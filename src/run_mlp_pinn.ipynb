{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline Simple MLP with just MSE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading models, go to the ``src/results/models``:\n",
    "- Baseline NO2 2017 with MLP and MSE loss: ``best_mlp_no2_baseline.pth``\n",
    "- Exp 1: NO2 2017 with MLP and option 1 simple physics loss: ``best_mlp_no2_adjusted_dist.pth`` (naming because I updated the distance between T and B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "\n",
    "from modelling.MLP import BasicMLP\n",
    "from modelling import *\n",
    "\n",
    "\n",
    "import optuna\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rachel/forecasting_smog_PEML/src')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/rachel/forecasting_smog_PEML\n",
      "MODEL_PATH:  /home/rachel/forecasting_smog_PEML/src/results/models\n",
      "MINMAX_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/pollutants_minmax.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f26e0bf1fb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "\n",
    "BASE_DIR = Path.cwd().parents[0] # set it to the root directory of the project, not src\n",
    "MODEL_PATH = BASE_DIR /\"src\" / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR  / \"data\" / \"data_combined\" / \"pollutants_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 24 * 3               # number of hours to use for input (number of days * 24 hours)\n",
    "N_HOURS_Y = 24                    # number of hours to predict (1 day * 24 hours)\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "\n",
    "# Change this according to the data you want to use\n",
    "YEARS = [2017]\n",
    "TRAIN_YEARS = [2017]\n",
    "VAL_YEARS = [2017]\n",
    "TEST_YEARS = [2017]\n",
    "\n",
    "torch.random.manual_seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2017_combined_u.csv\n",
      "Imported train_2017_combined_y.csv\n",
      "Imported val_2017_combined_u.csv\n",
      "Imported val_2017_combined_y.csv\n",
      "Imported test_2017_combined_u.csv\n",
      "Imported test_2017_combined_y.csv\n",
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', YEARS)\n",
    "train_output_frames = get_dataframes('train', 'y', YEARS)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', YEARS)\n",
    "val_output_frames = get_dataframes('val', 'y', YEARS)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', YEARS)\n",
    "test_output_frames = get_dataframes('test', 'y', YEARS) \n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    len(TRAIN_YEARS),                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    len(VAL_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    len(TEST_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                           DD   FF        FH        FX       NO2         P  \\\n",
       " DateTime                                                                     \n",
       " 2017-08-01 00:00:00  0.166667  0.1  0.111111  0.000000  0.242115  0.562982   \n",
       " 2017-08-01 01:00:00  0.000000  0.0  0.111111  0.052632  0.223158  0.570694   \n",
       " 2017-08-01 02:00:00  0.000000  0.0  0.000000  0.000000  0.165911  0.560411   \n",
       " 2017-08-01 03:00:00  0.277778  0.1  0.000000  0.000000  0.142363  0.555270   \n",
       " 2017-08-01 04:00:00  0.805556  0.2  0.111111  0.105263  0.156297  0.555270   \n",
       " ...                       ...  ...       ...       ...       ...       ...   \n",
       " 2017-11-16 19:00:00  0.750000  0.2  0.333333  0.210526  0.523871  0.789203   \n",
       " 2017-11-16 20:00:00  0.972222  0.3  0.333333  0.421053  0.512314  0.814910   \n",
       " 2017-11-16 21:00:00  0.888889  0.1  0.222222  0.263158  0.232880  0.827763   \n",
       " 2017-11-16 22:00:00  0.944444  0.2  0.111111  0.105263  0.108123  0.832905   \n",
       " 2017-11-16 23:00:00  0.861111  0.1  0.222222  0.105263  0.205120  0.845758   \n",
       " \n",
       "                       SQ         T        TD  \n",
       " DateTime                                      \n",
       " 2017-08-01 00:00:00  0.0  0.536667  0.726852  \n",
       " 2017-08-01 01:00:00  0.0  0.546667  0.740741  \n",
       " 2017-08-01 02:00:00  0.0  0.506667  0.689815  \n",
       " 2017-08-01 03:00:00  0.0  0.463333  0.634259  \n",
       " 2017-08-01 04:00:00  0.0  0.493333  0.662037  \n",
       " ...                  ...       ...       ...  \n",
       " 2017-11-16 19:00:00  0.0  0.390000  0.513889  \n",
       " 2017-11-16 20:00:00  0.0  0.353333  0.462963  \n",
       " 2017-11-16 21:00:00  0.0  0.330000  0.435185  \n",
       " 2017-11-16 22:00:00  0.0  0.306667  0.407407  \n",
       " 2017-11-16 23:00:00  0.0  0.250000  0.319444  \n",
       " \n",
       " [2592 rows x 9 columns]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                          NO2\n",
       " DateTime                     \n",
       " 2017-08-01 00:00:00  0.223698\n",
       " 2017-08-01 01:00:00  0.145496\n",
       " 2017-08-01 02:00:00  0.275978\n",
       " 2017-08-01 03:00:00  0.423742\n",
       " 2017-08-01 04:00:00  0.478721\n",
       " ...                       ...\n",
       " 2017-11-16 19:00:00  0.606502\n",
       " 2017-11-16 20:00:00  0.456470\n",
       " 2017-11-16 21:00:00  0.483258\n",
       " 2017-11-16 22:00:00  0.468784\n",
       " 2017-11-16 23:00:00  0.473428\n",
       " \n",
       " [2592 rows x 1 columns]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.1000, 0.1111, 0.0000, 0.2421, 0.5630, 0.0000, 0.5367, 0.7269],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.2232, 0.5707, 0.0000, 0.5467, 0.7407],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1659, 0.5604, 0.0000, 0.5067, 0.6898],\n",
       "        [0.2778, 0.1000, 0.0000, 0.0000, 0.1424, 0.5553, 0.0000, 0.4633, 0.6343],\n",
       "        [0.8056, 0.2000, 0.1111, 0.1053, 0.1563, 0.5553, 0.0000, 0.4933, 0.6620],\n",
       "        [0.0000, 0.0000, 0.1111, 0.1053, 0.3135, 0.5681, 0.3000, 0.6200, 0.7593],\n",
       "        [0.7222, 0.1000, 0.1111, 0.0526, 0.5326, 0.5913, 0.0000, 0.6433, 0.7269],\n",
       "        [0.7500, 0.1000, 0.1111, 0.1053, 0.5367, 0.5938, 0.0000, 0.6500, 0.7037],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1053, 0.5172, 0.5964, 0.0000, 0.6733, 0.6574],\n",
       "        [0.7500, 0.2000, 0.2222, 0.2105, 0.4459, 0.5990, 0.3000, 0.7133, 0.6157],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3129, 0.6041, 0.0000, 0.7167, 0.6019],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3478, 0.6067, 0.0000, 0.7133, 0.5926],\n",
       "        [0.6528, 0.1000, 0.2222, 0.1053, 0.3649, 0.6041, 0.2000, 0.7800, 0.6435],\n",
       "        [0.6944, 0.2000, 0.2222, 0.1053, 0.3019, 0.6015, 0.2000, 0.7867, 0.4861],\n",
       "        [0.7222, 0.2000, 0.2222, 0.2105, 0.2268, 0.5990, 0.4000, 0.7733, 0.5046],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.2246, 0.6015, 0.1000, 0.7633, 0.5972],\n",
       "        [0.6944, 0.2000, 0.2222, 0.2105, 0.2855, 0.6041, 0.7000, 0.7700, 0.5880],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.2469, 0.6015, 0.2000, 0.7267, 0.6435],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1579, 0.2171, 0.6144, 0.2000, 0.6967, 0.6944],\n",
       "        [0.6944, 0.1000, 0.1111, 0.1053, 0.2834, 0.6298, 0.0000, 0.5933, 0.7130],\n",
       "        [0.4167, 0.1000, 0.1111, 0.0526, 0.3918, 0.6298, 0.0000, 0.5267, 0.6944],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0000, 0.4752, 0.6375, 0.0000, 0.5200, 0.6991],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0526, 0.5745, 0.6298, 0.0000, 0.5067, 0.6852],\n",
       "        [0.5000, 0.1000, 0.1111, 0.1053, 0.5891, 0.6247, 0.0000, 0.5033, 0.6713],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.5491, 0.6298, 0.0000, 0.5000, 0.6806],\n",
       "        [0.4444, 0.1000, 0.1111, 0.0526, 0.5092, 0.6221, 0.0000, 0.5300, 0.6944],\n",
       "        [0.4167, 0.2000, 0.1111, 0.0526, 0.3212, 0.6144, 0.0000, 0.5267, 0.6898],\n",
       "        [0.3889, 0.1000, 0.2222, 0.1053, 0.2835, 0.6118, 0.0000, 0.5467, 0.6898],\n",
       "        [0.5000, 0.1000, 0.1111, 0.0526, 0.4099, 0.6144, 0.0000, 0.5533, 0.7083],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.4797, 0.6272, 0.0000, 0.6067, 0.7083],\n",
       "        [0.6389, 0.2000, 0.2222, 0.2105, 0.5086, 0.6272, 0.0000, 0.6333, 0.6991],\n",
       "        [0.6667, 0.4000, 0.3333, 0.2632, 0.4155, 0.6375, 0.0000, 0.6567, 0.6574],\n",
       "        [0.6667, 0.4000, 0.4444, 0.3158, 0.3375, 0.6478, 0.1000, 0.6800, 0.6574],\n",
       "        [0.6389, 0.3000, 0.3333, 0.3684, 0.2610, 0.6478, 0.1000, 0.7067, 0.6713],\n",
       "        [0.5556, 0.4000, 0.4444, 0.2632, 0.2420, 0.6555, 0.3000, 0.7300, 0.6296],\n",
       "        [0.5833, 0.4000, 0.4444, 0.3158, 0.2146, 0.6427, 0.2000, 0.7433, 0.5741],\n",
       "        [0.5556, 0.3000, 0.3333, 0.2632, 0.1782, 0.6221, 0.1000, 0.7600, 0.5370],\n",
       "        [0.5000, 0.4000, 0.4444, 0.2632, 0.1985, 0.6195, 0.0000, 0.7533, 0.5556],\n",
       "        [0.5000, 0.5000, 0.4444, 0.3684, 0.2416, 0.5964, 0.0000, 0.7600, 0.6204],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4737, 0.2883, 0.5938, 0.0000, 0.7267, 0.5833],\n",
       "        [0.5833, 0.4000, 0.7778, 0.5789, 0.2718, 0.6093, 0.0000, 0.5933, 0.6806],\n",
       "        [0.5556, 0.2000, 0.3333, 0.3158, 0.1936, 0.5964, 0.0000, 0.5600, 0.7083],\n",
       "        [0.4861, 0.1000, 0.1111, 0.1579, 0.2238, 0.5835, 0.0000, 0.5533, 0.7361],\n",
       "        [0.4167, 0.2000, 0.2222, 0.1579, 0.2430, 0.5656, 0.0000, 0.5533, 0.7269],\n",
       "        [0.3889, 0.2000, 0.2222, 0.1579, 0.3154, 0.5681, 0.0000, 0.5567, 0.7315],\n",
       "        [0.4444, 0.3000, 0.2222, 0.1579, 0.2860, 0.5553, 0.0000, 0.5800, 0.7639],\n",
       "        [0.5000, 0.3000, 0.3333, 0.2632, 0.2077, 0.5373, 0.0000, 0.5967, 0.7778],\n",
       "        [0.5000, 0.2000, 0.2222, 0.2105, 0.1640, 0.5167, 0.0000, 0.6000, 0.7778],\n",
       "        [0.4444, 0.3000, 0.3333, 0.2632, 0.1525, 0.4961, 0.0000, 0.5967, 0.7824],\n",
       "        [0.4722, 0.4000, 0.3333, 0.3158, 0.1328, 0.4730, 0.0000, 0.6000, 0.7824],\n",
       "        [0.5000, 0.4000, 0.4444, 0.3158, 0.1252, 0.4422, 0.0000, 0.6100, 0.7639],\n",
       "        [0.4722, 0.2000, 0.3333, 0.2632, 0.1161, 0.4293, 0.0000, 0.6000, 0.7639],\n",
       "        [0.4722, 0.2000, 0.2222, 0.1579, 0.1766, 0.4165, 0.0000, 0.5967, 0.7731],\n",
       "        [0.5556, 0.4000, 0.3333, 0.3158, 0.2840, 0.4139, 0.0000, 0.6533, 0.8056],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4211, 0.3435, 0.4010, 0.5000, 0.6967, 0.8009],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.3057, 0.3985, 0.0000, 0.6867, 0.8194],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.2615, 0.4036, 0.0000, 0.6633, 0.8519],\n",
       "        [0.6389, 0.5000, 0.6667, 0.5789, 0.2453, 0.4190, 0.0000, 0.6133, 0.7454],\n",
       "        [0.6389, 0.7000, 0.7778, 0.6316, 0.1434, 0.4216, 0.6000, 0.7067, 0.6806],\n",
       "        [0.6667, 0.9000, 0.8889, 0.7368, 0.1046, 0.4267, 0.7000, 0.7467, 0.6435],\n",
       "        [0.6667, 0.8000, 0.7778, 0.7368, 0.0607, 0.4319, 0.8000, 0.7667, 0.6019],\n",
       "        [0.6667, 0.9000, 1.0000, 0.8947, 0.0700, 0.4319, 1.0000, 0.7867, 0.6065],\n",
       "        [0.6944, 0.8000, 0.8889, 0.7895, 0.0594, 0.4422, 0.4000, 0.7400, 0.6389],\n",
       "        [0.6667, 0.8000, 0.7778, 0.6842, 0.0823, 0.4370, 0.5000, 0.7633, 0.6389],\n",
       "        [0.6667, 0.7000, 0.8889, 0.7368, 0.1016, 0.4473, 1.0000, 0.7467, 0.6944],\n",
       "        [0.6389, 0.9000, 0.8889, 0.8421, 0.1005, 0.4550, 0.5000, 0.7067, 0.5278],\n",
       "        [0.6667, 0.7000, 1.0000, 0.8947, 0.0701, 0.4627, 0.3000, 0.6900, 0.5602],\n",
       "        [0.6389, 1.0000, 0.8889, 0.9474, 0.0572, 0.4653, 0.0000, 0.6800, 0.5648],\n",
       "        [0.6667, 0.9000, 0.8889, 0.8421, 0.0533, 0.4627, 0.0000, 0.6567, 0.5741],\n",
       "        [0.6667, 0.7000, 0.7778, 0.7368, 0.0475, 0.4627, 0.0000, 0.6300, 0.6019],\n",
       "        [0.6389, 0.4000, 0.5556, 0.5789, 0.0376, 0.4576, 0.0000, 0.6100, 0.6204],\n",
       "        [0.6111, 0.5000, 0.4444, 0.4211, 0.0373, 0.4499, 0.0000, 0.5933, 0.6296]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1965],\n",
       "        [0.1501],\n",
       "        [0.1518],\n",
       "        [0.2622],\n",
       "        [0.5524],\n",
       "        [0.4840],\n",
       "        [0.3544],\n",
       "        [0.2754],\n",
       "        [0.1948],\n",
       "        [0.1734],\n",
       "        [0.1505],\n",
       "        [0.1352],\n",
       "        [0.0778],\n",
       "        [0.1184],\n",
       "        [0.1293],\n",
       "        [0.1238],\n",
       "        [0.1043],\n",
       "        [0.0997],\n",
       "        [0.0812],\n",
       "        [0.0823],\n",
       "        [0.1155],\n",
       "        [0.0837],\n",
       "        [0.0570],\n",
       "        [0.1006]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2 index:  4\n",
      "DD index (wind direction):  0\n",
      "FH index (Hourly wind speed):  2\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataset.u[0] is a pandas Index object with column names\n",
    "column_names = list(train_dataset.u[0])  # Convert Index to list\n",
    "\n",
    "# Now, find the indices of the columns 'NO2', 'DD', 'FH'\n",
    "no2_idx = column_names.index('NO2')\n",
    "dd_idx = column_names.index('DD')\n",
    "fh_idx = column_names.index('FH')\n",
    "\n",
    "print(\"NO2 index: \", no2_idx)\n",
    "print(\"DD index (wind direction): \", dd_idx)\n",
    "print(\"FH index (Hourly wind speed): \", fh_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.242115\n",
       "2017-08-01 01:00:00    0.223158\n",
       "2017-08-01 02:00:00    0.165911\n",
       "2017-08-01 03:00:00    0.142363\n",
       "2017-08-01 04:00:00    0.156297\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.523871\n",
       "2017-11-16 20:00:00    0.512314\n",
       "2017-11-16 21:00:00    0.232880\n",
       "2017-11-16 22:00:00    0.108123\n",
       "2017-11-16 23:00:00    0.205120\n",
       "Name: NO2, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,no2_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.166667\n",
       "2017-08-01 01:00:00    0.000000\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.277778\n",
       "2017-08-01 04:00:00    0.805556\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.750000\n",
       "2017-11-16 20:00:00    0.972222\n",
       "2017-11-16 21:00:00    0.888889\n",
       "2017-11-16 22:00:00    0.944444\n",
       "2017-11-16 23:00:00    0.861111\n",
       "Name: DD, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,dd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.111111\n",
       "2017-08-01 01:00:00    0.111111\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.000000\n",
       "2017-08-01 04:00:00    0.111111\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.333333\n",
       "2017-11-16 20:00:00    0.333333\n",
       "2017-11-16 21:00:00    0.222222\n",
       "2017-11-16 22:00:00    0.111111\n",
       "2017-11-16 23:00:00    0.222222\n",
       "Name: FH, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,fh_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PhysicsMLP(nn.Module):\n",
    "    def __init__(self, N_INPUT_UNITS=72, N_HIDDEN_LAYERS=2, N_HIDDEN_UNITS=100, N_OUTPUT_UNITS=24, loss_function=\"MSE\"):\n",
    "        super(PhysicsMLP, self).__init__()\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        layers = [nn.Linear(N_INPUT_UNITS, N_HIDDEN_UNITS), nn.ReLU()]\n",
    "        for _ in range(N_HIDDEN_LAYERS):\n",
    "            layers.append(nn.Linear(N_HIDDEN_UNITS, N_HIDDEN_UNITS))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(N_HIDDEN_UNITS, N_OUTPUT_UNITS))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Compute constant dx, dy using lat/lon coordinates of Tuindorp & Breukelen\n",
    "        self.dx, self.dy = self.compute_dx_dy()\n",
    "\n",
    "    def compute_dx_dy(self):\n",
    "        \"\"\"Compute fixed spatial distances (dx, dy) between Tuindorp and Breukelen.\"\"\"\n",
    "        lat_tuindorp, lon_tuindorp = 52.10503, 5.12448  # Tuindorp coords\n",
    "        lat_breukelen, lon_breukelen = 52.20153, 4.98741  # Breukelen coords\n",
    "        R = 6371  # Earth radius in km\n",
    "        \n",
    "        x = (lon_breukelen - lon_tuindorp) * (math.pi / 180) * R * math.cos(math.radians((lat_tuindorp + lat_breukelen) / 2))\n",
    "        y = (lat_breukelen - lat_tuindorp) * (math.pi / 180) * R\n",
    "        return x, y\n",
    "\n",
    "    def compute_wind_components(self, u):\n",
    "        \"\"\"Compute wind speed components (v_x, v_y) from wind speed and direction.\"\"\"\n",
    "        dd_idx = 0  # Wind direction index\n",
    "        fh_idx = 2  # Wind speed index\n",
    "\n",
    "\n",
    "        wind_speed = u[:, :, fh_idx] * 3.6  # Convert m/s to km/h\n",
    "        wind_direction = u[:, :, dd_idx] * 360  # Convert normalized [0,1] to degrees\n",
    "\n",
    "        v_x = wind_speed * torch.cos(torch.deg2rad(wind_direction))  # X-component\n",
    "        v_y = wind_speed * torch.sin(torch.deg2rad(wind_direction))  # Y-component\n",
    "        return v_x, v_y\n",
    "\n",
    "    def forward(self, u):\n",
    "        batch_size, seq_length, n_features = u.size()\n",
    "        outputs = torch.empty((batch_size, seq_length, self.layers[-1].out_features), device=u.device)\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            u_t = u[:, t, :]\n",
    "            y_t = self.layers(u_t)\n",
    "            outputs[:, t, :] = y_t\n",
    "\n",
    "        return outputs[:, -24:, :]\n",
    "\n",
    "    def physics_loss(self, u, y_pred, v_x, v_y, dt):\n",
    "        \"\"\"\n",
    "        Compute physics loss enforcing the advection equation:\n",
    "        ∂c/∂t + v_x * ∂c/∂x + v_y * ∂c/∂y = 0\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = u.size()\n",
    "        loss_phy = 0.0\n",
    "        no2_idx = 4  # NO2 index\n",
    "\n",
    "        for t in range(1, 24):  \n",
    "            c_t = y_pred[:, t, :]\n",
    "            c_t_prev = y_pred[:, t - 1, :]\n",
    "\n",
    "            # Temporal derivative ∂c/∂t\n",
    "            dc_dt = (c_t - c_t_prev) / dt\n",
    "\n",
    "            # Spatial derivatives ∂c/∂x and ∂c/∂y using autograd\n",
    "            dc_dx = torch.autograd.grad(c_t, u, torch.ones_like(c_t), create_graph=True)[0] / self.dx\n",
    "            dc_dy = torch.autograd.grad(c_t, u, torch.ones_like(c_t), create_graph=True)[0] / self.dy\n",
    "            # 16, 72, 9\n",
    "            # 16 , 72\n",
    "\n",
    "            # Ensure that dc_dx and dc_dy have shape (batch_size, 72)\n",
    "            # Squeeze or select the first feature if necessary (from the extra dimension)\n",
    "            dc_dx = dc_dx[:, :, no2_idx]  # Assuming we want the first feature for simplicity\n",
    "            dc_dy = dc_dy[:, :, no2_idx]  # Assuming we want the first feature for simplicity\n",
    "\n",
    "            # print(\"vx shape\",v_x.shape)\n",
    "            # print(\"dc_dx shape\",dc_dx.shape)\n",
    "            # print(\"vy shape\",v_y.shape)\n",
    "            # print(\"dc_dy shape\",dc_dy.shape)\n",
    "            # print(\"dc_dt shape\",dc_dt.shape)\n",
    "\n",
    "            # Compute residual of PDE\n",
    "            residual = dc_dt + v_x * dc_dx + v_y * dc_dy\n",
    "            loss_phy += torch.mean(residual ** 2)\n",
    "\n",
    "        return loss_phy / (24 - 2)  # Average over time steps\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, epochs=50, lr=1e-3, weight_decay=1e-6, lambda_phy=1e-5, device=\"cpu\"):\n",
    "        self.to(device)\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_model_state = None\n",
    "        dt = 1  # Time step in hours\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "\n",
    "            for u, y in train_loader:\n",
    "                u, y = u.to(device), y.to(device)\n",
    "\n",
    "                u = u.float()  # Ensure float dtype\n",
    "                u.requires_grad = True  # Ensure gradient tracking is enabled for u\n",
    "            \n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self.forward(u)  # Forward pass should result in tensors that require gradients\n",
    "\n",
    "                # Compute wind components\n",
    "                v_x, v_y = self.compute_wind_components(u)\n",
    "\n",
    "                # Data-driven loss\n",
    "                loss_data = torch.nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "                # Physics loss\n",
    "                loss_phy = self.physics_loss(u, y_pred, v_x, v_y, dt)\n",
    "\n",
    "                # print(\"Loss phy: \", loss_phy)\n",
    "                # print(\"Loss data: \", loss_data)\n",
    "\n",
    "                # Combine losses\n",
    "                loss = loss_data + lambda_phy * loss_phy\n",
    "                # print(\"Total loss: \", loss)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "\n",
    "            # Validation step\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "\n",
    "            for u, y in val_loader:\n",
    "                u, y = u.to(device), y.to(device)\n",
    "\n",
    "                u = u.float()  # Ensure float dtype\n",
    "                u.requires_grad = True  # No need for gradients during validation\n",
    "\n",
    "                y_pred = self.forward(u)\n",
    "                v_x, v_y = self.compute_wind_components(u)\n",
    "\n",
    "                loss = torch.nn.functional.mse_loss(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = self.state_dict()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f} - Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        if best_model_state:\n",
    "            self.load_state_dict(best_model_state)\n",
    "\n",
    "        return best_val_loss\n",
    "\n",
    "\n",
    "    def test_model(self, test_loader, min_value=None, max_value=None, device=\"cpu\"):\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        mse_loss_fn = nn.MSELoss()\n",
    "        rmse_loss = 0.0\n",
    "        smape_loss = 0.0\n",
    "        total_elements = 0\n",
    "        epsilon = 1e-6  # To prevent division by zero\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for u, y in test_loader:\n",
    "                u, y = u.to(device), y.to(device)\n",
    "                output = self.forward(u)\n",
    "\n",
    "                # Compute SMAPE BEFORE denormalization\n",
    "                abs_diff = torch.abs(y - output)\n",
    "                sum_abs = torch.abs(y) + torch.abs(output) + epsilon  # Avoid division by zero\n",
    "                smape_batch = torch.sum(2 * abs_diff / sum_abs).item()\n",
    "\n",
    "                smape_loss += smape_batch\n",
    "                total_elements += y.numel()  # Count total number of elements\n",
    "\n",
    "                # Denormalize for RMSE and MSE calculation\n",
    "                if min_value is not None and max_value is not None:\n",
    "                    output = output * (max_value - min_value) + min_value\n",
    "                    y = y * (max_value - min_value) + min_value\n",
    "\n",
    "                mse_loss = mse_loss_fn(output, y)\n",
    "                rmse_loss += torch.sqrt(mse_loss).item()\n",
    "\n",
    "        # Final loss calculations\n",
    "        rmse_loss /= len(test_loader)  # Average over batches\n",
    "        smape_loss = (smape_loss / total_elements) * 100  # Average over all elements and convert to %\n",
    "\n",
    "        print(f\"Test MSE Loss: {mse_loss.item():.6f}\")\n",
    "        print(f\"Test RMSE Loss: {rmse_loss:.6f}\")\n",
    "        print(f\"Test SMAPE Loss: {smape_loss:.6f}%\")\n",
    "\n",
    "        return mse_loss.item(), rmse_loss, smape_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MSE Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.063185 - Val Loss: 0.039295\n",
      "Epoch 2/50 - Train Loss: 0.029938 - Val Loss: 0.057857\n",
      "Epoch 3/50 - Train Loss: 0.027003 - Val Loss: 0.033442\n",
      "Epoch 4/50 - Train Loss: 0.026025 - Val Loss: 0.037039\n",
      "Epoch 5/50 - Train Loss: 0.021648 - Val Loss: 0.027958\n",
      "Epoch 6/50 - Train Loss: 0.018225 - Val Loss: 0.025800\n",
      "Epoch 7/50 - Train Loss: 0.016240 - Val Loss: 0.023938\n",
      "Epoch 8/50 - Train Loss: 0.015933 - Val Loss: 0.022082\n",
      "Epoch 9/50 - Train Loss: 0.015482 - Val Loss: 0.021290\n",
      "Epoch 10/50 - Train Loss: 0.015093 - Val Loss: 0.021010\n",
      "Epoch 11/50 - Train Loss: 0.015702 - Val Loss: 0.021218\n",
      "Epoch 12/50 - Train Loss: 0.014756 - Val Loss: 0.022029\n",
      "Epoch 13/50 - Train Loss: 0.014722 - Val Loss: 0.020772\n",
      "Epoch 14/50 - Train Loss: 0.014679 - Val Loss: 0.021470\n",
      "Epoch 15/50 - Train Loss: 0.014835 - Val Loss: 0.020759\n",
      "Epoch 16/50 - Train Loss: 0.014754 - Val Loss: 0.022891\n",
      "Epoch 17/50 - Train Loss: 0.015242 - Val Loss: 0.024430\n",
      "Epoch 18/50 - Train Loss: 0.016656 - Val Loss: 0.021316\n",
      "Epoch 19/50 - Train Loss: 0.015776 - Val Loss: 0.025836\n",
      "Epoch 20/50 - Train Loss: 0.015843 - Val Loss: 0.020941\n",
      "Epoch 21/50 - Train Loss: 0.014404 - Val Loss: 0.022177\n",
      "Epoch 22/50 - Train Loss: 0.015433 - Val Loss: 0.021150\n",
      "Epoch 23/50 - Train Loss: 0.015261 - Val Loss: 0.021938\n",
      "Epoch 24/50 - Train Loss: 0.014280 - Val Loss: 0.022228\n",
      "Epoch 25/50 - Train Loss: 0.014836 - Val Loss: 0.021199\n",
      "Epoch 26/50 - Train Loss: 0.014086 - Val Loss: 0.020798\n",
      "Epoch 27/50 - Train Loss: 0.014278 - Val Loss: 0.022576\n",
      "Epoch 28/50 - Train Loss: 0.013532 - Val Loss: 0.021395\n",
      "Epoch 29/50 - Train Loss: 0.013848 - Val Loss: 0.022015\n",
      "Epoch 30/50 - Train Loss: 0.013726 - Val Loss: 0.021473\n",
      "Epoch 31/50 - Train Loss: 0.013732 - Val Loss: 0.021611\n",
      "Epoch 32/50 - Train Loss: 0.013284 - Val Loss: 0.021025\n",
      "Epoch 33/50 - Train Loss: 0.013938 - Val Loss: 0.021990\n",
      "Epoch 34/50 - Train Loss: 0.014327 - Val Loss: 0.026480\n",
      "Epoch 35/50 - Train Loss: 0.014408 - Val Loss: 0.022062\n",
      "Epoch 36/50 - Train Loss: 0.013694 - Val Loss: 0.023142\n",
      "Epoch 37/50 - Train Loss: 0.013334 - Val Loss: 0.024844\n",
      "Epoch 38/50 - Train Loss: 0.013209 - Val Loss: 0.023710\n",
      "Epoch 39/50 - Train Loss: 0.014272 - Val Loss: 0.021706\n",
      "Epoch 40/50 - Train Loss: 0.013090 - Val Loss: 0.023847\n",
      "Epoch 41/50 - Train Loss: 0.013501 - Val Loss: 0.022186\n",
      "Epoch 42/50 - Train Loss: 0.013179 - Val Loss: 0.022804\n",
      "Epoch 43/50 - Train Loss: 0.013412 - Val Loss: 0.020755\n",
      "Epoch 44/50 - Train Loss: 0.012954 - Val Loss: 0.022042\n",
      "Epoch 45/50 - Train Loss: 0.013406 - Val Loss: 0.022508\n",
      "Epoch 46/50 - Train Loss: 0.012885 - Val Loss: 0.021922\n",
      "Epoch 47/50 - Train Loss: 0.012557 - Val Loss: 0.032598\n",
      "Epoch 48/50 - Train Loss: 0.013981 - Val Loss: 0.030466\n",
      "Epoch 49/50 - Train Loss: 0.013190 - Val Loss: 0.022075\n",
      "Epoch 50/50 - Train Loss: 0.013384 - Val Loss: 0.022581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02075547818094492"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_hidden_layers': 5,\n",
    " 'n_hidden_units': 219,\n",
    " 'lr': 0.001400724556706755,\n",
    " 'weight_decay': 1.4564281375374214e-08,\n",
    " 'batch_size': 16}\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "Phy_mlp = PhysicsMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=params[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=params[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "    loss_function=\"MSE\",\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "Phy_mlp.train_model(train_loader, val_loader, epochs=50, lr=params[\"lr\"], weight_decay=params[\"weight_decay\"], device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def test_model(model, test_loader, device, min_value=None, max_value=None):\n",
    "    \"\"\"\n",
    "    Evaluates the given model on a test dataset and computes MSE, RMSE, and SMAPE.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        device (torch.device): The device to run inference on.\n",
    "        min_value (float, optional): Minimum value of the dataset (for denormalization).\n",
    "        max_value (float, optional): Maximum value of the dataset (for denormalization).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (mse_loss, rmse_loss, smape_loss) computed on the test set.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    rmse_loss = 0.0\n",
    "    smape_loss = 0.0\n",
    "    total_elements = 0\n",
    "    epsilon = 1e-6  # To prevent division by zero\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u, y in test_loader:\n",
    "            u, y = u.to(device), y.to(device)\n",
    "            output = model(u)  # Forward pass\n",
    "\n",
    "            # Compute SMAPE BEFORE denormalization\n",
    "            abs_diff = torch.abs(y - output)\n",
    "            sum_abs = torch.abs(y) + torch.abs(output) + epsilon  # Avoid division by zero\n",
    "            smape_batch = torch.sum(2 * abs_diff / sum_abs).item()\n",
    "\n",
    "            smape_loss += smape_batch\n",
    "            total_elements += y.numel()  # Count total number of elements\n",
    "\n",
    "            # Denormalize for RMSE and MSE calculation\n",
    "            if min_value is not None and max_value is not None:\n",
    "                output = output * (max_value - min_value) + min_value\n",
    "                y = y * (max_value - min_value) + min_value\n",
    "\n",
    "            mse_loss = mse_loss_fn(output, y)\n",
    "            rmse_loss += torch.sqrt(mse_loss).item()\n",
    "\n",
    "    # Final loss calculations\n",
    "    rmse_loss /= len(test_loader)  # Average over batches\n",
    "    smape_loss = (smape_loss / total_elements) * 100  # Average over all elements and convert to %\n",
    "\n",
    "    print(f\"Test MSE Loss: {mse_loss.item():.6f}\")\n",
    "    print(f\"Test RMSE Loss: {rmse_loss:.6f}\")\n",
    "    print(f\"Test SMAPE Loss: {smape_loss:.6f}%\")\n",
    "\n",
    "    return mse_loss.item(), rmse_loss, smape_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 148.015683\n",
      "Test RMSE Loss: 14.104732\n",
      "Test SMAPE Loss: 36.083454%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(148.01568267826852, 14.104732335989837, 36.083454114419446)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_model_baseline.load_state_dict(torch.load(f\"{MODEL_PATH}/best_mlp_no2_baseline.pth\"))\n",
    "# best_model_baseline.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# # Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "test_model(Phy_mlp, test_loader, device, min_value, max_value)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
