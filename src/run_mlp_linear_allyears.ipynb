{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PEML MLP Architecture 1 - computing physics output and feed in NN with regularisation**\n",
    "## Experiment 1: y_phy calculated by linear time shifting according to wind speed and dist\n",
    "### All years data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading models, go to the ``src/results/models``:\n",
    "- Baseline NO2 2017 with MLP and MSE loss: ``best_mlp_no2_baseline_2017.pth``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "\n",
    "from modelling.MLP import BasicMLP\n",
    "from modelling import *\n",
    "\n",
    "\n",
    "import optuna\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rachel/forecasting_smog_PEML/src')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/home/rachel/forecasting_smog_PEML/src/config.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/rachel/forecasting_smog_PEML\n",
      "MODEL_PATH:  /home/rachel/forecasting_smog_PEML/src/results/models\n",
      "Results path:  /home/rachel/forecasting_smog_PEML/src/results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5dac465fd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"Results path: \", RESULTS_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MODIFY THESE GLOBAL VARIABLES FOR YOUR MODEL SCENARIO\n",
    "## all other variables are defined in config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5dac465fd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change this according to the data you want to use\n",
    "YEARS = [2017, 2018, 2020, 2021, 2022, 2023]\n",
    "TRAIN_YEARS = [2017, 2018, 2020, 2021, 2022]\n",
    "VAL_YEARS = [2021, 2022, 2023]\n",
    "TEST_YEARS = [2021, 2022, 2023]\n",
    "\n",
    "LOSS_FUNC = \"LinearShift_MSE\"\n",
    "NN_TYPE = \"MLP\" # choose from \"MLP\", \"RNN\", \"LSTM\", \"GRU\"\n",
    "torch.random.manual_seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all years\n",
      "MINMAX_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/all_years/pollutants_minmax_allyears.csv\n",
      "DATASET_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/all_years\n",
      "MODEL_PATH_NAME:  best_MLP_no2_LinearShift_MSE_allyears.pth\n",
      "RESULTS_METRICS_FILENAME:  results_MLP_no2_LinearShift_MSE_allyears.csv\n",
      "BESTPARAMS_FILENAME:  best_params_MLP_no2_LinearShift_MSE_allyears.txt\n",
      "PLOT_FILENAME:  plot_MLP_no2_LinearShift_MSE_allyears.png\n"
     ]
    }
   ],
   "source": [
    "if YEARS == [2017, 2018, 2020, 2021, 2022, 2023]:\n",
    "    years = \"allyears\"\n",
    "    MINMAX_PATH = MINMAX_PATH_ALLYEARS\n",
    "    DATASET_PATH = DATASET_PATH_ALLYEARS\n",
    "    \n",
    "    print(\"Using all years\")\n",
    "    \n",
    "elif YEARS == [2017]:\n",
    "    years = \"2017\"\n",
    "    MINMAX_PATH = MINMAX_PATH_2017\n",
    "    DATASET_PATH = DATASET_PATH_2017\n",
    "    print(\"Using 2017\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid years selected\")\n",
    "\n",
    "\n",
    "MODEL_PATH_NAME = f'best_{NN_TYPE}_no2_{LOSS_FUNC}_{years}.pth'\n",
    "RESULTS_METRICS_FILENAME = f'results_{NN_TYPE}_no2_{LOSS_FUNC}_{years}.csv'\n",
    "BESTPARAMS_FILENAME = f'best_params_{NN_TYPE}_no2_{LOSS_FUNC}_{years}.txt'\n",
    "PLOT_FILENAME = f'plot_{NN_TYPE}_no2_{LOSS_FUNC}_{years}.png'\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "print(\"DATASET_PATH: \", DATASET_PATH)\n",
    "print(\"MODEL_PATH_NAME: \", MODEL_PATH_NAME)\n",
    "print(\"RESULTS_METRICS_FILENAME: \", RESULTS_METRICS_FILENAME)\n",
    "print(\"BESTPARAMS_FILENAME: \", BESTPARAMS_FILENAME)\n",
    "print(\"PLOT_FILENAME: \", PLOT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2017_combined_u.csv\n",
      "Imported train_2018_combined_u.csv\n",
      "Imported train_2020_combined_u.csv\n",
      "Imported train_2021_combined_u.csv\n",
      "Imported train_2022_combined_u.csv\n",
      "Warning: train_2023_combined_u.csv does not exist.\n",
      "Imported train_2017_combined_y.csv\n",
      "Imported train_2018_combined_y.csv\n",
      "Imported train_2020_combined_y.csv\n",
      "Imported train_2021_combined_y.csv\n",
      "Imported train_2022_combined_y.csv\n",
      "Warning: train_2023_combined_y.csv does not exist.\n",
      "Warning: val_2017_combined_u.csv does not exist.\n",
      "Warning: val_2018_combined_u.csv does not exist.\n",
      "Warning: val_2020_combined_u.csv does not exist.\n",
      "Imported val_2021_combined_u.csv\n",
      "Imported val_2022_combined_u.csv\n",
      "Imported val_2023_combined_u.csv\n",
      "Warning: val_2017_combined_y.csv does not exist.\n",
      "Warning: val_2018_combined_y.csv does not exist.\n",
      "Warning: val_2020_combined_y.csv does not exist.\n",
      "Imported val_2021_combined_y.csv\n",
      "Imported val_2022_combined_y.csv\n",
      "Imported val_2023_combined_y.csv\n",
      "Warning: test_2017_combined_u.csv does not exist.\n",
      "Warning: test_2018_combined_u.csv does not exist.\n",
      "Warning: test_2020_combined_u.csv does not exist.\n",
      "Imported test_2021_combined_u.csv\n",
      "Imported test_2022_combined_u.csv\n",
      "Imported test_2023_combined_u.csv\n",
      "Warning: test_2017_combined_y.csv does not exist.\n",
      "Warning: test_2018_combined_y.csv does not exist.\n",
      "Warning: test_2020_combined_y.csv does not exist.\n",
      "Imported test_2021_combined_y.csv\n",
      "Imported test_2022_combined_y.csv\n",
      "Imported test_2023_combined_y.csv\n",
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', YEARS, DATASET_PATH)\n",
    "train_output_frames = get_dataframes('train', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', YEARS, DATASET_PATH)\n",
    "val_output_frames = get_dataframes('val', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', YEARS, DATASET_PATH)\n",
    "test_output_frames = get_dataframes('test', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                           DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2017-08-01 00:00:00  0.166667  0.083333  0.083333  0.043478       0.207556   \n",
       " 2017-08-01 01:00:00  0.000000  0.000000  0.083333  0.086957       0.140351   \n",
       " 2017-08-01 02:00:00  0.000000  0.000000  0.000000  0.043478       0.252483   \n",
       " 2017-08-01 03:00:00  0.277778  0.083333  0.000000  0.043478       0.379467   \n",
       " 2017-08-01 04:00:00  0.805556  0.166667  0.083333  0.130435       0.426715   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2017-12-30 19:00:00  0.694444  0.416667  0.416667  0.478261       0.201615   \n",
       " 2017-12-30 20:00:00  0.694444  0.416667  0.333333  0.434783       0.217859   \n",
       " 2017-12-30 21:00:00  0.666667  0.333333  0.416667  0.434783       0.221480   \n",
       " 2017-12-30 22:00:00  0.694444  0.250000  0.250000  0.304348       0.209505   \n",
       " 2017-12-30 23:00:00  0.666667  0.416667  0.250000  0.304348       0.240787   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2017-08-01 00:00:00      0.223383  0.634943  0.0  0.497354  0.723022  \n",
       " 2017-08-01 01:00:00      0.207092  0.639205  0.0  0.505291  0.733813  \n",
       " 2017-08-01 02:00:00      0.157895  0.633523  0.0  0.473545  0.694245  \n",
       " 2017-08-01 03:00:00      0.137659  0.630682  0.0  0.439153  0.651079  \n",
       " 2017-08-01 04:00:00      0.149633  0.630682  0.0  0.462963  0.672662  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2017-12-30 19:00:00      0.168848  0.457386  0.0  0.396825  0.507194  \n",
       " 2017-12-30 20:00:00      0.140072  0.471591  0.0  0.388889  0.496403  \n",
       " 2017-12-30 21:00:00      0.191033  0.473011  0.0  0.375661  0.503597  \n",
       " 2017-12-30 22:00:00      0.147406  0.475852  0.0  0.370370  0.492806  \n",
       " 2017-12-30 23:00:00      0.158823  0.477273  0.0  0.373016  0.489209  \n",
       " \n",
       " [3648 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2018-08-01 00:00:00  0.166667  0.083333  0.083333  0.043478       0.572728   \n",
       " 2018-08-01 01:00:00  0.000000  0.000000  0.000000  0.043478       0.490485   \n",
       " 2018-08-01 02:00:00  0.000000  0.000000  0.000000  0.043478       0.511928   \n",
       " 2018-08-01 03:00:00  0.055556  0.083333  0.083333  0.043478       0.430428   \n",
       " 2018-08-01 04:00:00  0.194444  0.083333  0.083333  0.043478       0.387450   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2018-12-30 19:00:00  0.750000  0.166667  0.166667  0.173913       0.273554   \n",
       " 2018-12-30 20:00:00  0.750000  0.250000  0.166667  0.217391       0.245336   \n",
       " 2018-12-30 21:00:00  0.777778  0.250000  0.250000  0.217391       0.189548   \n",
       " 2018-12-30 22:00:00  0.805556  0.250000  0.166667  0.260870       0.169312   \n",
       " 2018-12-30 23:00:00  0.777778  0.166667  0.166667  0.217391       0.124478   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2018-08-01 00:00:00      0.247192  0.732955  0.0  0.428571  0.629496  \n",
       " 2018-08-01 01:00:00      0.252762  0.735795  0.0  0.420635  0.615108  \n",
       " 2018-08-01 02:00:00      0.217024  0.732955  0.0  0.404762  0.593525  \n",
       " 2018-08-01 03:00:00      0.189177  0.732955  0.0  0.404762  0.593525  \n",
       " 2018-08-01 04:00:00      0.243665  0.734375  0.3  0.436508  0.640288  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2018-12-30 19:00:00      0.255825  0.897727  0.0  0.346561  0.521583  \n",
       " 2018-12-30 20:00:00      0.263993  0.893466  0.0  0.349206  0.525180  \n",
       " 2018-12-30 21:00:00      0.199944  0.893466  0.0  0.349206  0.525180  \n",
       " 2018-12-30 22:00:00      0.178038  0.896307  0.0  0.351852  0.528777  \n",
       " 2018-12-30 23:00:00      0.145456  0.897727  0.0  0.354497  0.532374  \n",
       " \n",
       " [3648 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2020-08-01 00:00:00  0.527778  0.166667  0.250000  0.304348       0.115845   \n",
       " 2020-08-01 01:00:00  0.250000  0.083333  0.166667  0.217391       0.173953   \n",
       " 2020-08-01 02:00:00  0.611111  0.166667  0.250000  0.391304       0.117702   \n",
       " 2020-08-01 03:00:00  0.750000  0.083333  0.083333  0.173913       0.153439   \n",
       " 2020-08-01 04:00:00  0.666667  0.166667  0.166667  0.173913       0.236239   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2020-12-30 19:00:00  0.583333  0.250000  0.333333  0.304348       0.207649   \n",
       " 2020-12-30 20:00:00  0.583333  0.333333  0.250000  0.260870       0.238838   \n",
       " 2020-12-30 21:00:00  0.555556  0.250000  0.250000  0.260870       0.231133   \n",
       " 2020-12-30 22:00:00  0.555556  0.333333  0.333333  0.260870       0.287107   \n",
       " 2020-12-30 23:00:00  0.583333  0.166667  0.250000  0.304348       0.299638   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2020-08-01 00:00:00      0.140722  0.616477  0.0  0.706349  0.762590  \n",
       " 2020-08-01 01:00:00      0.162722  0.590909  0.0  0.640212  0.856115  \n",
       " 2020-08-01 02:00:00      0.172654  0.610795  0.0  0.685185  0.848921  \n",
       " 2020-08-01 03:00:00      0.150283  0.615057  0.0  0.653439  0.859712  \n",
       " 2020-08-01 04:00:00      0.161886  0.616477  0.0  0.642857  0.823741  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2020-12-30 19:00:00      0.227513  0.473011  0.0  0.203704  0.280576  \n",
       " 2020-12-30 20:00:00      0.231505  0.473011  0.0  0.206349  0.280576  \n",
       " 2020-12-30 21:00:00      0.271048  0.475852  0.0  0.198413  0.266187  \n",
       " 2020-12-30 22:00:00      0.285157  0.474432  0.0  0.195767  0.266187  \n",
       " 2020-12-30 23:00:00      0.263715  0.474432  0.0  0.187831  0.284173  \n",
       " \n",
       " [3648 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2021-08-01 00:00:00  0.583333  0.250000  0.166667  0.173913       0.235310   \n",
       " 2021-08-01 01:00:00  0.611111  0.166667  0.166667  0.173913       0.171540   \n",
       " 2021-08-01 02:00:00  0.583333  0.250000  0.250000  0.217391       0.143971   \n",
       " 2021-08-01 03:00:00  0.611111  0.166667  0.250000  0.217391       0.130883   \n",
       " 2021-08-01 04:00:00  0.611111  0.250000  0.250000  0.217391       0.130511   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2021-11-18 19:00:00  0.666667  0.333333  0.333333  0.304348       0.299452   \n",
       " 2021-11-18 20:00:00  0.638889  0.250000  0.250000  0.260870       0.288870   \n",
       " 2021-11-18 21:00:00  0.611111  0.333333  0.250000  0.304348       0.284693   \n",
       " 2021-11-18 22:00:00  0.611111  0.333333  0.250000  0.260870       0.297317   \n",
       " 2021-11-18 23:00:00  0.611111  0.333333  0.250000  0.260870       0.264086   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2021-08-01 00:00:00      0.152975  0.559659  0.0  0.492063  0.712230  \n",
       " 2021-08-01 01:00:00      0.134689  0.555398  0.0  0.492063  0.712230  \n",
       " 2021-08-01 02:00:00      0.121786  0.553977  0.0  0.500000  0.712230  \n",
       " 2021-08-01 03:00:00      0.111482  0.553977  0.0  0.497354  0.712230  \n",
       " 2021-08-01 04:00:00      0.104335  0.555398  0.0  0.497354  0.708633  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2021-11-18 19:00:00      0.339738  0.829545  0.0  0.404762  0.550360  \n",
       " 2021-11-18 20:00:00      0.286828  0.838068  0.0  0.402116  0.557554  \n",
       " 2021-11-18 21:00:00      0.288221  0.836648  0.0  0.407407  0.564748  \n",
       " 2021-11-18 22:00:00      0.283672  0.833807  0.0  0.399471  0.564748  \n",
       " 2021-11-18 23:00:00      0.249977  0.833807  0.0  0.410053  0.568345  \n",
       " \n",
       " [2640 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2022-08-01 00:00:00  0.722222  0.166667  0.166667  0.130435       0.088833   \n",
       " 2022-08-01 01:00:00  0.694444  0.166667  0.166667  0.217391       0.083821   \n",
       " 2022-08-01 02:00:00  0.777778  0.083333  0.166667  0.173913       0.078344   \n",
       " 2022-08-01 03:00:00  0.861111  0.083333  0.166667  0.173913       0.096166   \n",
       " 2022-08-01 04:00:00  0.916667  0.166667  0.166667  0.130435       0.138959   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2022-11-18 19:00:00  0.222222  0.333333  0.333333  0.304348       0.138773   \n",
       " 2022-11-18 20:00:00  0.222222  0.416667  0.416667  0.391304       0.188156   \n",
       " 2022-11-18 21:00:00  0.222222  0.416667  0.416667  0.391304       0.177574   \n",
       " 2022-11-18 22:00:00  0.222222  0.416667  0.416667  0.391304       0.160679   \n",
       " 2022-11-18 23:00:00  0.222222  0.333333  0.333333  0.347826       0.155481   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2022-08-01 00:00:00      0.051239  0.673295  0.0  0.584656  0.812950  \n",
       " 2022-08-01 01:00:00      0.052910  0.671875  0.0  0.584656  0.812950  \n",
       " 2022-08-01 02:00:00      0.052724  0.671875  0.0  0.574074  0.812950  \n",
       " 2022-08-01 03:00:00      0.061821  0.674716  0.0  0.566138  0.802158  \n",
       " 2022-08-01 04:00:00      0.098023  0.680398  0.0  0.568783  0.809353  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2022-11-18 19:00:00      0.333519  0.521307  0.0  0.304233  0.449640  \n",
       " 2022-11-18 20:00:00      0.269470  0.531250  0.0  0.264550  0.395683  \n",
       " 2022-11-18 21:00:00      0.177666  0.539773  0.0  0.230159  0.345324  \n",
       " 2022-11-18 22:00:00      0.165135  0.546875  0.0  0.208995  0.309353  \n",
       " 2022-11-18 23:00:00      0.153625  0.555398  0.0  0.203704  0.294964  \n",
       " \n",
       " [2640 rows x 10 columns]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    len(TRAIN_YEARS),                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    len(VAL_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    len(TEST_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                           DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2017-08-01 00:00:00  0.166667  0.083333  0.083333  0.043478       0.207556   \n",
       " 2017-08-01 01:00:00  0.000000  0.000000  0.083333  0.086957       0.140351   \n",
       " 2017-08-01 02:00:00  0.000000  0.000000  0.000000  0.043478       0.252483   \n",
       " 2017-08-01 03:00:00  0.277778  0.083333  0.000000  0.043478       0.379467   \n",
       " 2017-08-01 04:00:00  0.805556  0.166667  0.083333  0.130435       0.426715   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2017-12-30 19:00:00  0.694444  0.416667  0.416667  0.478261       0.201615   \n",
       " 2017-12-30 20:00:00  0.694444  0.416667  0.333333  0.434783       0.217859   \n",
       " 2017-12-30 21:00:00  0.666667  0.333333  0.416667  0.434783       0.221480   \n",
       " 2017-12-30 22:00:00  0.694444  0.250000  0.250000  0.304348       0.209505   \n",
       " 2017-12-30 23:00:00  0.666667  0.416667  0.250000  0.304348       0.240787   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2017-08-01 00:00:00      0.223383  0.634943  0.0  0.497354  0.723022  \n",
       " 2017-08-01 01:00:00      0.207092  0.639205  0.0  0.505291  0.733813  \n",
       " 2017-08-01 02:00:00      0.157895  0.633523  0.0  0.473545  0.694245  \n",
       " 2017-08-01 03:00:00      0.137659  0.630682  0.0  0.439153  0.651079  \n",
       " 2017-08-01 04:00:00      0.149633  0.630682  0.0  0.462963  0.672662  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2017-12-30 19:00:00      0.168848  0.457386  0.0  0.396825  0.507194  \n",
       " 2017-12-30 20:00:00      0.140072  0.471591  0.0  0.388889  0.496403  \n",
       " 2017-12-30 21:00:00      0.191033  0.473011  0.0  0.375661  0.503597  \n",
       " 2017-12-30 22:00:00      0.147406  0.475852  0.0  0.370370  0.492806  \n",
       " 2017-12-30 23:00:00      0.158823  0.477273  0.0  0.373016  0.489209  \n",
       " \n",
       " [3648 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2018-08-01 00:00:00  0.166667  0.083333  0.083333  0.043478       0.572728   \n",
       " 2018-08-01 01:00:00  0.000000  0.000000  0.000000  0.043478       0.490485   \n",
       " 2018-08-01 02:00:00  0.000000  0.000000  0.000000  0.043478       0.511928   \n",
       " 2018-08-01 03:00:00  0.055556  0.083333  0.083333  0.043478       0.430428   \n",
       " 2018-08-01 04:00:00  0.194444  0.083333  0.083333  0.043478       0.387450   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2018-12-30 19:00:00  0.750000  0.166667  0.166667  0.173913       0.273554   \n",
       " 2018-12-30 20:00:00  0.750000  0.250000  0.166667  0.217391       0.245336   \n",
       " 2018-12-30 21:00:00  0.777778  0.250000  0.250000  0.217391       0.189548   \n",
       " 2018-12-30 22:00:00  0.805556  0.250000  0.166667  0.260870       0.169312   \n",
       " 2018-12-30 23:00:00  0.777778  0.166667  0.166667  0.217391       0.124478   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2018-08-01 00:00:00      0.247192  0.732955  0.0  0.428571  0.629496  \n",
       " 2018-08-01 01:00:00      0.252762  0.735795  0.0  0.420635  0.615108  \n",
       " 2018-08-01 02:00:00      0.217024  0.732955  0.0  0.404762  0.593525  \n",
       " 2018-08-01 03:00:00      0.189177  0.732955  0.0  0.404762  0.593525  \n",
       " 2018-08-01 04:00:00      0.243665  0.734375  0.3  0.436508  0.640288  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2018-12-30 19:00:00      0.255825  0.897727  0.0  0.346561  0.521583  \n",
       " 2018-12-30 20:00:00      0.263993  0.893466  0.0  0.349206  0.525180  \n",
       " 2018-12-30 21:00:00      0.199944  0.893466  0.0  0.349206  0.525180  \n",
       " 2018-12-30 22:00:00      0.178038  0.896307  0.0  0.351852  0.528777  \n",
       " 2018-12-30 23:00:00      0.145456  0.897727  0.0  0.354497  0.532374  \n",
       " \n",
       " [3648 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2020-08-01 00:00:00  0.527778  0.166667  0.250000  0.304348       0.115845   \n",
       " 2020-08-01 01:00:00  0.250000  0.083333  0.166667  0.217391       0.173953   \n",
       " 2020-08-01 02:00:00  0.611111  0.166667  0.250000  0.391304       0.117702   \n",
       " 2020-08-01 03:00:00  0.750000  0.083333  0.083333  0.173913       0.153439   \n",
       " 2020-08-01 04:00:00  0.666667  0.166667  0.166667  0.173913       0.236239   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2020-12-30 19:00:00  0.583333  0.250000  0.333333  0.304348       0.207649   \n",
       " 2020-12-30 20:00:00  0.583333  0.333333  0.250000  0.260870       0.238838   \n",
       " 2020-12-30 21:00:00  0.555556  0.250000  0.250000  0.260870       0.231133   \n",
       " 2020-12-30 22:00:00  0.555556  0.333333  0.333333  0.260870       0.287107   \n",
       " 2020-12-30 23:00:00  0.583333  0.166667  0.250000  0.304348       0.299638   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2020-08-01 00:00:00      0.140722  0.616477  0.0  0.706349  0.762590  \n",
       " 2020-08-01 01:00:00      0.162722  0.590909  0.0  0.640212  0.856115  \n",
       " 2020-08-01 02:00:00      0.172654  0.610795  0.0  0.685185  0.848921  \n",
       " 2020-08-01 03:00:00      0.150283  0.615057  0.0  0.653439  0.859712  \n",
       " 2020-08-01 04:00:00      0.161886  0.616477  0.0  0.642857  0.823741  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2020-12-30 19:00:00      0.227513  0.473011  0.0  0.203704  0.280576  \n",
       " 2020-12-30 20:00:00      0.231505  0.473011  0.0  0.206349  0.280576  \n",
       " 2020-12-30 21:00:00      0.271048  0.475852  0.0  0.198413  0.266187  \n",
       " 2020-12-30 22:00:00      0.285157  0.474432  0.0  0.195767  0.266187  \n",
       " 2020-12-30 23:00:00      0.263715  0.474432  0.0  0.187831  0.284173  \n",
       " \n",
       " [3648 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2021-08-01 00:00:00  0.583333  0.250000  0.166667  0.173913       0.235310   \n",
       " 2021-08-01 01:00:00  0.611111  0.166667  0.166667  0.173913       0.171540   \n",
       " 2021-08-01 02:00:00  0.583333  0.250000  0.250000  0.217391       0.143971   \n",
       " 2021-08-01 03:00:00  0.611111  0.166667  0.250000  0.217391       0.130883   \n",
       " 2021-08-01 04:00:00  0.611111  0.250000  0.250000  0.217391       0.130511   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2021-11-18 19:00:00  0.666667  0.333333  0.333333  0.304348       0.299452   \n",
       " 2021-11-18 20:00:00  0.638889  0.250000  0.250000  0.260870       0.288870   \n",
       " 2021-11-18 21:00:00  0.611111  0.333333  0.250000  0.304348       0.284693   \n",
       " 2021-11-18 22:00:00  0.611111  0.333333  0.250000  0.260870       0.297317   \n",
       " 2021-11-18 23:00:00  0.611111  0.333333  0.250000  0.260870       0.264086   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2021-08-01 00:00:00      0.152975  0.559659  0.0  0.492063  0.712230  \n",
       " 2021-08-01 01:00:00      0.134689  0.555398  0.0  0.492063  0.712230  \n",
       " 2021-08-01 02:00:00      0.121786  0.553977  0.0  0.500000  0.712230  \n",
       " 2021-08-01 03:00:00      0.111482  0.553977  0.0  0.497354  0.712230  \n",
       " 2021-08-01 04:00:00      0.104335  0.555398  0.0  0.497354  0.708633  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2021-11-18 19:00:00      0.339738  0.829545  0.0  0.404762  0.550360  \n",
       " 2021-11-18 20:00:00      0.286828  0.838068  0.0  0.402116  0.557554  \n",
       " 2021-11-18 21:00:00      0.288221  0.836648  0.0  0.407407  0.564748  \n",
       " 2021-11-18 22:00:00      0.283672  0.833807  0.0  0.399471  0.564748  \n",
       " 2021-11-18 23:00:00      0.249977  0.833807  0.0  0.410053  0.568345  \n",
       " \n",
       " [2640 rows x 10 columns],\n",
       "                            DD        FF        FH        FX  NO2_BREUKELEN  \\\n",
       " DateTime                                                                     \n",
       " 2022-08-01 00:00:00  0.722222  0.166667  0.166667  0.130435       0.088833   \n",
       " 2022-08-01 01:00:00  0.694444  0.166667  0.166667  0.217391       0.083821   \n",
       " 2022-08-01 02:00:00  0.777778  0.083333  0.166667  0.173913       0.078344   \n",
       " 2022-08-01 03:00:00  0.861111  0.083333  0.166667  0.173913       0.096166   \n",
       " 2022-08-01 04:00:00  0.916667  0.166667  0.166667  0.130435       0.138959   \n",
       " ...                       ...       ...       ...       ...            ...   \n",
       " 2022-11-18 19:00:00  0.222222  0.333333  0.333333  0.304348       0.138773   \n",
       " 2022-11-18 20:00:00  0.222222  0.416667  0.416667  0.391304       0.188156   \n",
       " 2022-11-18 21:00:00  0.222222  0.416667  0.416667  0.391304       0.177574   \n",
       " 2022-11-18 22:00:00  0.222222  0.416667  0.416667  0.391304       0.160679   \n",
       " 2022-11-18 23:00:00  0.222222  0.333333  0.333333  0.347826       0.155481   \n",
       " \n",
       "                      NO2_TUINDORP         P   SQ         T        TD  \n",
       " DateTime                                                              \n",
       " 2022-08-01 00:00:00      0.051239  0.673295  0.0  0.584656  0.812950  \n",
       " 2022-08-01 01:00:00      0.052910  0.671875  0.0  0.584656  0.812950  \n",
       " 2022-08-01 02:00:00      0.052724  0.671875  0.0  0.574074  0.812950  \n",
       " 2022-08-01 03:00:00      0.061821  0.674716  0.0  0.566138  0.802158  \n",
       " 2022-08-01 04:00:00      0.098023  0.680398  0.0  0.568783  0.809353  \n",
       " ...                           ...       ...  ...       ...       ...  \n",
       " 2022-11-18 19:00:00      0.333519  0.521307  0.0  0.304233  0.449640  \n",
       " 2022-11-18 20:00:00      0.269470  0.531250  0.0  0.264550  0.395683  \n",
       " 2022-11-18 21:00:00      0.177666  0.539773  0.0  0.230159  0.345324  \n",
       " 2022-11-18 22:00:00      0.165135  0.546875  0.0  0.208995  0.309353  \n",
       " 2022-11-18 23:00:00      0.153625  0.555398  0.0  0.203704  0.294964  \n",
       " \n",
       " [2640 rows x 10 columns]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                          NO2\n",
       " DateTime                     \n",
       " 2017-08-01 00:00:00  0.207556\n",
       " 2017-08-01 01:00:00  0.140351\n",
       " 2017-08-01 02:00:00  0.252483\n",
       " 2017-08-01 03:00:00  0.379467\n",
       " 2017-08-01 04:00:00  0.426715\n",
       " ...                       ...\n",
       " 2017-12-30 19:00:00  0.201615\n",
       " 2017-12-30 20:00:00  0.217859\n",
       " 2017-12-30 21:00:00  0.221480\n",
       " 2017-12-30 22:00:00  0.209505\n",
       " 2017-12-30 23:00:00  0.240787\n",
       " \n",
       " [3648 rows x 1 columns],\n",
       "                           NO2\n",
       " DateTime                     \n",
       " 2018-08-01 00:00:00  0.572728\n",
       " 2018-08-01 01:00:00  0.490485\n",
       " 2018-08-01 02:00:00  0.511928\n",
       " 2018-08-01 03:00:00  0.430428\n",
       " 2018-08-01 04:00:00  0.387450\n",
       " ...                       ...\n",
       " 2018-12-30 19:00:00  0.273554\n",
       " 2018-12-30 20:00:00  0.245336\n",
       " 2018-12-30 21:00:00  0.189548\n",
       " 2018-12-30 22:00:00  0.169312\n",
       " 2018-12-30 23:00:00  0.124478\n",
       " \n",
       " [3648 rows x 1 columns],\n",
       "                           NO2\n",
       " DateTime                     \n",
       " 2020-08-01 00:00:00  0.115845\n",
       " 2020-08-01 01:00:00  0.173953\n",
       " 2020-08-01 02:00:00  0.117702\n",
       " 2020-08-01 03:00:00  0.153439\n",
       " 2020-08-01 04:00:00  0.236239\n",
       " ...                       ...\n",
       " 2020-12-30 19:00:00  0.207649\n",
       " 2020-12-30 20:00:00  0.238838\n",
       " 2020-12-30 21:00:00  0.231133\n",
       " 2020-12-30 22:00:00  0.287107\n",
       " 2020-12-30 23:00:00  0.299638\n",
       " \n",
       " [3648 rows x 1 columns],\n",
       "                           NO2\n",
       " DateTime                     \n",
       " 2021-08-01 00:00:00  0.235310\n",
       " 2021-08-01 01:00:00  0.171540\n",
       " 2021-08-01 02:00:00  0.143971\n",
       " 2021-08-01 03:00:00  0.130883\n",
       " 2021-08-01 04:00:00  0.130511\n",
       " ...                       ...\n",
       " 2021-11-18 19:00:00  0.299452\n",
       " 2021-11-18 20:00:00  0.288870\n",
       " 2021-11-18 21:00:00  0.284693\n",
       " 2021-11-18 22:00:00  0.297317\n",
       " 2021-11-18 23:00:00  0.264086\n",
       " \n",
       " [2640 rows x 1 columns],\n",
       "                           NO2\n",
       " DateTime                     \n",
       " 2022-08-01 00:00:00  0.088833\n",
       " 2022-08-01 01:00:00  0.083821\n",
       " 2022-08-01 02:00:00  0.078344\n",
       " 2022-08-01 03:00:00  0.096166\n",
       " 2022-08-01 04:00:00  0.138959\n",
       " ...                       ...\n",
       " 2022-11-18 19:00:00  0.138773\n",
       " 2022-11-18 20:00:00  0.188156\n",
       " 2022-11-18 21:00:00  0.177574\n",
       " 2022-11-18 22:00:00  0.160679\n",
       " 2022-11-18 23:00:00  0.155481\n",
       " \n",
       " [2640 rows x 1 columns]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.0833, 0.0833, 0.0435, 0.2076, 0.2234, 0.6349, 0.0000, 0.4974,\n",
       "         0.7230],\n",
       "        [0.0000, 0.0000, 0.0833, 0.0870, 0.1404, 0.2071, 0.6392, 0.0000, 0.5053,\n",
       "         0.7338],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0435, 0.2525, 0.1579, 0.6335, 0.0000, 0.4735,\n",
       "         0.6942],\n",
       "        [0.2778, 0.0833, 0.0000, 0.0435, 0.3795, 0.1377, 0.6307, 0.0000, 0.4392,\n",
       "         0.6511],\n",
       "        [0.8056, 0.1667, 0.0833, 0.1304, 0.4267, 0.1496, 0.6307, 0.0000, 0.4630,\n",
       "         0.6727],\n",
       "        [0.0000, 0.0000, 0.0833, 0.1304, 0.4310, 0.2847, 0.6378, 0.3000, 0.5635,\n",
       "         0.7482],\n",
       "        [0.7222, 0.0833, 0.0833, 0.0870, 0.3823, 0.4730, 0.6506, 0.0000, 0.5820,\n",
       "         0.7230],\n",
       "        [0.7500, 0.0833, 0.0833, 0.1304, 0.3783, 0.4766, 0.6520, 0.0000, 0.5873,\n",
       "         0.7050],\n",
       "        [0.7222, 0.1667, 0.1667, 0.1304, 0.3354, 0.4598, 0.6534, 0.0000, 0.6058,\n",
       "         0.6691],\n",
       "        [0.7500, 0.1667, 0.1667, 0.2174, 0.3959, 0.3985, 0.6548, 0.3000, 0.6376,\n",
       "         0.6367],\n",
       "        [0.6111, 0.1667, 0.1667, 0.1739, 0.3266, 0.2842, 0.6577, 0.0000, 0.6402,\n",
       "         0.6259],\n",
       "        [0.6111, 0.1667, 0.1667, 0.1739, 0.2639, 0.3142, 0.6591, 0.0000, 0.6376,\n",
       "         0.6187],\n",
       "        [0.6528, 0.0833, 0.1667, 0.1304, 0.3375, 0.3289, 0.6577, 0.2000, 0.6905,\n",
       "         0.6583],\n",
       "        [0.6944, 0.1667, 0.1667, 0.1304, 0.2600, 0.2748, 0.6562, 0.2000, 0.6958,\n",
       "         0.5360],\n",
       "        [0.7222, 0.1667, 0.1667, 0.2174, 0.3125, 0.2102, 0.6548, 0.4000, 0.6852,\n",
       "         0.5504],\n",
       "        [0.6111, 0.1667, 0.1667, 0.1739, 0.3502, 0.2083, 0.6562, 0.1000, 0.6772,\n",
       "         0.6223],\n",
       "        [0.6944, 0.1667, 0.1667, 0.2174, 0.3312, 0.2607, 0.6577, 0.7000, 0.6825,\n",
       "         0.6151],\n",
       "        [0.5833, 0.1667, 0.1667, 0.1739, 0.2937, 0.2275, 0.6562, 0.2000, 0.6481,\n",
       "         0.6583],\n",
       "        [0.7222, 0.1667, 0.1667, 0.1739, 0.3748, 0.2019, 0.6634, 0.2000, 0.6243,\n",
       "         0.6978],\n",
       "        [0.6944, 0.0833, 0.0833, 0.1304, 0.3481, 0.2589, 0.6719, 0.0000, 0.5423,\n",
       "         0.7122],\n",
       "        [0.4167, 0.0833, 0.0833, 0.0870, 0.4018, 0.3520, 0.6719, 0.0000, 0.4894,\n",
       "         0.6978],\n",
       "        [0.4722, 0.0833, 0.0833, 0.0435, 0.4651, 0.4237, 0.6761, 0.0000, 0.4841,\n",
       "         0.7014],\n",
       "        [0.4722, 0.0833, 0.0833, 0.0870, 0.5232, 0.5091, 0.6719, 0.0000, 0.4735,\n",
       "         0.6906],\n",
       "        [0.5000, 0.0833, 0.0833, 0.1304, 0.5465, 0.5216, 0.6690, 0.0000, 0.4709,\n",
       "         0.6799],\n",
       "        [0.0000, 0.0000, 0.0833, 0.0870, 0.4333, 0.4872, 0.6719, 0.0000, 0.4683,\n",
       "         0.6871],\n",
       "        [0.4444, 0.0833, 0.0833, 0.0870, 0.3848, 0.4529, 0.6676, 0.0000, 0.4921,\n",
       "         0.6978],\n",
       "        [0.4167, 0.1667, 0.0833, 0.0870, 0.2990, 0.2914, 0.6634, 0.0000, 0.4894,\n",
       "         0.6942],\n",
       "        [0.3889, 0.0833, 0.1667, 0.1304, 0.3332, 0.2590, 0.6619, 0.0000, 0.5053,\n",
       "         0.6942],\n",
       "        [0.5000, 0.0833, 0.0833, 0.0870, 0.3998, 0.3676, 0.6634, 0.0000, 0.5106,\n",
       "         0.7086],\n",
       "        [0.5833, 0.1667, 0.1667, 0.1739, 0.5275, 0.4276, 0.6705, 0.0000, 0.5529,\n",
       "         0.7086],\n",
       "        [0.6389, 0.1667, 0.1667, 0.2174, 0.4581, 0.4524, 0.6705, 0.0000, 0.5741,\n",
       "         0.7014],\n",
       "        [0.6667, 0.3333, 0.2500, 0.2609, 0.4007, 0.3724, 0.6761, 0.0000, 0.5926,\n",
       "         0.6691],\n",
       "        [0.6667, 0.3333, 0.3333, 0.3043, 0.3140, 0.3054, 0.6818, 0.1000, 0.6111,\n",
       "         0.6691],\n",
       "        [0.6389, 0.2500, 0.2500, 0.3478, 0.2963, 0.2396, 0.6818, 0.1000, 0.6323,\n",
       "         0.6799],\n",
       "        [0.5556, 0.3333, 0.3333, 0.2609, 0.3106, 0.2232, 0.6861, 0.3000, 0.6508,\n",
       "         0.6475],\n",
       "        [0.5833, 0.3333, 0.3333, 0.3043, 0.3101, 0.1998, 0.6790, 0.2000, 0.6614,\n",
       "         0.6043],\n",
       "        [0.5556, 0.2500, 0.2500, 0.2609, 0.3055, 0.1685, 0.6676, 0.1000, 0.6746,\n",
       "         0.5755],\n",
       "        [0.5000, 0.3333, 0.3333, 0.2609, 0.3454, 0.1859, 0.6662, 0.0000, 0.6693,\n",
       "         0.5899],\n",
       "        [0.5000, 0.4167, 0.3333, 0.3478, 0.3333, 0.2230, 0.6534, 0.0000, 0.6746,\n",
       "         0.6403],\n",
       "        [0.5833, 0.4167, 0.3333, 0.4348, 0.3390, 0.2631, 0.6520, 0.0000, 0.6481,\n",
       "         0.6115],\n",
       "        [0.5833, 0.3333, 0.5833, 0.5217, 0.3920, 0.2489, 0.6605, 0.0000, 0.5423,\n",
       "         0.6871],\n",
       "        [0.5556, 0.1667, 0.2500, 0.3043, 0.2634, 0.1817, 0.6534, 0.0000, 0.5159,\n",
       "         0.7086],\n",
       "        [0.4861, 0.0833, 0.0833, 0.1739, 0.4256, 0.2076, 0.6463, 0.0000, 0.5106,\n",
       "         0.7302],\n",
       "        [0.4167, 0.1667, 0.1667, 0.1739, 0.3067, 0.2242, 0.6364, 0.0000, 0.5106,\n",
       "         0.7230],\n",
       "        [0.3889, 0.1667, 0.1667, 0.1739, 0.1685, 0.2864, 0.6378, 0.0000, 0.5132,\n",
       "         0.7266],\n",
       "        [0.4444, 0.2500, 0.1667, 0.1739, 0.2038, 0.2611, 0.6307, 0.0000, 0.5317,\n",
       "         0.7518],\n",
       "        [0.5000, 0.2500, 0.2500, 0.2609, 0.1856, 0.1938, 0.6207, 0.0000, 0.5450,\n",
       "         0.7626],\n",
       "        [0.5000, 0.1667, 0.1667, 0.2174, 0.1620, 0.1562, 0.6094, 0.0000, 0.5476,\n",
       "         0.7626],\n",
       "        [0.4444, 0.2500, 0.2500, 0.2609, 0.1565, 0.1464, 0.5980, 0.0000, 0.5450,\n",
       "         0.7662],\n",
       "        [0.4722, 0.3333, 0.2500, 0.3043, 0.1842, 0.1294, 0.5852, 0.0000, 0.5476,\n",
       "         0.7662],\n",
       "        [0.5000, 0.3333, 0.3333, 0.3043, 0.1443, 0.1229, 0.5682, 0.0000, 0.5556,\n",
       "         0.7518],\n",
       "        [0.4722, 0.1667, 0.2500, 0.2609, 0.1457, 0.1151, 0.5611, 0.0000, 0.5476,\n",
       "         0.7518],\n",
       "        [0.4722, 0.1667, 0.1667, 0.1739, 0.2406, 0.1671, 0.5540, 0.0000, 0.5450,\n",
       "         0.7590],\n",
       "        [0.5556, 0.3333, 0.2500, 0.3043, 0.4900, 0.2594, 0.5526, 0.0000, 0.5899,\n",
       "         0.7842],\n",
       "        [0.5833, 0.4167, 0.3333, 0.3913, 0.4313, 0.3105, 0.5455, 0.5000, 0.6243,\n",
       "         0.7806],\n",
       "        [0.5833, 0.5000, 0.4167, 0.4348, 0.3199, 0.2780, 0.5440, 0.0000, 0.6164,\n",
       "         0.7950],\n",
       "        [0.5833, 0.5000, 0.4167, 0.4348, 0.2520, 0.2400, 0.5469, 0.0000, 0.5979,\n",
       "         0.8201],\n",
       "        [0.6389, 0.4167, 0.5000, 0.5217, 0.1827, 0.2261, 0.5554, 0.0000, 0.5582,\n",
       "         0.7374],\n",
       "        [0.6389, 0.5833, 0.5833, 0.5652, 0.1643, 0.1386, 0.5568, 0.6000, 0.6323,\n",
       "         0.6871],\n",
       "        [0.6667, 0.7500, 0.6667, 0.6522, 0.1446, 0.1052, 0.5597, 0.7000, 0.6640,\n",
       "         0.6583],\n",
       "        [0.6667, 0.6667, 0.5833, 0.6522, 0.1315, 0.0675, 0.5625, 0.8000, 0.6799,\n",
       "         0.6259],\n",
       "        [0.6667, 0.7500, 0.7500, 0.7826, 0.0821, 0.0755, 0.5625, 1.0000, 0.6958,\n",
       "         0.6295],\n",
       "        [0.6944, 0.6667, 0.6667, 0.6957, 0.1171, 0.0664, 0.5682, 0.4000, 0.6587,\n",
       "         0.6547],\n",
       "        [0.6667, 0.6667, 0.5833, 0.6087, 0.1264, 0.0860, 0.5653, 0.5000, 0.6772,\n",
       "         0.6547],\n",
       "        [0.6667, 0.5833, 0.6667, 0.6522, 0.1217, 0.1027, 0.5710, 1.0000, 0.6640,\n",
       "         0.6978],\n",
       "        [0.6389, 0.7500, 0.6667, 0.7391, 0.1050, 0.1016, 0.5753, 0.5000, 0.6323,\n",
       "         0.5683],\n",
       "        [0.6667, 0.5833, 0.7500, 0.7826, 0.1010, 0.0756, 0.5795, 0.3000, 0.6190,\n",
       "         0.5935],\n",
       "        [0.6389, 0.8333, 0.6667, 0.8261, 0.0851, 0.0645, 0.5810, 0.0000, 0.6111,\n",
       "         0.5971],\n",
       "        [0.6667, 0.7500, 0.6667, 0.7391, 0.0860, 0.0611, 0.5795, 0.0000, 0.5926,\n",
       "         0.6043],\n",
       "        [0.6667, 0.5833, 0.5833, 0.6522, 0.1145, 0.0562, 0.5795, 0.0000, 0.5714,\n",
       "         0.6259],\n",
       "        [0.6389, 0.3333, 0.4167, 0.5217, 0.0873, 0.0476, 0.5767, 0.0000, 0.5556,\n",
       "         0.6403],\n",
       "        [0.6111, 0.4167, 0.3333, 0.3913, 0.0643, 0.0473, 0.5724, 0.0000, 0.5423,\n",
       "         0.6475]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1842],\n",
       "        [0.1443],\n",
       "        [0.1457],\n",
       "        [0.2406],\n",
       "        [0.4900],\n",
       "        [0.4313],\n",
       "        [0.3199],\n",
       "        [0.2520],\n",
       "        [0.1827],\n",
       "        [0.1643],\n",
       "        [0.1446],\n",
       "        [0.1315],\n",
       "        [0.0821],\n",
       "        [0.1171],\n",
       "        [0.1264],\n",
       "        [0.1217],\n",
       "        [0.1050],\n",
       "        [0.1010],\n",
       "        [0.0851],\n",
       "        [0.0860],\n",
       "        [0.1145],\n",
       "        [0.0873],\n",
       "        [0.0643],\n",
       "        [0.1017]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No2 tuindorp idx:  5\n",
      "No2 breukelen idx:  4\n",
      "wind dir (dd) idx:  0\n",
      "wind speed (fh) idx:  2\n",
      "Column indices are same as config.py\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataset.u[0] is a pandas Index object with column names\n",
    "column_names = list(train_dataset.u[0])  # Convert Index to list\n",
    "\n",
    "\n",
    "print(\"No2 tuindorp idx: \", column_names.index('NO2_TUINDORP'))\n",
    "print(\"No2 breukelen idx: \", column_names.index('NO2_BREUKELEN'))\n",
    "print(\"wind dir (dd) idx: \", column_names.index('DD'))\n",
    "print(\"wind speed (fh) idx: \", column_names.index('FH'))\n",
    "\n",
    "# check if the indices are the same as whats defined in config.py\n",
    "assert column_names.index('NO2_TUINDORP')== NO2_TUINDORP_IDX\n",
    "assert column_names.index('NO2_BREUKELEN') == NO2_BREUKELEN_IDX\n",
    "assert column_names.index('DD') == WIND_DIR_IDX\n",
    "assert column_names.index('FH') == WIND_SPEED_IDX\n",
    "print(\"Column indices are same as config.py\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.223383\n",
       "2017-08-01 01:00:00    0.207092\n",
       "2017-08-01 02:00:00    0.157895\n",
       "2017-08-01 03:00:00    0.137659\n",
       "2017-08-01 04:00:00    0.149633\n",
       "                         ...   \n",
       "2017-12-30 19:00:00    0.168848\n",
       "2017-12-30 20:00:00    0.140072\n",
       "2017-12-30 21:00:00    0.191033\n",
       "2017-12-30 22:00:00    0.147406\n",
       "2017-12-30 23:00:00    0.158823\n",
       "Name: NO2_TUINDORP, Length: 3648, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,NO2_TUINDORP_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.207556\n",
       "2017-08-01 01:00:00    0.140351\n",
       "2017-08-01 02:00:00    0.252483\n",
       "2017-08-01 03:00:00    0.379467\n",
       "2017-08-01 04:00:00    0.426715\n",
       "                         ...   \n",
       "2017-12-30 19:00:00    0.201615\n",
       "2017-12-30 20:00:00    0.217859\n",
       "2017-12-30 21:00:00    0.221480\n",
       "2017-12-30 22:00:00    0.209505\n",
       "2017-12-30 23:00:00    0.240787\n",
       "Name: NO2_BREUKELEN, Length: 3648, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,NO2_BREUKELEN_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.166667\n",
       "2017-08-01 01:00:00    0.000000\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.277778\n",
       "2017-08-01 04:00:00    0.805556\n",
       "                         ...   \n",
       "2017-12-30 19:00:00    0.694444\n",
       "2017-12-30 20:00:00    0.694444\n",
       "2017-12-30 21:00:00    0.666667\n",
       "2017-12-30 22:00:00    0.694444\n",
       "2017-12-30 23:00:00    0.666667\n",
       "Name: DD, Length: 3648, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,WIND_DIR_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.083333\n",
       "2017-08-01 01:00:00    0.083333\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.000000\n",
       "2017-08-01 04:00:00    0.083333\n",
       "                         ...   \n",
       "2017-12-30 19:00:00    0.416667\n",
       "2017-12-30 20:00:00    0.333333\n",
       "2017-12-30 21:00:00    0.416667\n",
       "2017-12-30 22:00:00    0.250000\n",
       "2017-12-30 23:00:00    0.250000\n",
       "Name: FH, Length: 3648, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,WIND_SPEED_IDX]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning with loss function:  LinearShift_MSE\n",
      "tuning with nn type:  MLP\n"
     ]
    }
   ],
   "source": [
    "print(\"tuning with loss function: \", LOSS_FUNC)\n",
    "print(\"tuning with nn type: \", NN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:01:13,906] A new study created in RDB with name: mlp_hyperparameter_optimization_linearshift_mse_allyears\n",
      "/tmp/ipykernel_12613/2082688131.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_12613/2082688131.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n",
      "/tmp/ipykernel_12613/2082688131.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_phy = trial.suggest_loguniform(\"lambda_phy\", 1e-5, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.015932 - Val Loss (simple RMSE, no physics involved): 0.074919\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007464 - Val Loss (simple RMSE, no physics involved): 0.070691\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007257 - Val Loss (simple RMSE, no physics involved): 0.074675\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007108 - Val Loss (simple RMSE, no physics involved): 0.071011\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007123 - Val Loss (simple RMSE, no physics involved): 0.068931\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007213 - Val Loss (simple RMSE, no physics involved): 0.066955\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007133 - Val Loss (simple RMSE, no physics involved): 0.069343\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007066 - Val Loss (simple RMSE, no physics involved): 0.071833\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007192 - Val Loss (simple RMSE, no physics involved): 0.068072\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007021 - Val Loss (simple RMSE, no physics involved): 0.065835\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.068727\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006994 - Val Loss (simple RMSE, no physics involved): 0.074787\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007063 - Val Loss (simple RMSE, no physics involved): 0.069239\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.066042\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007026 - Val Loss (simple RMSE, no physics involved): 0.065963\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.066410\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006978 - Val Loss (simple RMSE, no physics involved): 0.075245\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006980 - Val Loss (simple RMSE, no physics involved): 0.078611\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007090 - Val Loss (simple RMSE, no physics involved): 0.076529\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.065301\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007016 - Val Loss (simple RMSE, no physics involved): 0.068551\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006862 - Val Loss (simple RMSE, no physics involved): 0.065745\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007031 - Val Loss (simple RMSE, no physics involved): 0.066130\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.078997\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007233 - Val Loss (simple RMSE, no physics involved): 0.065558\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.065311\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006926 - Val Loss (simple RMSE, no physics involved): 0.068337\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006903 - Val Loss (simple RMSE, no physics involved): 0.067184\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006861 - Val Loss (simple RMSE, no physics involved): 0.068228\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006927 - Val Loss (simple RMSE, no physics involved): 0.068803\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006874 - Val Loss (simple RMSE, no physics involved): 0.068028\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006833 - Val Loss (simple RMSE, no physics involved): 0.067492\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006911 - Val Loss (simple RMSE, no physics involved): 0.074584\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007090 - Val Loss (simple RMSE, no physics involved): 0.070274\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006928 - Val Loss (simple RMSE, no physics involved): 0.067499\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.069059\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006857 - Val Loss (simple RMSE, no physics involved): 0.067348\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006867 - Val Loss (simple RMSE, no physics involved): 0.068061\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.077171\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006939 - Val Loss (simple RMSE, no physics involved): 0.065827\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006786 - Val Loss (simple RMSE, no physics involved): 0.068384\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006891 - Val Loss (simple RMSE, no physics involved): 0.067218\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006860 - Val Loss (simple RMSE, no physics involved): 0.068232\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006859 - Val Loss (simple RMSE, no physics involved): 0.069768\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006891 - Val Loss (simple RMSE, no physics involved): 0.068582\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007048 - Val Loss (simple RMSE, no physics involved): 0.064992\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006818 - Val Loss (simple RMSE, no physics involved): 0.065419\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006945 - Val Loss (simple RMSE, no physics involved): 0.069036\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.069655\n",
      "Epoch 50/50\n",
      "Epoch 50/50 - Train Loss: 0.006813 - Val Loss (simple RMSE, no physics involved): 0.069657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:06:27,361] Trial 0 finished with value: 0.06499167283376057 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 211, 'lr': 0.0007373374686952379, 'weight_decay': 3.5793194295136316e-05, 'batch_size': 8, 'lambda_phy': 3.081287066241128e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.054261 - Val Loss (simple RMSE, no physics involved): 0.162174\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.048371 - Val Loss (simple RMSE, no physics involved): 0.150011\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.042763 - Val Loss (simple RMSE, no physics involved): 0.138157\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.037473 - Val Loss (simple RMSE, no physics involved): 0.127002\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.032604 - Val Loss (simple RMSE, no physics involved): 0.117691\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.028439 - Val Loss (simple RMSE, no physics involved): 0.110965\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.025048 - Val Loss (simple RMSE, no physics involved): 0.107497\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.022576 - Val Loss (simple RMSE, no physics involved): 0.106860\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.020917 - Val Loss (simple RMSE, no physics involved): 0.107967\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.019886 - Val Loss (simple RMSE, no physics involved): 0.109895\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.019230 - Val Loss (simple RMSE, no physics involved): 0.111290\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.018777 - Val Loss (simple RMSE, no physics involved): 0.111900\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.018460 - Val Loss (simple RMSE, no physics involved): 0.112237\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.018181 - Val Loss (simple RMSE, no physics involved): 0.112252\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.017913 - Val Loss (simple RMSE, no physics involved): 0.111722\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.017646 - Val Loss (simple RMSE, no physics involved): 0.111086\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017374 - Val Loss (simple RMSE, no physics involved): 0.110426\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017082 - Val Loss (simple RMSE, no physics involved): 0.109699\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016746 - Val Loss (simple RMSE, no physics involved): 0.107599\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016399 - Val Loss (simple RMSE, no physics involved): 0.106821\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015927 - Val Loss (simple RMSE, no physics involved): 0.105430\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015470 - Val Loss (simple RMSE, no physics involved): 0.103813\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015051 - Val Loss (simple RMSE, no physics involved): 0.102664\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014639 - Val Loss (simple RMSE, no physics involved): 0.100821\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014206 - Val Loss (simple RMSE, no physics involved): 0.099277\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013781 - Val Loss (simple RMSE, no physics involved): 0.098344\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013306 - Val Loss (simple RMSE, no physics involved): 0.095834\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.012841 - Val Loss (simple RMSE, no physics involved): 0.095726\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.012420 - Val Loss (simple RMSE, no physics involved): 0.092713\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.011993 - Val Loss (simple RMSE, no physics involved): 0.091577\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.011580 - Val Loss (simple RMSE, no physics involved): 0.089967\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.011191 - Val Loss (simple RMSE, no physics involved): 0.087821\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.010838 - Val Loss (simple RMSE, no physics involved): 0.085776\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.010521 - Val Loss (simple RMSE, no physics involved): 0.084503\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.010229 - Val Loss (simple RMSE, no physics involved): 0.084625\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.009981 - Val Loss (simple RMSE, no physics involved): 0.083625\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.009761 - Val Loss (simple RMSE, no physics involved): 0.082383\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.009552 - Val Loss (simple RMSE, no physics involved): 0.080015\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.009380 - Val Loss (simple RMSE, no physics involved): 0.080333\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.009214 - Val Loss (simple RMSE, no physics involved): 0.080082\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.009087 - Val Loss (simple RMSE, no physics involved): 0.079659\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.008966 - Val Loss (simple RMSE, no physics involved): 0.077760\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.008877 - Val Loss (simple RMSE, no physics involved): 0.077896\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.008787 - Val Loss (simple RMSE, no physics involved): 0.077789\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.008711 - Val Loss (simple RMSE, no physics involved): 0.076302\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.008648 - Val Loss (simple RMSE, no physics involved): 0.076500\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.008582 - Val Loss (simple RMSE, no physics involved): 0.076792\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.008541 - Val Loss (simple RMSE, no physics involved): 0.077216\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.008487 - Val Loss (simple RMSE, no physics involved): 0.076197\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:09:03,727] Trial 1 finished with value: 0.07515470311045647 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 54, 'lr': 1.531611130579815e-05, 'weight_decay': 2.4929254251452253e-05, 'batch_size': 8, 'lambda_phy': 0.013349638195138788}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.008439 - Val Loss (simple RMSE, no physics involved): 0.075155\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.062216 - Val Loss (simple RMSE, no physics involved): 0.110583\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.022743 - Val Loss (simple RMSE, no physics involved): 0.137057\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.018409 - Val Loss (simple RMSE, no physics involved): 0.109441\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.016288 - Val Loss (simple RMSE, no physics involved): 0.105696\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.012585 - Val Loss (simple RMSE, no physics involved): 0.084582\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.009528 - Val Loss (simple RMSE, no physics involved): 0.075153\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.008527 - Val Loss (simple RMSE, no physics involved): 0.073817\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007959 - Val Loss (simple RMSE, no physics involved): 0.073288\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007627 - Val Loss (simple RMSE, no physics involved): 0.072832\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007673 - Val Loss (simple RMSE, no physics involved): 0.071248\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007598 - Val Loss (simple RMSE, no physics involved): 0.072617\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007385 - Val Loss (simple RMSE, no physics involved): 0.071120\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007503 - Val Loss (simple RMSE, no physics involved): 0.074420\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007338 - Val Loss (simple RMSE, no physics involved): 0.067856\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007254 - Val Loss (simple RMSE, no physics involved): 0.072054\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007105 - Val Loss (simple RMSE, no physics involved): 0.067684\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007063 - Val Loss (simple RMSE, no physics involved): 0.068841\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007221 - Val Loss (simple RMSE, no physics involved): 0.068098\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007287 - Val Loss (simple RMSE, no physics involved): 0.069446\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007204 - Val Loss (simple RMSE, no physics involved): 0.069086\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007038 - Val Loss (simple RMSE, no physics involved): 0.067737\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007117 - Val Loss (simple RMSE, no physics involved): 0.069655\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007241 - Val Loss (simple RMSE, no physics involved): 0.073284\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007393 - Val Loss (simple RMSE, no physics involved): 0.067690\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007136 - Val Loss (simple RMSE, no physics involved): 0.068402\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007152 - Val Loss (simple RMSE, no physics involved): 0.069309\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.070877\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007268 - Val Loss (simple RMSE, no physics involved): 0.071997\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007233 - Val Loss (simple RMSE, no physics involved): 0.067612\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007101 - Val Loss (simple RMSE, no physics involved): 0.071858\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006994 - Val Loss (simple RMSE, no physics involved): 0.067207\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007219 - Val Loss (simple RMSE, no physics involved): 0.071917\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007239 - Val Loss (simple RMSE, no physics involved): 0.069046\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007069 - Val Loss (simple RMSE, no physics involved): 0.068716\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007175 - Val Loss (simple RMSE, no physics involved): 0.069372\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007137 - Val Loss (simple RMSE, no physics involved): 0.070392\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006951 - Val Loss (simple RMSE, no physics involved): 0.066925\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007167 - Val Loss (simple RMSE, no physics involved): 0.072550\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007097 - Val Loss (simple RMSE, no physics involved): 0.067223\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007345 - Val Loss (simple RMSE, no physics involved): 0.071872\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007019 - Val Loss (simple RMSE, no physics involved): 0.066317\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007057 - Val Loss (simple RMSE, no physics involved): 0.067918\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006997 - Val Loss (simple RMSE, no physics involved): 0.068953\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007079 - Val Loss (simple RMSE, no physics involved): 0.068147\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007009 - Val Loss (simple RMSE, no physics involved): 0.068869\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006985 - Val Loss (simple RMSE, no physics involved): 0.069246\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.070729\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006852 - Val Loss (simple RMSE, no physics involved): 0.066996\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006910 - Val Loss (simple RMSE, no physics involved): 0.069955\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:10:10,519] Trial 2 finished with value: 0.06631694734096527 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 155, 'lr': 0.0004888794137698953, 'weight_decay': 9.284624552988477e-08, 'batch_size': 64, 'lambda_phy': 0.008661939958391028}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006776 - Val Loss (simple RMSE, no physics involved): 0.069463\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.039837 - Val Loss (simple RMSE, no physics involved): 0.111184\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.012742 - Val Loss (simple RMSE, no physics involved): 0.083317\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.008467 - Val Loss (simple RMSE, no physics involved): 0.072550\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007794 - Val Loss (simple RMSE, no physics involved): 0.074391\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007521 - Val Loss (simple RMSE, no physics involved): 0.069855\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007381 - Val Loss (simple RMSE, no physics involved): 0.068457\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007380 - Val Loss (simple RMSE, no physics involved): 0.069035\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007329 - Val Loss (simple RMSE, no physics involved): 0.067690\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007396 - Val Loss (simple RMSE, no physics involved): 0.072532\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007220 - Val Loss (simple RMSE, no physics involved): 0.072120\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007203 - Val Loss (simple RMSE, no physics involved): 0.068151\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007150 - Val Loss (simple RMSE, no physics involved): 0.067880\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007144 - Val Loss (simple RMSE, no physics involved): 0.069722\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007171 - Val Loss (simple RMSE, no physics involved): 0.067106\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007242 - Val Loss (simple RMSE, no physics involved): 0.069936\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007156 - Val Loss (simple RMSE, no physics involved): 0.070222\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007149 - Val Loss (simple RMSE, no physics involved): 0.078040\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007207 - Val Loss (simple RMSE, no physics involved): 0.070553\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007104 - Val Loss (simple RMSE, no physics involved): 0.069238\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007094 - Val Loss (simple RMSE, no physics involved): 0.070029\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007122 - Val Loss (simple RMSE, no physics involved): 0.070095\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007118 - Val Loss (simple RMSE, no physics involved): 0.071976\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007124 - Val Loss (simple RMSE, no physics involved): 0.068597\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007097 - Val Loss (simple RMSE, no physics involved): 0.067102\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007090 - Val Loss (simple RMSE, no physics involved): 0.068609\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007132 - Val Loss (simple RMSE, no physics involved): 0.068216\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007104 - Val Loss (simple RMSE, no physics involved): 0.070253\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007142 - Val Loss (simple RMSE, no physics involved): 0.069946\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007101 - Val Loss (simple RMSE, no physics involved): 0.069053\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007128 - Val Loss (simple RMSE, no physics involved): 0.069534\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007091 - Val Loss (simple RMSE, no physics involved): 0.070846\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007159 - Val Loss (simple RMSE, no physics involved): 0.067728\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007155 - Val Loss (simple RMSE, no physics involved): 0.068256\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007119 - Val Loss (simple RMSE, no physics involved): 0.067086\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007110 - Val Loss (simple RMSE, no physics involved): 0.072763\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007090 - Val Loss (simple RMSE, no physics involved): 0.070817\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007109 - Val Loss (simple RMSE, no physics involved): 0.066845\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007135 - Val Loss (simple RMSE, no physics involved): 0.072439\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007138 - Val Loss (simple RMSE, no physics involved): 0.070082\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007179 - Val Loss (simple RMSE, no physics involved): 0.069364\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007083 - Val Loss (simple RMSE, no physics involved): 0.067148\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007130 - Val Loss (simple RMSE, no physics involved): 0.066949\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007098 - Val Loss (simple RMSE, no physics involved): 0.071073\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007297 - Val Loss (simple RMSE, no physics involved): 0.067282\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007130 - Val Loss (simple RMSE, no physics involved): 0.072658\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007130 - Val Loss (simple RMSE, no physics involved): 0.068773\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.007098 - Val Loss (simple RMSE, no physics involved): 0.068631\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007075 - Val Loss (simple RMSE, no physics involved): 0.066657\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007066 - Val Loss (simple RMSE, no physics involved): 0.068105\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:11:22,467] Trial 3 finished with value: 0.06665714395542939 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 39, 'lr': 0.0009321403522552886, 'weight_decay': 0.0002743682638618613, 'batch_size': 16, 'lambda_phy': 2.0526750174374002e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.007110 - Val Loss (simple RMSE, no physics involved): 0.066866\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.092795 - Val Loss (simple RMSE, no physics involved): 0.224632\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.084221 - Val Loss (simple RMSE, no physics involved): 0.210427\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.075823 - Val Loss (simple RMSE, no physics involved): 0.195668\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.067437 - Val Loss (simple RMSE, no physics involved): 0.180017\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.058754 - Val Loss (simple RMSE, no physics involved): 0.162722\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.049745 - Val Loss (simple RMSE, no physics involved): 0.143800\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.040670 - Val Loss (simple RMSE, no physics involved): 0.124805\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.031710 - Val Loss (simple RMSE, no physics involved): 0.110490\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.025225 - Val Loss (simple RMSE, no physics involved): 0.107452\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.021813 - Val Loss (simple RMSE, no physics involved): 0.111380\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.020580 - Val Loss (simple RMSE, no physics involved): 0.115812\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.020113 - Val Loss (simple RMSE, no physics involved): 0.117101\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.019755 - Val Loss (simple RMSE, no physics involved): 0.116475\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.019382 - Val Loss (simple RMSE, no physics involved): 0.115103\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.018921 - Val Loss (simple RMSE, no physics involved): 0.113770\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.018419 - Val Loss (simple RMSE, no physics involved): 0.112885\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017884 - Val Loss (simple RMSE, no physics involved): 0.110695\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017295 - Val Loss (simple RMSE, no physics involved): 0.108272\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016618 - Val Loss (simple RMSE, no physics involved): 0.106128\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015747 - Val Loss (simple RMSE, no physics involved): 0.102244\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014772 - Val Loss (simple RMSE, no physics involved): 0.100217\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.013884 - Val Loss (simple RMSE, no physics involved): 0.094786\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.012942 - Val Loss (simple RMSE, no physics involved): 0.091272\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.012151 - Val Loss (simple RMSE, no physics involved): 0.085957\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.011490 - Val Loss (simple RMSE, no physics involved): 0.084337\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.010949 - Val Loss (simple RMSE, no physics involved): 0.083184\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.010562 - Val Loss (simple RMSE, no physics involved): 0.080531\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.010305 - Val Loss (simple RMSE, no physics involved): 0.080790\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.010100 - Val Loss (simple RMSE, no physics involved): 0.080341\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.009952 - Val Loss (simple RMSE, no physics involved): 0.077284\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.009849 - Val Loss (simple RMSE, no physics involved): 0.078350\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.009740 - Val Loss (simple RMSE, no physics involved): 0.075515\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.009663 - Val Loss (simple RMSE, no physics involved): 0.076303\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.009582 - Val Loss (simple RMSE, no physics involved): 0.077763\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.009542 - Val Loss (simple RMSE, no physics involved): 0.076435\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.009506 - Val Loss (simple RMSE, no physics involved): 0.076692\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.009466 - Val Loss (simple RMSE, no physics involved): 0.077086\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.009380 - Val Loss (simple RMSE, no physics involved): 0.074203\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.009340 - Val Loss (simple RMSE, no physics involved): 0.075571\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.009314 - Val Loss (simple RMSE, no physics involved): 0.074871\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.009275 - Val Loss (simple RMSE, no physics involved): 0.073046\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.009248 - Val Loss (simple RMSE, no physics involved): 0.073833\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.009212 - Val Loss (simple RMSE, no physics involved): 0.073398\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.009180 - Val Loss (simple RMSE, no physics involved): 0.072399\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.009158 - Val Loss (simple RMSE, no physics involved): 0.073219\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.009134 - Val Loss (simple RMSE, no physics involved): 0.073322\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.009114 - Val Loss (simple RMSE, no physics involved): 0.072178\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.009097 - Val Loss (simple RMSE, no physics involved): 0.073382\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.009070 - Val Loss (simple RMSE, no physics involved): 0.071223\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:13:13,758] Trial 4 finished with value: 0.07122332789003849 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 71, 'lr': 2.7209433202332614e-05, 'weight_decay': 2.9665279001332357e-08, 'batch_size': 16, 'lambda_phy': 0.0761186980967188}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.009056 - Val Loss (simple RMSE, no physics involved): 0.072367\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.029272 - Val Loss (simple RMSE, no physics involved): 0.088666\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.010882 - Val Loss (simple RMSE, no physics involved): 0.076047\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.008353 - Val Loss (simple RMSE, no physics involved): 0.077487\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007831 - Val Loss (simple RMSE, no physics involved): 0.073883\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007491 - Val Loss (simple RMSE, no physics involved): 0.071469\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007360 - Val Loss (simple RMSE, no physics involved): 0.073377\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007220 - Val Loss (simple RMSE, no physics involved): 0.068954\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007185 - Val Loss (simple RMSE, no physics involved): 0.070069\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007137 - Val Loss (simple RMSE, no physics involved): 0.069271\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007149 - Val Loss (simple RMSE, no physics involved): 0.068828\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007034 - Val Loss (simple RMSE, no physics involved): 0.068445\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007064 - Val Loss (simple RMSE, no physics involved): 0.069275\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007097 - Val Loss (simple RMSE, no physics involved): 0.067184\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007075 - Val Loss (simple RMSE, no physics involved): 0.068018\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007076 - Val Loss (simple RMSE, no physics involved): 0.075876\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007074 - Val Loss (simple RMSE, no physics involved): 0.069040\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006998 - Val Loss (simple RMSE, no physics involved): 0.067143\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.067270\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006943 - Val Loss (simple RMSE, no physics involved): 0.067995\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.067081\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007039 - Val Loss (simple RMSE, no physics involved): 0.068479\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006927 - Val Loss (simple RMSE, no physics involved): 0.066917\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.066670\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006950 - Val Loss (simple RMSE, no physics involved): 0.069925\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006890 - Val Loss (simple RMSE, no physics involved): 0.070076\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006883 - Val Loss (simple RMSE, no physics involved): 0.066311\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006933 - Val Loss (simple RMSE, no physics involved): 0.069062\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.069248\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006957 - Val Loss (simple RMSE, no physics involved): 0.066794\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.069747\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.068642\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006797 - Val Loss (simple RMSE, no physics involved): 0.068845\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006803 - Val Loss (simple RMSE, no physics involved): 0.070254\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006890 - Val Loss (simple RMSE, no physics involved): 0.068587\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006972 - Val Loss (simple RMSE, no physics involved): 0.075687\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006994 - Val Loss (simple RMSE, no physics involved): 0.066941\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006841 - Val Loss (simple RMSE, no physics involved): 0.071016\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006884 - Val Loss (simple RMSE, no physics involved): 0.070287\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006799 - Val Loss (simple RMSE, no physics involved): 0.066978\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006806 - Val Loss (simple RMSE, no physics involved): 0.068435\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006753 - Val Loss (simple RMSE, no physics involved): 0.071553\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006830 - Val Loss (simple RMSE, no physics involved): 0.067169\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006879 - Val Loss (simple RMSE, no physics involved): 0.074697\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006883 - Val Loss (simple RMSE, no physics involved): 0.066781\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006777 - Val Loss (simple RMSE, no physics involved): 0.069452\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006756 - Val Loss (simple RMSE, no physics involved): 0.070635\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006759 - Val Loss (simple RMSE, no physics involved): 0.066999\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006787 - Val Loss (simple RMSE, no physics involved): 0.067035\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006741 - Val Loss (simple RMSE, no physics involved): 0.068490\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:14:37,019] Trial 5 finished with value: 0.06631120170156161 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 169, 'lr': 0.0008210547457074133, 'weight_decay': 9.18153319933937e-07, 'batch_size': 32, 'lambda_phy': 0.004984448953423662}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006728 - Val Loss (simple RMSE, no physics involved): 0.068138\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.025932 - Val Loss (simple RMSE, no physics involved): 0.125265\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.009868 - Val Loss (simple RMSE, no physics involved): 0.074196\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.008341 - Val Loss (simple RMSE, no physics involved): 0.080922\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007919 - Val Loss (simple RMSE, no physics involved): 0.073705\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007531 - Val Loss (simple RMSE, no physics involved): 0.072090\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007338 - Val Loss (simple RMSE, no physics involved): 0.072029\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007431 - Val Loss (simple RMSE, no physics involved): 0.073390\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007221 - Val Loss (simple RMSE, no physics involved): 0.072257\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007525 - Val Loss (simple RMSE, no physics involved): 0.071056\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007399 - Val Loss (simple RMSE, no physics involved): 0.071355\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007149 - Val Loss (simple RMSE, no physics involved): 0.068074\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007421 - Val Loss (simple RMSE, no physics involved): 0.076322\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007472 - Val Loss (simple RMSE, no physics involved): 0.081318\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007695 - Val Loss (simple RMSE, no physics involved): 0.078777\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007709 - Val Loss (simple RMSE, no physics involved): 0.076121\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007409 - Val Loss (simple RMSE, no physics involved): 0.078196\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007460 - Val Loss (simple RMSE, no physics involved): 0.073322\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007209 - Val Loss (simple RMSE, no physics involved): 0.068632\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007148 - Val Loss (simple RMSE, no physics involved): 0.070632\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007171 - Val Loss (simple RMSE, no physics involved): 0.079234\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007421 - Val Loss (simple RMSE, no physics involved): 0.073365\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007100 - Val Loss (simple RMSE, no physics involved): 0.068704\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007170 - Val Loss (simple RMSE, no physics involved): 0.067952\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007379 - Val Loss (simple RMSE, no physics involved): 0.068048\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007631 - Val Loss (simple RMSE, no physics involved): 0.068375\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007149 - Val Loss (simple RMSE, no physics involved): 0.073430\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007081 - Val Loss (simple RMSE, no physics involved): 0.069550\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007209 - Val Loss (simple RMSE, no physics involved): 0.069403\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007097 - Val Loss (simple RMSE, no physics involved): 0.069274\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007160 - Val Loss (simple RMSE, no physics involved): 0.071396\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007039 - Val Loss (simple RMSE, no physics involved): 0.067905\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007366 - Val Loss (simple RMSE, no physics involved): 0.075760\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007264 - Val Loss (simple RMSE, no physics involved): 0.070363\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007142 - Val Loss (simple RMSE, no physics involved): 0.069890\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007118 - Val Loss (simple RMSE, no physics involved): 0.070663\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007109 - Val Loss (simple RMSE, no physics involved): 0.069230\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007055 - Val Loss (simple RMSE, no physics involved): 0.072814\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007206 - Val Loss (simple RMSE, no physics involved): 0.071788\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007098 - Val Loss (simple RMSE, no physics involved): 0.070798\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007097 - Val Loss (simple RMSE, no physics involved): 0.067783\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007052 - Val Loss (simple RMSE, no physics involved): 0.068356\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007557 - Val Loss (simple RMSE, no physics involved): 0.073782\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007203 - Val Loss (simple RMSE, no physics involved): 0.071551\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007329 - Val Loss (simple RMSE, no physics involved): 0.075670\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007232 - Val Loss (simple RMSE, no physics involved): 0.070717\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007520 - Val Loss (simple RMSE, no physics involved): 0.071599\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.007220 - Val Loss (simple RMSE, no physics involved): 0.071061\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007223 - Val Loss (simple RMSE, no physics involved): 0.067986\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007312 - Val Loss (simple RMSE, no physics involved): 0.067803\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:15:31,184] Trial 6 finished with value: 0.06778349354863167 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 121, 'lr': 0.006845107889957535, 'weight_decay': 0.0007393651179388028, 'batch_size': 64, 'lambda_phy': 2.2088780901933665e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.007155 - Val Loss (simple RMSE, no physics involved): 0.068792\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.024643 - Val Loss (simple RMSE, no physics involved): 0.116947\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.016334 - Val Loss (simple RMSE, no physics involved): 0.098126\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.012947 - Val Loss (simple RMSE, no physics involved): 0.086719\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.010352 - Val Loss (simple RMSE, no physics involved): 0.076969\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.008833 - Val Loss (simple RMSE, no physics involved): 0.074130\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.008127 - Val Loss (simple RMSE, no physics involved): 0.071529\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007833 - Val Loss (simple RMSE, no physics involved): 0.072409\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007685 - Val Loss (simple RMSE, no physics involved): 0.069835\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007607 - Val Loss (simple RMSE, no physics involved): 0.069432\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007569 - Val Loss (simple RMSE, no physics involved): 0.068707\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007476 - Val Loss (simple RMSE, no physics involved): 0.068642\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007439 - Val Loss (simple RMSE, no physics involved): 0.069803\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007417 - Val Loss (simple RMSE, no physics involved): 0.068697\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007389 - Val Loss (simple RMSE, no physics involved): 0.068426\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007362 - Val Loss (simple RMSE, no physics involved): 0.069242\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007335 - Val Loss (simple RMSE, no physics involved): 0.070518\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007312 - Val Loss (simple RMSE, no physics involved): 0.068037\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007297 - Val Loss (simple RMSE, no physics involved): 0.070023\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007276 - Val Loss (simple RMSE, no physics involved): 0.067057\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007273 - Val Loss (simple RMSE, no physics involved): 0.069405\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007256 - Val Loss (simple RMSE, no physics involved): 0.068549\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007231 - Val Loss (simple RMSE, no physics involved): 0.068323\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007229 - Val Loss (simple RMSE, no physics involved): 0.067853\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007249 - Val Loss (simple RMSE, no physics involved): 0.070101\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007203 - Val Loss (simple RMSE, no physics involved): 0.067592\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007218 - Val Loss (simple RMSE, no physics involved): 0.069431\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007194 - Val Loss (simple RMSE, no physics involved): 0.067127\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007177 - Val Loss (simple RMSE, no physics involved): 0.071027\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007188 - Val Loss (simple RMSE, no physics involved): 0.067392\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007175 - Val Loss (simple RMSE, no physics involved): 0.067678\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007153 - Val Loss (simple RMSE, no physics involved): 0.068413\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007149 - Val Loss (simple RMSE, no physics involved): 0.067387\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007161 - Val Loss (simple RMSE, no physics involved): 0.071647\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007142 - Val Loss (simple RMSE, no physics involved): 0.069189\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007131 - Val Loss (simple RMSE, no physics involved): 0.068114\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007157 - Val Loss (simple RMSE, no physics involved): 0.066867\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007104 - Val Loss (simple RMSE, no physics involved): 0.067947\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007106 - Val Loss (simple RMSE, no physics involved): 0.069494\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007109 - Val Loss (simple RMSE, no physics involved): 0.066976\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007094 - Val Loss (simple RMSE, no physics involved): 0.070132\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007094 - Val Loss (simple RMSE, no physics involved): 0.067102\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007090 - Val Loss (simple RMSE, no physics involved): 0.067533\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007071 - Val Loss (simple RMSE, no physics involved): 0.067770\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007104 - Val Loss (simple RMSE, no physics involved): 0.067391\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007069 - Val Loss (simple RMSE, no physics involved): 0.067137\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007059 - Val Loss (simple RMSE, no physics involved): 0.066165\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.007062 - Val Loss (simple RMSE, no physics involved): 0.066555\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007050 - Val Loss (simple RMSE, no physics involved): 0.067479\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007045 - Val Loss (simple RMSE, no physics involved): 0.067972\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:19:15,838] Trial 7 finished with value: 0.06616523768752813 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 210, 'lr': 3.162185934312986e-05, 'weight_decay': 1.1922255592006932e-05, 'batch_size': 8, 'lambda_phy': 0.012528424462493193}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.067862\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.025474 - Val Loss (simple RMSE, no physics involved): 0.092089\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.009276 - Val Loss (simple RMSE, no physics involved): 0.082399\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007889 - Val Loss (simple RMSE, no physics involved): 0.072957\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007515 - Val Loss (simple RMSE, no physics involved): 0.073209\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007541 - Val Loss (simple RMSE, no physics involved): 0.069631\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007326 - Val Loss (simple RMSE, no physics involved): 0.069737\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007283 - Val Loss (simple RMSE, no physics involved): 0.069890\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007367 - Val Loss (simple RMSE, no physics involved): 0.067987\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007357 - Val Loss (simple RMSE, no physics involved): 0.069293\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007525 - Val Loss (simple RMSE, no physics involved): 0.067925\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007292 - Val Loss (simple RMSE, no physics involved): 0.068750\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007383 - Val Loss (simple RMSE, no physics involved): 0.068018\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007284 - Val Loss (simple RMSE, no physics involved): 0.071595\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007499 - Val Loss (simple RMSE, no physics involved): 0.067994\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007389 - Val Loss (simple RMSE, no physics involved): 0.076354\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.008007 - Val Loss (simple RMSE, no physics involved): 0.070281\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007618 - Val Loss (simple RMSE, no physics involved): 0.072209\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007306 - Val Loss (simple RMSE, no physics involved): 0.077612\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007457 - Val Loss (simple RMSE, no physics involved): 0.068642\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007328 - Val Loss (simple RMSE, no physics involved): 0.077039\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007321 - Val Loss (simple RMSE, no physics involved): 0.072680\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007308 - Val Loss (simple RMSE, no physics involved): 0.067983\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007441 - Val Loss (simple RMSE, no physics involved): 0.071855\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007467 - Val Loss (simple RMSE, no physics involved): 0.077061\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007493 - Val Loss (simple RMSE, no physics involved): 0.068570\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007391 - Val Loss (simple RMSE, no physics involved): 0.075261\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007351 - Val Loss (simple RMSE, no physics involved): 0.072790\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007288 - Val Loss (simple RMSE, no physics involved): 0.072756\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007317 - Val Loss (simple RMSE, no physics involved): 0.073746\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007241 - Val Loss (simple RMSE, no physics involved): 0.071185\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007729 - Val Loss (simple RMSE, no physics involved): 0.067886\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007626 - Val Loss (simple RMSE, no physics involved): 0.073138\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007262 - Val Loss (simple RMSE, no physics involved): 0.069167\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007300 - Val Loss (simple RMSE, no physics involved): 0.069697\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007347 - Val Loss (simple RMSE, no physics involved): 0.068984\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007226 - Val Loss (simple RMSE, no physics involved): 0.067906\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007345 - Val Loss (simple RMSE, no physics involved): 0.072454\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007508 - Val Loss (simple RMSE, no physics involved): 0.068790\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007429 - Val Loss (simple RMSE, no physics involved): 0.068410\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007450 - Val Loss (simple RMSE, no physics involved): 0.069072\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007275 - Val Loss (simple RMSE, no physics involved): 0.069560\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007310 - Val Loss (simple RMSE, no physics involved): 0.072715\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007287 - Val Loss (simple RMSE, no physics involved): 0.070987\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007337 - Val Loss (simple RMSE, no physics involved): 0.067755\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007308 - Val Loss (simple RMSE, no physics involved): 0.069020\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007306 - Val Loss (simple RMSE, no physics involved): 0.069406\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.007291 - Val Loss (simple RMSE, no physics involved): 0.068099\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007241 - Val Loss (simple RMSE, no physics involved): 0.072662\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007406 - Val Loss (simple RMSE, no physics involved): 0.073410\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:20:39,309] Trial 8 finished with value: 0.06775500004490216 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 190, 'lr': 0.002923372059156733, 'weight_decay': 0.0006864909375926936, 'batch_size': 32, 'lambda_phy': 0.0037338194854727247}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.007403 - Val Loss (simple RMSE, no physics involved): 0.068360\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.084278 - Val Loss (simple RMSE, no physics involved): 0.216600\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.074405 - Val Loss (simple RMSE, no physics involved): 0.200009\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.066148 - Val Loss (simple RMSE, no physics involved): 0.184454\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.058580 - Val Loss (simple RMSE, no physics involved): 0.169689\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.050626 - Val Loss (simple RMSE, no physics involved): 0.155778\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.044005 - Val Loss (simple RMSE, no physics involved): 0.142922\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.038998 - Val Loss (simple RMSE, no physics involved): 0.131442\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.033393 - Val Loss (simple RMSE, no physics involved): 0.121891\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.029647 - Val Loss (simple RMSE, no physics involved): 0.115556\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.026299 - Val Loss (simple RMSE, no physics involved): 0.111747\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.023531 - Val Loss (simple RMSE, no physics involved): 0.110176\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.021941 - Val Loss (simple RMSE, no physics involved): 0.109857\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.020374 - Val Loss (simple RMSE, no physics involved): 0.110347\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.019635 - Val Loss (simple RMSE, no physics involved): 0.110768\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.018641 - Val Loss (simple RMSE, no physics involved): 0.110940\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.018079 - Val Loss (simple RMSE, no physics involved): 0.110618\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017740 - Val Loss (simple RMSE, no physics involved): 0.109633\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016944 - Val Loss (simple RMSE, no physics involved): 0.108894\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016583 - Val Loss (simple RMSE, no physics involved): 0.107358\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016023 - Val Loss (simple RMSE, no physics involved): 0.105930\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015525 - Val Loss (simple RMSE, no physics involved): 0.104137\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015111 - Val Loss (simple RMSE, no physics involved): 0.102147\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014587 - Val Loss (simple RMSE, no physics involved): 0.100729\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014080 - Val Loss (simple RMSE, no physics involved): 0.099045\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013760 - Val Loss (simple RMSE, no physics involved): 0.097402\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013276 - Val Loss (simple RMSE, no physics involved): 0.096302\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.012782 - Val Loss (simple RMSE, no physics involved): 0.094304\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.012443 - Val Loss (simple RMSE, no physics involved): 0.093131\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.012052 - Val Loss (simple RMSE, no physics involved): 0.092050\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.011688 - Val Loss (simple RMSE, no physics involved): 0.090967\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.011393 - Val Loss (simple RMSE, no physics involved): 0.089198\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.011092 - Val Loss (simple RMSE, no physics involved): 0.088340\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.010827 - Val Loss (simple RMSE, no physics involved): 0.087938\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.010461 - Val Loss (simple RMSE, no physics involved): 0.086293\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.010247 - Val Loss (simple RMSE, no physics involved): 0.085368\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.010050 - Val Loss (simple RMSE, no physics involved): 0.084672\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.009864 - Val Loss (simple RMSE, no physics involved): 0.083484\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.009618 - Val Loss (simple RMSE, no physics involved): 0.083442\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.009439 - Val Loss (simple RMSE, no physics involved): 0.082538\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.009229 - Val Loss (simple RMSE, no physics involved): 0.082081\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.009199 - Val Loss (simple RMSE, no physics involved): 0.081057\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.008979 - Val Loss (simple RMSE, no physics involved): 0.080801\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.008867 - Val Loss (simple RMSE, no physics involved): 0.080339\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.008695 - Val Loss (simple RMSE, no physics involved): 0.080113\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.008640 - Val Loss (simple RMSE, no physics involved): 0.079024\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.008543 - Val Loss (simple RMSE, no physics involved): 0.079563\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.008530 - Val Loss (simple RMSE, no physics involved): 0.078695\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.008498 - Val Loss (simple RMSE, no physics involved): 0.077831\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.008344 - Val Loss (simple RMSE, no physics involved): 0.079297\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:22:02,284] Trial 9 finished with value: 0.07744763046503067 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 194, 'lr': 1.1544252556541883e-05, 'weight_decay': 2.757660164737376e-06, 'batch_size': 32, 'lambda_phy': 2.8692013711912484e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.008313 - Val Loss (simple RMSE, no physics involved): 0.077448\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.028841 - Val Loss (simple RMSE, no physics involved): 0.113045\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.012406 - Val Loss (simple RMSE, no physics involved): 0.084183\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.008008 - Val Loss (simple RMSE, no physics involved): 0.073149\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007514 - Val Loss (simple RMSE, no physics involved): 0.069703\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007297 - Val Loss (simple RMSE, no physics involved): 0.071801\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007150 - Val Loss (simple RMSE, no physics involved): 0.071900\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007097 - Val Loss (simple RMSE, no physics involved): 0.068188\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007126 - Val Loss (simple RMSE, no physics involved): 0.070507\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007049 - Val Loss (simple RMSE, no physics involved): 0.066325\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007082 - Val Loss (simple RMSE, no physics involved): 0.066980\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007024 - Val Loss (simple RMSE, no physics involved): 0.069907\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006984 - Val Loss (simple RMSE, no physics involved): 0.066596\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006988 - Val Loss (simple RMSE, no physics involved): 0.066538\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007038 - Val Loss (simple RMSE, no physics involved): 0.073297\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006954 - Val Loss (simple RMSE, no physics involved): 0.066547\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006947 - Val Loss (simple RMSE, no physics involved): 0.068557\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.069222\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006913 - Val Loss (simple RMSE, no physics involved): 0.066770\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006948 - Val Loss (simple RMSE, no physics involved): 0.067900\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006945 - Val Loss (simple RMSE, no physics involved): 0.069014\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006920 - Val Loss (simple RMSE, no physics involved): 0.068258\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006948 - Val Loss (simple RMSE, no physics involved): 0.069873\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007005 - Val Loss (simple RMSE, no physics involved): 0.065620\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006891 - Val Loss (simple RMSE, no physics involved): 0.068630\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006904 - Val Loss (simple RMSE, no physics involved): 0.068341\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006899 - Val Loss (simple RMSE, no physics involved): 0.066965\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006929 - Val Loss (simple RMSE, no physics involved): 0.068349\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006924 - Val Loss (simple RMSE, no physics involved): 0.066293\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006901 - Val Loss (simple RMSE, no physics involved): 0.066277\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.067268\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006890 - Val Loss (simple RMSE, no physics involved): 0.068866\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006922 - Val Loss (simple RMSE, no physics involved): 0.071369\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006927 - Val Loss (simple RMSE, no physics involved): 0.067363\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006925 - Val Loss (simple RMSE, no physics involved): 0.069967\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.065520\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006869 - Val Loss (simple RMSE, no physics involved): 0.067636\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006952 - Val Loss (simple RMSE, no physics involved): 0.068491\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006844 - Val Loss (simple RMSE, no physics involved): 0.072866\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.067918\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006840 - Val Loss (simple RMSE, no physics involved): 0.071506\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006922 - Val Loss (simple RMSE, no physics involved): 0.067373\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006989 - Val Loss (simple RMSE, no physics involved): 0.068488\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006906 - Val Loss (simple RMSE, no physics involved): 0.069644\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006796 - Val Loss (simple RMSE, no physics involved): 0.068214\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006909 - Val Loss (simple RMSE, no physics involved): 0.075555\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006845 - Val Loss (simple RMSE, no physics involved): 0.067342\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006791 - Val Loss (simple RMSE, no physics involved): 0.069989\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006818 - Val Loss (simple RMSE, no physics involved): 0.067657\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006808 - Val Loss (simple RMSE, no physics involved): 0.065719\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:29:58,434] Trial 10 finished with value: 0.06551977961013715 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 250, 'lr': 0.00010015888580030069, 'weight_decay': 4.6054023052220814e-05, 'batch_size': 8, 'lambda_phy': 0.0002952051120945112}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006844 - Val Loss (simple RMSE, no physics involved): 0.066732\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.031784 - Val Loss (simple RMSE, no physics involved): 0.112218\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.012781 - Val Loss (simple RMSE, no physics involved): 0.073245\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007729 - Val Loss (simple RMSE, no physics involved): 0.075385\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007446 - Val Loss (simple RMSE, no physics involved): 0.072128\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007304 - Val Loss (simple RMSE, no physics involved): 0.068174\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007254 - Val Loss (simple RMSE, no physics involved): 0.068483\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007146 - Val Loss (simple RMSE, no physics involved): 0.073114\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007151 - Val Loss (simple RMSE, no physics involved): 0.071111\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007051 - Val Loss (simple RMSE, no physics involved): 0.069994\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007053 - Val Loss (simple RMSE, no physics involved): 0.071624\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007083 - Val Loss (simple RMSE, no physics involved): 0.068960\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007040 - Val Loss (simple RMSE, no physics involved): 0.067817\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007154 - Val Loss (simple RMSE, no physics involved): 0.067022\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007015 - Val Loss (simple RMSE, no physics involved): 0.069563\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007045 - Val Loss (simple RMSE, no physics involved): 0.069102\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007029 - Val Loss (simple RMSE, no physics involved): 0.073608\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006986 - Val Loss (simple RMSE, no physics involved): 0.067217\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006972 - Val Loss (simple RMSE, no physics involved): 0.068618\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006985 - Val Loss (simple RMSE, no physics involved): 0.066412\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006985 - Val Loss (simple RMSE, no physics involved): 0.069971\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006941 - Val Loss (simple RMSE, no physics involved): 0.067162\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006988 - Val Loss (simple RMSE, no physics involved): 0.067730\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007024 - Val Loss (simple RMSE, no physics involved): 0.065746\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006985 - Val Loss (simple RMSE, no physics involved): 0.072466\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006930 - Val Loss (simple RMSE, no physics involved): 0.066805\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.073430\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006925 - Val Loss (simple RMSE, no physics involved): 0.068636\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006943 - Val Loss (simple RMSE, no physics involved): 0.067249\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006897 - Val Loss (simple RMSE, no physics involved): 0.072593\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006919 - Val Loss (simple RMSE, no physics involved): 0.068811\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006919 - Val Loss (simple RMSE, no physics involved): 0.066621\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006945 - Val Loss (simple RMSE, no physics involved): 0.065361\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006977 - Val Loss (simple RMSE, no physics involved): 0.069807\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006972 - Val Loss (simple RMSE, no physics involved): 0.066221\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.069306\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006860 - Val Loss (simple RMSE, no physics involved): 0.070256\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006951 - Val Loss (simple RMSE, no physics involved): 0.069723\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006943 - Val Loss (simple RMSE, no physics involved): 0.075210\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006906 - Val Loss (simple RMSE, no physics involved): 0.065755\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006852 - Val Loss (simple RMSE, no physics involved): 0.066305\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006907 - Val Loss (simple RMSE, no physics involved): 0.073355\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006990 - Val Loss (simple RMSE, no physics involved): 0.065954\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007049 - Val Loss (simple RMSE, no physics involved): 0.066702\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006825 - Val Loss (simple RMSE, no physics involved): 0.069564\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.073449\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006930 - Val Loss (simple RMSE, no physics involved): 0.069312\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006845 - Val Loss (simple RMSE, no physics involved): 0.068944\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006898 - Val Loss (simple RMSE, no physics involved): 0.066571\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006844 - Val Loss (simple RMSE, no physics involved): 0.068104\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:37:59,606] Trial 11 finished with value: 0.06536118406802416 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 250, 'lr': 0.00010034950189600522, 'weight_decay': 5.315650735638783e-05, 'batch_size': 8, 'lambda_phy': 0.00025823853722502785}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006838 - Val Loss (simple RMSE, no physics involved): 0.065364\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.029542 - Val Loss (simple RMSE, no physics involved): 0.094463\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.008876 - Val Loss (simple RMSE, no physics involved): 0.073976\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007420 - Val Loss (simple RMSE, no physics involved): 0.070593\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007229 - Val Loss (simple RMSE, no physics involved): 0.066853\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007170 - Val Loss (simple RMSE, no physics involved): 0.066805\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007250 - Val Loss (simple RMSE, no physics involved): 0.067022\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007079 - Val Loss (simple RMSE, no physics involved): 0.068085\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007037 - Val Loss (simple RMSE, no physics involved): 0.069873\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007029 - Val Loss (simple RMSE, no physics involved): 0.067998\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006978 - Val Loss (simple RMSE, no physics involved): 0.067087\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007009 - Val Loss (simple RMSE, no physics involved): 0.067606\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007031 - Val Loss (simple RMSE, no physics involved): 0.071069\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006980 - Val Loss (simple RMSE, no physics involved): 0.066049\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006949 - Val Loss (simple RMSE, no physics involved): 0.066546\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006919 - Val Loss (simple RMSE, no physics involved): 0.066235\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006988 - Val Loss (simple RMSE, no physics involved): 0.069182\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007000 - Val Loss (simple RMSE, no physics involved): 0.068984\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006897 - Val Loss (simple RMSE, no physics involved): 0.065929\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006944 - Val Loss (simple RMSE, no physics involved): 0.067803\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006976 - Val Loss (simple RMSE, no physics involved): 0.070634\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006953 - Val Loss (simple RMSE, no physics involved): 0.066263\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006966 - Val Loss (simple RMSE, no physics involved): 0.066760\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007026 - Val Loss (simple RMSE, no physics involved): 0.066225\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006918 - Val Loss (simple RMSE, no physics involved): 0.066031\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.065964\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.066679\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006910 - Val Loss (simple RMSE, no physics involved): 0.065587\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007022 - Val Loss (simple RMSE, no physics involved): 0.068068\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006846 - Val Loss (simple RMSE, no physics involved): 0.066303\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.074650\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006949 - Val Loss (simple RMSE, no physics involved): 0.065684\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006843 - Val Loss (simple RMSE, no physics involved): 0.066875\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006909 - Val Loss (simple RMSE, no physics involved): 0.065801\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.070673\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006895 - Val Loss (simple RMSE, no physics involved): 0.067642\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006844 - Val Loss (simple RMSE, no physics involved): 0.065908\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006832 - Val Loss (simple RMSE, no physics involved): 0.070263\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006815 - Val Loss (simple RMSE, no physics involved): 0.066019\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006905 - Val Loss (simple RMSE, no physics involved): 0.068660\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006835 - Val Loss (simple RMSE, no physics involved): 0.066373\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006847 - Val Loss (simple RMSE, no physics involved): 0.068901\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006862 - Val Loss (simple RMSE, no physics involved): 0.068806\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006852 - Val Loss (simple RMSE, no physics involved): 0.066705\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006846 - Val Loss (simple RMSE, no physics involved): 0.066625\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006803 - Val Loss (simple RMSE, no physics involved): 0.067838\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006893 - Val Loss (simple RMSE, no physics involved): 0.066064\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006842 - Val Loss (simple RMSE, no physics involved): 0.069688\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006867 - Val Loss (simple RMSE, no physics involved): 0.067243\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006923 - Val Loss (simple RMSE, no physics involved): 0.068957\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:45:27,293] Trial 12 finished with value: 0.065586866500477 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 251, 'lr': 0.00013146369461565033, 'weight_decay': 7.161800392112195e-05, 'batch_size': 8, 'lambda_phy': 0.00016520599912371492}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006890 - Val Loss (simple RMSE, no physics involved): 0.068707\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.033537 - Val Loss (simple RMSE, no physics involved): 0.111991\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.010191 - Val Loss (simple RMSE, no physics involved): 0.073408\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007519 - Val Loss (simple RMSE, no physics involved): 0.070731\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007310 - Val Loss (simple RMSE, no physics involved): 0.068547\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007165 - Val Loss (simple RMSE, no physics involved): 0.071444\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007174 - Val Loss (simple RMSE, no physics involved): 0.070629\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007122 - Val Loss (simple RMSE, no physics involved): 0.070891\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007080 - Val Loss (simple RMSE, no physics involved): 0.068704\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007014 - Val Loss (simple RMSE, no physics involved): 0.070744\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007009 - Val Loss (simple RMSE, no physics involved): 0.066716\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007154 - Val Loss (simple RMSE, no physics involved): 0.071740\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007025 - Val Loss (simple RMSE, no physics involved): 0.066328\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006952 - Val Loss (simple RMSE, no physics involved): 0.071733\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.066279\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006944 - Val Loss (simple RMSE, no physics involved): 0.069745\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006975 - Val Loss (simple RMSE, no physics involved): 0.065788\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006902 - Val Loss (simple RMSE, no physics involved): 0.073789\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006958 - Val Loss (simple RMSE, no physics involved): 0.071598\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007037 - Val Loss (simple RMSE, no physics involved): 0.066539\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006918 - Val Loss (simple RMSE, no physics involved): 0.065940\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006852 - Val Loss (simple RMSE, no physics involved): 0.067259\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006820 - Val Loss (simple RMSE, no physics involved): 0.068053\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006835 - Val Loss (simple RMSE, no physics involved): 0.068736\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006843 - Val Loss (simple RMSE, no physics involved): 0.067265\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006849 - Val Loss (simple RMSE, no physics involved): 0.069846\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.071349\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006807 - Val Loss (simple RMSE, no physics involved): 0.067832\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007057 - Val Loss (simple RMSE, no physics involved): 0.067159\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006828 - Val Loss (simple RMSE, no physics involved): 0.067457\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.072705\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006766 - Val Loss (simple RMSE, no physics involved): 0.068265\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006886 - Val Loss (simple RMSE, no physics involved): 0.065682\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006745 - Val Loss (simple RMSE, no physics involved): 0.069641\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006763 - Val Loss (simple RMSE, no physics involved): 0.067386\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006704 - Val Loss (simple RMSE, no physics involved): 0.068326\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006786 - Val Loss (simple RMSE, no physics involved): 0.068448\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006719 - Val Loss (simple RMSE, no physics involved): 0.072575\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006860 - Val Loss (simple RMSE, no physics involved): 0.066909\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006692 - Val Loss (simple RMSE, no physics involved): 0.067137\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006722 - Val Loss (simple RMSE, no physics involved): 0.073286\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006705 - Val Loss (simple RMSE, no physics involved): 0.066958\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006684 - Val Loss (simple RMSE, no physics involved): 0.069408\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006711 - Val Loss (simple RMSE, no physics involved): 0.069199\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006719 - Val Loss (simple RMSE, no physics involved): 0.068969\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006686 - Val Loss (simple RMSE, no physics involved): 0.069108\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006692 - Val Loss (simple RMSE, no physics involved): 0.068602\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006700 - Val Loss (simple RMSE, no physics involved): 0.066862\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006728 - Val Loss (simple RMSE, no physics involved): 0.067578\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006641 - Val Loss (simple RMSE, no physics involved): 0.067788\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:51:12,982] Trial 13 finished with value: 0.06568188002953927 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 223, 'lr': 0.0001505557300394664, 'weight_decay': 3.847965565732921e-06, 'batch_size': 8, 'lambda_phy': 0.0001181126639731938}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006708 - Val Loss (simple RMSE, no physics involved): 0.074129\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.018945 - Val Loss (simple RMSE, no physics involved): 0.070546\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007662 - Val Loss (simple RMSE, no physics involved): 0.070415\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007390 - Val Loss (simple RMSE, no physics involved): 0.070052\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007260 - Val Loss (simple RMSE, no physics involved): 0.071118\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007361 - Val Loss (simple RMSE, no physics involved): 0.071991\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007248 - Val Loss (simple RMSE, no physics involved): 0.070650\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007277 - Val Loss (simple RMSE, no physics involved): 0.066216\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007239 - Val Loss (simple RMSE, no physics involved): 0.066338\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007250 - Val Loss (simple RMSE, no physics involved): 0.066454\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007549 - Val Loss (simple RMSE, no physics involved): 0.067954\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007170 - Val Loss (simple RMSE, no physics involved): 0.075432\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007224 - Val Loss (simple RMSE, no physics involved): 0.067648\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007274 - Val Loss (simple RMSE, no physics involved): 0.071148\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007218 - Val Loss (simple RMSE, no physics involved): 0.084223\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007414 - Val Loss (simple RMSE, no physics involved): 0.072018\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007171 - Val Loss (simple RMSE, no physics involved): 0.071984\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007397 - Val Loss (simple RMSE, no physics involved): 0.067489\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007438 - Val Loss (simple RMSE, no physics involved): 0.067220\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007236 - Val Loss (simple RMSE, no physics involved): 0.065957\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007249 - Val Loss (simple RMSE, no physics involved): 0.070937\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007152 - Val Loss (simple RMSE, no physics involved): 0.065836\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007210 - Val Loss (simple RMSE, no physics involved): 0.070919\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007212 - Val Loss (simple RMSE, no physics involved): 0.066450\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007221 - Val Loss (simple RMSE, no physics involved): 0.067389\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007083 - Val Loss (simple RMSE, no physics involved): 0.071172\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007116 - Val Loss (simple RMSE, no physics involved): 0.066274\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007200 - Val Loss (simple RMSE, no physics involved): 0.068199\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007177 - Val Loss (simple RMSE, no physics involved): 0.074659\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007159 - Val Loss (simple RMSE, no physics involved): 0.066041\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007272 - Val Loss (simple RMSE, no physics involved): 0.067578\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007201 - Val Loss (simple RMSE, no physics involved): 0.065910\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007318 - Val Loss (simple RMSE, no physics involved): 0.065832\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007151 - Val Loss (simple RMSE, no physics involved): 0.066841\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007288 - Val Loss (simple RMSE, no physics involved): 0.065611\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007182 - Val Loss (simple RMSE, no physics involved): 0.066962\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007048 - Val Loss (simple RMSE, no physics involved): 0.071993\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007142 - Val Loss (simple RMSE, no physics involved): 0.077577\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007228 - Val Loss (simple RMSE, no physics involved): 0.074746\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007177 - Val Loss (simple RMSE, no physics involved): 0.067102\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007145 - Val Loss (simple RMSE, no physics involved): 0.071180\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007098 - Val Loss (simple RMSE, no physics involved): 0.067284\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007261 - Val Loss (simple RMSE, no physics involved): 0.067641\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007088 - Val Loss (simple RMSE, no physics involved): 0.068072\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007126 - Val Loss (simple RMSE, no physics involved): 0.065495\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007138 - Val Loss (simple RMSE, no physics involved): 0.065481\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007064 - Val Loss (simple RMSE, no physics involved): 0.071671\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.007102 - Val Loss (simple RMSE, no physics involved): 0.068257\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007123 - Val Loss (simple RMSE, no physics involved): 0.065374\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007319 - Val Loss (simple RMSE, no physics involved): 0.067512\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 13:55:40,668] Trial 14 finished with value: 0.06537400620679061 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 116, 'lr': 0.0018211775519268868, 'weight_decay': 0.00010929876678106549, 'batch_size': 8, 'lambda_phy': 0.0006909822519165009}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.007091 - Val Loss (simple RMSE, no physics involved): 0.067867\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.028934 - Val Loss (simple RMSE, no physics involved): 0.095294\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.009337 - Val Loss (simple RMSE, no physics involved): 0.077840\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007374 - Val Loss (simple RMSE, no physics involved): 0.069329\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007197 - Val Loss (simple RMSE, no physics involved): 0.069819\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007061 - Val Loss (simple RMSE, no physics involved): 0.069157\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007106 - Val Loss (simple RMSE, no physics involved): 0.067262\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.069877\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007025 - Val Loss (simple RMSE, no physics involved): 0.067214\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007017 - Val Loss (simple RMSE, no physics involved): 0.065908\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006998 - Val Loss (simple RMSE, no physics involved): 0.067833\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006988 - Val Loss (simple RMSE, no physics involved): 0.067897\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006947 - Val Loss (simple RMSE, no physics involved): 0.071632\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.066695\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006965 - Val Loss (simple RMSE, no physics involved): 0.066793\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.066467\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.073554\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007009 - Val Loss (simple RMSE, no physics involved): 0.070006\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006887 - Val Loss (simple RMSE, no physics involved): 0.067554\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006881 - Val Loss (simple RMSE, no physics involved): 0.066050\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006852 - Val Loss (simple RMSE, no physics involved): 0.067926\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006932 - Val Loss (simple RMSE, no physics involved): 0.070574\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006832 - Val Loss (simple RMSE, no physics involved): 0.069370\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.069147\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006859 - Val Loss (simple RMSE, no physics involved): 0.072003\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006780 - Val Loss (simple RMSE, no physics involved): 0.070172\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006831 - Val Loss (simple RMSE, no physics involved): 0.068305\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006852 - Val Loss (simple RMSE, no physics involved): 0.069006\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006738 - Val Loss (simple RMSE, no physics involved): 0.065701\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006770 - Val Loss (simple RMSE, no physics involved): 0.066155\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006847 - Val Loss (simple RMSE, no physics involved): 0.074626\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006813 - Val Loss (simple RMSE, no physics involved): 0.069296\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006790 - Val Loss (simple RMSE, no physics involved): 0.070812\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006719 - Val Loss (simple RMSE, no physics involved): 0.069764\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006745 - Val Loss (simple RMSE, no physics involved): 0.076642\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.071305\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006673 - Val Loss (simple RMSE, no physics involved): 0.065837\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006721 - Val Loss (simple RMSE, no physics involved): 0.065162\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.068139\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006672 - Val Loss (simple RMSE, no physics involved): 0.067596\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006668 - Val Loss (simple RMSE, no physics involved): 0.066221\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006680 - Val Loss (simple RMSE, no physics involved): 0.071544\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006837 - Val Loss (simple RMSE, no physics involved): 0.065095\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006695 - Val Loss (simple RMSE, no physics involved): 0.065675\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006635 - Val Loss (simple RMSE, no physics involved): 0.066742\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006799 - Val Loss (simple RMSE, no physics involved): 0.075551\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006610 - Val Loss (simple RMSE, no physics involved): 0.065846\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006654 - Val Loss (simple RMSE, no physics involved): 0.066339\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006551 - Val Loss (simple RMSE, no physics involved): 0.068804\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006713 - Val Loss (simple RMSE, no physics involved): 0.069526\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:02:03,343] Trial 15 finished with value: 0.06509513780474663 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 224, 'lr': 0.00027147003980217593, 'weight_decay': 3.1742584907316727e-07, 'batch_size': 8, 'lambda_phy': 8.569631821589522e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006534 - Val Loss (simple RMSE, no physics involved): 0.065950\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.020089 - Val Loss (simple RMSE, no physics involved): 0.076709\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007774 - Val Loss (simple RMSE, no physics involved): 0.082111\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007659 - Val Loss (simple RMSE, no physics involved): 0.084797\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007272 - Val Loss (simple RMSE, no physics involved): 0.068618\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007059 - Val Loss (simple RMSE, no physics involved): 0.066258\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007048 - Val Loss (simple RMSE, no physics involved): 0.067423\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007094 - Val Loss (simple RMSE, no physics involved): 0.071847\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007022 - Val Loss (simple RMSE, no physics involved): 0.080118\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006999 - Val Loss (simple RMSE, no physics involved): 0.069579\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006959 - Val Loss (simple RMSE, no physics involved): 0.068557\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007006 - Val Loss (simple RMSE, no physics involved): 0.066602\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006946 - Val Loss (simple RMSE, no physics involved): 0.070217\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006887 - Val Loss (simple RMSE, no physics involved): 0.067680\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.068064\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006955 - Val Loss (simple RMSE, no physics involved): 0.071880\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006955 - Val Loss (simple RMSE, no physics involved): 0.066283\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006924 - Val Loss (simple RMSE, no physics involved): 0.065965\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006850 - Val Loss (simple RMSE, no physics involved): 0.065545\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006854 - Val Loss (simple RMSE, no physics involved): 0.067506\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006775 - Val Loss (simple RMSE, no physics involved): 0.065860\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006836 - Val Loss (simple RMSE, no physics involved): 0.066576\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006884 - Val Loss (simple RMSE, no physics involved): 0.069121\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006875 - Val Loss (simple RMSE, no physics involved): 0.069953\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006790 - Val Loss (simple RMSE, no physics involved): 0.071752\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006758 - Val Loss (simple RMSE, no physics involved): 0.065260\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007001 - Val Loss (simple RMSE, no physics involved): 0.071345\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.068052\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006775 - Val Loss (simple RMSE, no physics involved): 0.065968\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006684 - Val Loss (simple RMSE, no physics involved): 0.065912\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006750 - Val Loss (simple RMSE, no physics involved): 0.069938\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006718 - Val Loss (simple RMSE, no physics involved): 0.067035\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006709 - Val Loss (simple RMSE, no physics involved): 0.066213\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006746 - Val Loss (simple RMSE, no physics involved): 0.065511\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006803 - Val Loss (simple RMSE, no physics involved): 0.066023\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006727 - Val Loss (simple RMSE, no physics involved): 0.067268\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.066170\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006747 - Val Loss (simple RMSE, no physics involved): 0.069933\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006660 - Val Loss (simple RMSE, no physics involved): 0.068551\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006726 - Val Loss (simple RMSE, no physics involved): 0.069287\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006643 - Val Loss (simple RMSE, no physics involved): 0.067593\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006703 - Val Loss (simple RMSE, no physics involved): 0.068039\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006713 - Val Loss (simple RMSE, no physics involved): 0.066057\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006625 - Val Loss (simple RMSE, no physics involved): 0.075515\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006594 - Val Loss (simple RMSE, no physics involved): 0.072848\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006689 - Val Loss (simple RMSE, no physics involved): 0.068124\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006686 - Val Loss (simple RMSE, no physics involved): 0.068019\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006610 - Val Loss (simple RMSE, no physics involved): 0.066952\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006599 - Val Loss (simple RMSE, no physics involved): 0.068329\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006635 - Val Loss (simple RMSE, no physics involved): 0.069523\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:06:41,736] Trial 16 finished with value: 0.06525968542943399 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 221, 'lr': 0.0002974208660489574, 'weight_decay': 2.641900503352436e-07, 'batch_size': 8, 'lambda_phy': 6.303268718731705e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006588 - Val Loss (simple RMSE, no physics involved): 0.065790\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.024340 - Val Loss (simple RMSE, no physics involved): 0.118713\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.012352 - Val Loss (simple RMSE, no physics involved): 0.072603\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007809 - Val Loss (simple RMSE, no physics involved): 0.073166\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007319 - Val Loss (simple RMSE, no physics involved): 0.074239\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007218 - Val Loss (simple RMSE, no physics involved): 0.069509\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007131 - Val Loss (simple RMSE, no physics involved): 0.070379\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007071 - Val Loss (simple RMSE, no physics involved): 0.068781\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007001 - Val Loss (simple RMSE, no physics involved): 0.071891\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007033 - Val Loss (simple RMSE, no physics involved): 0.067235\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007083 - Val Loss (simple RMSE, no physics involved): 0.066044\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007023 - Val Loss (simple RMSE, no physics involved): 0.065824\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006954 - Val Loss (simple RMSE, no physics involved): 0.068370\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006931 - Val Loss (simple RMSE, no physics involved): 0.069224\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.066997\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006966 - Val Loss (simple RMSE, no physics involved): 0.068473\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.067988\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006888 - Val Loss (simple RMSE, no physics involved): 0.066059\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006840 - Val Loss (simple RMSE, no physics involved): 0.069915\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006851 - Val Loss (simple RMSE, no physics involved): 0.070110\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006803 - Val Loss (simple RMSE, no physics involved): 0.069730\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006836 - Val Loss (simple RMSE, no physics involved): 0.069553\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.067788\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.072046\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.065657\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006930 - Val Loss (simple RMSE, no physics involved): 0.067333\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006758 - Val Loss (simple RMSE, no physics involved): 0.073397\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006853 - Val Loss (simple RMSE, no physics involved): 0.065729\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006751 - Val Loss (simple RMSE, no physics involved): 0.066136\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006828 - Val Loss (simple RMSE, no physics involved): 0.065912\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006743 - Val Loss (simple RMSE, no physics involved): 0.065786\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006733 - Val Loss (simple RMSE, no physics involved): 0.068938\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006730 - Val Loss (simple RMSE, no physics involved): 0.068296\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006758 - Val Loss (simple RMSE, no physics involved): 0.065179\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007045 - Val Loss (simple RMSE, no physics involved): 0.072433\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006718 - Val Loss (simple RMSE, no physics involved): 0.074420\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006693 - Val Loss (simple RMSE, no physics involved): 0.069788\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006790 - Val Loss (simple RMSE, no physics involved): 0.069138\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006695 - Val Loss (simple RMSE, no physics involved): 0.066036\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006688 - Val Loss (simple RMSE, no physics involved): 0.069435\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006690 - Val Loss (simple RMSE, no physics involved): 0.065447\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006727 - Val Loss (simple RMSE, no physics involved): 0.077522\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006765 - Val Loss (simple RMSE, no physics involved): 0.067885\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006663 - Val Loss (simple RMSE, no physics involved): 0.065448\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006651 - Val Loss (simple RMSE, no physics involved): 0.066152\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006680 - Val Loss (simple RMSE, no physics involved): 0.067840\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006789 - Val Loss (simple RMSE, no physics involved): 0.073647\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006670 - Val Loss (simple RMSE, no physics involved): 0.067100\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.070633\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006647 - Val Loss (simple RMSE, no physics involved): 0.066320\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:09:36,026] Trial 17 finished with value: 0.06517862590650718 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 178, 'lr': 0.00036103248116047345, 'weight_decay': 5.294566141429274e-07, 'batch_size': 16, 'lambda_phy': 1.0320284454184048e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006664 - Val Loss (simple RMSE, no physics involved): 0.068848\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.029293 - Val Loss (simple RMSE, no physics involved): 0.094108\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.014452 - Val Loss (simple RMSE, no physics involved): 0.088156\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.008289 - Val Loss (simple RMSE, no physics involved): 0.072541\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007801 - Val Loss (simple RMSE, no physics involved): 0.070359\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007399 - Val Loss (simple RMSE, no physics involved): 0.071318\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007216 - Val Loss (simple RMSE, no physics involved): 0.069870\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007143 - Val Loss (simple RMSE, no physics involved): 0.069591\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.006990 - Val Loss (simple RMSE, no physics involved): 0.068708\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007233 - Val Loss (simple RMSE, no physics involved): 0.068774\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007106 - Val Loss (simple RMSE, no physics involved): 0.071636\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007068 - Val Loss (simple RMSE, no physics involved): 0.069174\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007168 - Val Loss (simple RMSE, no physics involved): 0.074263\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007100 - Val Loss (simple RMSE, no physics involved): 0.067432\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007281 - Val Loss (simple RMSE, no physics involved): 0.071405\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007031 - Val Loss (simple RMSE, no physics involved): 0.068267\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006830 - Val Loss (simple RMSE, no physics involved): 0.069445\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006945 - Val Loss (simple RMSE, no physics involved): 0.067357\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006990 - Val Loss (simple RMSE, no physics involved): 0.070005\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006965 - Val Loss (simple RMSE, no physics involved): 0.067232\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006932 - Val Loss (simple RMSE, no physics involved): 0.068101\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006938 - Val Loss (simple RMSE, no physics involved): 0.068936\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006953 - Val Loss (simple RMSE, no physics involved): 0.066783\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007072 - Val Loss (simple RMSE, no physics involved): 0.070428\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006911 - Val Loss (simple RMSE, no physics involved): 0.068737\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006838 - Val Loss (simple RMSE, no physics involved): 0.069961\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006744 - Val Loss (simple RMSE, no physics involved): 0.067287\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006911 - Val Loss (simple RMSE, no physics involved): 0.069700\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.069023\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006822 - Val Loss (simple RMSE, no physics involved): 0.067781\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006872 - Val Loss (simple RMSE, no physics involved): 0.067059\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006891 - Val Loss (simple RMSE, no physics involved): 0.068725\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006854 - Val Loss (simple RMSE, no physics involved): 0.068693\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006841 - Val Loss (simple RMSE, no physics involved): 0.068419\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006793 - Val Loss (simple RMSE, no physics involved): 0.070160\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006843 - Val Loss (simple RMSE, no physics involved): 0.069405\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006692 - Val Loss (simple RMSE, no physics involved): 0.068187\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006654 - Val Loss (simple RMSE, no physics involved): 0.073119\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006922 - Val Loss (simple RMSE, no physics involved): 0.071155\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006677 - Val Loss (simple RMSE, no physics involved): 0.071422\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006631 - Val Loss (simple RMSE, no physics involved): 0.069192\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006914 - Val Loss (simple RMSE, no physics involved): 0.069286\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006726 - Val Loss (simple RMSE, no physics involved): 0.069031\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006861 - Val Loss (simple RMSE, no physics involved): 0.068912\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006751 - Val Loss (simple RMSE, no physics involved): 0.066899\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006614 - Val Loss (simple RMSE, no physics involved): 0.067124\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006692 - Val Loss (simple RMSE, no physics involved): 0.067743\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006906 - Val Loss (simple RMSE, no physics involved): 0.067220\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006736 - Val Loss (simple RMSE, no physics involved): 0.067242\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006768 - Val Loss (simple RMSE, no physics involved): 0.066605\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:10:19,815] Trial 18 finished with value: 0.06660457700490952 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 130, 'lr': 0.0024949282007164296, 'weight_decay': 2.0883571031531552e-08, 'batch_size': 64, 'lambda_phy': 0.0012231478880006309}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006632 - Val Loss (simple RMSE, no physics involved): 0.067035\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.012697 - Val Loss (simple RMSE, no physics involved): 0.080319\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007523 - Val Loss (simple RMSE, no physics involved): 0.067265\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007405 - Val Loss (simple RMSE, no physics involved): 0.068153\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007559 - Val Loss (simple RMSE, no physics involved): 0.066933\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007457 - Val Loss (simple RMSE, no physics involved): 0.066005\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007779 - Val Loss (simple RMSE, no physics involved): 0.066666\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007231 - Val Loss (simple RMSE, no physics involved): 0.066023\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007354 - Val Loss (simple RMSE, no physics involved): 0.066974\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007133 - Val Loss (simple RMSE, no physics involved): 0.078254\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007213 - Val Loss (simple RMSE, no physics involved): 0.067614\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007148 - Val Loss (simple RMSE, no physics involved): 0.069193\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007131 - Val Loss (simple RMSE, no physics involved): 0.066637\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007290 - Val Loss (simple RMSE, no physics involved): 0.072603\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007125 - Val Loss (simple RMSE, no physics involved): 0.068597\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007148 - Val Loss (simple RMSE, no physics involved): 0.067628\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007128 - Val Loss (simple RMSE, no physics involved): 0.068610\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007064 - Val Loss (simple RMSE, no physics involved): 0.071097\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007099 - Val Loss (simple RMSE, no physics involved): 0.067110\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007397 - Val Loss (simple RMSE, no physics involved): 0.066297\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007159 - Val Loss (simple RMSE, no physics involved): 0.066349\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007203 - Val Loss (simple RMSE, no physics involved): 0.066137\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007037 - Val Loss (simple RMSE, no physics involved): 0.071730\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007228 - Val Loss (simple RMSE, no physics involved): 0.068413\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.077549\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007081 - Val Loss (simple RMSE, no physics involved): 0.077236\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007051 - Val Loss (simple RMSE, no physics involved): 0.072009\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007056 - Val Loss (simple RMSE, no physics involved): 0.065582\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007086 - Val Loss (simple RMSE, no physics involved): 0.071983\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.066915\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007042 - Val Loss (simple RMSE, no physics involved): 0.068709\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.067941\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007110 - Val Loss (simple RMSE, no physics involved): 0.073336\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007085 - Val Loss (simple RMSE, no physics involved): 0.066864\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.068492\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006940 - Val Loss (simple RMSE, no physics involved): 0.071043\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006979 - Val Loss (simple RMSE, no physics involved): 0.066851\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006892 - Val Loss (simple RMSE, no physics involved): 0.069162\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006939 - Val Loss (simple RMSE, no physics involved): 0.069142\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006969 - Val Loss (simple RMSE, no physics involved): 0.066838\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007219 - Val Loss (simple RMSE, no physics involved): 0.067600\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006950 - Val Loss (simple RMSE, no physics involved): 0.066076\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006961 - Val Loss (simple RMSE, no physics involved): 0.067875\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007104 - Val Loss (simple RMSE, no physics involved): 0.066076\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007026 - Val Loss (simple RMSE, no physics involved): 0.066320\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007022 - Val Loss (simple RMSE, no physics involved): 0.066622\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006969 - Val Loss (simple RMSE, no physics involved): 0.069057\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006974 - Val Loss (simple RMSE, no physics involved): 0.068835\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007138 - Val Loss (simple RMSE, no physics involved): 0.077045\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.065985\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:14:24,257] Trial 19 finished with value: 0.06558201586206754 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 223, 'lr': 0.006306200134647129, 'weight_decay': 6.708879867304567e-06, 'batch_size': 8, 'lambda_phy': 4.775651461491471e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006926 - Val Loss (simple RMSE, no physics involved): 0.070053\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.014654 - Val Loss (simple RMSE, no physics involved): 0.071068\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007489 - Val Loss (simple RMSE, no physics involved): 0.067618\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007353 - Val Loss (simple RMSE, no physics involved): 0.067953\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007225 - Val Loss (simple RMSE, no physics involved): 0.066534\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007209 - Val Loss (simple RMSE, no physics involved): 0.066983\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007095 - Val Loss (simple RMSE, no physics involved): 0.067633\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007187 - Val Loss (simple RMSE, no physics involved): 0.068817\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007129 - Val Loss (simple RMSE, no physics involved): 0.068798\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007042 - Val Loss (simple RMSE, no physics involved): 0.066950\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007012 - Val Loss (simple RMSE, no physics involved): 0.066627\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007085 - Val Loss (simple RMSE, no physics involved): 0.069043\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007012 - Val Loss (simple RMSE, no physics involved): 0.066072\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006983 - Val Loss (simple RMSE, no physics involved): 0.072918\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007085 - Val Loss (simple RMSE, no physics involved): 0.066292\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006968 - Val Loss (simple RMSE, no physics involved): 0.070262\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006906 - Val Loss (simple RMSE, no physics involved): 0.068183\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006916 - Val Loss (simple RMSE, no physics involved): 0.069021\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.066964\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006945 - Val Loss (simple RMSE, no physics involved): 0.066016\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006953 - Val Loss (simple RMSE, no physics involved): 0.066749\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006942 - Val Loss (simple RMSE, no physics involved): 0.066955\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007092 - Val Loss (simple RMSE, no physics involved): 0.068403\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007016 - Val Loss (simple RMSE, no physics involved): 0.070644\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006904 - Val Loss (simple RMSE, no physics involved): 0.067379\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006812 - Val Loss (simple RMSE, no physics involved): 0.069566\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006873 - Val Loss (simple RMSE, no physics involved): 0.072430\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.066660\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006820 - Val Loss (simple RMSE, no physics involved): 0.068411\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.066721\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006909 - Val Loss (simple RMSE, no physics involved): 0.073472\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006866 - Val Loss (simple RMSE, no physics involved): 0.068058\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006773 - Val Loss (simple RMSE, no physics involved): 0.074803\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.069010\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006757 - Val Loss (simple RMSE, no physics involved): 0.067018\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006820 - Val Loss (simple RMSE, no physics involved): 0.071681\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006696 - Val Loss (simple RMSE, no physics involved): 0.068885\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006686 - Val Loss (simple RMSE, no physics involved): 0.067695\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.073405\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006713 - Val Loss (simple RMSE, no physics involved): 0.070094\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006791 - Val Loss (simple RMSE, no physics involved): 0.066967\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006745 - Val Loss (simple RMSE, no physics involved): 0.066462\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006767 - Val Loss (simple RMSE, no physics involved): 0.069304\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006684 - Val Loss (simple RMSE, no physics involved): 0.066847\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006759 - Val Loss (simple RMSE, no physics involved): 0.074023\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006815 - Val Loss (simple RMSE, no physics involved): 0.067968\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006654 - Val Loss (simple RMSE, no physics involved): 0.070047\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006719 - Val Loss (simple RMSE, no physics involved): 0.070077\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006745 - Val Loss (simple RMSE, no physics involved): 0.065990\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006716 - Val Loss (simple RMSE, no physics involved): 0.072030\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:17:03,846] Trial 20 finished with value: 0.06599011675765117 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 96, 'lr': 0.001036468322877998, 'weight_decay': 1.5971433287743958e-06, 'batch_size': 8, 'lambda_phy': 1.06650911577737e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006672 - Val Loss (simple RMSE, no physics involved): 0.066938\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.022994 - Val Loss (simple RMSE, no physics involved): 0.107315\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.010287 - Val Loss (simple RMSE, no physics involved): 0.071047\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007523 - Val Loss (simple RMSE, no physics involved): 0.075970\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007291 - Val Loss (simple RMSE, no physics involved): 0.070693\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007161 - Val Loss (simple RMSE, no physics involved): 0.067647\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007148 - Val Loss (simple RMSE, no physics involved): 0.071184\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007155 - Val Loss (simple RMSE, no physics involved): 0.071562\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007059 - Val Loss (simple RMSE, no physics involved): 0.067938\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006977 - Val Loss (simple RMSE, no physics involved): 0.067984\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006961 - Val Loss (simple RMSE, no physics involved): 0.067335\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007007 - Val Loss (simple RMSE, no physics involved): 0.070344\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007014 - Val Loss (simple RMSE, no physics involved): 0.068187\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.067495\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006961 - Val Loss (simple RMSE, no physics involved): 0.076285\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007086 - Val Loss (simple RMSE, no physics involved): 0.069209\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.066273\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.065895\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.066560\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006971 - Val Loss (simple RMSE, no physics involved): 0.067262\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007019 - Val Loss (simple RMSE, no physics involved): 0.072707\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006858 - Val Loss (simple RMSE, no physics involved): 0.068199\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006824 - Val Loss (simple RMSE, no physics involved): 0.067913\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006783 - Val Loss (simple RMSE, no physics involved): 0.067651\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006873 - Val Loss (simple RMSE, no physics involved): 0.069607\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006786 - Val Loss (simple RMSE, no physics involved): 0.071420\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006783 - Val Loss (simple RMSE, no physics involved): 0.067145\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006891 - Val Loss (simple RMSE, no physics involved): 0.065344\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.071450\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006775 - Val Loss (simple RMSE, no physics involved): 0.068176\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006824 - Val Loss (simple RMSE, no physics involved): 0.065827\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006763 - Val Loss (simple RMSE, no physics involved): 0.066904\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006776 - Val Loss (simple RMSE, no physics involved): 0.066039\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006803 - Val Loss (simple RMSE, no physics involved): 0.072334\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006733 - Val Loss (simple RMSE, no physics involved): 0.068038\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006800 - Val Loss (simple RMSE, no physics involved): 0.065434\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006870 - Val Loss (simple RMSE, no physics involved): 0.073038\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006684 - Val Loss (simple RMSE, no physics involved): 0.071237\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006845 - Val Loss (simple RMSE, no physics involved): 0.071209\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.067430\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006733 - Val Loss (simple RMSE, no physics involved): 0.067593\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006654 - Val Loss (simple RMSE, no physics involved): 0.066683\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006690 - Val Loss (simple RMSE, no physics involved): 0.065938\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006659 - Val Loss (simple RMSE, no physics involved): 0.070308\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006667 - Val Loss (simple RMSE, no physics involved): 0.066565\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006644 - Val Loss (simple RMSE, no physics involved): 0.067604\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006674 - Val Loss (simple RMSE, no physics involved): 0.067499\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006655 - Val Loss (simple RMSE, no physics involved): 0.070253\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006699 - Val Loss (simple RMSE, no physics involved): 0.067090\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006681 - Val Loss (simple RMSE, no physics involved): 0.068516\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:19:52,324] Trial 21 finished with value: 0.06534370221197605 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 176, 'lr': 0.0003743436860588015, 'weight_decay': 4.345718386502407e-07, 'batch_size': 16, 'lambda_phy': 1.1060788185702604e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006726 - Val Loss (simple RMSE, no physics involved): 0.065744\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.027820 - Val Loss (simple RMSE, no physics involved): 0.114351\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.012751 - Val Loss (simple RMSE, no physics involved): 0.089369\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007988 - Val Loss (simple RMSE, no physics involved): 0.068817\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007527 - Val Loss (simple RMSE, no physics involved): 0.073974\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007235 - Val Loss (simple RMSE, no physics involved): 0.071697\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007162 - Val Loss (simple RMSE, no physics involved): 0.075520\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007222 - Val Loss (simple RMSE, no physics involved): 0.072333\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007043 - Val Loss (simple RMSE, no physics involved): 0.067724\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007031 - Val Loss (simple RMSE, no physics involved): 0.066738\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007007 - Val Loss (simple RMSE, no physics involved): 0.069037\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007033 - Val Loss (simple RMSE, no physics involved): 0.066072\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007003 - Val Loss (simple RMSE, no physics involved): 0.068072\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006959 - Val Loss (simple RMSE, no physics involved): 0.070820\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006912 - Val Loss (simple RMSE, no physics involved): 0.067244\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006937 - Val Loss (simple RMSE, no physics involved): 0.074559\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006905 - Val Loss (simple RMSE, no physics involved): 0.071032\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006870 - Val Loss (simple RMSE, no physics involved): 0.066789\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006915 - Val Loss (simple RMSE, no physics involved): 0.066063\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006890 - Val Loss (simple RMSE, no physics involved): 0.068129\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006866 - Val Loss (simple RMSE, no physics involved): 0.068286\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.067680\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006805 - Val Loss (simple RMSE, no physics involved): 0.068607\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006776 - Val Loss (simple RMSE, no physics involved): 0.066963\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006864 - Val Loss (simple RMSE, no physics involved): 0.066520\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006747 - Val Loss (simple RMSE, no physics involved): 0.067497\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006767 - Val Loss (simple RMSE, no physics involved): 0.066028\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006859 - Val Loss (simple RMSE, no physics involved): 0.069963\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006832 - Val Loss (simple RMSE, no physics involved): 0.078859\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006817 - Val Loss (simple RMSE, no physics involved): 0.067694\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.068523\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006740 - Val Loss (simple RMSE, no physics involved): 0.069557\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006773 - Val Loss (simple RMSE, no physics involved): 0.065673\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006673 - Val Loss (simple RMSE, no physics involved): 0.070128\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006694 - Val Loss (simple RMSE, no physics involved): 0.067645\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006694 - Val Loss (simple RMSE, no physics involved): 0.068096\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006710 - Val Loss (simple RMSE, no physics involved): 0.066469\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006738 - Val Loss (simple RMSE, no physics involved): 0.067324\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006679 - Val Loss (simple RMSE, no physics involved): 0.065877\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006722 - Val Loss (simple RMSE, no physics involved): 0.070420\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006620 - Val Loss (simple RMSE, no physics involved): 0.066117\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006703 - Val Loss (simple RMSE, no physics involved): 0.066099\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006694 - Val Loss (simple RMSE, no physics involved): 0.072149\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006639 - Val Loss (simple RMSE, no physics involved): 0.068632\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006669 - Val Loss (simple RMSE, no physics involved): 0.067893\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006630 - Val Loss (simple RMSE, no physics involved): 0.068089\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006606 - Val Loss (simple RMSE, no physics involved): 0.069755\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006617 - Val Loss (simple RMSE, no physics involved): 0.068675\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006587 - Val Loss (simple RMSE, no physics involved): 0.068386\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006605 - Val Loss (simple RMSE, no physics involved): 0.069188\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:23:24,383] Trial 22 finished with value: 0.0656732985128959 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 199, 'lr': 0.00031253238578805745, 'weight_decay': 1.3822210502950244e-07, 'batch_size': 16, 'lambda_phy': 6.435950038000571e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006607 - Val Loss (simple RMSE, no physics involved): 0.069597\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.032431 - Val Loss (simple RMSE, no physics involved): 0.128778\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.017208 - Val Loss (simple RMSE, no physics involved): 0.104187\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.011188 - Val Loss (simple RMSE, no physics involved): 0.074107\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007773 - Val Loss (simple RMSE, no physics involved): 0.069657\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007499 - Val Loss (simple RMSE, no physics involved): 0.068262\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007419 - Val Loss (simple RMSE, no physics involved): 0.068228\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007309 - Val Loss (simple RMSE, no physics involved): 0.068556\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007217 - Val Loss (simple RMSE, no physics involved): 0.068436\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007135 - Val Loss (simple RMSE, no physics involved): 0.070998\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007123 - Val Loss (simple RMSE, no physics involved): 0.069779\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007046 - Val Loss (simple RMSE, no physics involved): 0.068600\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007008 - Val Loss (simple RMSE, no physics involved): 0.069385\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006982 - Val Loss (simple RMSE, no physics involved): 0.067004\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006980 - Val Loss (simple RMSE, no physics involved): 0.068515\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006997 - Val Loss (simple RMSE, no physics involved): 0.067024\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006972 - Val Loss (simple RMSE, no physics involved): 0.068622\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006912 - Val Loss (simple RMSE, no physics involved): 0.066420\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006893 - Val Loss (simple RMSE, no physics involved): 0.071274\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.072101\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006920 - Val Loss (simple RMSE, no physics involved): 0.066174\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006893 - Val Loss (simple RMSE, no physics involved): 0.067265\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006855 - Val Loss (simple RMSE, no physics involved): 0.067991\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006833 - Val Loss (simple RMSE, no physics involved): 0.069037\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006834 - Val Loss (simple RMSE, no physics involved): 0.066606\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006917 - Val Loss (simple RMSE, no physics involved): 0.067785\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006845 - Val Loss (simple RMSE, no physics involved): 0.067955\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006850 - Val Loss (simple RMSE, no physics involved): 0.066617\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006799 - Val Loss (simple RMSE, no physics involved): 0.068217\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.067047\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006781 - Val Loss (simple RMSE, no physics involved): 0.070392\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.066497\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.069305\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.067295\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006832 - Val Loss (simple RMSE, no physics involved): 0.071585\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006755 - Val Loss (simple RMSE, no physics involved): 0.069805\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006788 - Val Loss (simple RMSE, no physics involved): 0.066247\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006739 - Val Loss (simple RMSE, no physics involved): 0.068558\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006712 - Val Loss (simple RMSE, no physics involved): 0.067480\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006722 - Val Loss (simple RMSE, no physics involved): 0.067845\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006785 - Val Loss (simple RMSE, no physics involved): 0.066028\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006769 - Val Loss (simple RMSE, no physics involved): 0.067907\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006696 - Val Loss (simple RMSE, no physics involved): 0.066128\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006723 - Val Loss (simple RMSE, no physics involved): 0.065513\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006712 - Val Loss (simple RMSE, no physics involved): 0.065934\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006685 - Val Loss (simple RMSE, no physics involved): 0.067138\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006721 - Val Loss (simple RMSE, no physics involved): 0.073338\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006680 - Val Loss (simple RMSE, no physics involved): 0.068076\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006729 - Val Loss (simple RMSE, no physics involved): 0.066472\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006834 - Val Loss (simple RMSE, no physics involved): 0.067361\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:26:06,451] Trial 23 finished with value: 0.06551296326021354 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 153, 'lr': 0.00020993360914864075, 'weight_decay': 7.539870104705704e-07, 'batch_size': 16, 'lambda_phy': 3.6500051020198414e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006648 - Val Loss (simple RMSE, no physics involved): 0.071303\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.043663 - Val Loss (simple RMSE, no physics involved): 0.123633\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.024422 - Val Loss (simple RMSE, no physics involved): 0.115374\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.017802 - Val Loss (simple RMSE, no physics involved): 0.118604\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.015151 - Val Loss (simple RMSE, no physics involved): 0.105353\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.011305 - Val Loss (simple RMSE, no physics involved): 0.084818\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.008620 - Val Loss (simple RMSE, no physics involved): 0.077262\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007847 - Val Loss (simple RMSE, no physics involved): 0.073397\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007604 - Val Loss (simple RMSE, no physics involved): 0.071125\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007415 - Val Loss (simple RMSE, no physics involved): 0.070853\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007289 - Val Loss (simple RMSE, no physics involved): 0.069537\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007282 - Val Loss (simple RMSE, no physics involved): 0.068092\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007198 - Val Loss (simple RMSE, no physics involved): 0.069296\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007145 - Val Loss (simple RMSE, no physics involved): 0.067946\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007108 - Val Loss (simple RMSE, no physics involved): 0.068758\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007084 - Val Loss (simple RMSE, no physics involved): 0.070450\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007089 - Val Loss (simple RMSE, no physics involved): 0.067306\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007078 - Val Loss (simple RMSE, no physics involved): 0.067129\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007032 - Val Loss (simple RMSE, no physics involved): 0.068709\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007067 - Val Loss (simple RMSE, no physics involved): 0.067768\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007004 - Val Loss (simple RMSE, no physics involved): 0.068768\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007015 - Val Loss (simple RMSE, no physics involved): 0.067740\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006978 - Val Loss (simple RMSE, no physics involved): 0.068072\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006972 - Val Loss (simple RMSE, no physics involved): 0.071203\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006975 - Val Loss (simple RMSE, no physics involved): 0.070587\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.066931\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006948 - Val Loss (simple RMSE, no physics involved): 0.069056\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006929 - Val Loss (simple RMSE, no physics involved): 0.069394\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006933 - Val Loss (simple RMSE, no physics involved): 0.066950\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006983 - Val Loss (simple RMSE, no physics involved): 0.069030\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006924 - Val Loss (simple RMSE, no physics involved): 0.067127\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006937 - Val Loss (simple RMSE, no physics involved): 0.070532\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006933 - Val Loss (simple RMSE, no physics involved): 0.068144\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006924 - Val Loss (simple RMSE, no physics involved): 0.069376\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006906 - Val Loss (simple RMSE, no physics involved): 0.066567\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006928 - Val Loss (simple RMSE, no physics involved): 0.072642\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006902 - Val Loss (simple RMSE, no physics involved): 0.067373\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006907 - Val Loss (simple RMSE, no physics involved): 0.067096\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006912 - Val Loss (simple RMSE, no physics involved): 0.068630\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006884 - Val Loss (simple RMSE, no physics involved): 0.067947\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006868 - Val Loss (simple RMSE, no physics involved): 0.068065\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006875 - Val Loss (simple RMSE, no physics involved): 0.067534\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006892 - Val Loss (simple RMSE, no physics involved): 0.067962\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006881 - Val Loss (simple RMSE, no physics involved): 0.067410\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006849 - Val Loss (simple RMSE, no physics involved): 0.068946\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006861 - Val Loss (simple RMSE, no physics involved): 0.066411\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006865 - Val Loss (simple RMSE, no physics involved): 0.072067\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006904 - Val Loss (simple RMSE, no physics involved): 0.066340\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006826 - Val Loss (simple RMSE, no physics involved): 0.074854\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006899 - Val Loss (simple RMSE, no physics involved): 0.066686\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:29:28,157] Trial 24 finished with value: 0.06633966850737731 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 175, 'lr': 5.9469953316265734e-05, 'weight_decay': 1.4581889948248316e-05, 'batch_size': 16, 'lambda_phy': 0.00010930700931144035}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006831 - Val Loss (simple RMSE, no physics involved): 0.068467\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.019853 - Val Loss (simple RMSE, no physics involved): 0.110112\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.008889 - Val Loss (simple RMSE, no physics involved): 0.070388\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007506 - Val Loss (simple RMSE, no physics involved): 0.071845\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007296 - Val Loss (simple RMSE, no physics involved): 0.069761\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007160 - Val Loss (simple RMSE, no physics involved): 0.067872\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007196 - Val Loss (simple RMSE, no physics involved): 0.073264\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007101 - Val Loss (simple RMSE, no physics involved): 0.067104\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007008 - Val Loss (simple RMSE, no physics involved): 0.071647\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007267 - Val Loss (simple RMSE, no physics involved): 0.067783\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007139 - Val Loss (simple RMSE, no physics involved): 0.067049\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.069087\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007008 - Val Loss (simple RMSE, no physics involved): 0.070161\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007042 - Val Loss (simple RMSE, no physics involved): 0.066152\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007007 - Val Loss (simple RMSE, no physics involved): 0.068316\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006960 - Val Loss (simple RMSE, no physics involved): 0.066862\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006911 - Val Loss (simple RMSE, no physics involved): 0.069144\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006894 - Val Loss (simple RMSE, no physics involved): 0.069332\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006966 - Val Loss (simple RMSE, no physics involved): 0.069742\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006912 - Val Loss (simple RMSE, no physics involved): 0.067861\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006919 - Val Loss (simple RMSE, no physics involved): 0.067527\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.066646\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006939 - Val Loss (simple RMSE, no physics involved): 0.065971\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006922 - Val Loss (simple RMSE, no physics involved): 0.066330\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.066734\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006946 - Val Loss (simple RMSE, no physics involved): 0.066935\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006836 - Val Loss (simple RMSE, no physics involved): 0.068321\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006799 - Val Loss (simple RMSE, no physics involved): 0.069106\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006884 - Val Loss (simple RMSE, no physics involved): 0.074136\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006786 - Val Loss (simple RMSE, no physics involved): 0.068409\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006761 - Val Loss (simple RMSE, no physics involved): 0.068923\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006874 - Val Loss (simple RMSE, no physics involved): 0.066811\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006912 - Val Loss (simple RMSE, no physics involved): 0.068119\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006848 - Val Loss (simple RMSE, no physics involved): 0.066900\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006721 - Val Loss (simple RMSE, no physics involved): 0.067962\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006756 - Val Loss (simple RMSE, no physics involved): 0.065659\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006695 - Val Loss (simple RMSE, no physics involved): 0.069586\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006765 - Val Loss (simple RMSE, no physics involved): 0.070104\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006705 - Val Loss (simple RMSE, no physics involved): 0.068178\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006728 - Val Loss (simple RMSE, no physics involved): 0.067883\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006714 - Val Loss (simple RMSE, no physics involved): 0.072688\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006686 - Val Loss (simple RMSE, no physics involved): 0.065693\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006731 - Val Loss (simple RMSE, no physics involved): 0.068189\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006739 - Val Loss (simple RMSE, no physics involved): 0.065429\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006954 - Val Loss (simple RMSE, no physics involved): 0.069837\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006730 - Val Loss (simple RMSE, no physics involved): 0.072272\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006766 - Val Loss (simple RMSE, no physics involved): 0.074567\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006722 - Val Loss (simple RMSE, no physics involved): 0.065742\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006587 - Val Loss (simple RMSE, no physics involved): 0.066775\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006595 - Val Loss (simple RMSE, no physics involved): 0.066925\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:32:36,250] Trial 25 finished with value: 0.06542873072127502 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 207, 'lr': 0.0005315390906202606, 'weight_decay': 9.498162594376547e-08, 'batch_size': 16, 'lambda_phy': 0.0006649370687235865}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006640 - Val Loss (simple RMSE, no physics involved): 0.068528\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.031706 - Val Loss (simple RMSE, no physics involved): 0.109444\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.018154 - Val Loss (simple RMSE, no physics involved): 0.113891\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.012150 - Val Loss (simple RMSE, no physics involved): 0.089760\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.008862 - Val Loss (simple RMSE, no physics involved): 0.082060\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007706 - Val Loss (simple RMSE, no physics involved): 0.073825\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007437 - Val Loss (simple RMSE, no physics involved): 0.070270\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007136 - Val Loss (simple RMSE, no physics involved): 0.076117\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007222 - Val Loss (simple RMSE, no physics involved): 0.069458\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006884 - Val Loss (simple RMSE, no physics involved): 0.067309\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007323 - Val Loss (simple RMSE, no physics involved): 0.066753\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007173 - Val Loss (simple RMSE, no physics involved): 0.069510\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006992 - Val Loss (simple RMSE, no physics involved): 0.071036\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.070056\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006981 - Val Loss (simple RMSE, no physics involved): 0.069160\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006813 - Val Loss (simple RMSE, no physics involved): 0.068841\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006750 - Val Loss (simple RMSE, no physics involved): 0.067642\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006765 - Val Loss (simple RMSE, no physics involved): 0.066640\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006998 - Val Loss (simple RMSE, no physics involved): 0.066263\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006926 - Val Loss (simple RMSE, no physics involved): 0.067378\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007086 - Val Loss (simple RMSE, no physics involved): 0.065972\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006756 - Val Loss (simple RMSE, no physics involved): 0.067882\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006961 - Val Loss (simple RMSE, no physics involved): 0.074966\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007032 - Val Loss (simple RMSE, no physics involved): 0.077133\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007102 - Val Loss (simple RMSE, no physics involved): 0.074460\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007004 - Val Loss (simple RMSE, no physics involved): 0.075510\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006930 - Val Loss (simple RMSE, no physics involved): 0.066652\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006608 - Val Loss (simple RMSE, no physics involved): 0.066786\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007373 - Val Loss (simple RMSE, no physics involved): 0.065932\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007152 - Val Loss (simple RMSE, no physics involved): 0.080123\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007005 - Val Loss (simple RMSE, no physics involved): 0.066730\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006655 - Val Loss (simple RMSE, no physics involved): 0.067689\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006691 - Val Loss (simple RMSE, no physics involved): 0.068473\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006762 - Val Loss (simple RMSE, no physics involved): 0.067033\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006683 - Val Loss (simple RMSE, no physics involved): 0.067997\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006729 - Val Loss (simple RMSE, no physics involved): 0.069854\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006891 - Val Loss (simple RMSE, no physics involved): 0.073034\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006872 - Val Loss (simple RMSE, no physics involved): 0.070769\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006702 - Val Loss (simple RMSE, no physics involved): 0.069976\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006707 - Val Loss (simple RMSE, no physics involved): 0.068218\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006638 - Val Loss (simple RMSE, no physics involved): 0.066776\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006825 - Val Loss (simple RMSE, no physics involved): 0.066887\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006603 - Val Loss (simple RMSE, no physics involved): 0.069018\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006546 - Val Loss (simple RMSE, no physics involved): 0.067606\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006742 - Val Loss (simple RMSE, no physics involved): 0.070590\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006652 - Val Loss (simple RMSE, no physics involved): 0.066982\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006709 - Val Loss (simple RMSE, no physics involved): 0.067532\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006840 - Val Loss (simple RMSE, no physics involved): 0.070621\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006761 - Val Loss (simple RMSE, no physics involved): 0.073298\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006957 - Val Loss (simple RMSE, no physics involved): 0.073641\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:34:29,030] Trial 26 finished with value: 0.06593229249119759 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 233, 'lr': 0.0013580100463632231, 'weight_decay': 3.0207181241584454e-07, 'batch_size': 64, 'lambda_phy': 2.2406259188587695e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006869 - Val Loss (simple RMSE, no physics involved): 0.071791\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.043933 - Val Loss (simple RMSE, no physics involved): 0.120168\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.016919 - Val Loss (simple RMSE, no physics involved): 0.098881\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.010930 - Val Loss (simple RMSE, no physics involved): 0.093656\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.008286 - Val Loss (simple RMSE, no physics involved): 0.075987\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007590 - Val Loss (simple RMSE, no physics involved): 0.071952\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007457 - Val Loss (simple RMSE, no physics involved): 0.072416\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007243 - Val Loss (simple RMSE, no physics involved): 0.073261\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007123 - Val Loss (simple RMSE, no physics involved): 0.068903\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007185 - Val Loss (simple RMSE, no physics involved): 0.070982\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007076 - Val Loss (simple RMSE, no physics involved): 0.070414\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007055 - Val Loss (simple RMSE, no physics involved): 0.069805\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007070 - Val Loss (simple RMSE, no physics involved): 0.067945\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007036 - Val Loss (simple RMSE, no physics involved): 0.067556\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006932 - Val Loss (simple RMSE, no physics involved): 0.068465\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.067552\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006951 - Val Loss (simple RMSE, no physics involved): 0.070231\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006955 - Val Loss (simple RMSE, no physics involved): 0.074689\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.068679\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007021 - Val Loss (simple RMSE, no physics involved): 0.071497\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006909 - Val Loss (simple RMSE, no physics involved): 0.071545\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007032 - Val Loss (simple RMSE, no physics involved): 0.069191\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006885 - Val Loss (simple RMSE, no physics involved): 0.068830\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006768 - Val Loss (simple RMSE, no physics involved): 0.067841\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006872 - Val Loss (simple RMSE, no physics involved): 0.067096\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006847 - Val Loss (simple RMSE, no physics involved): 0.067711\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006847 - Val Loss (simple RMSE, no physics involved): 0.068753\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006809 - Val Loss (simple RMSE, no physics involved): 0.071847\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006886 - Val Loss (simple RMSE, no physics involved): 0.067407\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006871 - Val Loss (simple RMSE, no physics involved): 0.067940\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006849 - Val Loss (simple RMSE, no physics involved): 0.076054\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006893 - Val Loss (simple RMSE, no physics involved): 0.068857\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007056 - Val Loss (simple RMSE, no physics involved): 0.066322\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006909 - Val Loss (simple RMSE, no physics involved): 0.068623\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006928 - Val Loss (simple RMSE, no physics involved): 0.071857\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006815 - Val Loss (simple RMSE, no physics involved): 0.071999\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006767 - Val Loss (simple RMSE, no physics involved): 0.069257\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006738 - Val Loss (simple RMSE, no physics involved): 0.068923\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006663 - Val Loss (simple RMSE, no physics involved): 0.068883\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006806 - Val Loss (simple RMSE, no physics involved): 0.070407\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006703 - Val Loss (simple RMSE, no physics involved): 0.068494\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006789 - Val Loss (simple RMSE, no physics involved): 0.067589\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006716 - Val Loss (simple RMSE, no physics involved): 0.066900\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006702 - Val Loss (simple RMSE, no physics involved): 0.069061\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006744 - Val Loss (simple RMSE, no physics involved): 0.068810\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006674 - Val Loss (simple RMSE, no physics involved): 0.067931\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.067039\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006692 - Val Loss (simple RMSE, no physics involved): 0.069872\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006709 - Val Loss (simple RMSE, no physics involved): 0.072194\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006705 - Val Loss (simple RMSE, no physics involved): 0.067229\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:36:23,514] Trial 27 finished with value: 0.06632168715198834 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 183, 'lr': 0.0005262712381300281, 'weight_decay': 4.115542247418014e-08, 'batch_size': 32, 'lambda_phy': 1.1355771711308634e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006937 - Val Loss (simple RMSE, no physics involved): 0.066449\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.017478 - Val Loss (simple RMSE, no physics involved): 0.084754\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007756 - Val Loss (simple RMSE, no physics involved): 0.071083\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007407 - Val Loss (simple RMSE, no physics involved): 0.068852\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007192 - Val Loss (simple RMSE, no physics involved): 0.071464\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007243 - Val Loss (simple RMSE, no physics involved): 0.069688\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007075 - Val Loss (simple RMSE, no physics involved): 0.070166\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007040 - Val Loss (simple RMSE, no physics involved): 0.066507\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007054 - Val Loss (simple RMSE, no physics involved): 0.066018\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007071 - Val Loss (simple RMSE, no physics involved): 0.066231\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.067836\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007120 - Val Loss (simple RMSE, no physics involved): 0.074653\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006996 - Val Loss (simple RMSE, no physics involved): 0.067261\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006925 - Val Loss (simple RMSE, no physics involved): 0.067859\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006898 - Val Loss (simple RMSE, no physics involved): 0.069568\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006848 - Val Loss (simple RMSE, no physics involved): 0.068458\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006980 - Val Loss (simple RMSE, no physics involved): 0.071929\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006899 - Val Loss (simple RMSE, no physics involved): 0.068152\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.070373\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006919 - Val Loss (simple RMSE, no physics involved): 0.067801\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.065543\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006927 - Val Loss (simple RMSE, no physics involved): 0.065276\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.066075\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006738 - Val Loss (simple RMSE, no physics involved): 0.067500\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006844 - Val Loss (simple RMSE, no physics involved): 0.065661\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006766 - Val Loss (simple RMSE, no physics involved): 0.068319\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006733 - Val Loss (simple RMSE, no physics involved): 0.067261\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006813 - Val Loss (simple RMSE, no physics involved): 0.071686\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006807 - Val Loss (simple RMSE, no physics involved): 0.067849\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006762 - Val Loss (simple RMSE, no physics involved): 0.067537\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006752 - Val Loss (simple RMSE, no physics involved): 0.071093\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006713 - Val Loss (simple RMSE, no physics involved): 0.067492\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006705 - Val Loss (simple RMSE, no physics involved): 0.067968\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006689 - Val Loss (simple RMSE, no physics involved): 0.069996\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006668 - Val Loss (simple RMSE, no physics involved): 0.066162\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006668 - Val Loss (simple RMSE, no physics involved): 0.068328\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006733 - Val Loss (simple RMSE, no physics involved): 0.065772\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006672 - Val Loss (simple RMSE, no physics involved): 0.070095\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006659 - Val Loss (simple RMSE, no physics involved): 0.067899\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006612 - Val Loss (simple RMSE, no physics involved): 0.067025\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006631 - Val Loss (simple RMSE, no physics involved): 0.066420\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006693 - Val Loss (simple RMSE, no physics involved): 0.071486\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006653 - Val Loss (simple RMSE, no physics involved): 0.066402\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006636 - Val Loss (simple RMSE, no physics involved): 0.066617\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006741 - Val Loss (simple RMSE, no physics involved): 0.067390\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006624 - Val Loss (simple RMSE, no physics involved): 0.069800\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006527 - Val Loss (simple RMSE, no physics involved): 0.068575\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006596 - Val Loss (simple RMSE, no physics involved): 0.066682\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006564 - Val Loss (simple RMSE, no physics involved): 0.067185\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006618 - Val Loss (simple RMSE, no physics involved): 0.070173\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:41:03,544] Trial 28 finished with value: 0.06527590689559777 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 237, 'lr': 0.00020245741315629408, 'weight_decay': 1.262210349019658e-08, 'batch_size': 8, 'lambda_phy': 6.660340338064146e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006539 - Val Loss (simple RMSE, no physics involved): 0.066144\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.070324 - Val Loss (simple RMSE, no physics involved): 0.129525\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.022494 - Val Loss (simple RMSE, no physics involved): 0.116086\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.016134 - Val Loss (simple RMSE, no physics involved): 0.100732\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.012082 - Val Loss (simple RMSE, no physics involved): 0.078987\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.009137 - Val Loss (simple RMSE, no physics involved): 0.074839\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.008097 - Val Loss (simple RMSE, no physics involved): 0.073673\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007719 - Val Loss (simple RMSE, no physics involved): 0.071271\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007550 - Val Loss (simple RMSE, no physics involved): 0.072858\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007463 - Val Loss (simple RMSE, no physics involved): 0.069900\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007386 - Val Loss (simple RMSE, no physics involved): 0.068788\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007348 - Val Loss (simple RMSE, no physics involved): 0.070085\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007285 - Val Loss (simple RMSE, no physics involved): 0.070763\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007242 - Val Loss (simple RMSE, no physics involved): 0.069575\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007199 - Val Loss (simple RMSE, no physics involved): 0.071455\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007185 - Val Loss (simple RMSE, no physics involved): 0.067890\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007156 - Val Loss (simple RMSE, no physics involved): 0.070429\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007139 - Val Loss (simple RMSE, no physics involved): 0.070698\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007115 - Val Loss (simple RMSE, no physics involved): 0.069613\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007093 - Val Loss (simple RMSE, no physics involved): 0.067762\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007083 - Val Loss (simple RMSE, no physics involved): 0.067083\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007083 - Val Loss (simple RMSE, no physics involved): 0.068633\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007052 - Val Loss (simple RMSE, no physics involved): 0.068911\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.070770\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007025 - Val Loss (simple RMSE, no physics involved): 0.068776\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007011 - Val Loss (simple RMSE, no physics involved): 0.068650\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006997 - Val Loss (simple RMSE, no physics involved): 0.067551\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007000 - Val Loss (simple RMSE, no physics involved): 0.067642\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006992 - Val Loss (simple RMSE, no physics involved): 0.067233\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006978 - Val Loss (simple RMSE, no physics involved): 0.068045\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006968 - Val Loss (simple RMSE, no physics involved): 0.068318\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.066799\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.068229\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006958 - Val Loss (simple RMSE, no physics involved): 0.066935\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.069048\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.068050\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006959 - Val Loss (simple RMSE, no physics involved): 0.067191\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006920 - Val Loss (simple RMSE, no physics involved): 0.069218\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006896 - Val Loss (simple RMSE, no physics involved): 0.066519\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006903 - Val Loss (simple RMSE, no physics involved): 0.067723\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.068511\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006904 - Val Loss (simple RMSE, no physics involved): 0.066852\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006905 - Val Loss (simple RMSE, no physics involved): 0.068299\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.067339\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006883 - Val Loss (simple RMSE, no physics involved): 0.069553\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.070175\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006909 - Val Loss (simple RMSE, no physics involved): 0.067852\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006870 - Val Loss (simple RMSE, no physics involved): 0.067881\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006853 - Val Loss (simple RMSE, no physics involved): 0.068449\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006847 - Val Loss (simple RMSE, no physics involved): 0.067683\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:45:01,376] Trial 29 finished with value: 0.06651856719205777 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 143, 'lr': 5.165548056319211e-05, 'weight_decay': 1.5206889817704892e-06, 'batch_size': 8, 'lambda_phy': 0.0014743906481157626}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006864 - Val Loss (simple RMSE, no physics involved): 0.067115\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.019690 - Val Loss (simple RMSE, no physics involved): 0.077080\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.009317 - Val Loss (simple RMSE, no physics involved): 0.072334\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.008621 - Val Loss (simple RMSE, no physics involved): 0.067083\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.008645 - Val Loss (simple RMSE, no physics involved): 0.067021\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.008557 - Val Loss (simple RMSE, no physics involved): 0.075313\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.008412 - Val Loss (simple RMSE, no physics involved): 0.069757\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.008540 - Val Loss (simple RMSE, no physics involved): 0.066827\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.008563 - Val Loss (simple RMSE, no physics involved): 0.071956\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.008423 - Val Loss (simple RMSE, no physics involved): 0.067601\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.008453 - Val Loss (simple RMSE, no physics involved): 0.080360\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.008335 - Val Loss (simple RMSE, no physics involved): 0.067341\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.008604 - Val Loss (simple RMSE, no physics involved): 0.068936\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.009415 - Val Loss (simple RMSE, no physics involved): 0.074095\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.008642 - Val Loss (simple RMSE, no physics involved): 0.067171\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.008230 - Val Loss (simple RMSE, no physics involved): 0.069087\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.008261 - Val Loss (simple RMSE, no physics involved): 0.066965\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.008302 - Val Loss (simple RMSE, no physics involved): 0.066091\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.008358 - Val Loss (simple RMSE, no physics involved): 0.068299\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.008281 - Val Loss (simple RMSE, no physics involved): 0.066486\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.008354 - Val Loss (simple RMSE, no physics involved): 0.069430\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.008464 - Val Loss (simple RMSE, no physics involved): 0.068909\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.008205 - Val Loss (simple RMSE, no physics involved): 0.073453\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.008326 - Val Loss (simple RMSE, no physics involved): 0.066041\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.008783 - Val Loss (simple RMSE, no physics involved): 0.066716\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.008441 - Val Loss (simple RMSE, no physics involved): 0.069303\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.008451 - Val Loss (simple RMSE, no physics involved): 0.070135\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.008297 - Val Loss (simple RMSE, no physics involved): 0.065900\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.008966 - Val Loss (simple RMSE, no physics involved): 0.067716\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.008358 - Val Loss (simple RMSE, no physics involved): 0.067182\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.008262 - Val Loss (simple RMSE, no physics involved): 0.066063\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.008317 - Val Loss (simple RMSE, no physics involved): 0.075791\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.008201 - Val Loss (simple RMSE, no physics involved): 0.067329\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.008128 - Val Loss (simple RMSE, no physics involved): 0.066697\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.008157 - Val Loss (simple RMSE, no physics involved): 0.068712\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.008343 - Val Loss (simple RMSE, no physics involved): 0.076833\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.008422 - Val Loss (simple RMSE, no physics involved): 0.068802\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.008179 - Val Loss (simple RMSE, no physics involved): 0.066021\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.008195 - Val Loss (simple RMSE, no physics involved): 0.065598\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.008504 - Val Loss (simple RMSE, no physics involved): 0.069087\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.008105 - Val Loss (simple RMSE, no physics involved): 0.066429\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.008271 - Val Loss (simple RMSE, no physics involved): 0.066451\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.008157 - Val Loss (simple RMSE, no physics involved): 0.070078\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.008179 - Val Loss (simple RMSE, no physics involved): 0.065416\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.008195 - Val Loss (simple RMSE, no physics involved): 0.066065\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.008154 - Val Loss (simple RMSE, no physics involved): 0.065579\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.008282 - Val Loss (simple RMSE, no physics involved): 0.067365\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.008154 - Val Loss (simple RMSE, no physics involved): 0.065707\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.008296 - Val Loss (simple RMSE, no physics involved): 0.068854\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.008270 - Val Loss (simple RMSE, no physics involved): 0.067527\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:48:38,625] Trial 30 finished with value: 0.06541624044378598 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 163, 'lr': 0.0034269785707426487, 'weight_decay': 2.3409250404061675e-05, 'batch_size': 16, 'lambda_phy': 0.0636556510853346}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.008433 - Val Loss (simple RMSE, no physics involved): 0.065589\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.014995 - Val Loss (simple RMSE, no physics involved): 0.079395\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007591 - Val Loss (simple RMSE, no physics involved): 0.069555\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007483 - Val Loss (simple RMSE, no physics involved): 0.069149\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007190 - Val Loss (simple RMSE, no physics involved): 0.067562\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007391 - Val Loss (simple RMSE, no physics involved): 0.073428\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007152 - Val Loss (simple RMSE, no physics involved): 0.070864\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007063 - Val Loss (simple RMSE, no physics involved): 0.067083\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007126 - Val Loss (simple RMSE, no physics involved): 0.066311\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007047 - Val Loss (simple RMSE, no physics involved): 0.067305\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007024 - Val Loss (simple RMSE, no physics involved): 0.069962\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006959 - Val Loss (simple RMSE, no physics involved): 0.072674\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006934 - Val Loss (simple RMSE, no physics involved): 0.069526\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006883 - Val Loss (simple RMSE, no physics involved): 0.068119\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006860 - Val Loss (simple RMSE, no physics involved): 0.067111\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006857 - Val Loss (simple RMSE, no physics involved): 0.073587\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007004 - Val Loss (simple RMSE, no physics involved): 0.066443\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006874 - Val Loss (simple RMSE, no physics involved): 0.065471\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006854 - Val Loss (simple RMSE, no physics involved): 0.069374\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.069365\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.066067\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.068883\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006973 - Val Loss (simple RMSE, no physics involved): 0.068190\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006887 - Val Loss (simple RMSE, no physics involved): 0.067600\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006758 - Val Loss (simple RMSE, no physics involved): 0.072045\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006745 - Val Loss (simple RMSE, no physics involved): 0.073120\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006827 - Val Loss (simple RMSE, no physics involved): 0.066696\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.065956\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006802 - Val Loss (simple RMSE, no physics involved): 0.066623\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.068453\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006775 - Val Loss (simple RMSE, no physics involved): 0.067713\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006707 - Val Loss (simple RMSE, no physics involved): 0.068733\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006725 - Val Loss (simple RMSE, no physics involved): 0.070735\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006672 - Val Loss (simple RMSE, no physics involved): 0.072997\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006643 - Val Loss (simple RMSE, no physics involved): 0.070005\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006652 - Val Loss (simple RMSE, no physics involved): 0.066420\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006680 - Val Loss (simple RMSE, no physics involved): 0.067876\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006689 - Val Loss (simple RMSE, no physics involved): 0.070993\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006600 - Val Loss (simple RMSE, no physics involved): 0.073751\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006856 - Val Loss (simple RMSE, no physics involved): 0.075926\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006619 - Val Loss (simple RMSE, no physics involved): 0.067058\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006608 - Val Loss (simple RMSE, no physics involved): 0.069958\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006615 - Val Loss (simple RMSE, no physics involved): 0.067045\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006600 - Val Loss (simple RMSE, no physics involved): 0.073257\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006642 - Val Loss (simple RMSE, no physics involved): 0.067380\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006661 - Val Loss (simple RMSE, no physics involved): 0.072379\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006593 - Val Loss (simple RMSE, no physics involved): 0.066836\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006585 - Val Loss (simple RMSE, no physics involved): 0.066486\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006534 - Val Loss (simple RMSE, no physics involved): 0.066315\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006539 - Val Loss (simple RMSE, no physics involved): 0.067224\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:53:06,086] Trial 31 finished with value: 0.06547138808916013 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 214, 'lr': 0.0002789081070176508, 'weight_decay': 3.0600844935852513e-07, 'batch_size': 8, 'lambda_phy': 9.646358348101096e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006590 - Val Loss (simple RMSE, no physics involved): 0.067122\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.012626 - Val Loss (simple RMSE, no physics involved): 0.069084\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007497 - Val Loss (simple RMSE, no physics involved): 0.075794\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007218 - Val Loss (simple RMSE, no physics involved): 0.073796\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007135 - Val Loss (simple RMSE, no physics involved): 0.069582\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007232 - Val Loss (simple RMSE, no physics involved): 0.067027\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007049 - Val Loss (simple RMSE, no physics involved): 0.066385\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.006985 - Val Loss (simple RMSE, no physics involved): 0.068239\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.071531\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006914 - Val Loss (simple RMSE, no physics involved): 0.070836\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006993 - Val Loss (simple RMSE, no physics involved): 0.073706\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007093 - Val Loss (simple RMSE, no physics involved): 0.066428\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007153 - Val Loss (simple RMSE, no physics involved): 0.066573\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006993 - Val Loss (simple RMSE, no physics involved): 0.070324\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006831 - Val Loss (simple RMSE, no physics involved): 0.076164\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006849 - Val Loss (simple RMSE, no physics involved): 0.066580\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006806 - Val Loss (simple RMSE, no physics involved): 0.067265\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006812 - Val Loss (simple RMSE, no physics involved): 0.069458\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006845 - Val Loss (simple RMSE, no physics involved): 0.071933\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006954 - Val Loss (simple RMSE, no physics involved): 0.068383\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006820 - Val Loss (simple RMSE, no physics involved): 0.067510\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006851 - Val Loss (simple RMSE, no physics involved): 0.069311\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.066013\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006746 - Val Loss (simple RMSE, no physics involved): 0.067273\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006766 - Val Loss (simple RMSE, no physics involved): 0.065414\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.067078\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006708 - Val Loss (simple RMSE, no physics involved): 0.065501\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006756 - Val Loss (simple RMSE, no physics involved): 0.066323\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.068346\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006727 - Val Loss (simple RMSE, no physics involved): 0.068837\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006623 - Val Loss (simple RMSE, no physics involved): 0.068300\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.067060\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006787 - Val Loss (simple RMSE, no physics involved): 0.074089\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006752 - Val Loss (simple RMSE, no physics involved): 0.065607\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006648 - Val Loss (simple RMSE, no physics involved): 0.065535\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006700 - Val Loss (simple RMSE, no physics involved): 0.067916\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006634 - Val Loss (simple RMSE, no physics involved): 0.069856\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006627 - Val Loss (simple RMSE, no physics involved): 0.067838\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006624 - Val Loss (simple RMSE, no physics involved): 0.071055\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006605 - Val Loss (simple RMSE, no physics involved): 0.069332\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006594 - Val Loss (simple RMSE, no physics involved): 0.072146\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006659 - Val Loss (simple RMSE, no physics involved): 0.065260\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006610 - Val Loss (simple RMSE, no physics involved): 0.073703\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006545 - Val Loss (simple RMSE, no physics involved): 0.070872\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006591 - Val Loss (simple RMSE, no physics involved): 0.070379\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006577 - Val Loss (simple RMSE, no physics involved): 0.070334\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006577 - Val Loss (simple RMSE, no physics involved): 0.070226\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006485 - Val Loss (simple RMSE, no physics involved): 0.071010\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006537 - Val Loss (simple RMSE, no physics involved): 0.071495\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006527 - Val Loss (simple RMSE, no physics involved): 0.067196\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 14:57:41,494] Trial 32 finished with value: 0.06526027868191402 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 232, 'lr': 0.0006525078746961807, 'weight_decay': 1.1371497311059903e-07, 'batch_size': 8, 'lambda_phy': 0.00030804159088720364}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006433 - Val Loss (simple RMSE, no physics involved): 0.069682\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.020176 - Val Loss (simple RMSE, no physics involved): 0.078875\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007870 - Val Loss (simple RMSE, no physics involved): 0.069391\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007326 - Val Loss (simple RMSE, no physics involved): 0.075191\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007273 - Val Loss (simple RMSE, no physics involved): 0.072681\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007057 - Val Loss (simple RMSE, no physics involved): 0.066289\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007110 - Val Loss (simple RMSE, no physics involved): 0.071125\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007024 - Val Loss (simple RMSE, no physics involved): 0.067962\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007122 - Val Loss (simple RMSE, no physics involved): 0.068274\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.065682\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006904 - Val Loss (simple RMSE, no physics involved): 0.065895\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006919 - Val Loss (simple RMSE, no physics involved): 0.065472\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006945 - Val Loss (simple RMSE, no physics involved): 0.067137\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.073342\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006993 - Val Loss (simple RMSE, no physics involved): 0.076917\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006890 - Val Loss (simple RMSE, no physics involved): 0.071583\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006967 - Val Loss (simple RMSE, no physics involved): 0.065334\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006818 - Val Loss (simple RMSE, no physics involved): 0.067016\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006827 - Val Loss (simple RMSE, no physics involved): 0.065373\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.066439\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006808 - Val Loss (simple RMSE, no physics involved): 0.066717\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006888 - Val Loss (simple RMSE, no physics involved): 0.068364\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006813 - Val Loss (simple RMSE, no physics involved): 0.066319\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006837 - Val Loss (simple RMSE, no physics involved): 0.073079\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006755 - Val Loss (simple RMSE, no physics involved): 0.068094\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.066140\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006777 - Val Loss (simple RMSE, no physics involved): 0.073023\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006755 - Val Loss (simple RMSE, no physics involved): 0.071998\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006799 - Val Loss (simple RMSE, no physics involved): 0.070282\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006713 - Val Loss (simple RMSE, no physics involved): 0.065671\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006728 - Val Loss (simple RMSE, no physics involved): 0.065657\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006775 - Val Loss (simple RMSE, no physics involved): 0.067812\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.066986\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006726 - Val Loss (simple RMSE, no physics involved): 0.067347\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006755 - Val Loss (simple RMSE, no physics involved): 0.069903\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.071114\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006683 - Val Loss (simple RMSE, no physics involved): 0.070278\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006761 - Val Loss (simple RMSE, no physics involved): 0.069362\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006608 - Val Loss (simple RMSE, no physics involved): 0.067614\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006641 - Val Loss (simple RMSE, no physics involved): 0.066090\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006612 - Val Loss (simple RMSE, no physics involved): 0.070107\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006700 - Val Loss (simple RMSE, no physics involved): 0.067069\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006684 - Val Loss (simple RMSE, no physics involved): 0.066454\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006683 - Val Loss (simple RMSE, no physics involved): 0.067668\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006568 - Val Loss (simple RMSE, no physics involved): 0.067792\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006659 - Val Loss (simple RMSE, no physics involved): 0.065467\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006625 - Val Loss (simple RMSE, no physics involved): 0.068282\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006611 - Val Loss (simple RMSE, no physics involved): 0.068739\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006581 - Val Loss (simple RMSE, no physics involved): 0.070742\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006665 - Val Loss (simple RMSE, no physics involved): 0.074535\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:01:15,179] Trial 33 finished with value: 0.06533390035231908 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 198, 'lr': 0.00037566795488420246, 'weight_decay': 2.5506687108930395e-07, 'batch_size': 8, 'lambda_phy': 4.251133171161939e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006618 - Val Loss (simple RMSE, no physics involved): 0.074460\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.017595 - Val Loss (simple RMSE, no physics involved): 0.092759\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007742 - Val Loss (simple RMSE, no physics involved): 0.070842\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007251 - Val Loss (simple RMSE, no physics involved): 0.072890\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007133 - Val Loss (simple RMSE, no physics involved): 0.066523\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007065 - Val Loss (simple RMSE, no physics involved): 0.066974\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007040 - Val Loss (simple RMSE, no physics involved): 0.068846\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007138 - Val Loss (simple RMSE, no physics involved): 0.077028\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007039 - Val Loss (simple RMSE, no physics involved): 0.070107\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006996 - Val Loss (simple RMSE, no physics involved): 0.066357\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006961 - Val Loss (simple RMSE, no physics involved): 0.072896\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006905 - Val Loss (simple RMSE, no physics involved): 0.070593\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006926 - Val Loss (simple RMSE, no physics involved): 0.069729\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006886 - Val Loss (simple RMSE, no physics involved): 0.072262\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006943 - Val Loss (simple RMSE, no physics involved): 0.067931\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006940 - Val Loss (simple RMSE, no physics involved): 0.074068\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006946 - Val Loss (simple RMSE, no physics involved): 0.070979\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006845 - Val Loss (simple RMSE, no physics involved): 0.070810\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007110 - Val Loss (simple RMSE, no physics involved): 0.073501\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006898 - Val Loss (simple RMSE, no physics involved): 0.070258\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006809 - Val Loss (simple RMSE, no physics involved): 0.074698\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006921 - Val Loss (simple RMSE, no physics involved): 0.071859\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006806 - Val Loss (simple RMSE, no physics involved): 0.070896\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.067894\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006801 - Val Loss (simple RMSE, no physics involved): 0.066084\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006807 - Val Loss (simple RMSE, no physics involved): 0.067536\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006750 - Val Loss (simple RMSE, no physics involved): 0.067440\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006746 - Val Loss (simple RMSE, no physics involved): 0.069599\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006771 - Val Loss (simple RMSE, no physics involved): 0.072389\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.067612\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006706 - Val Loss (simple RMSE, no physics involved): 0.067954\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006721 - Val Loss (simple RMSE, no physics involved): 0.071981\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006730 - Val Loss (simple RMSE, no physics involved): 0.069158\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006729 - Val Loss (simple RMSE, no physics involved): 0.066507\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.075447\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.065224\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006735 - Val Loss (simple RMSE, no physics involved): 0.069152\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006735 - Val Loss (simple RMSE, no physics involved): 0.065867\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006677 - Val Loss (simple RMSE, no physics involved): 0.069068\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006733 - Val Loss (simple RMSE, no physics involved): 0.066224\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006649 - Val Loss (simple RMSE, no physics involved): 0.065407\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006623 - Val Loss (simple RMSE, no physics involved): 0.067716\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006679 - Val Loss (simple RMSE, no physics involved): 0.072490\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006812 - Val Loss (simple RMSE, no physics involved): 0.068413\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006730 - Val Loss (simple RMSE, no physics involved): 0.070482\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006630 - Val Loss (simple RMSE, no physics involved): 0.071866\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006592 - Val Loss (simple RMSE, no physics involved): 0.068990\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006605 - Val Loss (simple RMSE, no physics involved): 0.070359\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006706 - Val Loss (simple RMSE, no physics involved): 0.070614\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006581 - Val Loss (simple RMSE, no physics involved): 0.070827\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:05:43,030] Trial 34 finished with value: 0.06522431255628665 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 220, 'lr': 0.00022740591589469764, 'weight_decay': 4.8880530996707476e-08, 'batch_size': 8, 'lambda_phy': 1.549942226803522e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006616 - Val Loss (simple RMSE, no physics involved): 0.071085\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.073859 - Val Loss (simple RMSE, no physics involved): 0.160927\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.036633 - Val Loss (simple RMSE, no physics involved): 0.109734\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.020446 - Val Loss (simple RMSE, no physics involved): 0.144600\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.018665 - Val Loss (simple RMSE, no physics involved): 0.109770\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.016663 - Val Loss (simple RMSE, no physics involved): 0.114287\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.015916 - Val Loss (simple RMSE, no physics involved): 0.107941\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.013369 - Val Loss (simple RMSE, no physics involved): 0.099663\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.010997 - Val Loss (simple RMSE, no physics involved): 0.086525\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.008978 - Val Loss (simple RMSE, no physics involved): 0.079390\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.008140 - Val Loss (simple RMSE, no physics involved): 0.076045\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007667 - Val Loss (simple RMSE, no physics involved): 0.073886\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007611 - Val Loss (simple RMSE, no physics involved): 0.075457\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007590 - Val Loss (simple RMSE, no physics involved): 0.074304\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007301 - Val Loss (simple RMSE, no physics involved): 0.072275\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007129 - Val Loss (simple RMSE, no physics involved): 0.069087\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007194 - Val Loss (simple RMSE, no physics involved): 0.069132\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007084 - Val Loss (simple RMSE, no physics involved): 0.070138\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007081 - Val Loss (simple RMSE, no physics involved): 0.069824\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007205 - Val Loss (simple RMSE, no physics involved): 0.070294\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007156 - Val Loss (simple RMSE, no physics involved): 0.071253\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007008 - Val Loss (simple RMSE, no physics involved): 0.069557\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007187 - Val Loss (simple RMSE, no physics involved): 0.068207\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006978 - Val Loss (simple RMSE, no physics involved): 0.069065\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007072 - Val Loss (simple RMSE, no physics involved): 0.068841\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006754 - Val Loss (simple RMSE, no physics involved): 0.067755\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006973 - Val Loss (simple RMSE, no physics involved): 0.070418\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006937 - Val Loss (simple RMSE, no physics involved): 0.070215\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007016 - Val Loss (simple RMSE, no physics involved): 0.069109\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007116 - Val Loss (simple RMSE, no physics involved): 0.073362\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.069431\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006885 - Val Loss (simple RMSE, no physics involved): 0.070690\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006881 - Val Loss (simple RMSE, no physics involved): 0.067235\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006824 - Val Loss (simple RMSE, no physics involved): 0.067684\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006848 - Val Loss (simple RMSE, no physics involved): 0.069451\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007137 - Val Loss (simple RMSE, no physics involved): 0.069845\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.067486\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.070326\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006832 - Val Loss (simple RMSE, no physics involved): 0.067865\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007133 - Val Loss (simple RMSE, no physics involved): 0.067029\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.068492\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006811 - Val Loss (simple RMSE, no physics involved): 0.068214\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007028 - Val Loss (simple RMSE, no physics involved): 0.068840\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006824 - Val Loss (simple RMSE, no physics involved): 0.071526\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006974 - Val Loss (simple RMSE, no physics involved): 0.069361\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006734 - Val Loss (simple RMSE, no physics involved): 0.067439\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006994 - Val Loss (simple RMSE, no physics involved): 0.069802\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006977 - Val Loss (simple RMSE, no physics involved): 0.069824\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006758 - Val Loss (simple RMSE, no physics involved): 0.068218\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006940 - Val Loss (simple RMSE, no physics involved): 0.072360\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:07:06,340] Trial 35 finished with value: 0.06702867150306702 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 187, 'lr': 0.00019790508613289122, 'weight_decay': 3.563112926996187e-08, 'batch_size': 64, 'lambda_phy': 1.6158104799414545e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006844 - Val Loss (simple RMSE, no physics involved): 0.069716\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.018467 - Val Loss (simple RMSE, no physics involved): 0.082824\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007846 - Val Loss (simple RMSE, no physics involved): 0.072990\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007301 - Val Loss (simple RMSE, no physics involved): 0.070419\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007149 - Val Loss (simple RMSE, no physics involved): 0.072740\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007034 - Val Loss (simple RMSE, no physics involved): 0.068295\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007059 - Val Loss (simple RMSE, no physics involved): 0.066364\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007011 - Val Loss (simple RMSE, no physics involved): 0.066191\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007077 - Val Loss (simple RMSE, no physics involved): 0.066334\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.068759\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006998 - Val Loss (simple RMSE, no physics involved): 0.073382\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007053 - Val Loss (simple RMSE, no physics involved): 0.069922\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006886 - Val Loss (simple RMSE, no physics involved): 0.067245\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006841 - Val Loss (simple RMSE, no physics involved): 0.066019\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006955 - Val Loss (simple RMSE, no physics involved): 0.066511\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006918 - Val Loss (simple RMSE, no physics involved): 0.066395\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.071063\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006820 - Val Loss (simple RMSE, no physics involved): 0.065871\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.067131\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006841 - Val Loss (simple RMSE, no physics involved): 0.074297\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006803 - Val Loss (simple RMSE, no physics involved): 0.074676\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006850 - Val Loss (simple RMSE, no physics involved): 0.066265\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.065732\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006848 - Val Loss (simple RMSE, no physics involved): 0.069164\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006839 - Val Loss (simple RMSE, no physics involved): 0.067934\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.065727\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006825 - Val Loss (simple RMSE, no physics involved): 0.065709\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006939 - Val Loss (simple RMSE, no physics involved): 0.070341\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006862 - Val Loss (simple RMSE, no physics involved): 0.072723\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006713 - Val Loss (simple RMSE, no physics involved): 0.068312\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006701 - Val Loss (simple RMSE, no physics involved): 0.068876\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006830 - Val Loss (simple RMSE, no physics involved): 0.066369\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006721 - Val Loss (simple RMSE, no physics involved): 0.067377\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006730 - Val Loss (simple RMSE, no physics involved): 0.073779\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006902 - Val Loss (simple RMSE, no physics involved): 0.065303\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.066118\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006800 - Val Loss (simple RMSE, no physics involved): 0.069862\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006658 - Val Loss (simple RMSE, no physics involved): 0.067655\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006979 - Val Loss (simple RMSE, no physics involved): 0.066660\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006748 - Val Loss (simple RMSE, no physics involved): 0.075944\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006720 - Val Loss (simple RMSE, no physics involved): 0.065464\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006764 - Val Loss (simple RMSE, no physics involved): 0.065798\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006773 - Val Loss (simple RMSE, no physics involved): 0.069126\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006639 - Val Loss (simple RMSE, no physics involved): 0.065269\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006632 - Val Loss (simple RMSE, no physics involved): 0.067396\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006696 - Val Loss (simple RMSE, no physics involved): 0.070712\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006625 - Val Loss (simple RMSE, no physics involved): 0.066272\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006639 - Val Loss (simple RMSE, no physics involved): 0.068033\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006598 - Val Loss (simple RMSE, no physics involved): 0.070087\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006609 - Val Loss (simple RMSE, no physics involved): 0.069290\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:08:47,533] Trial 36 finished with value: 0.06526868728299935 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 238, 'lr': 0.0008022050169435659, 'weight_decay': 6.024717625939958e-08, 'batch_size': 16, 'lambda_phy': 1.8145890781342935e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006543 - Val Loss (simple RMSE, no physics involved): 0.065849\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.036482 - Val Loss (simple RMSE, no physics involved): 0.113431\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.013758 - Val Loss (simple RMSE, no physics involved): 0.090943\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.009758 - Val Loss (simple RMSE, no physics involved): 0.080142\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.008178 - Val Loss (simple RMSE, no physics involved): 0.073870\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007611 - Val Loss (simple RMSE, no physics involved): 0.077697\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007398 - Val Loss (simple RMSE, no physics involved): 0.068518\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007255 - Val Loss (simple RMSE, no physics involved): 0.072440\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007162 - Val Loss (simple RMSE, no physics involved): 0.069108\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007091 - Val Loss (simple RMSE, no physics involved): 0.071093\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007073 - Val Loss (simple RMSE, no physics involved): 0.068984\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007028 - Val Loss (simple RMSE, no physics involved): 0.068272\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007004 - Val Loss (simple RMSE, no physics involved): 0.068064\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006976 - Val Loss (simple RMSE, no physics involved): 0.067007\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006999 - Val Loss (simple RMSE, no physics involved): 0.072476\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006960 - Val Loss (simple RMSE, no physics involved): 0.067379\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006942 - Val Loss (simple RMSE, no physics involved): 0.071340\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006938 - Val Loss (simple RMSE, no physics involved): 0.067464\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006924 - Val Loss (simple RMSE, no physics involved): 0.070050\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006906 - Val Loss (simple RMSE, no physics involved): 0.070436\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.068378\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006887 - Val Loss (simple RMSE, no physics involved): 0.068647\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.068123\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006890 - Val Loss (simple RMSE, no physics involved): 0.067475\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.067040\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.067013\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006873 - Val Loss (simple RMSE, no physics involved): 0.067116\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006846 - Val Loss (simple RMSE, no physics involved): 0.066832\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006831 - Val Loss (simple RMSE, no physics involved): 0.069394\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006824 - Val Loss (simple RMSE, no physics involved): 0.068342\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006815 - Val Loss (simple RMSE, no physics involved): 0.068975\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006830 - Val Loss (simple RMSE, no physics involved): 0.068872\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006832 - Val Loss (simple RMSE, no physics involved): 0.070904\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006850 - Val Loss (simple RMSE, no physics involved): 0.070863\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.068238\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006800 - Val Loss (simple RMSE, no physics involved): 0.067858\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006780 - Val Loss (simple RMSE, no physics involved): 0.068505\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.067026\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006797 - Val Loss (simple RMSE, no physics involved): 0.072050\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006807 - Val Loss (simple RMSE, no physics involved): 0.069161\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006778 - Val Loss (simple RMSE, no physics involved): 0.067469\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006756 - Val Loss (simple RMSE, no physics involved): 0.073703\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.067259\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006735 - Val Loss (simple RMSE, no physics involved): 0.066522\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006719 - Val Loss (simple RMSE, no physics involved): 0.068073\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006747 - Val Loss (simple RMSE, no physics involved): 0.067902\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006727 - Val Loss (simple RMSE, no physics involved): 0.067590\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006708 - Val Loss (simple RMSE, no physics involved): 0.070515\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006708 - Val Loss (simple RMSE, no physics involved): 0.066566\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.065984\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:12:19,318] Trial 37 finished with value: 0.0659839113553365 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 207, 'lr': 6.896682541779389e-05, 'weight_decay': 6.859482737751319e-07, 'batch_size': 8, 'lambda_phy': 3.226819193358119e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006721 - Val Loss (simple RMSE, no physics involved): 0.067473\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.020548 - Val Loss (simple RMSE, no physics involved): 0.124662\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.011824 - Val Loss (simple RMSE, no physics involved): 0.074741\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.008243 - Val Loss (simple RMSE, no physics involved): 0.079072\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007615 - Val Loss (simple RMSE, no physics involved): 0.073753\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007425 - Val Loss (simple RMSE, no physics involved): 0.070454\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007672 - Val Loss (simple RMSE, no physics involved): 0.069549\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007330 - Val Loss (simple RMSE, no physics involved): 0.074545\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007120 - Val Loss (simple RMSE, no physics involved): 0.072657\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007374 - Val Loss (simple RMSE, no physics involved): 0.067702\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007332 - Val Loss (simple RMSE, no physics involved): 0.068226\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007078 - Val Loss (simple RMSE, no physics involved): 0.071384\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007101 - Val Loss (simple RMSE, no physics involved): 0.067495\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007391 - Val Loss (simple RMSE, no physics involved): 0.068823\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007098 - Val Loss (simple RMSE, no physics involved): 0.072504\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007146 - Val Loss (simple RMSE, no physics involved): 0.067745\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007162 - Val Loss (simple RMSE, no physics involved): 0.067773\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007316 - Val Loss (simple RMSE, no physics involved): 0.072646\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007089 - Val Loss (simple RMSE, no physics involved): 0.068773\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007194 - Val Loss (simple RMSE, no physics involved): 0.072688\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007258 - Val Loss (simple RMSE, no physics involved): 0.067653\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007447 - Val Loss (simple RMSE, no physics involved): 0.070037\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007077 - Val Loss (simple RMSE, no physics involved): 0.075389\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007215 - Val Loss (simple RMSE, no physics involved): 0.067754\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007327 - Val Loss (simple RMSE, no physics involved): 0.067761\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007261 - Val Loss (simple RMSE, no physics involved): 0.073556\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007342 - Val Loss (simple RMSE, no physics involved): 0.072980\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007166 - Val Loss (simple RMSE, no physics involved): 0.073995\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007066 - Val Loss (simple RMSE, no physics involved): 0.070271\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007042 - Val Loss (simple RMSE, no physics involved): 0.068087\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007353 - Val Loss (simple RMSE, no physics involved): 0.068636\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007120 - Val Loss (simple RMSE, no physics involved): 0.069321\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007064 - Val Loss (simple RMSE, no physics involved): 0.067550\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007108 - Val Loss (simple RMSE, no physics involved): 0.072058\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007089 - Val Loss (simple RMSE, no physics involved): 0.068533\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007144 - Val Loss (simple RMSE, no physics involved): 0.073690\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007079 - Val Loss (simple RMSE, no physics involved): 0.070854\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007036 - Val Loss (simple RMSE, no physics involved): 0.068936\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007051 - Val Loss (simple RMSE, no physics involved): 0.074343\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007131 - Val Loss (simple RMSE, no physics involved): 0.067213\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007051 - Val Loss (simple RMSE, no physics involved): 0.070586\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007174 - Val Loss (simple RMSE, no physics involved): 0.074110\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007102 - Val Loss (simple RMSE, no physics involved): 0.070218\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007041 - Val Loss (simple RMSE, no physics involved): 0.067388\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007099 - Val Loss (simple RMSE, no physics involved): 0.068167\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007014 - Val Loss (simple RMSE, no physics involved): 0.067912\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007027 - Val Loss (simple RMSE, no physics involved): 0.072679\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.007042 - Val Loss (simple RMSE, no physics involved): 0.067039\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007099 - Val Loss (simple RMSE, no physics involved): 0.069108\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007101 - Val Loss (simple RMSE, no physics involved): 0.069670\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:14:06,282] Trial 38 finished with value: 0.06703913335998853 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 166, 'lr': 0.0013575365421843082, 'weight_decay': 0.00021804620808251637, 'batch_size': 32, 'lambda_phy': 1.7926401769707202e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.007136 - Val Loss (simple RMSE, no physics involved): 0.071083\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.059158 - Val Loss (simple RMSE, no physics involved): 0.140835\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.028960 - Val Loss (simple RMSE, no physics involved): 0.106163\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.019110 - Val Loss (simple RMSE, no physics involved): 0.105491\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.016084 - Val Loss (simple RMSE, no physics involved): 0.100328\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.014055 - Val Loss (simple RMSE, no physics involved): 0.095185\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.012302 - Val Loss (simple RMSE, no physics involved): 0.085647\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.010861 - Val Loss (simple RMSE, no physics involved): 0.081125\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.009729 - Val Loss (simple RMSE, no physics involved): 0.078314\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.008854 - Val Loss (simple RMSE, no physics involved): 0.075973\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.008320 - Val Loss (simple RMSE, no physics involved): 0.073918\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007968 - Val Loss (simple RMSE, no physics involved): 0.073023\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007746 - Val Loss (simple RMSE, no physics involved): 0.072330\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007607 - Val Loss (simple RMSE, no physics involved): 0.071510\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007500 - Val Loss (simple RMSE, no physics involved): 0.070289\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007436 - Val Loss (simple RMSE, no physics involved): 0.070534\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007370 - Val Loss (simple RMSE, no physics involved): 0.070805\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007333 - Val Loss (simple RMSE, no physics involved): 0.070309\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007312 - Val Loss (simple RMSE, no physics involved): 0.069050\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007262 - Val Loss (simple RMSE, no physics involved): 0.072256\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007233 - Val Loss (simple RMSE, no physics involved): 0.069752\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007216 - Val Loss (simple RMSE, no physics involved): 0.069409\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007193 - Val Loss (simple RMSE, no physics involved): 0.068809\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007175 - Val Loss (simple RMSE, no physics involved): 0.068999\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007154 - Val Loss (simple RMSE, no physics involved): 0.070092\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007162 - Val Loss (simple RMSE, no physics involved): 0.068508\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007146 - Val Loss (simple RMSE, no physics involved): 0.069303\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007116 - Val Loss (simple RMSE, no physics involved): 0.068146\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007124 - Val Loss (simple RMSE, no physics involved): 0.069579\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007092 - Val Loss (simple RMSE, no physics involved): 0.069332\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007086 - Val Loss (simple RMSE, no physics involved): 0.068943\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007065 - Val Loss (simple RMSE, no physics involved): 0.067783\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007061 - Val Loss (simple RMSE, no physics involved): 0.068228\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007047 - Val Loss (simple RMSE, no physics involved): 0.068396\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007045 - Val Loss (simple RMSE, no physics involved): 0.069037\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007044 - Val Loss (simple RMSE, no physics involved): 0.068850\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007029 - Val Loss (simple RMSE, no physics involved): 0.068495\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007026 - Val Loss (simple RMSE, no physics involved): 0.068655\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007019 - Val Loss (simple RMSE, no physics involved): 0.067956\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.070243\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.007015 - Val Loss (simple RMSE, no physics involved): 0.068496\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006993 - Val Loss (simple RMSE, no physics involved): 0.068286\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006987 - Val Loss (simple RMSE, no physics involved): 0.067919\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007004 - Val Loss (simple RMSE, no physics involved): 0.068173\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006988 - Val Loss (simple RMSE, no physics involved): 0.069284\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006985 - Val Loss (simple RMSE, no physics involved): 0.068748\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006971 - Val Loss (simple RMSE, no physics involved): 0.067243\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006979 - Val Loss (simple RMSE, no physics involved): 0.068327\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006966 - Val Loss (simple RMSE, no physics involved): 0.070329\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006974 - Val Loss (simple RMSE, no physics involved): 0.067616\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:17:21,279] Trial 39 finished with value: 0.06724333111196756 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 148, 'lr': 2.5497592157031065e-05, 'weight_decay': 1.0672724061811563e-08, 'batch_size': 8, 'lambda_phy': 2.673930303682319e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006957 - Val Loss (simple RMSE, no physics involved): 0.068786\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.022521 - Val Loss (simple RMSE, no physics involved): 0.112609\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.019638 - Val Loss (simple RMSE, no physics involved): 0.120442\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.018946 - Val Loss (simple RMSE, no physics involved): 0.117128\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.018490 - Val Loss (simple RMSE, no physics involved): 0.112005\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.016918 - Val Loss (simple RMSE, no physics involved): 0.107426\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.015467 - Val Loss (simple RMSE, no physics involved): 0.098288\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.013982 - Val Loss (simple RMSE, no physics involved): 0.095036\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.012229 - Val Loss (simple RMSE, no physics involved): 0.084769\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.010754 - Val Loss (simple RMSE, no physics involved): 0.085861\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.009888 - Val Loss (simple RMSE, no physics involved): 0.082016\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.009126 - Val Loss (simple RMSE, no physics involved): 0.077472\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.008873 - Val Loss (simple RMSE, no physics involved): 0.077947\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.008719 - Val Loss (simple RMSE, no physics involved): 0.076670\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.008596 - Val Loss (simple RMSE, no physics involved): 0.073745\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.008600 - Val Loss (simple RMSE, no physics involved): 0.074266\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.008145 - Val Loss (simple RMSE, no physics involved): 0.073404\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.008097 - Val Loss (simple RMSE, no physics involved): 0.072109\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.008096 - Val Loss (simple RMSE, no physics involved): 0.071492\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.008060 - Val Loss (simple RMSE, no physics involved): 0.070144\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.008215 - Val Loss (simple RMSE, no physics involved): 0.070785\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.008070 - Val Loss (simple RMSE, no physics involved): 0.071735\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.008039 - Val Loss (simple RMSE, no physics involved): 0.072092\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.008017 - Val Loss (simple RMSE, no physics involved): 0.071558\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007816 - Val Loss (simple RMSE, no physics involved): 0.071049\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007970 - Val Loss (simple RMSE, no physics involved): 0.070184\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007876 - Val Loss (simple RMSE, no physics involved): 0.069379\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007904 - Val Loss (simple RMSE, no physics involved): 0.070015\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.008087 - Val Loss (simple RMSE, no physics involved): 0.071139\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007794 - Val Loss (simple RMSE, no physics involved): 0.069682\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.007837 - Val Loss (simple RMSE, no physics involved): 0.069745\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.007656 - Val Loss (simple RMSE, no physics involved): 0.069826\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.007749 - Val Loss (simple RMSE, no physics involved): 0.068374\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.007633 - Val Loss (simple RMSE, no physics involved): 0.068965\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.007784 - Val Loss (simple RMSE, no physics involved): 0.069067\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007775 - Val Loss (simple RMSE, no physics involved): 0.068884\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007733 - Val Loss (simple RMSE, no physics involved): 0.068015\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.007583 - Val Loss (simple RMSE, no physics involved): 0.068314\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.007857 - Val Loss (simple RMSE, no physics involved): 0.068125\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.007807 - Val Loss (simple RMSE, no physics involved): 0.070955\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.008036 - Val Loss (simple RMSE, no physics involved): 0.070645\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.007739 - Val Loss (simple RMSE, no physics involved): 0.072549\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.007881 - Val Loss (simple RMSE, no physics involved): 0.070054\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.007661 - Val Loss (simple RMSE, no physics involved): 0.069785\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.007664 - Val Loss (simple RMSE, no physics involved): 0.069968\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.007623 - Val Loss (simple RMSE, no physics involved): 0.070093\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.007640 - Val Loss (simple RMSE, no physics involved): 0.068977\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.007889 - Val Loss (simple RMSE, no physics involved): 0.068854\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.007679 - Val Loss (simple RMSE, no physics involved): 0.068948\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.007596 - Val Loss (simple RMSE, no physics involved): 0.070390\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:18:00,456] Trial 40 finished with value: 0.06801469624042511 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 37, 'lr': 0.00045940440951977975, 'weight_decay': 1.5831647278803736e-07, 'batch_size': 64, 'lambda_phy': 0.036148419228011264}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.007868 - Val Loss (simple RMSE, no physics involved): 0.070383\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.014552 - Val Loss (simple RMSE, no physics involved): 0.072696\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.007605 - Val Loss (simple RMSE, no physics involved): 0.070661\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007356 - Val Loss (simple RMSE, no physics involved): 0.067695\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007263 - Val Loss (simple RMSE, no physics involved): 0.067932\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007148 - Val Loss (simple RMSE, no physics involved): 0.071146\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007063 - Val Loss (simple RMSE, no physics involved): 0.068098\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.066179\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.006958 - Val Loss (simple RMSE, no physics involved): 0.068945\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006932 - Val Loss (simple RMSE, no physics involved): 0.074053\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007089 - Val Loss (simple RMSE, no physics involved): 0.066712\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006924 - Val Loss (simple RMSE, no physics involved): 0.074167\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006983 - Val Loss (simple RMSE, no physics involved): 0.070935\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006974 - Val Loss (simple RMSE, no physics involved): 0.067084\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006852 - Val Loss (simple RMSE, no physics involved): 0.068442\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006885 - Val Loss (simple RMSE, no physics involved): 0.067505\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.073455\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006807 - Val Loss (simple RMSE, no physics involved): 0.065752\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006888 - Val Loss (simple RMSE, no physics involved): 0.067713\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006898 - Val Loss (simple RMSE, no physics involved): 0.080679\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006830 - Val Loss (simple RMSE, no physics involved): 0.067196\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006760 - Val Loss (simple RMSE, no physics involved): 0.066328\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.065694\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006759 - Val Loss (simple RMSE, no physics involved): 0.070203\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006750 - Val Loss (simple RMSE, no physics involved): 0.074730\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007046 - Val Loss (simple RMSE, no physics involved): 0.067336\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006841 - Val Loss (simple RMSE, no physics involved): 0.075470\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006799 - Val Loss (simple RMSE, no physics involved): 0.066851\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006751 - Val Loss (simple RMSE, no physics involved): 0.070629\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006692 - Val Loss (simple RMSE, no physics involved): 0.068526\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006743 - Val Loss (simple RMSE, no physics involved): 0.065912\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006710 - Val Loss (simple RMSE, no physics involved): 0.067304\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006808 - Val Loss (simple RMSE, no physics involved): 0.068236\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006788 - Val Loss (simple RMSE, no physics involved): 0.073930\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006647 - Val Loss (simple RMSE, no physics involved): 0.067571\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006633 - Val Loss (simple RMSE, no physics involved): 0.067611\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006629 - Val Loss (simple RMSE, no physics involved): 0.070460\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.069538\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006676 - Val Loss (simple RMSE, no physics involved): 0.069612\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006661 - Val Loss (simple RMSE, no physics involved): 0.066388\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006599 - Val Loss (simple RMSE, no physics involved): 0.066420\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006616 - Val Loss (simple RMSE, no physics involved): 0.068376\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006642 - Val Loss (simple RMSE, no physics involved): 0.072230\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006640 - Val Loss (simple RMSE, no physics involved): 0.068191\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006615 - Val Loss (simple RMSE, no physics involved): 0.068504\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006610 - Val Loss (simple RMSE, no physics involved): 0.069505\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006571 - Val Loss (simple RMSE, no physics involved): 0.072280\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006733 - Val Loss (simple RMSE, no physics involved): 0.072794\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006627 - Val Loss (simple RMSE, no physics involved): 0.071069\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006506 - Val Loss (simple RMSE, no physics involved): 0.073387\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:22:29,702] Trial 41 finished with value: 0.06564181391149759 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 221, 'lr': 0.00025052761454084585, 'weight_decay': 6.088197250939076e-08, 'batch_size': 8, 'lambda_phy': 7.146808901579117e-05}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006632 - Val Loss (simple RMSE, no physics involved): 0.065642\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.024160 - Val Loss (simple RMSE, no physics involved): 0.097719\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.008932 - Val Loss (simple RMSE, no physics involved): 0.069650\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007549 - Val Loss (simple RMSE, no physics involved): 0.071895\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007284 - Val Loss (simple RMSE, no physics involved): 0.074143\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007128 - Val Loss (simple RMSE, no physics involved): 0.069718\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007133 - Val Loss (simple RMSE, no physics involved): 0.067593\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007072 - Val Loss (simple RMSE, no physics involved): 0.070886\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007060 - Val Loss (simple RMSE, no physics involved): 0.067123\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007009 - Val Loss (simple RMSE, no physics involved): 0.067047\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007004 - Val Loss (simple RMSE, no physics involved): 0.068197\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006968 - Val Loss (simple RMSE, no physics involved): 0.071349\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006963 - Val Loss (simple RMSE, no physics involved): 0.069735\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006873 - Val Loss (simple RMSE, no physics involved): 0.070015\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006859 - Val Loss (simple RMSE, no physics involved): 0.067056\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.065869\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006827 - Val Loss (simple RMSE, no physics involved): 0.071907\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006903 - Val Loss (simple RMSE, no physics involved): 0.065802\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006806 - Val Loss (simple RMSE, no physics involved): 0.069312\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006785 - Val Loss (simple RMSE, no physics involved): 0.066081\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006826 - Val Loss (simple RMSE, no physics involved): 0.066314\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006785 - Val Loss (simple RMSE, no physics involved): 0.066915\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006789 - Val Loss (simple RMSE, no physics involved): 0.067800\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006800 - Val Loss (simple RMSE, no physics involved): 0.067155\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006772 - Val Loss (simple RMSE, no physics involved): 0.065407\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006841 - Val Loss (simple RMSE, no physics involved): 0.066223\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006753 - Val Loss (simple RMSE, no physics involved): 0.065095\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006780 - Val Loss (simple RMSE, no physics involved): 0.065769\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006748 - Val Loss (simple RMSE, no physics involved): 0.068594\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006715 - Val Loss (simple RMSE, no physics involved): 0.065924\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006694 - Val Loss (simple RMSE, no physics involved): 0.067324\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006667 - Val Loss (simple RMSE, no physics involved): 0.069028\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006715 - Val Loss (simple RMSE, no physics involved): 0.066601\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006695 - Val Loss (simple RMSE, no physics involved): 0.066681\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006681 - Val Loss (simple RMSE, no physics involved): 0.078726\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006654 - Val Loss (simple RMSE, no physics involved): 0.071668\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006653 - Val Loss (simple RMSE, no physics involved): 0.073079\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006629 - Val Loss (simple RMSE, no physics involved): 0.068132\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006643 - Val Loss (simple RMSE, no physics involved): 0.067735\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006607 - Val Loss (simple RMSE, no physics involved): 0.066653\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006620 - Val Loss (simple RMSE, no physics involved): 0.065478\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006599 - Val Loss (simple RMSE, no physics involved): 0.073115\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006645 - Val Loss (simple RMSE, no physics involved): 0.065429\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006603 - Val Loss (simple RMSE, no physics involved): 0.068822\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006565 - Val Loss (simple RMSE, no physics involved): 0.067063\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006559 - Val Loss (simple RMSE, no physics involved): 0.067432\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006656 - Val Loss (simple RMSE, no physics involved): 0.069060\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006587 - Val Loss (simple RMSE, no physics involved): 0.065744\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006545 - Val Loss (simple RMSE, no physics involved): 0.073296\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006598 - Val Loss (simple RMSE, no physics involved): 0.072315\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:26:59,282] Trial 42 finished with value: 0.065094539274772 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 217, 'lr': 0.0001536853327477055, 'weight_decay': 1.4535666015525662e-06, 'batch_size': 8, 'lambda_phy': 0.0001561176912380596}. Best is trial 0 with value: 0.06499167283376057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006637 - Val Loss (simple RMSE, no physics involved): 0.065952\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.028829 - Val Loss (simple RMSE, no physics involved): 0.108008\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.009557 - Val Loss (simple RMSE, no physics involved): 0.076422\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007671 - Val Loss (simple RMSE, no physics involved): 0.069257\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007387 - Val Loss (simple RMSE, no physics involved): 0.068140\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007252 - Val Loss (simple RMSE, no physics involved): 0.068588\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007149 - Val Loss (simple RMSE, no physics involved): 0.067163\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007160 - Val Loss (simple RMSE, no physics involved): 0.071877\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007110 - Val Loss (simple RMSE, no physics involved): 0.067634\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007063 - Val Loss (simple RMSE, no physics involved): 0.068014\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007023 - Val Loss (simple RMSE, no physics involved): 0.065805\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007054 - Val Loss (simple RMSE, no physics involved): 0.065925\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007099 - Val Loss (simple RMSE, no physics involved): 0.072752\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007064 - Val Loss (simple RMSE, no physics involved): 0.068519\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006965 - Val Loss (simple RMSE, no physics involved): 0.066897\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.065966\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006952 - Val Loss (simple RMSE, no physics involved): 0.074379\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006943 - Val Loss (simple RMSE, no physics involved): 0.065257\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006981 - Val Loss (simple RMSE, no physics involved): 0.066921\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.069113\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006982 - Val Loss (simple RMSE, no physics involved): 0.067254\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007007 - Val Loss (simple RMSE, no physics involved): 0.067322\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006832 - Val Loss (simple RMSE, no physics involved): 0.072451\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006833 - Val Loss (simple RMSE, no physics involved): 0.069830\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006818 - Val Loss (simple RMSE, no physics involved): 0.065402\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006797 - Val Loss (simple RMSE, no physics involved): 0.066398\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006912 - Val Loss (simple RMSE, no physics involved): 0.065980\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006811 - Val Loss (simple RMSE, no physics involved): 0.071949\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006773 - Val Loss (simple RMSE, no physics involved): 0.068391\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006767 - Val Loss (simple RMSE, no physics involved): 0.069116\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.065596\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006804 - Val Loss (simple RMSE, no physics involved): 0.066950\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006763 - Val Loss (simple RMSE, no physics involved): 0.065088\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006759 - Val Loss (simple RMSE, no physics involved): 0.072085\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006761 - Val Loss (simple RMSE, no physics involved): 0.068863\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006803 - Val Loss (simple RMSE, no physics involved): 0.067635\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006727 - Val Loss (simple RMSE, no physics involved): 0.072318\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006827 - Val Loss (simple RMSE, no physics involved): 0.065075\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006704 - Val Loss (simple RMSE, no physics involved): 0.065401\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006800 - Val Loss (simple RMSE, no physics involved): 0.067541\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006687 - Val Loss (simple RMSE, no physics involved): 0.067648\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.065127\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006794 - Val Loss (simple RMSE, no physics involved): 0.064599\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006744 - Val Loss (simple RMSE, no physics involved): 0.065603\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006692 - Val Loss (simple RMSE, no physics involved): 0.067091\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006682 - Val Loss (simple RMSE, no physics involved): 0.066621\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006662 - Val Loss (simple RMSE, no physics involved): 0.066182\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006674 - Val Loss (simple RMSE, no physics involved): 0.064865\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006690 - Val Loss (simple RMSE, no physics involved): 0.065810\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006642 - Val Loss (simple RMSE, no physics involved): 0.066940\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:31:23,746] Trial 43 finished with value: 0.06459878354022901 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 201, 'lr': 0.00013655206277505007, 'weight_decay': 2.9657486231251276e-06, 'batch_size': 8, 'lambda_phy': 0.0002330713151186184}. Best is trial 43 with value: 0.06459878354022901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006613 - Val Loss (simple RMSE, no physics involved): 0.065643\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.022961 - Val Loss (simple RMSE, no physics involved): 0.092741\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.009026 - Val Loss (simple RMSE, no physics involved): 0.077420\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007619 - Val Loss (simple RMSE, no physics involved): 0.072344\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007436 - Val Loss (simple RMSE, no physics involved): 0.069208\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007258 - Val Loss (simple RMSE, no physics involved): 0.071658\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007214 - Val Loss (simple RMSE, no physics involved): 0.070415\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007114 - Val Loss (simple RMSE, no physics involved): 0.069159\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007089 - Val Loss (simple RMSE, no physics involved): 0.066689\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007008 - Val Loss (simple RMSE, no physics involved): 0.067660\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007008 - Val Loss (simple RMSE, no physics involved): 0.066766\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006988 - Val Loss (simple RMSE, no physics involved): 0.066070\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007001 - Val Loss (simple RMSE, no physics involved): 0.068770\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006959 - Val Loss (simple RMSE, no physics involved): 0.066285\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006923 - Val Loss (simple RMSE, no physics involved): 0.066719\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.065955\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006898 - Val Loss (simple RMSE, no physics involved): 0.067799\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006894 - Val Loss (simple RMSE, no physics involved): 0.066486\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.067171\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006870 - Val Loss (simple RMSE, no physics involved): 0.068056\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006867 - Val Loss (simple RMSE, no physics involved): 0.065484\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006848 - Val Loss (simple RMSE, no physics involved): 0.066124\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006823 - Val Loss (simple RMSE, no physics involved): 0.068535\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006818 - Val Loss (simple RMSE, no physics involved): 0.066986\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006867 - Val Loss (simple RMSE, no physics involved): 0.067187\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006805 - Val Loss (simple RMSE, no physics involved): 0.066391\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.069653\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006843 - Val Loss (simple RMSE, no physics involved): 0.070504\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006914 - Val Loss (simple RMSE, no physics involved): 0.066196\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006826 - Val Loss (simple RMSE, no physics involved): 0.066693\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006798 - Val Loss (simple RMSE, no physics involved): 0.066454\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006770 - Val Loss (simple RMSE, no physics involved): 0.067711\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006758 - Val Loss (simple RMSE, no physics involved): 0.070429\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006787 - Val Loss (simple RMSE, no physics involved): 0.069369\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006755 - Val Loss (simple RMSE, no physics involved): 0.072072\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006749 - Val Loss (simple RMSE, no physics involved): 0.070634\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006718 - Val Loss (simple RMSE, no physics involved): 0.067181\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006751 - Val Loss (simple RMSE, no physics involved): 0.067912\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006697 - Val Loss (simple RMSE, no physics involved): 0.071962\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006736 - Val Loss (simple RMSE, no physics involved): 0.065215\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006742 - Val Loss (simple RMSE, no physics involved): 0.070470\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006707 - Val Loss (simple RMSE, no physics involved): 0.066707\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006691 - Val Loss (simple RMSE, no physics involved): 0.067270\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006730 - Val Loss (simple RMSE, no physics involved): 0.066399\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006680 - Val Loss (simple RMSE, no physics involved): 0.067243\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006718 - Val Loss (simple RMSE, no physics involved): 0.068894\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006747 - Val Loss (simple RMSE, no physics involved): 0.066072\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006679 - Val Loss (simple RMSE, no physics involved): 0.065350\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006754 - Val Loss (simple RMSE, no physics involved): 0.068244\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006655 - Val Loss (simple RMSE, no physics involved): 0.067000\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:34:56,462] Trial 44 finished with value: 0.0652146004140377 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 199, 'lr': 0.00011922997013825561, 'weight_decay': 4.1770669141269154e-06, 'batch_size': 8, 'lambda_phy': 0.00017806832008267402}. Best is trial 43 with value: 0.06459878354022901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006677 - Val Loss (simple RMSE, no physics involved): 0.069306\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.063796 - Val Loss (simple RMSE, no physics involved): 0.113743\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.016889 - Val Loss (simple RMSE, no physics involved): 0.099331\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.012150 - Val Loss (simple RMSE, no physics involved): 0.083910\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.009293 - Val Loss (simple RMSE, no physics involved): 0.077468\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.008177 - Val Loss (simple RMSE, no physics involved): 0.076969\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007802 - Val Loss (simple RMSE, no physics involved): 0.073271\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007594 - Val Loss (simple RMSE, no physics involved): 0.072346\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007502 - Val Loss (simple RMSE, no physics involved): 0.071828\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007412 - Val Loss (simple RMSE, no physics involved): 0.069830\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007338 - Val Loss (simple RMSE, no physics involved): 0.070763\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007268 - Val Loss (simple RMSE, no physics involved): 0.070687\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007214 - Val Loss (simple RMSE, no physics involved): 0.069285\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007159 - Val Loss (simple RMSE, no physics involved): 0.070260\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007115 - Val Loss (simple RMSE, no physics involved): 0.070041\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007077 - Val Loss (simple RMSE, no physics involved): 0.068074\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007051 - Val Loss (simple RMSE, no physics involved): 0.069952\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007015 - Val Loss (simple RMSE, no physics involved): 0.069905\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007007 - Val Loss (simple RMSE, no physics involved): 0.068247\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006982 - Val Loss (simple RMSE, no physics involved): 0.067652\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007002 - Val Loss (simple RMSE, no physics involved): 0.068083\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006950 - Val Loss (simple RMSE, no physics involved): 0.070230\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006960 - Val Loss (simple RMSE, no physics involved): 0.071021\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006928 - Val Loss (simple RMSE, no physics involved): 0.068072\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006943 - Val Loss (simple RMSE, no physics involved): 0.067160\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006918 - Val Loss (simple RMSE, no physics involved): 0.068551\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006932 - Val Loss (simple RMSE, no physics involved): 0.069867\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006916 - Val Loss (simple RMSE, no physics involved): 0.067629\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006893 - Val Loss (simple RMSE, no physics involved): 0.068923\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006896 - Val Loss (simple RMSE, no physics involved): 0.068380\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006872 - Val Loss (simple RMSE, no physics involved): 0.067031\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006886 - Val Loss (simple RMSE, no physics involved): 0.066447\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006881 - Val Loss (simple RMSE, no physics involved): 0.067332\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006856 - Val Loss (simple RMSE, no physics involved): 0.066349\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.070122\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006840 - Val Loss (simple RMSE, no physics involved): 0.070032\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006853 - Val Loss (simple RMSE, no physics involved): 0.067899\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006835 - Val Loss (simple RMSE, no physics involved): 0.067796\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006831 - Val Loss (simple RMSE, no physics involved): 0.067103\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.068191\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006848 - Val Loss (simple RMSE, no physics involved): 0.068015\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006846 - Val Loss (simple RMSE, no physics involved): 0.068562\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006811 - Val Loss (simple RMSE, no physics involved): 0.068496\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006792 - Val Loss (simple RMSE, no physics involved): 0.066156\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006819 - Val Loss (simple RMSE, no physics involved): 0.070021\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006804 - Val Loss (simple RMSE, no physics involved): 0.066968\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006879 - Val Loss (simple RMSE, no physics involved): 0.065954\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006834 - Val Loss (simple RMSE, no physics involved): 0.067217\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006787 - Val Loss (simple RMSE, no physics involved): 0.067515\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006807 - Val Loss (simple RMSE, no physics involved): 0.066553\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:37:34,246] Trial 45 finished with value: 0.0659539761642615 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 183, 'lr': 7.535974764675306e-05, 'weight_decay': 1.4670015755850156e-06, 'batch_size': 8, 'lambda_phy': 0.0003628281497631251}. Best is trial 43 with value: 0.06459878354022901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006768 - Val Loss (simple RMSE, no physics involved): 0.068923\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.063796 - Val Loss (simple RMSE, no physics involved): 0.173710\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.049045 - Val Loss (simple RMSE, no physics involved): 0.142135\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.034655 - Val Loss (simple RMSE, no physics involved): 0.111418\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.021748 - Val Loss (simple RMSE, no physics involved): 0.123182\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.018487 - Val Loss (simple RMSE, no physics involved): 0.116932\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.017298 - Val Loss (simple RMSE, no physics involved): 0.113357\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.015544 - Val Loss (simple RMSE, no physics involved): 0.105973\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.013305 - Val Loss (simple RMSE, no physics involved): 0.095333\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.010783 - Val Loss (simple RMSE, no physics involved): 0.086300\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.009000 - Val Loss (simple RMSE, no physics involved): 0.077304\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.008243 - Val Loss (simple RMSE, no physics involved): 0.074876\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007848 - Val Loss (simple RMSE, no physics involved): 0.073763\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007679 - Val Loss (simple RMSE, no physics involved): 0.074471\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007566 - Val Loss (simple RMSE, no physics involved): 0.072178\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007508 - Val Loss (simple RMSE, no physics involved): 0.071830\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007341 - Val Loss (simple RMSE, no physics involved): 0.071031\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007249 - Val Loss (simple RMSE, no physics involved): 0.072681\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007275 - Val Loss (simple RMSE, no physics involved): 0.069781\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007166 - Val Loss (simple RMSE, no physics involved): 0.070076\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007151 - Val Loss (simple RMSE, no physics involved): 0.071509\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007085 - Val Loss (simple RMSE, no physics involved): 0.070431\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007074 - Val Loss (simple RMSE, no physics involved): 0.070297\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007064 - Val Loss (simple RMSE, no physics involved): 0.068777\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007045 - Val Loss (simple RMSE, no physics involved): 0.070051\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007101 - Val Loss (simple RMSE, no physics involved): 0.071159\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007040 - Val Loss (simple RMSE, no physics involved): 0.069971\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007034 - Val Loss (simple RMSE, no physics involved): 0.070664\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007017 - Val Loss (simple RMSE, no physics involved): 0.069982\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.007059 - Val Loss (simple RMSE, no physics involved): 0.070145\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006999 - Val Loss (simple RMSE, no physics involved): 0.069842\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006950 - Val Loss (simple RMSE, no physics involved): 0.068667\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006949 - Val Loss (simple RMSE, no physics involved): 0.069341\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006966 - Val Loss (simple RMSE, no physics involved): 0.068446\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006963 - Val Loss (simple RMSE, no physics involved): 0.068185\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007009 - Val Loss (simple RMSE, no physics involved): 0.068147\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006992 - Val Loss (simple RMSE, no physics involved): 0.069161\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006898 - Val Loss (simple RMSE, no physics involved): 0.069260\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006913 - Val Loss (simple RMSE, no physics involved): 0.071914\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006903 - Val Loss (simple RMSE, no physics involved): 0.067841\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006868 - Val Loss (simple RMSE, no physics involved): 0.067681\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006859 - Val Loss (simple RMSE, no physics involved): 0.069572\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006868 - Val Loss (simple RMSE, no physics involved): 0.068144\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006926 - Val Loss (simple RMSE, no physics involved): 0.067253\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006862 - Val Loss (simple RMSE, no physics involved): 0.070210\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006897 - Val Loss (simple RMSE, no physics involved): 0.068178\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006941 - Val Loss (simple RMSE, no physics involved): 0.067567\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006894 - Val Loss (simple RMSE, no physics involved): 0.069090\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006874 - Val Loss (simple RMSE, no physics involved): 0.069222\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006836 - Val Loss (simple RMSE, no physics involved): 0.067971\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:40:19,458] Trial 46 finished with value: 0.06725334872802098 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 256, 'lr': 3.915872406029062e-05, 'weight_decay': 6.683636239188702e-06, 'batch_size': 32, 'lambda_phy': 0.00018373491368344383}. Best is trial 43 with value: 0.06459878354022901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006865 - Val Loss (simple RMSE, no physics involved): 0.068048\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.053002 - Val Loss (simple RMSE, no physics involved): 0.107322\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.019350 - Val Loss (simple RMSE, no physics involved): 0.116758\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.016903 - Val Loss (simple RMSE, no physics involved): 0.106044\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.012066 - Val Loss (simple RMSE, no physics involved): 0.087884\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.008936 - Val Loss (simple RMSE, no physics involved): 0.074320\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.008129 - Val Loss (simple RMSE, no physics involved): 0.075510\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007792 - Val Loss (simple RMSE, no physics involved): 0.069922\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007609 - Val Loss (simple RMSE, no physics involved): 0.071481\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007480 - Val Loss (simple RMSE, no physics involved): 0.070637\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007406 - Val Loss (simple RMSE, no physics involved): 0.070525\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007360 - Val Loss (simple RMSE, no physics involved): 0.070500\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007310 - Val Loss (simple RMSE, no physics involved): 0.068798\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007332 - Val Loss (simple RMSE, no physics involved): 0.072680\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007217 - Val Loss (simple RMSE, no physics involved): 0.067648\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.007207 - Val Loss (simple RMSE, no physics involved): 0.069417\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.007176 - Val Loss (simple RMSE, no physics involved): 0.071042\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.007103 - Val Loss (simple RMSE, no physics involved): 0.071267\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.007112 - Val Loss (simple RMSE, no physics involved): 0.069289\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.007114 - Val Loss (simple RMSE, no physics involved): 0.069688\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.007071 - Val Loss (simple RMSE, no physics involved): 0.066906\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.007126 - Val Loss (simple RMSE, no physics involved): 0.067136\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.007047 - Val Loss (simple RMSE, no physics involved): 0.066270\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.007159 - Val Loss (simple RMSE, no physics involved): 0.072474\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.007034 - Val Loss (simple RMSE, no physics involved): 0.067904\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.007001 - Val Loss (simple RMSE, no physics involved): 0.070570\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.007053 - Val Loss (simple RMSE, no physics involved): 0.066690\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.007000 - Val Loss (simple RMSE, no physics involved): 0.067152\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.007012 - Val Loss (simple RMSE, no physics involved): 0.067062\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006979 - Val Loss (simple RMSE, no physics involved): 0.067814\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006966 - Val Loss (simple RMSE, no physics involved): 0.069048\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.068428\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.072399\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.068162\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.066887\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006935 - Val Loss (simple RMSE, no physics involved): 0.065942\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.007005 - Val Loss (simple RMSE, no physics involved): 0.067718\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006969 - Val Loss (simple RMSE, no physics involved): 0.070142\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006951 - Val Loss (simple RMSE, no physics involved): 0.072705\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006888 - Val Loss (simple RMSE, no physics involved): 0.067561\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006936 - Val Loss (simple RMSE, no physics involved): 0.066837\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006980 - Val Loss (simple RMSE, no physics involved): 0.069527\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006896 - Val Loss (simple RMSE, no physics involved): 0.067461\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006913 - Val Loss (simple RMSE, no physics involved): 0.066869\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006909 - Val Loss (simple RMSE, no physics involved): 0.066547\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006974 - Val Loss (simple RMSE, no physics involved): 0.069981\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006970 - Val Loss (simple RMSE, no physics involved): 0.067242\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006900 - Val Loss (simple RMSE, no physics involved): 0.068290\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006938 - Val Loss (simple RMSE, no physics involved): 0.069554\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.066314\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:42:58,363] Trial 47 finished with value: 0.06594191584736109 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 64, 'lr': 0.00015500213868295025, 'weight_decay': 2.234032403175075e-06, 'batch_size': 8, 'lambda_phy': 0.0004505509150167949}. Best is trial 43 with value: 0.06459878354022901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006873 - Val Loss (simple RMSE, no physics involved): 0.066582\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.023147 - Val Loss (simple RMSE, no physics involved): 0.095843\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.010395 - Val Loss (simple RMSE, no physics involved): 0.072645\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007900 - Val Loss (simple RMSE, no physics involved): 0.072137\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007479 - Val Loss (simple RMSE, no physics involved): 0.069951\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007362 - Val Loss (simple RMSE, no physics involved): 0.071250\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007264 - Val Loss (simple RMSE, no physics involved): 0.069709\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007143 - Val Loss (simple RMSE, no physics involved): 0.069093\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007111 - Val Loss (simple RMSE, no physics involved): 0.068829\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007046 - Val Loss (simple RMSE, no physics involved): 0.071263\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007034 - Val Loss (simple RMSE, no physics involved): 0.074035\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007070 - Val Loss (simple RMSE, no physics involved): 0.066396\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007106 - Val Loss (simple RMSE, no physics involved): 0.069698\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006962 - Val Loss (simple RMSE, no physics involved): 0.068546\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006956 - Val Loss (simple RMSE, no physics involved): 0.067086\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006964 - Val Loss (simple RMSE, no physics involved): 0.068750\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006987 - Val Loss (simple RMSE, no physics involved): 0.069478\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006908 - Val Loss (simple RMSE, no physics involved): 0.068363\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006906 - Val Loss (simple RMSE, no physics involved): 0.069149\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006873 - Val Loss (simple RMSE, no physics involved): 0.067453\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006878 - Val Loss (simple RMSE, no physics involved): 0.066770\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006879 - Val Loss (simple RMSE, no physics involved): 0.068914\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006861 - Val Loss (simple RMSE, no physics involved): 0.066694\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006862 - Val Loss (simple RMSE, no physics involved): 0.067132\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006833 - Val Loss (simple RMSE, no physics involved): 0.068847\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006834 - Val Loss (simple RMSE, no physics involved): 0.071700\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006833 - Val Loss (simple RMSE, no physics involved): 0.067764\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006796 - Val Loss (simple RMSE, no physics involved): 0.068008\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.068384\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006801 - Val Loss (simple RMSE, no physics involved): 0.071153\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006818 - Val Loss (simple RMSE, no physics involved): 0.069039\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006829 - Val Loss (simple RMSE, no physics involved): 0.067241\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006782 - Val Loss (simple RMSE, no physics involved): 0.068437\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006735 - Val Loss (simple RMSE, no physics involved): 0.067355\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006773 - Val Loss (simple RMSE, no physics involved): 0.068429\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006790 - Val Loss (simple RMSE, no physics involved): 0.069923\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006752 - Val Loss (simple RMSE, no physics involved): 0.066138\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006751 - Val Loss (simple RMSE, no physics involved): 0.069040\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006760 - Val Loss (simple RMSE, no physics involved): 0.067228\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006704 - Val Loss (simple RMSE, no physics involved): 0.068412\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006722 - Val Loss (simple RMSE, no physics involved): 0.067645\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006714 - Val Loss (simple RMSE, no physics involved): 0.069610\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006753 - Val Loss (simple RMSE, no physics involved): 0.074936\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006703 - Val Loss (simple RMSE, no physics involved): 0.069308\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006688 - Val Loss (simple RMSE, no physics involved): 0.066506\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006750 - Val Loss (simple RMSE, no physics involved): 0.068894\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006663 - Val Loss (simple RMSE, no physics involved): 0.067684\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006677 - Val Loss (simple RMSE, no physics involved): 0.070350\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006681 - Val Loss (simple RMSE, no physics involved): 0.069195\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006701 - Val Loss (simple RMSE, no physics involved): 0.070416\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:46:34,586] Trial 48 finished with value: 0.066137522769471 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 208, 'lr': 8.563763138617673e-05, 'weight_decay': 5.850910978979016e-07, 'batch_size': 8, 'lambda_phy': 0.00012637506449173551}. Best is trial 43 with value: 0.06459878354022901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006663 - Val Loss (simple RMSE, no physics involved): 0.068140\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.049169 - Val Loss (simple RMSE, no physics involved): 0.137960\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.017211 - Val Loss (simple RMSE, no physics involved): 0.102115\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.010819 - Val Loss (simple RMSE, no physics involved): 0.082969\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.008062 - Val Loss (simple RMSE, no physics involved): 0.077439\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007651 - Val Loss (simple RMSE, no physics involved): 0.070940\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007391 - Val Loss (simple RMSE, no physics involved): 0.069625\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007290 - Val Loss (simple RMSE, no physics involved): 0.067481\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007248 - Val Loss (simple RMSE, no physics involved): 0.069143\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.007143 - Val Loss (simple RMSE, no physics involved): 0.068368\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.007103 - Val Loss (simple RMSE, no physics involved): 0.066930\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.007118 - Val Loss (simple RMSE, no physics involved): 0.070605\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.007115 - Val Loss (simple RMSE, no physics involved): 0.066365\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.007054 - Val Loss (simple RMSE, no physics involved): 0.070842\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.007016 - Val Loss (simple RMSE, no physics involved): 0.072785\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006982 - Val Loss (simple RMSE, no physics involved): 0.070466\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006952 - Val Loss (simple RMSE, no physics involved): 0.066829\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006976 - Val Loss (simple RMSE, no physics involved): 0.070289\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006924 - Val Loss (simple RMSE, no physics involved): 0.069592\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006918 - Val Loss (simple RMSE, no physics involved): 0.065836\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006959 - Val Loss (simple RMSE, no physics involved): 0.066008\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006990 - Val Loss (simple RMSE, no physics involved): 0.070832\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006865 - Val Loss (simple RMSE, no physics involved): 0.066527\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006891 - Val Loss (simple RMSE, no physics involved): 0.066174\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006876 - Val Loss (simple RMSE, no physics involved): 0.066829\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006843 - Val Loss (simple RMSE, no physics involved): 0.067228\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006824 - Val Loss (simple RMSE, no physics involved): 0.067025\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006840 - Val Loss (simple RMSE, no physics involved): 0.066938\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006882 - Val Loss (simple RMSE, no physics involved): 0.066374\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.067231\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006809 - Val Loss (simple RMSE, no physics involved): 0.065898\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006810 - Val Loss (simple RMSE, no physics involved): 0.070084\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006878 - Val Loss (simple RMSE, no physics involved): 0.068220\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006805 - Val Loss (simple RMSE, no physics involved): 0.069835\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006842 - Val Loss (simple RMSE, no physics involved): 0.071523\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.007032 - Val Loss (simple RMSE, no physics involved): 0.066288\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006816 - Val Loss (simple RMSE, no physics involved): 0.070478\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006793 - Val Loss (simple RMSE, no physics involved): 0.067763\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006790 - Val Loss (simple RMSE, no physics involved): 0.068482\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006771 - Val Loss (simple RMSE, no physics involved): 0.066457\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006752 - Val Loss (simple RMSE, no physics involved): 0.070320\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006820 - Val Loss (simple RMSE, no physics involved): 0.065336\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006922 - Val Loss (simple RMSE, no physics involved): 0.067326\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006866 - Val Loss (simple RMSE, no physics involved): 0.068222\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006772 - Val Loss (simple RMSE, no physics involved): 0.066190\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006737 - Val Loss (simple RMSE, no physics involved): 0.065627\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006744 - Val Loss (simple RMSE, no physics involved): 0.065268\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006822 - Val Loss (simple RMSE, no physics involved): 0.066726\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006751 - Val Loss (simple RMSE, no physics involved): 0.067763\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006722 - Val Loss (simple RMSE, no physics involved): 0.065811\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-23 15:49:29,369] Trial 49 finished with value: 0.0652682955066363 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 192, 'lr': 0.00015484186631150486, 'weight_decay': 6.44524885931707e-06, 'batch_size': 16, 'lambda_phy': 0.0007927843984444024}. Best is trial 43 with value: 0.06459878354022901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.006753 - Val Loss (simple RMSE, no physics involved): 0.066669\n",
      "Best Hyperparameters: {'n_hidden_layers': 3, 'n_hidden_units': 201, 'lr': 0.00013655206277505007, 'weight_decay': 2.9657486231251276e-06, 'batch_size': 8, 'lambda_phy': 0.0002330713151186184}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to search over\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 5)\n",
    "    n_hidden_units = trial.suggest_int(\"n_hidden_units\", 32, 256)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64]) \n",
    "    lambda_phy = trial.suggest_loguniform(\"lambda_phy\", 1e-5, 1e-1)\n",
    "\n",
    "    # Create train & validation loaders (following the original code)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize MLP model\n",
    "    model = BasicMLP(\n",
    "        N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "        N_HIDDEN_LAYERS=n_hidden_layers,\n",
    "        N_HIDDEN_UNITS=n_hidden_units,\n",
    "        N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "        loss_function=LOSS_FUNC,\n",
    "    )\n",
    "\n",
    "    # Train and return validation loss\n",
    "    val_loss, _ = model.train_model(train_loader, val_loader, epochs=50, lr=lr, weight_decay=weight_decay, lambda_phy=lambda_phy, device=device)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"mlp_hyperparameter_optimization_linearshift_mse_allyears\", storage=\"sqlite:///mlp_hyperparameter_optimization_phy.db\", load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for baseline mlp 2017: {'n_hidden_layers': 3, 'n_hidden_units': 201, 'lr': 0.00013655206277505007, 'weight_decay': 2.9657486231251276e-06, 'batch_size': 8, 'lambda_phy': 0.0002330713151186184}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters for baseline mlp 2017:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_params_MLP_no2_LinearShift_MSE_allyears.txt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BESTPARAMS_FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters saved to file /home/rachel/forecasting_smog_PEML/src/results/best_params/best_params_MLP_no2_LinearShift_MSE_allyears.txt\n"
     ]
    }
   ],
   "source": [
    "# save best params to a file\n",
    "with open(f'{RESULTS_PATH}/best_params/{BESTPARAMS_FILENAME}', 'w') as f:\n",
    "    for key in best_params.keys():\n",
    "        f.write(\"%s: %s\\n\" % (key, best_params[key]))\n",
    "print(f\"Best Hyperparameters saved to file {RESULTS_PATH}/best_params/{BESTPARAMS_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.021300 - Val Loss (simple RMSE, no physics involved): 0.089954\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.009424 - Val Loss (simple RMSE, no physics involved): 0.075835\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.007453 - Val Loss (simple RMSE, no physics involved): 0.070698\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.007279 - Val Loss (simple RMSE, no physics involved): 0.071115\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.007151 - Val Loss (simple RMSE, no physics involved): 0.071266\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.007165 - Val Loss (simple RMSE, no physics involved): 0.070450\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.007110 - Val Loss (simple RMSE, no physics involved): 0.067357\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.007061 - Val Loss (simple RMSE, no physics involved): 0.068868\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.006979 - Val Loss (simple RMSE, no physics involved): 0.068090\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.006971 - Val Loss (simple RMSE, no physics involved): 0.067224\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.006969 - Val Loss (simple RMSE, no physics involved): 0.068683\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.006986 - Val Loss (simple RMSE, no physics involved): 0.069462\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.006961 - Val Loss (simple RMSE, no physics involved): 0.069797\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.006892 - Val Loss (simple RMSE, no physics involved): 0.071221\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.006889 - Val Loss (simple RMSE, no physics involved): 0.069130\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.006880 - Val Loss (simple RMSE, no physics involved): 0.066165\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.006872 - Val Loss (simple RMSE, no physics involved): 0.072009\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.006857 - Val Loss (simple RMSE, no physics involved): 0.067673\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.006939 - Val Loss (simple RMSE, no physics involved): 0.068955\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.006828 - Val Loss (simple RMSE, no physics involved): 0.066403\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.006879 - Val Loss (simple RMSE, no physics involved): 0.068864\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.006941 - Val Loss (simple RMSE, no physics involved): 0.069139\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.006786 - Val Loss (simple RMSE, no physics involved): 0.067054\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.006767 - Val Loss (simple RMSE, no physics involved): 0.069651\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.006809 - Val Loss (simple RMSE, no physics involved): 0.072017\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.006762 - Val Loss (simple RMSE, no physics involved): 0.068789\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.006755 - Val Loss (simple RMSE, no physics involved): 0.070155\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.006719 - Val Loss (simple RMSE, no physics involved): 0.072886\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.006747 - Val Loss (simple RMSE, no physics involved): 0.065789\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.006726 - Val Loss (simple RMSE, no physics involved): 0.070315\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.006744 - Val Loss (simple RMSE, no physics involved): 0.072369\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.006753 - Val Loss (simple RMSE, no physics involved): 0.067200\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.006696 - Val Loss (simple RMSE, no physics involved): 0.066676\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.006737 - Val Loss (simple RMSE, no physics involved): 0.066292\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.006744 - Val Loss (simple RMSE, no physics involved): 0.068865\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.006719 - Val Loss (simple RMSE, no physics involved): 0.065323\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.006671 - Val Loss (simple RMSE, no physics involved): 0.065499\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.006691 - Val Loss (simple RMSE, no physics involved): 0.069519\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.006704 - Val Loss (simple RMSE, no physics involved): 0.068994\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.006619 - Val Loss (simple RMSE, no physics involved): 0.066791\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.006623 - Val Loss (simple RMSE, no physics involved): 0.067403\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.006628 - Val Loss (simple RMSE, no physics involved): 0.067229\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.006659 - Val Loss (simple RMSE, no physics involved): 0.072629\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.006659 - Val Loss (simple RMSE, no physics involved): 0.065216\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.006654 - Val Loss (simple RMSE, no physics involved): 0.076134\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.006598 - Val Loss (simple RMSE, no physics involved): 0.067418\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.006569 - Val Loss (simple RMSE, no physics involved): 0.070251\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.006628 - Val Loss (simple RMSE, no physics involved): 0.071873\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.006613 - Val Loss (simple RMSE, no physics involved): 0.067889\n",
      "Epoch 50/50\n",
      "Epoch 50/50 - Train Loss: 0.006580 - Val Loss (simple RMSE, no physics involved): 0.067111\n",
      "Training time: 227.02057099342346\n",
      "Model saved as best_MLP_no2_LinearShift_MSE_allyears.pth in Model folder\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(34)\n",
    "# Train the model with the best hyperparameters\n",
    "best_model_baseline = BasicMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=best_params[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=best_params[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "    loss_function=\"MSE\",\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "_, training_time = best_model_baseline.train_model(train_loader, val_loader, epochs=50, lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"], lambda_phy= best_params[\"lambda_phy\"], device=device)\n",
    "\n",
    "print(f\"Training time: {training_time}\")\n",
    "# Save the trained model\n",
    "torch.save(best_model_baseline.state_dict(), f\"{MODEL_PATH}/{MODEL_PATH_NAME}\")\n",
    "print(f\"Model saved as {MODEL_PATH_NAME} in Model folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 62.572534\n",
      "Test RMSE Loss: 6.705590\n",
      "Test SMAPE Loss: 27.778922%\n",
      "Total Inference Time: 0.23 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(34)  # Set seed for reproducibility\n",
    "best_model_baseline.load_state_dict(torch.load(f\"{MODEL_PATH}/{MODEL_PATH_NAME}\"))\n",
    "best_model_baseline.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "mse, rmse, smape, inference_time = best_model_baseline.test_model(test_loader, min_value=min_value, max_value=max_value, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved as results_MLP_no2_LinearShift_MSE_allyears.csv in Results/metrics folder\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the CSV file path\n",
    "results_csv_path = f\"{RESULTS_PATH}/metrics/{RESULTS_METRICS_FILENAME}\"\n",
    "\n",
    "# Save metrics in a proper CSV format (header + values in one row)\n",
    "with open(results_csv_path, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow([\"MSE\", \"RMSE\", \"SMAPE\", \"Inference Time\", \"Training Time\"])\n",
    "    \n",
    "    # Write values\n",
    "    writer.writerow([mse, rmse, smape, inference_time, training_time])\n",
    "\n",
    "print(f\"Results saved as {RESULTS_METRICS_FILENAME} in Results/metrics folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "best_model_baseline.eval()\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "# Iterate through the test set and collect predictions & ground truth\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_test, y_true = batch  # Get input and ground truth\n",
    "        x_test = x_test.to(\"cpu\")  # Ensure data is on CPU if needed\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = best_model_baseline(x_test)\n",
    "\n",
    "        # Store results\n",
    "        y_preds.append(y_pred.cpu())\n",
    "        y_trues.append(y_true.cpu())\n",
    "\n",
    "# Convert lists to tensors\n",
    "y_preds = torch.cat(y_preds, dim=0).numpy()\n",
    "y_trues = torch.cat(y_trues, dim=0).numpy()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_trues.flatten(), label=\"Ground Truth (NO)\", linestyle=\"-\", color=\"blue\")\n",
    "plt.scatter(range(len(y_preds.flatten())), y_preds.flatten(), label=\"Predictions\", color=\"black\", s=10)\n",
    "\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"NO Level\")\n",
    "plt.title(\"Predictions vs. Ground Truth\")\n",
    "plt.legend()\n",
    "#save the plot\n",
    "plt.savefig(f\"{RESULTS_PATH}/plots/{PLOT_FILENAME}\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
