{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLP with extended physics loss function simple baseline**\n",
    "## y_phy calculated by linear time shifting according to wind speed and dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading models, go to the ``src/results/models``:\n",
    "- Baseline NO2 2017 with MLP and MSE loss: ``best_mlp_no2_baseline.pth``\n",
    "- Exp 1: NO2 2017 with MLP and option 1 simple physics loss: ``best_mlp_no2_adjusted_dist.pth`` (naming because I updated the distance between T and B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "\n",
    "from modelling.MLP import BasicMLP\n",
    "from modelling import *\n",
    "\n",
    "\n",
    "import optuna\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rachel/forecasting_smog_PEML/src')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/rachel/forecasting_smog_PEML\n",
      "MODEL_PATH:  /home/rachel/forecasting_smog_PEML/src/results/models\n",
      "MINMAX_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/pollutants_minmax.csv\n"
     ]
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "\n",
    "BASE_DIR = Path.cwd().parents[0] # set it to the root directory of the project, not src\n",
    "MODEL_PATH = BASE_DIR /\"src\" / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR  / \"data\" / \"data_combined\" / \"pollutants_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 24 * 3               # number of hours to use for input (number of days * 24 hours)\n",
    "N_HOURS_Y = 24                    # number of hours to predict (1 day * 24 hours)\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "\n",
    "# Change this according to the data you want to use\n",
    "YEARS = [2017]\n",
    "TRAIN_YEARS = [2017]\n",
    "VAL_YEARS = [2017]\n",
    "TEST_YEARS = [2017]\n",
    "\n",
    "LOSS_FUNC = \"Physics_Linear_MSE\" # choose from \"MSE\" and \"Physics_Linear_MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2017_combined_u.csv\n",
      "Imported train_2017_combined_y.csv\n",
      "Imported val_2017_combined_u.csv\n",
      "Imported val_2017_combined_y.csv\n",
      "Imported test_2017_combined_u.csv\n",
      "Imported test_2017_combined_y.csv\n",
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', YEARS)\n",
    "train_output_frames = get_dataframes('train', 'y', YEARS)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', YEARS)\n",
    "val_output_frames = get_dataframes('val', 'y', YEARS)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', YEARS)\n",
    "test_output_frames = get_dataframes('test', 'y', YEARS) \n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    len(TRAIN_YEARS),                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    len(VAL_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    len(TEST_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                           DD   FF        FH        FX       NO2         P  \\\n",
       " DateTime                                                                     \n",
       " 2017-08-01 00:00:00  0.166667  0.1  0.111111  0.000000  0.242115  0.562982   \n",
       " 2017-08-01 01:00:00  0.000000  0.0  0.111111  0.052632  0.223158  0.570694   \n",
       " 2017-08-01 02:00:00  0.000000  0.0  0.000000  0.000000  0.165911  0.560411   \n",
       " 2017-08-01 03:00:00  0.277778  0.1  0.000000  0.000000  0.142363  0.555270   \n",
       " 2017-08-01 04:00:00  0.805556  0.2  0.111111  0.105263  0.156297  0.555270   \n",
       " ...                       ...  ...       ...       ...       ...       ...   \n",
       " 2017-11-16 19:00:00  0.750000  0.2  0.333333  0.210526  0.523871  0.789203   \n",
       " 2017-11-16 20:00:00  0.972222  0.3  0.333333  0.421053  0.512314  0.814910   \n",
       " 2017-11-16 21:00:00  0.888889  0.1  0.222222  0.263158  0.232880  0.827763   \n",
       " 2017-11-16 22:00:00  0.944444  0.2  0.111111  0.105263  0.108123  0.832905   \n",
       " 2017-11-16 23:00:00  0.861111  0.1  0.222222  0.105263  0.205120  0.845758   \n",
       " \n",
       "                       SQ         T        TD  \n",
       " DateTime                                      \n",
       " 2017-08-01 00:00:00  0.0  0.536667  0.726852  \n",
       " 2017-08-01 01:00:00  0.0  0.546667  0.740741  \n",
       " 2017-08-01 02:00:00  0.0  0.506667  0.689815  \n",
       " 2017-08-01 03:00:00  0.0  0.463333  0.634259  \n",
       " 2017-08-01 04:00:00  0.0  0.493333  0.662037  \n",
       " ...                  ...       ...       ...  \n",
       " 2017-11-16 19:00:00  0.0  0.390000  0.513889  \n",
       " 2017-11-16 20:00:00  0.0  0.353333  0.462963  \n",
       " 2017-11-16 21:00:00  0.0  0.330000  0.435185  \n",
       " 2017-11-16 22:00:00  0.0  0.306667  0.407407  \n",
       " 2017-11-16 23:00:00  0.0  0.250000  0.319444  \n",
       " \n",
       " [2592 rows x 9 columns]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                          NO2\n",
       " DateTime                     \n",
       " 2017-08-01 00:00:00  0.223698\n",
       " 2017-08-01 01:00:00  0.145496\n",
       " 2017-08-01 02:00:00  0.275978\n",
       " 2017-08-01 03:00:00  0.423742\n",
       " 2017-08-01 04:00:00  0.478721\n",
       " ...                       ...\n",
       " 2017-11-16 19:00:00  0.606502\n",
       " 2017-11-16 20:00:00  0.456470\n",
       " 2017-11-16 21:00:00  0.483258\n",
       " 2017-11-16 22:00:00  0.468784\n",
       " 2017-11-16 23:00:00  0.473428\n",
       " \n",
       " [2592 rows x 1 columns]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.1000, 0.1111, 0.0000, 0.2421, 0.5630, 0.0000, 0.5367, 0.7269],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.2232, 0.5707, 0.0000, 0.5467, 0.7407],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1659, 0.5604, 0.0000, 0.5067, 0.6898],\n",
       "        [0.2778, 0.1000, 0.0000, 0.0000, 0.1424, 0.5553, 0.0000, 0.4633, 0.6343],\n",
       "        [0.8056, 0.2000, 0.1111, 0.1053, 0.1563, 0.5553, 0.0000, 0.4933, 0.6620],\n",
       "        [0.0000, 0.0000, 0.1111, 0.1053, 0.3135, 0.5681, 0.3000, 0.6200, 0.7593],\n",
       "        [0.7222, 0.1000, 0.1111, 0.0526, 0.5326, 0.5913, 0.0000, 0.6433, 0.7269],\n",
       "        [0.7500, 0.1000, 0.1111, 0.1053, 0.5367, 0.5938, 0.0000, 0.6500, 0.7037],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1053, 0.5172, 0.5964, 0.0000, 0.6733, 0.6574],\n",
       "        [0.7500, 0.2000, 0.2222, 0.2105, 0.4459, 0.5990, 0.3000, 0.7133, 0.6157],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3129, 0.6041, 0.0000, 0.7167, 0.6019],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3478, 0.6067, 0.0000, 0.7133, 0.5926],\n",
       "        [0.6528, 0.1000, 0.2222, 0.1053, 0.3649, 0.6041, 0.2000, 0.7800, 0.6435],\n",
       "        [0.6944, 0.2000, 0.2222, 0.1053, 0.3019, 0.6015, 0.2000, 0.7867, 0.4861],\n",
       "        [0.7222, 0.2000, 0.2222, 0.2105, 0.2268, 0.5990, 0.4000, 0.7733, 0.5046],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.2246, 0.6015, 0.1000, 0.7633, 0.5972],\n",
       "        [0.6944, 0.2000, 0.2222, 0.2105, 0.2855, 0.6041, 0.7000, 0.7700, 0.5880],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.2469, 0.6015, 0.2000, 0.7267, 0.6435],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1579, 0.2171, 0.6144, 0.2000, 0.6967, 0.6944],\n",
       "        [0.6944, 0.1000, 0.1111, 0.1053, 0.2834, 0.6298, 0.0000, 0.5933, 0.7130],\n",
       "        [0.4167, 0.1000, 0.1111, 0.0526, 0.3918, 0.6298, 0.0000, 0.5267, 0.6944],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0000, 0.4752, 0.6375, 0.0000, 0.5200, 0.6991],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0526, 0.5745, 0.6298, 0.0000, 0.5067, 0.6852],\n",
       "        [0.5000, 0.1000, 0.1111, 0.1053, 0.5891, 0.6247, 0.0000, 0.5033, 0.6713],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.5491, 0.6298, 0.0000, 0.5000, 0.6806],\n",
       "        [0.4444, 0.1000, 0.1111, 0.0526, 0.5092, 0.6221, 0.0000, 0.5300, 0.6944],\n",
       "        [0.4167, 0.2000, 0.1111, 0.0526, 0.3212, 0.6144, 0.0000, 0.5267, 0.6898],\n",
       "        [0.3889, 0.1000, 0.2222, 0.1053, 0.2835, 0.6118, 0.0000, 0.5467, 0.6898],\n",
       "        [0.5000, 0.1000, 0.1111, 0.0526, 0.4099, 0.6144, 0.0000, 0.5533, 0.7083],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.4797, 0.6272, 0.0000, 0.6067, 0.7083],\n",
       "        [0.6389, 0.2000, 0.2222, 0.2105, 0.5086, 0.6272, 0.0000, 0.6333, 0.6991],\n",
       "        [0.6667, 0.4000, 0.3333, 0.2632, 0.4155, 0.6375, 0.0000, 0.6567, 0.6574],\n",
       "        [0.6667, 0.4000, 0.4444, 0.3158, 0.3375, 0.6478, 0.1000, 0.6800, 0.6574],\n",
       "        [0.6389, 0.3000, 0.3333, 0.3684, 0.2610, 0.6478, 0.1000, 0.7067, 0.6713],\n",
       "        [0.5556, 0.4000, 0.4444, 0.2632, 0.2420, 0.6555, 0.3000, 0.7300, 0.6296],\n",
       "        [0.5833, 0.4000, 0.4444, 0.3158, 0.2146, 0.6427, 0.2000, 0.7433, 0.5741],\n",
       "        [0.5556, 0.3000, 0.3333, 0.2632, 0.1782, 0.6221, 0.1000, 0.7600, 0.5370],\n",
       "        [0.5000, 0.4000, 0.4444, 0.2632, 0.1985, 0.6195, 0.0000, 0.7533, 0.5556],\n",
       "        [0.5000, 0.5000, 0.4444, 0.3684, 0.2416, 0.5964, 0.0000, 0.7600, 0.6204],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4737, 0.2883, 0.5938, 0.0000, 0.7267, 0.5833],\n",
       "        [0.5833, 0.4000, 0.7778, 0.5789, 0.2718, 0.6093, 0.0000, 0.5933, 0.6806],\n",
       "        [0.5556, 0.2000, 0.3333, 0.3158, 0.1936, 0.5964, 0.0000, 0.5600, 0.7083],\n",
       "        [0.4861, 0.1000, 0.1111, 0.1579, 0.2238, 0.5835, 0.0000, 0.5533, 0.7361],\n",
       "        [0.4167, 0.2000, 0.2222, 0.1579, 0.2430, 0.5656, 0.0000, 0.5533, 0.7269],\n",
       "        [0.3889, 0.2000, 0.2222, 0.1579, 0.3154, 0.5681, 0.0000, 0.5567, 0.7315],\n",
       "        [0.4444, 0.3000, 0.2222, 0.1579, 0.2860, 0.5553, 0.0000, 0.5800, 0.7639],\n",
       "        [0.5000, 0.3000, 0.3333, 0.2632, 0.2077, 0.5373, 0.0000, 0.5967, 0.7778],\n",
       "        [0.5000, 0.2000, 0.2222, 0.2105, 0.1640, 0.5167, 0.0000, 0.6000, 0.7778],\n",
       "        [0.4444, 0.3000, 0.3333, 0.2632, 0.1525, 0.4961, 0.0000, 0.5967, 0.7824],\n",
       "        [0.4722, 0.4000, 0.3333, 0.3158, 0.1328, 0.4730, 0.0000, 0.6000, 0.7824],\n",
       "        [0.5000, 0.4000, 0.4444, 0.3158, 0.1252, 0.4422, 0.0000, 0.6100, 0.7639],\n",
       "        [0.4722, 0.2000, 0.3333, 0.2632, 0.1161, 0.4293, 0.0000, 0.6000, 0.7639],\n",
       "        [0.4722, 0.2000, 0.2222, 0.1579, 0.1766, 0.4165, 0.0000, 0.5967, 0.7731],\n",
       "        [0.5556, 0.4000, 0.3333, 0.3158, 0.2840, 0.4139, 0.0000, 0.6533, 0.8056],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4211, 0.3435, 0.4010, 0.5000, 0.6967, 0.8009],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.3057, 0.3985, 0.0000, 0.6867, 0.8194],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.2615, 0.4036, 0.0000, 0.6633, 0.8519],\n",
       "        [0.6389, 0.5000, 0.6667, 0.5789, 0.2453, 0.4190, 0.0000, 0.6133, 0.7454],\n",
       "        [0.6389, 0.7000, 0.7778, 0.6316, 0.1434, 0.4216, 0.6000, 0.7067, 0.6806],\n",
       "        [0.6667, 0.9000, 0.8889, 0.7368, 0.1046, 0.4267, 0.7000, 0.7467, 0.6435],\n",
       "        [0.6667, 0.8000, 0.7778, 0.7368, 0.0607, 0.4319, 0.8000, 0.7667, 0.6019],\n",
       "        [0.6667, 0.9000, 1.0000, 0.8947, 0.0700, 0.4319, 1.0000, 0.7867, 0.6065],\n",
       "        [0.6944, 0.8000, 0.8889, 0.7895, 0.0594, 0.4422, 0.4000, 0.7400, 0.6389],\n",
       "        [0.6667, 0.8000, 0.7778, 0.6842, 0.0823, 0.4370, 0.5000, 0.7633, 0.6389],\n",
       "        [0.6667, 0.7000, 0.8889, 0.7368, 0.1016, 0.4473, 1.0000, 0.7467, 0.6944],\n",
       "        [0.6389, 0.9000, 0.8889, 0.8421, 0.1005, 0.4550, 0.5000, 0.7067, 0.5278],\n",
       "        [0.6667, 0.7000, 1.0000, 0.8947, 0.0701, 0.4627, 0.3000, 0.6900, 0.5602],\n",
       "        [0.6389, 1.0000, 0.8889, 0.9474, 0.0572, 0.4653, 0.0000, 0.6800, 0.5648],\n",
       "        [0.6667, 0.9000, 0.8889, 0.8421, 0.0533, 0.4627, 0.0000, 0.6567, 0.5741],\n",
       "        [0.6667, 0.7000, 0.7778, 0.7368, 0.0475, 0.4627, 0.0000, 0.6300, 0.6019],\n",
       "        [0.6389, 0.4000, 0.5556, 0.5789, 0.0376, 0.4576, 0.0000, 0.6100, 0.6204],\n",
       "        [0.6111, 0.5000, 0.4444, 0.4211, 0.0373, 0.4499, 0.0000, 0.5933, 0.6296]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1965],\n",
       "        [0.1501],\n",
       "        [0.1518],\n",
       "        [0.2622],\n",
       "        [0.5524],\n",
       "        [0.4840],\n",
       "        [0.3544],\n",
       "        [0.2754],\n",
       "        [0.1948],\n",
       "        [0.1734],\n",
       "        [0.1505],\n",
       "        [0.1352],\n",
       "        [0.0778],\n",
       "        [0.1184],\n",
       "        [0.1293],\n",
       "        [0.1238],\n",
       "        [0.1043],\n",
       "        [0.0997],\n",
       "        [0.0812],\n",
       "        [0.0823],\n",
       "        [0.1155],\n",
       "        [0.0837],\n",
       "        [0.0570],\n",
       "        [0.1006]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2 index:  4\n",
      "DD index (wind direction):  0\n",
      "FH index (Hourly wind speed):  2\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataset.u[0] is a pandas Index object with column names\n",
    "column_names = list(train_dataset.u[0])  # Convert Index to list\n",
    "\n",
    "# Now, find the indices of the columns 'NO2', 'DD', 'FH'\n",
    "no2_idx = column_names.index('NO2')\n",
    "dd_idx = column_names.index('DD')\n",
    "fh_idx = column_names.index('FH')\n",
    "\n",
    "print(\"NO2 index: \", no2_idx)\n",
    "print(\"DD index (wind direction): \", dd_idx)\n",
    "print(\"FH index (Hourly wind speed): \", fh_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.242115\n",
       "2017-08-01 01:00:00    0.223158\n",
       "2017-08-01 02:00:00    0.165911\n",
       "2017-08-01 03:00:00    0.142363\n",
       "2017-08-01 04:00:00    0.156297\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.523871\n",
       "2017-11-16 20:00:00    0.512314\n",
       "2017-11-16 21:00:00    0.232880\n",
       "2017-11-16 22:00:00    0.108123\n",
       "2017-11-16 23:00:00    0.205120\n",
       "Name: NO2, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,no2_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.166667\n",
       "2017-08-01 01:00:00    0.000000\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.277778\n",
       "2017-08-01 04:00:00    0.805556\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.750000\n",
       "2017-11-16 20:00:00    0.972222\n",
       "2017-11-16 21:00:00    0.888889\n",
       "2017-11-16 22:00:00    0.944444\n",
       "2017-11-16 23:00:00    0.861111\n",
       "Name: DD, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,dd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.111111\n",
       "2017-08-01 01:00:00    0.111111\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.000000\n",
       "2017-08-01 04:00:00    0.111111\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.333333\n",
       "2017-11-16 20:00:00    0.333333\n",
       "2017-11-16 21:00:00    0.222222\n",
       "2017-11-16 22:00:00    0.111111\n",
       "2017-11-16 23:00:00    0.222222\n",
       "Name: FH, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,fh_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start hyperparameter searching with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:37:04,784] A new study created in RDB with name: mlp_hyperparameter_optimization_phy_newvalloss\n",
      "/tmp/ipykernel_14772/3588211214.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_14772/3588211214.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.057498 - Val Loss (DD rmse only): 0.253030\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.031450 - Val Loss (DD rmse only): 0.209131\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.029257 - Val Loss (DD rmse only): 0.226954\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028275 - Val Loss (DD rmse only): 0.207710\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.026481 - Val Loss (DD rmse only): 0.209031\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.024124 - Val Loss (DD rmse only): 0.191513\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.020319 - Val Loss (DD rmse only): 0.163155\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.018375 - Val Loss (DD rmse only): 0.155444\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.017125 - Val Loss (DD rmse only): 0.156737\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.017258 - Val Loss (DD rmse only): 0.152558\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016496 - Val Loss (DD rmse only): 0.160905\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016177 - Val Loss (DD rmse only): 0.151340\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015712 - Val Loss (DD rmse only): 0.145414\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015132 - Val Loss (DD rmse only): 0.145378\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014987 - Val Loss (DD rmse only): 0.143660\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015289 - Val Loss (DD rmse only): 0.149362\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015327 - Val Loss (DD rmse only): 0.145249\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015126 - Val Loss (DD rmse only): 0.142622\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014832 - Val Loss (DD rmse only): 0.142827\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014849 - Val Loss (DD rmse only): 0.145277\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015024 - Val Loss (DD rmse only): 0.143746\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014989 - Val Loss (DD rmse only): 0.143619\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015260 - Val Loss (DD rmse only): 0.144658\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015118 - Val Loss (DD rmse only): 0.146766\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015386 - Val Loss (DD rmse only): 0.145876\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014967 - Val Loss (DD rmse only): 0.143306\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015210 - Val Loss (DD rmse only): 0.152498\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.016096 - Val Loss (DD rmse only): 0.149546\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014806 - Val Loss (DD rmse only): 0.142929\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014448 - Val Loss (DD rmse only): 0.149525\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.016224 - Val Loss (DD rmse only): 0.144207\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015909 - Val Loss (DD rmse only): 0.153086\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015239 - Val Loss (DD rmse only): 0.143266\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014619 - Val Loss (DD rmse only): 0.143011\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014559 - Val Loss (DD rmse only): 0.142429\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014517 - Val Loss (DD rmse only): 0.145776\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014466 - Val Loss (DD rmse only): 0.145383\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014538 - Val Loss (DD rmse only): 0.142695\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014254 - Val Loss (DD rmse only): 0.142659\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014350 - Val Loss (DD rmse only): 0.143077\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014249 - Val Loss (DD rmse only): 0.142312\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014405 - Val Loss (DD rmse only): 0.142912\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014343 - Val Loss (DD rmse only): 0.145030\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014077 - Val Loss (DD rmse only): 0.145875\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014281 - Val Loss (DD rmse only): 0.147401\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014127 - Val Loss (DD rmse only): 0.143325\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014062 - Val Loss (DD rmse only): 0.144964\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014317 - Val Loss (DD rmse only): 0.150403\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014352 - Val Loss (DD rmse only): 0.149376\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:37:39,476] Trial 0 finished with value: 0.14231162269910178 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 103, 'lr': 0.0006756794635066747, 'weight_decay': 1.836695118777897e-05, 'batch_size': 8}. Best is trial 0 with value: 0.14231162269910178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014195 - Val Loss (DD rmse only): 0.144855\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14772/3588211214.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_14772/3588211214.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.034124 - Val Loss (DD rmse only): 0.183880\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.018383 - Val Loss (DD rmse only): 0.156823\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.016386 - Val Loss (DD rmse only): 0.153362\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.015554 - Val Loss (DD rmse only): 0.145674\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.015748 - Val Loss (DD rmse only): 0.146594\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.015571 - Val Loss (DD rmse only): 0.145802\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.014877 - Val Loss (DD rmse only): 0.144151\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.014710 - Val Loss (DD rmse only): 0.143578\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015162 - Val Loss (DD rmse only): 0.145768\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016670 - Val Loss (DD rmse only): 0.149028\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015758 - Val Loss (DD rmse only): 0.149546\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015249 - Val Loss (DD rmse only): 0.142328\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015195 - Val Loss (DD rmse only): 0.154373\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015117 - Val Loss (DD rmse only): 0.152005\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014236 - Val Loss (DD rmse only): 0.145566\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.013951 - Val Loss (DD rmse only): 0.143785\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.013428 - Val Loss (DD rmse only): 0.149235\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.013923 - Val Loss (DD rmse only): 0.154482\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.013773 - Val Loss (DD rmse only): 0.149837\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.013722 - Val Loss (DD rmse only): 0.163602\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015105 - Val Loss (DD rmse only): 0.155896\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.013956 - Val Loss (DD rmse only): 0.145320\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014449 - Val Loss (DD rmse only): 0.156152\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.013444 - Val Loss (DD rmse only): 0.157683\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014671 - Val Loss (DD rmse only): 0.159460\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013804 - Val Loss (DD rmse only): 0.200184\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014787 - Val Loss (DD rmse only): 0.147765\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013428 - Val Loss (DD rmse only): 0.168878\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013083 - Val Loss (DD rmse only): 0.154455\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013673 - Val Loss (DD rmse only): 0.161270\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013486 - Val Loss (DD rmse only): 0.156499\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015301 - Val Loss (DD rmse only): 0.158345\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014365 - Val Loss (DD rmse only): 0.178762\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013729 - Val Loss (DD rmse only): 0.150562\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013211 - Val Loss (DD rmse only): 0.165576\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014523 - Val Loss (DD rmse only): 0.163409\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014399 - Val Loss (DD rmse only): 0.196900\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013817 - Val Loss (DD rmse only): 0.154553\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013579 - Val Loss (DD rmse only): 0.176353\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013091 - Val Loss (DD rmse only): 0.159913\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012786 - Val Loss (DD rmse only): 0.162111\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014838 - Val Loss (DD rmse only): 0.193191\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013566 - Val Loss (DD rmse only): 0.168365\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012586 - Val Loss (DD rmse only): 0.163573\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012686 - Val Loss (DD rmse only): 0.159505\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014901 - Val Loss (DD rmse only): 0.183361\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013946 - Val Loss (DD rmse only): 0.212954\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014617 - Val Loss (DD rmse only): 0.155688\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013163 - Val Loss (DD rmse only): 0.180762\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:38:10,213] Trial 1 finished with value: 0.14232835670312247 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 143, 'lr': 0.007499640120782385, 'weight_decay': 2.1412243757571026e-08, 'batch_size': 8}. Best is trial 0 with value: 0.14231162269910178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012507 - Val Loss (DD rmse only): 0.182983\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.067462 - Val Loss (DD rmse only): 0.319907\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.039971 - Val Loss (DD rmse only): 0.236477\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.033842 - Val Loss (DD rmse only): 0.209238\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.031748 - Val Loss (DD rmse only): 0.224325\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.026482 - Val Loss (DD rmse only): 0.238940\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.027303 - Val Loss (DD rmse only): 0.237610\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.028074 - Val Loss (DD rmse only): 0.220667\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.025928 - Val Loss (DD rmse only): 0.205495\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.025566 - Val Loss (DD rmse only): 0.198046\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.023717 - Val Loss (DD rmse only): 0.196788\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.021361 - Val Loss (DD rmse only): 0.182773\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.020819 - Val Loss (DD rmse only): 0.171539\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.018859 - Val Loss (DD rmse only): 0.166049\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.017302 - Val Loss (DD rmse only): 0.159384\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.017264 - Val Loss (DD rmse only): 0.158432\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016439 - Val Loss (DD rmse only): 0.157322\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016404 - Val Loss (DD rmse only): 0.155875\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014458 - Val Loss (DD rmse only): 0.155670\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016732 - Val Loss (DD rmse only): 0.152817\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015083 - Val Loss (DD rmse only): 0.150991\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015602 - Val Loss (DD rmse only): 0.149587\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015079 - Val Loss (DD rmse only): 0.148125\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014016 - Val Loss (DD rmse only): 0.148502\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015722 - Val Loss (DD rmse only): 0.148615\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016863 - Val Loss (DD rmse only): 0.144938\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014203 - Val Loss (DD rmse only): 0.144233\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013965 - Val Loss (DD rmse only): 0.143908\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013976 - Val Loss (DD rmse only): 0.144373\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013377 - Val Loss (DD rmse only): 0.144231\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015464 - Val Loss (DD rmse only): 0.144747\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014884 - Val Loss (DD rmse only): 0.143682\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014892 - Val Loss (DD rmse only): 0.142970\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014085 - Val Loss (DD rmse only): 0.143749\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014964 - Val Loss (DD rmse only): 0.144965\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.012894 - Val Loss (DD rmse only): 0.145540\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014598 - Val Loss (DD rmse only): 0.146353\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014523 - Val Loss (DD rmse only): 0.147127\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014164 - Val Loss (DD rmse only): 0.146131\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014174 - Val Loss (DD rmse only): 0.145375\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013817 - Val Loss (DD rmse only): 0.146521\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013224 - Val Loss (DD rmse only): 0.147133\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013912 - Val Loss (DD rmse only): 0.148263\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014315 - Val Loss (DD rmse only): 0.150178\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013541 - Val Loss (DD rmse only): 0.146975\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013643 - Val Loss (DD rmse only): 0.146739\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015590 - Val Loss (DD rmse only): 0.147943\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013477 - Val Loss (DD rmse only): 0.146737\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013542 - Val Loss (DD rmse only): 0.147366\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014171 - Val Loss (DD rmse only): 0.148264\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:38:28,673] Trial 2 finished with value: 0.142970472574234 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 145, 'lr': 0.0006971572429625183, 'weight_decay': 2.618345192430675e-08, 'batch_size': 32}. Best is trial 0 with value: 0.14231162269910178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013255 - Val Loss (DD rmse only): 0.150534\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.155645 - Val Loss (DD rmse only): 0.453674\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.114881 - Val Loss (DD rmse only): 0.402096\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.083144 - Val Loss (DD rmse only): 0.346857\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.056858 - Val Loss (DD rmse only): 0.291901\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.037404 - Val Loss (DD rmse only): 0.242240\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.030419 - Val Loss (DD rmse only): 0.215655\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.038005 - Val Loss (DD rmse only): 0.211474\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.042267 - Val Loss (DD rmse only): 0.209006\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.038379 - Val Loss (DD rmse only): 0.209702\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.031361 - Val Loss (DD rmse only): 0.218516\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.027852 - Val Loss (DD rmse only): 0.231532\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.028501 - Val Loss (DD rmse only): 0.242334\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.028814 - Val Loss (DD rmse only): 0.247162\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.029031 - Val Loss (DD rmse only): 0.245147\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.028688 - Val Loss (DD rmse only): 0.237349\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.027157 - Val Loss (DD rmse only): 0.225373\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.024711 - Val Loss (DD rmse only): 0.211815\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.024276 - Val Loss (DD rmse only): 0.200001\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.023224 - Val Loss (DD rmse only): 0.192192\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.023171 - Val Loss (DD rmse only): 0.189088\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.022716 - Val Loss (DD rmse only): 0.189909\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.021909 - Val Loss (DD rmse only): 0.192584\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.020813 - Val Loss (DD rmse only): 0.195057\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.021125 - Val Loss (DD rmse only): 0.195803\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.020152 - Val Loss (DD rmse only): 0.192000\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.019461 - Val Loss (DD rmse only): 0.184529\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.018289 - Val Loss (DD rmse only): 0.176741\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.018462 - Val Loss (DD rmse only): 0.170788\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.017813 - Val Loss (DD rmse only): 0.167668\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.017215 - Val Loss (DD rmse only): 0.165770\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.016647 - Val Loss (DD rmse only): 0.164520\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.016839 - Val Loss (DD rmse only): 0.163078\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015723 - Val Loss (DD rmse only): 0.160141\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.016225 - Val Loss (DD rmse only): 0.157454\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015897 - Val Loss (DD rmse only): 0.155836\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015834 - Val Loss (DD rmse only): 0.154901\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015826 - Val Loss (DD rmse only): 0.153659\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015873 - Val Loss (DD rmse only): 0.152291\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015715 - Val Loss (DD rmse only): 0.151310\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015483 - Val Loss (DD rmse only): 0.150558\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015163 - Val Loss (DD rmse only): 0.149795\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015350 - Val Loss (DD rmse only): 0.149142\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.015293 - Val Loss (DD rmse only): 0.148578\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015328 - Val Loss (DD rmse only): 0.147880\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015376 - Val Loss (DD rmse only): 0.147185\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015004 - Val Loss (DD rmse only): 0.146630\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014726 - Val Loss (DD rmse only): 0.146431\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015139 - Val Loss (DD rmse only): 0.146352\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014686 - Val Loss (DD rmse only): 0.145671\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:38:39,151] Trial 3 finished with value: 0.1451338827610016 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 81, 'lr': 0.0013129394643645767, 'weight_decay': 2.0054601111524816e-08, 'batch_size': 64}. Best is trial 0 with value: 0.14231162269910178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014815 - Val Loss (DD rmse only): 0.145134\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.080669 - Val Loss (DD rmse only): 0.386819\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.078449 - Val Loss (DD rmse only): 0.376563\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.072899 - Val Loss (DD rmse only): 0.366439\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.064515 - Val Loss (DD rmse only): 0.356489\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.061790 - Val Loss (DD rmse only): 0.346778\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.054237 - Val Loss (DD rmse only): 0.337291\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.051215 - Val Loss (DD rmse only): 0.328124\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.047685 - Val Loss (DD rmse only): 0.319266\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.043665 - Val Loss (DD rmse only): 0.310763\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.042772 - Val Loss (DD rmse only): 0.302652\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.038652 - Val Loss (DD rmse only): 0.294890\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.037522 - Val Loss (DD rmse only): 0.287617\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.034890 - Val Loss (DD rmse only): 0.280769\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.034761 - Val Loss (DD rmse only): 0.274387\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.032236 - Val Loss (DD rmse only): 0.268453\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.031171 - Val Loss (DD rmse only): 0.263037\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.030292 - Val Loss (DD rmse only): 0.258074\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.029774 - Val Loss (DD rmse only): 0.253612\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.030141 - Val Loss (DD rmse only): 0.249578\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.028670 - Val Loss (DD rmse only): 0.245914\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.028588 - Val Loss (DD rmse only): 0.242633\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.028290 - Val Loss (DD rmse only): 0.239717\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.029026 - Val Loss (DD rmse only): 0.237132\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.028482 - Val Loss (DD rmse only): 0.234865\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.026971 - Val Loss (DD rmse only): 0.232822\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.027449 - Val Loss (DD rmse only): 0.231174\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.026378 - Val Loss (DD rmse only): 0.229656\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.026726 - Val Loss (DD rmse only): 0.228342\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.026146 - Val Loss (DD rmse only): 0.227195\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.026116 - Val Loss (DD rmse only): 0.226161\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.026376 - Val Loss (DD rmse only): 0.225343\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.025760 - Val Loss (DD rmse only): 0.224568\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.024846 - Val Loss (DD rmse only): 0.223775\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.024277 - Val Loss (DD rmse only): 0.223022\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.023985 - Val Loss (DD rmse only): 0.222347\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.024156 - Val Loss (DD rmse only): 0.221595\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.024133 - Val Loss (DD rmse only): 0.220980\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.023407 - Val Loss (DD rmse only): 0.220220\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.023714 - Val Loss (DD rmse only): 0.219325\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.023279 - Val Loss (DD rmse only): 0.218289\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.023925 - Val Loss (DD rmse only): 0.217304\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.023693 - Val Loss (DD rmse only): 0.216163\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.022293 - Val Loss (DD rmse only): 0.214847\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.022154 - Val Loss (DD rmse only): 0.213377\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.021678 - Val Loss (DD rmse only): 0.211866\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.021207 - Val Loss (DD rmse only): 0.210208\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.021178 - Val Loss (DD rmse only): 0.208583\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.022016 - Val Loss (DD rmse only): 0.207066\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.020709 - Val Loss (DD rmse only): 0.205458\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:38:50,795] Trial 4 finished with value: 0.20391800999641418 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 147, 'lr': 9.815192685733519e-05, 'weight_decay': 7.08164806201331e-06, 'batch_size': 64}. Best is trial 0 with value: 0.14231162269910178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.020797 - Val Loss (DD rmse only): 0.203918\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.149377 - Val Loss (DD rmse only): 0.480039\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.146819 - Val Loss (DD rmse only): 0.476668\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.144270 - Val Loss (DD rmse only): 0.473193\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.141643 - Val Loss (DD rmse only): 0.469718\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.139036 - Val Loss (DD rmse only): 0.466175\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.136470 - Val Loss (DD rmse only): 0.462682\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.133966 - Val Loss (DD rmse only): 0.459260\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.131492 - Val Loss (DD rmse only): 0.455853\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.129040 - Val Loss (DD rmse only): 0.452398\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.126595 - Val Loss (DD rmse only): 0.448863\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.124090 - Val Loss (DD rmse only): 0.445227\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.121553 - Val Loss (DD rmse only): 0.441427\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.118939 - Val Loss (DD rmse only): 0.437566\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.116332 - Val Loss (DD rmse only): 0.433589\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.113629 - Val Loss (DD rmse only): 0.429617\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.111011 - Val Loss (DD rmse only): 0.425395\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.108225 - Val Loss (DD rmse only): 0.421084\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.105423 - Val Loss (DD rmse only): 0.416553\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.102521 - Val Loss (DD rmse only): 0.411770\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.099514 - Val Loss (DD rmse only): 0.406683\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.096317 - Val Loss (DD rmse only): 0.401394\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.093116 - Val Loss (DD rmse only): 0.395431\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.089581 - Val Loss (DD rmse only): 0.389232\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.085887 - Val Loss (DD rmse only): 0.382549\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.081969 - Val Loss (DD rmse only): 0.375197\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.077836 - Val Loss (DD rmse only): 0.367015\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.073337 - Val Loss (DD rmse only): 0.357994\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.068631 - Val Loss (DD rmse only): 0.348588\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.064124 - Val Loss (DD rmse only): 0.338674\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.059668 - Val Loss (DD rmse only): 0.328912\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.055419 - Val Loss (DD rmse only): 0.318675\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.051238 - Val Loss (DD rmse only): 0.308125\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.047248 - Val Loss (DD rmse only): 0.297618\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.043642 - Val Loss (DD rmse only): 0.286598\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.040294 - Val Loss (DD rmse only): 0.275819\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.037540 - Val Loss (DD rmse only): 0.265730\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.035175 - Val Loss (DD rmse only): 0.257366\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.033493 - Val Loss (DD rmse only): 0.250392\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.032390 - Val Loss (DD rmse only): 0.243702\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.031530 - Val Loss (DD rmse only): 0.238691\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.031007 - Val Loss (DD rmse only): 0.234728\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.030626 - Val Loss (DD rmse only): 0.231634\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.030346 - Val Loss (DD rmse only): 0.229975\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.030157 - Val Loss (DD rmse only): 0.228049\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.029967 - Val Loss (DD rmse only): 0.227134\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.029834 - Val Loss (DD rmse only): 0.225328\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.029637 - Val Loss (DD rmse only): 0.224879\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.029474 - Val Loss (DD rmse only): 0.224402\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.029322 - Val Loss (DD rmse only): 0.223769\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:40:23,005] Trial 5 finished with value: 0.2230584273735682 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 148, 'lr': 1.1620157636519109e-05, 'weight_decay': 5.878099150790295e-08, 'batch_size': 8}. Best is trial 0 with value: 0.14231162269910178.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029171 - Val Loss (DD rmse only): 0.223058\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.064993 - Val Loss (DD rmse only): 0.202497\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.023670 - Val Loss (DD rmse only): 0.182316\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.020789 - Val Loss (DD rmse only): 0.184866\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.020328 - Val Loss (DD rmse only): 0.165482\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.017626 - Val Loss (DD rmse only): 0.160565\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.016376 - Val Loss (DD rmse only): 0.156144\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016538 - Val Loss (DD rmse only): 0.158975\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015361 - Val Loss (DD rmse only): 0.151518\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015580 - Val Loss (DD rmse only): 0.146179\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.014630 - Val Loss (DD rmse only): 0.153619\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016345 - Val Loss (DD rmse only): 0.155987\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.017545 - Val Loss (DD rmse only): 0.147893\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014558 - Val Loss (DD rmse only): 0.146573\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014613 - Val Loss (DD rmse only): 0.148110\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014996 - Val Loss (DD rmse only): 0.144205\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014374 - Val Loss (DD rmse only): 0.141828\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014525 - Val Loss (DD rmse only): 0.142022\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014440 - Val Loss (DD rmse only): 0.142799\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015234 - Val Loss (DD rmse only): 0.148339\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014337 - Val Loss (DD rmse only): 0.147739\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014784 - Val Loss (DD rmse only): 0.150626\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014947 - Val Loss (DD rmse only): 0.145885\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013413 - Val Loss (DD rmse only): 0.145701\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.013701 - Val Loss (DD rmse only): 0.145616\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014137 - Val Loss (DD rmse only): 0.153667\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014303 - Val Loss (DD rmse only): 0.159208\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014582 - Val Loss (DD rmse only): 0.146186\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013248 - Val Loss (DD rmse only): 0.146334\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013615 - Val Loss (DD rmse only): 0.147472\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013627 - Val Loss (DD rmse only): 0.172707\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015048 - Val Loss (DD rmse only): 0.156488\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014560 - Val Loss (DD rmse only): 0.151622\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013097 - Val Loss (DD rmse only): 0.151970\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013622 - Val Loss (DD rmse only): 0.161864\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013584 - Val Loss (DD rmse only): 0.158710\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013477 - Val Loss (DD rmse only): 0.156687\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.012851 - Val Loss (DD rmse only): 0.156561\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013448 - Val Loss (DD rmse only): 0.166079\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013546 - Val Loss (DD rmse only): 0.174736\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013192 - Val Loss (DD rmse only): 0.149685\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012766 - Val Loss (DD rmse only): 0.156663\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013444 - Val Loss (DD rmse only): 0.170870\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012446 - Val Loss (DD rmse only): 0.151746\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012638 - Val Loss (DD rmse only): 0.167870\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012509 - Val Loss (DD rmse only): 0.168911\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012653 - Val Loss (DD rmse only): 0.162076\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012766 - Val Loss (DD rmse only): 0.173546\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013329 - Val Loss (DD rmse only): 0.168291\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013035 - Val Loss (DD rmse only): 0.161320\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:40:47,701] Trial 6 finished with value: 0.14182832092046738 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 185, 'lr': 0.001950841137569128, 'weight_decay': 4.5370473777414546e-08, 'batch_size': 16}. Best is trial 6 with value: 0.14182832092046738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013042 - Val Loss (DD rmse only): 0.165839\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.124761 - Val Loss (DD rmse only): 0.456283\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.120745 - Val Loss (DD rmse only): 0.452295\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.125909 - Val Loss (DD rmse only): 0.448328\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.111194 - Val Loss (DD rmse only): 0.444381\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.110241 - Val Loss (DD rmse only): 0.440484\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.101867 - Val Loss (DD rmse only): 0.436643\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.112760 - Val Loss (DD rmse only): 0.432842\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.102224 - Val Loss (DD rmse only): 0.429023\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.100717 - Val Loss (DD rmse only): 0.425257\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.092567 - Val Loss (DD rmse only): 0.421541\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.093702 - Val Loss (DD rmse only): 0.417882\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.088977 - Val Loss (DD rmse only): 0.414261\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.089408 - Val Loss (DD rmse only): 0.410678\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.082623 - Val Loss (DD rmse only): 0.407129\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.086401 - Val Loss (DD rmse only): 0.403627\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.091502 - Val Loss (DD rmse only): 0.400097\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.087864 - Val Loss (DD rmse only): 0.396534\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.085488 - Val Loss (DD rmse only): 0.393022\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.083114 - Val Loss (DD rmse only): 0.389498\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.075798 - Val Loss (DD rmse only): 0.386010\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.078761 - Val Loss (DD rmse only): 0.382584\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.070726 - Val Loss (DD rmse only): 0.379188\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.067413 - Val Loss (DD rmse only): 0.375881\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.063250 - Val Loss (DD rmse only): 0.372639\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.068126 - Val Loss (DD rmse only): 0.369506\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.068968 - Val Loss (DD rmse only): 0.366377\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.066507 - Val Loss (DD rmse only): 0.363260\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.061887 - Val Loss (DD rmse only): 0.360144\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.060257 - Val Loss (DD rmse only): 0.357080\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.068525 - Val Loss (DD rmse only): 0.354052\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.060865 - Val Loss (DD rmse only): 0.350979\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.057278 - Val Loss (DD rmse only): 0.347952\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.058706 - Val Loss (DD rmse only): 0.344984\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.056650 - Val Loss (DD rmse only): 0.342032\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.055705 - Val Loss (DD rmse only): 0.339113\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.049882 - Val Loss (DD rmse only): 0.336235\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.047024 - Val Loss (DD rmse only): 0.333455\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.049785 - Val Loss (DD rmse only): 0.330778\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.050995 - Val Loss (DD rmse only): 0.328132\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.050347 - Val Loss (DD rmse only): 0.325503\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.048804 - Val Loss (DD rmse only): 0.322878\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.041690 - Val Loss (DD rmse only): 0.320300\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.043214 - Val Loss (DD rmse only): 0.317820\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.049519 - Val Loss (DD rmse only): 0.315361\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.042526 - Val Loss (DD rmse only): 0.312876\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.042516 - Val Loss (DD rmse only): 0.310486\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.042006 - Val Loss (DD rmse only): 0.308127\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.039982 - Val Loss (DD rmse only): 0.305785\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.042853 - Val Loss (DD rmse only): 0.303476\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:41:02,289] Trial 7 finished with value: 0.3011603057384491 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 184, 'lr': 1.324165697564972e-05, 'weight_decay': 5.973110493526922e-05, 'batch_size': 32}. Best is trial 6 with value: 0.14182832092046738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.038373 - Val Loss (DD rmse only): 0.301160\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.126374 - Val Loss (DD rmse only): 0.384812\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.066470 - Val Loss (DD rmse only): 0.283751\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.033162 - Val Loss (DD rmse only): 0.209426\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.046118 - Val Loss (DD rmse only): 0.205302\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.037169 - Val Loss (DD rmse only): 0.224147\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.027944 - Val Loss (DD rmse only): 0.255416\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.030651 - Val Loss (DD rmse only): 0.267589\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.032520 - Val Loss (DD rmse only): 0.257616\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.030433 - Val Loss (DD rmse only): 0.232892\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.025699 - Val Loss (DD rmse only): 0.205655\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.025985 - Val Loss (DD rmse only): 0.192335\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.025917 - Val Loss (DD rmse only): 0.191158\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.023809 - Val Loss (DD rmse only): 0.195937\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.022430 - Val Loss (DD rmse only): 0.203839\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.022041 - Val Loss (DD rmse only): 0.203996\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.021703 - Val Loss (DD rmse only): 0.192261\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.019869 - Val Loss (DD rmse only): 0.175343\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.019223 - Val Loss (DD rmse only): 0.166059\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019342 - Val Loss (DD rmse only): 0.166334\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.018068 - Val Loss (DD rmse only): 0.168342\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.017463 - Val Loss (DD rmse only): 0.164887\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.017026 - Val Loss (DD rmse only): 0.157596\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.016380 - Val Loss (DD rmse only): 0.155690\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016195 - Val Loss (DD rmse only): 0.154948\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016491 - Val Loss (DD rmse only): 0.154560\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015883 - Val Loss (DD rmse only): 0.151131\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015556 - Val Loss (DD rmse only): 0.150460\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015701 - Val Loss (DD rmse only): 0.148202\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015469 - Val Loss (DD rmse only): 0.147691\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014718 - Val Loss (DD rmse only): 0.146098\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015135 - Val Loss (DD rmse only): 0.145562\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015383 - Val Loss (DD rmse only): 0.144464\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014361 - Val Loss (DD rmse only): 0.143715\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014586 - Val Loss (DD rmse only): 0.142986\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014482 - Val Loss (DD rmse only): 0.142397\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014628 - Val Loss (DD rmse only): 0.141906\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014364 - Val Loss (DD rmse only): 0.141682\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014858 - Val Loss (DD rmse only): 0.142068\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014355 - Val Loss (DD rmse only): 0.141645\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014564 - Val Loss (DD rmse only): 0.141745\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014313 - Val Loss (DD rmse only): 0.142100\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014669 - Val Loss (DD rmse only): 0.142088\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014479 - Val Loss (DD rmse only): 0.141942\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014599 - Val Loss (DD rmse only): 0.141968\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014513 - Val Loss (DD rmse only): 0.141997\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013775 - Val Loss (DD rmse only): 0.142259\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014012 - Val Loss (DD rmse only): 0.142467\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014166 - Val Loss (DD rmse only): 0.142651\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014546 - Val Loss (DD rmse only): 0.143340\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:41:19,251] Trial 8 finished with value: 0.14164549112319946 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 241, 'lr': 0.0008282620015547203, 'weight_decay': 4.764296938693548e-05, 'batch_size': 64}. Best is trial 8 with value: 0.14164549112319946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014305 - Val Loss (DD rmse only): 0.142753\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.109356 - Val Loss (DD rmse only): 0.420109\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.107968 - Val Loss (DD rmse only): 0.416022\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.106756 - Val Loss (DD rmse only): 0.411951\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.102268 - Val Loss (DD rmse only): 0.407882\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.100654 - Val Loss (DD rmse only): 0.403853\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.099982 - Val Loss (DD rmse only): 0.399863\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.095631 - Val Loss (DD rmse only): 0.395868\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.091304 - Val Loss (DD rmse only): 0.391891\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.089875 - Val Loss (DD rmse only): 0.387993\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.089387 - Val Loss (DD rmse only): 0.384103\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.085379 - Val Loss (DD rmse only): 0.380206\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.085404 - Val Loss (DD rmse only): 0.376355\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.082174 - Val Loss (DD rmse only): 0.372560\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.080456 - Val Loss (DD rmse only): 0.368717\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.079488 - Val Loss (DD rmse only): 0.364913\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.077214 - Val Loss (DD rmse only): 0.361145\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.073723 - Val Loss (DD rmse only): 0.357373\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.071404 - Val Loss (DD rmse only): 0.353652\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.072333 - Val Loss (DD rmse only): 0.349959\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.068046 - Val Loss (DD rmse only): 0.346263\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.066829 - Val Loss (DD rmse only): 0.342564\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.066049 - Val Loss (DD rmse only): 0.338923\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.064008 - Val Loss (DD rmse only): 0.335199\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.060761 - Val Loss (DD rmse only): 0.331460\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.059940 - Val Loss (DD rmse only): 0.327826\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.059598 - Val Loss (DD rmse only): 0.324118\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.056856 - Val Loss (DD rmse only): 0.320449\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.053887 - Val Loss (DD rmse only): 0.316723\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.054066 - Val Loss (DD rmse only): 0.313070\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.053677 - Val Loss (DD rmse only): 0.309499\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.051827 - Val Loss (DD rmse only): 0.305877\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.052801 - Val Loss (DD rmse only): 0.302359\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.048066 - Val Loss (DD rmse only): 0.298649\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.048512 - Val Loss (DD rmse only): 0.295075\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.047921 - Val Loss (DD rmse only): 0.291497\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.044049 - Val Loss (DD rmse only): 0.287949\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.042965 - Val Loss (DD rmse only): 0.284498\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.042085 - Val Loss (DD rmse only): 0.281164\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.042154 - Val Loss (DD rmse only): 0.277834\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.040858 - Val Loss (DD rmse only): 0.274554\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.038910 - Val Loss (DD rmse only): 0.271269\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.038929 - Val Loss (DD rmse only): 0.268213\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.037782 - Val Loss (DD rmse only): 0.265099\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.037944 - Val Loss (DD rmse only): 0.262170\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.036089 - Val Loss (DD rmse only): 0.259166\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.035558 - Val Loss (DD rmse only): 0.256327\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.035265 - Val Loss (DD rmse only): 0.253529\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.034597 - Val Loss (DD rmse only): 0.250855\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.033525 - Val Loss (DD rmse only): 0.248229\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:41:33,390] Trial 9 finished with value: 0.24571146816015244 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 46, 'lr': 3.6091064649727436e-05, 'weight_decay': 2.608461594746753e-05, 'batch_size': 16}. Best is trial 8 with value: 0.14164549112319946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032481 - Val Loss (DD rmse only): 0.245711\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.094133 - Val Loss (DD rmse only): 0.397336\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.088761 - Val Loss (DD rmse only): 0.380525\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.072152 - Val Loss (DD rmse only): 0.363879\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.065234 - Val Loss (DD rmse only): 0.347542\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.057525 - Val Loss (DD rmse only): 0.331017\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.050154 - Val Loss (DD rmse only): 0.314225\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.043426 - Val Loss (DD rmse only): 0.297230\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.039193 - Val Loss (DD rmse only): 0.280331\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.035709 - Val Loss (DD rmse only): 0.264155\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.031302 - Val Loss (DD rmse only): 0.249411\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.031470 - Val Loss (DD rmse only): 0.237593\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.030470 - Val Loss (DD rmse only): 0.229122\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.031641 - Val Loss (DD rmse only): 0.223787\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.032214 - Val Loss (DD rmse only): 0.221416\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.031181 - Val Loss (DD rmse only): 0.221144\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.030470 - Val Loss (DD rmse only): 0.222919\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.030109 - Val Loss (DD rmse only): 0.226283\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.029601 - Val Loss (DD rmse only): 0.229993\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.028457 - Val Loss (DD rmse only): 0.233244\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.027598 - Val Loss (DD rmse only): 0.235573\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.027487 - Val Loss (DD rmse only): 0.236543\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.027927 - Val Loss (DD rmse only): 0.235883\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.027003 - Val Loss (DD rmse only): 0.233541\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.026413 - Val Loss (DD rmse only): 0.230061\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.025740 - Val Loss (DD rmse only): 0.226158\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.025982 - Val Loss (DD rmse only): 0.222196\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.024774 - Val Loss (DD rmse only): 0.218276\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.024564 - Val Loss (DD rmse only): 0.215019\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.024740 - Val Loss (DD rmse only): 0.212601\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.024986 - Val Loss (DD rmse only): 0.211539\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.023299 - Val Loss (DD rmse only): 0.210991\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.023455 - Val Loss (DD rmse only): 0.210554\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.023274 - Val Loss (DD rmse only): 0.209971\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.022118 - Val Loss (DD rmse only): 0.209264\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.022290 - Val Loss (DD rmse only): 0.207498\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.022069 - Val Loss (DD rmse only): 0.205431\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.022712 - Val Loss (DD rmse only): 0.202497\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.021463 - Val Loss (DD rmse only): 0.198581\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.021214 - Val Loss (DD rmse only): 0.194782\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.020620 - Val Loss (DD rmse only): 0.191823\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.019972 - Val Loss (DD rmse only): 0.189372\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.019468 - Val Loss (DD rmse only): 0.188068\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.019665 - Val Loss (DD rmse only): 0.187573\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.018896 - Val Loss (DD rmse only): 0.186651\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.018788 - Val Loss (DD rmse only): 0.185363\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.018321 - Val Loss (DD rmse only): 0.182976\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.018196 - Val Loss (DD rmse only): 0.180861\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.018320 - Val Loss (DD rmse only): 0.178551\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.018429 - Val Loss (DD rmse only): 0.176468\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:41:51,714] Trial 10 finished with value: 0.17505738139152527 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 241, 'lr': 0.0001506626651400638, 'weight_decay': 0.0009230255678561763, 'batch_size': 64}. Best is trial 8 with value: 0.14164549112319946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.017001 - Val Loss (DD rmse only): 0.175057\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.061680 - Val Loss (DD rmse only): 0.248400\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.027649 - Val Loss (DD rmse only): 0.201352\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.020061 - Val Loss (DD rmse only): 0.164426\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.018605 - Val Loss (DD rmse only): 0.183474\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.017091 - Val Loss (DD rmse only): 0.151170\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.015393 - Val Loss (DD rmse only): 0.147134\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.014928 - Val Loss (DD rmse only): 0.144392\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.014731 - Val Loss (DD rmse only): 0.142811\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.014722 - Val Loss (DD rmse only): 0.143238\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.014450 - Val Loss (DD rmse only): 0.149279\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.014833 - Val Loss (DD rmse only): 0.153756\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.014861 - Val Loss (DD rmse only): 0.146080\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014602 - Val Loss (DD rmse only): 0.164851\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014639 - Val Loss (DD rmse only): 0.160483\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014407 - Val Loss (DD rmse only): 0.162850\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015355 - Val Loss (DD rmse only): 0.154086\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.013973 - Val Loss (DD rmse only): 0.152691\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.013966 - Val Loss (DD rmse only): 0.145583\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.013831 - Val Loss (DD rmse only): 0.152794\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014078 - Val Loss (DD rmse only): 0.151152\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014922 - Val Loss (DD rmse only): 0.157020\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015085 - Val Loss (DD rmse only): 0.155858\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014855 - Val Loss (DD rmse only): 0.153240\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014911 - Val Loss (DD rmse only): 0.153509\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015020 - Val Loss (DD rmse only): 0.153359\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014271 - Val Loss (DD rmse only): 0.152227\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013577 - Val Loss (DD rmse only): 0.155084\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013583 - Val Loss (DD rmse only): 0.156242\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015306 - Val Loss (DD rmse only): 0.151523\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013121 - Val Loss (DD rmse only): 0.158984\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013660 - Val Loss (DD rmse only): 0.165746\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013189 - Val Loss (DD rmse only): 0.157937\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013781 - Val Loss (DD rmse only): 0.181099\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013132 - Val Loss (DD rmse only): 0.170637\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.012610 - Val Loss (DD rmse only): 0.159502\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.012953 - Val Loss (DD rmse only): 0.162830\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.012921 - Val Loss (DD rmse only): 0.175992\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013649 - Val Loss (DD rmse only): 0.181597\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.012708 - Val Loss (DD rmse only): 0.176500\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.012700 - Val Loss (DD rmse only): 0.171190\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012403 - Val Loss (DD rmse only): 0.170992\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.011870 - Val Loss (DD rmse only): 0.164775\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012494 - Val Loss (DD rmse only): 0.159048\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012955 - Val Loss (DD rmse only): 0.155621\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013247 - Val Loss (DD rmse only): 0.163955\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012693 - Val Loss (DD rmse only): 0.163460\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012209 - Val Loss (DD rmse only): 0.162239\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012352 - Val Loss (DD rmse only): 0.160999\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.015276 - Val Loss (DD rmse only): 0.157082\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:42:25,152] Trial 11 finished with value: 0.1428108587861061 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 249, 'lr': 0.002887127903332695, 'weight_decay': 5.892155904954996e-07, 'batch_size': 16}. Best is trial 8 with value: 0.14164549112319946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013599 - Val Loss (DD rmse only): 0.176287\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.034698 - Val Loss (DD rmse only): 0.174331\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.019278 - Val Loss (DD rmse only): 0.163077\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.016571 - Val Loss (DD rmse only): 0.155184\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.015492 - Val Loss (DD rmse only): 0.143252\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.016749 - Val Loss (DD rmse only): 0.153656\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.016212 - Val Loss (DD rmse only): 0.147256\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.014547 - Val Loss (DD rmse only): 0.145628\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015409 - Val Loss (DD rmse only): 0.143349\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.014827 - Val Loss (DD rmse only): 0.147567\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015004 - Val Loss (DD rmse only): 0.145541\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.014136 - Val Loss (DD rmse only): 0.145469\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.014322 - Val Loss (DD rmse only): 0.151104\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014197 - Val Loss (DD rmse only): 0.156201\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014245 - Val Loss (DD rmse only): 0.148357\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.013656 - Val Loss (DD rmse only): 0.145962\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014158 - Val Loss (DD rmse only): 0.148843\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.013696 - Val Loss (DD rmse only): 0.166553\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.013777 - Val Loss (DD rmse only): 0.154332\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.013361 - Val Loss (DD rmse only): 0.150819\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.012930 - Val Loss (DD rmse only): 0.152366\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.013273 - Val Loss (DD rmse only): 0.154030\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.013024 - Val Loss (DD rmse only): 0.162851\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013182 - Val Loss (DD rmse only): 0.156687\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014322 - Val Loss (DD rmse only): 0.186238\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014423 - Val Loss (DD rmse only): 0.193532\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013432 - Val Loss (DD rmse only): 0.182122\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013570 - Val Loss (DD rmse only): 0.179300\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013853 - Val Loss (DD rmse only): 0.174726\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013752 - Val Loss (DD rmse only): 0.199068\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013565 - Val Loss (DD rmse only): 0.157262\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013133 - Val Loss (DD rmse only): 0.154190\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013758 - Val Loss (DD rmse only): 0.159147\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.012990 - Val Loss (DD rmse only): 0.159801\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.012981 - Val Loss (DD rmse only): 0.164820\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013317 - Val Loss (DD rmse only): 0.173202\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013054 - Val Loss (DD rmse only): 0.189496\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013676 - Val Loss (DD rmse only): 0.195377\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013931 - Val Loss (DD rmse only): 0.184905\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.012565 - Val Loss (DD rmse only): 0.200146\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013405 - Val Loss (DD rmse only): 0.184612\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013014 - Val Loss (DD rmse only): 0.187117\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013482 - Val Loss (DD rmse only): 0.178452\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.011884 - Val Loss (DD rmse only): 0.163231\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013116 - Val Loss (DD rmse only): 0.204406\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013073 - Val Loss (DD rmse only): 0.181466\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012217 - Val Loss (DD rmse only): 0.170536\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012312 - Val Loss (DD rmse only): 0.174417\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012913 - Val Loss (DD rmse only): 0.184645\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012144 - Val Loss (DD rmse only): 0.157209\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:42:49,611] Trial 12 finished with value: 0.14325226098299026 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 213, 'lr': 0.004234063513106175, 'weight_decay': 7.827818867092288e-07, 'batch_size': 16}. Best is trial 8 with value: 0.14164549112319946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012364 - Val Loss (DD rmse only): 0.186312\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.073884 - Val Loss (DD rmse only): 0.191376\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.035312 - Val Loss (DD rmse only): 0.240724\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.028829 - Val Loss (DD rmse only): 0.181420\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.024274 - Val Loss (DD rmse only): 0.196788\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.021314 - Val Loss (DD rmse only): 0.171240\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.018480 - Val Loss (DD rmse only): 0.171166\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.018849 - Val Loss (DD rmse only): 0.168495\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017031 - Val Loss (DD rmse only): 0.167579\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016823 - Val Loss (DD rmse only): 0.157404\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016501 - Val Loss (DD rmse only): 0.153608\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016155 - Val Loss (DD rmse only): 0.151927\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015580 - Val Loss (DD rmse only): 0.148040\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015340 - Val Loss (DD rmse only): 0.145864\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015604 - Val Loss (DD rmse only): 0.145270\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015044 - Val Loss (DD rmse only): 0.146330\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015017 - Val Loss (DD rmse only): 0.144372\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014839 - Val Loss (DD rmse only): 0.145093\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014547 - Val Loss (DD rmse only): 0.149695\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014566 - Val Loss (DD rmse only): 0.150184\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015047 - Val Loss (DD rmse only): 0.154860\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014895 - Val Loss (DD rmse only): 0.149114\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014731 - Val Loss (DD rmse only): 0.148698\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015410 - Val Loss (DD rmse only): 0.147241\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015227 - Val Loss (DD rmse only): 0.146546\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014585 - Val Loss (DD rmse only): 0.145763\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014559 - Val Loss (DD rmse only): 0.145656\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015107 - Val Loss (DD rmse only): 0.154710\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014628 - Val Loss (DD rmse only): 0.146284\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014089 - Val Loss (DD rmse only): 0.148031\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014032 - Val Loss (DD rmse only): 0.144972\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014696 - Val Loss (DD rmse only): 0.145615\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015041 - Val Loss (DD rmse only): 0.146971\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015307 - Val Loss (DD rmse only): 0.153644\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015328 - Val Loss (DD rmse only): 0.149139\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015221 - Val Loss (DD rmse only): 0.179526\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.016001 - Val Loss (DD rmse only): 0.148354\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015173 - Val Loss (DD rmse only): 0.150028\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015050 - Val Loss (DD rmse only): 0.148200\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015162 - Val Loss (DD rmse only): 0.148581\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014534 - Val Loss (DD rmse only): 0.156884\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014507 - Val Loss (DD rmse only): 0.150494\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015204 - Val Loss (DD rmse only): 0.146718\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014221 - Val Loss (DD rmse only): 0.146377\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013942 - Val Loss (DD rmse only): 0.147045\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013950 - Val Loss (DD rmse only): 0.146797\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014343 - Val Loss (DD rmse only): 0.157883\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014489 - Val Loss (DD rmse only): 0.157011\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014391 - Val Loss (DD rmse only): 0.157732\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014266 - Val Loss (DD rmse only): 0.153514\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:43:23,977] Trial 13 finished with value: 0.1443721503019333 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 200, 'lr': 0.0016216756472595326, 'weight_decay': 0.0002070846774821972, 'batch_size': 16}. Best is trial 8 with value: 0.14164549112319946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.015128 - Val Loss (DD rmse only): 0.149614\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.144614 - Val Loss (DD rmse only): 0.447425\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.111787 - Val Loss (DD rmse only): 0.407077\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.086680 - Val Loss (DD rmse only): 0.366567\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.066159 - Val Loss (DD rmse only): 0.325147\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.048972 - Val Loss (DD rmse only): 0.282820\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.036055 - Val Loss (DD rmse only): 0.244602\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.031128 - Val Loss (DD rmse only): 0.221131\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.034329 - Val Loss (DD rmse only): 0.213957\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.038547 - Val Loss (DD rmse only): 0.211986\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.037359 - Val Loss (DD rmse only): 0.212645\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.031814 - Val Loss (DD rmse only): 0.218191\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.029044 - Val Loss (DD rmse only): 0.227400\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.027955 - Val Loss (DD rmse only): 0.237005\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.029724 - Val Loss (DD rmse only): 0.243390\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.028917 - Val Loss (DD rmse only): 0.244279\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.029064 - Val Loss (DD rmse only): 0.239757\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.026751 - Val Loss (DD rmse only): 0.231317\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.026078 - Val Loss (DD rmse only): 0.222036\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.025205 - Val Loss (DD rmse only): 0.213348\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.025000 - Val Loss (DD rmse only): 0.206864\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.024145 - Val Loss (DD rmse only): 0.203311\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.024223 - Val Loss (DD rmse only): 0.202036\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.023869 - Val Loss (DD rmse only): 0.202114\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.022894 - Val Loss (DD rmse only): 0.203487\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.022530 - Val Loss (DD rmse only): 0.204563\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.022436 - Val Loss (DD rmse only): 0.203999\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.022070 - Val Loss (DD rmse only): 0.200937\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.020335 - Val Loss (DD rmse only): 0.195204\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.020569 - Val Loss (DD rmse only): 0.189663\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.020064 - Val Loss (DD rmse only): 0.185156\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.019386 - Val Loss (DD rmse only): 0.182149\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.018542 - Val Loss (DD rmse only): 0.180643\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.018675 - Val Loss (DD rmse only): 0.180378\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.018347 - Val Loss (DD rmse only): 0.178728\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.018167 - Val Loss (DD rmse only): 0.175345\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.017053 - Val Loss (DD rmse only): 0.170796\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.016776 - Val Loss (DD rmse only): 0.167659\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.017009 - Val Loss (DD rmse only): 0.165858\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.016555 - Val Loss (DD rmse only): 0.164172\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.016233 - Val Loss (DD rmse only): 0.163195\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.016442 - Val Loss (DD rmse only): 0.161386\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.016079 - Val Loss (DD rmse only): 0.159716\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.015179 - Val Loss (DD rmse only): 0.158124\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015818 - Val Loss (DD rmse only): 0.156570\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015872 - Val Loss (DD rmse only): 0.155292\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015618 - Val Loss (DD rmse only): 0.154249\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015374 - Val Loss (DD rmse only): 0.153146\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015829 - Val Loss (DD rmse only): 0.152019\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.015108 - Val Loss (DD rmse only): 0.151018\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:43:40,064] Trial 14 finished with value: 0.15022499859333038 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 214, 'lr': 0.00031694706989430284, 'weight_decay': 3.4884408084415634e-07, 'batch_size': 64}. Best is trial 8 with value: 0.14164549112319946.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014793 - Val Loss (DD rmse only): 0.150225\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.060619 - Val Loss (DD rmse only): 0.221231\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.032984 - Val Loss (DD rmse only): 0.187239\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.027163 - Val Loss (DD rmse only): 0.209369\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025551 - Val Loss (DD rmse only): 0.201148\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.022321 - Val Loss (DD rmse only): 0.177494\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.022379 - Val Loss (DD rmse only): 0.178769\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.020236 - Val Loss (DD rmse only): 0.173561\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.018291 - Val Loss (DD rmse only): 0.162365\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.017515 - Val Loss (DD rmse only): 0.159192\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016270 - Val Loss (DD rmse only): 0.153221\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015476 - Val Loss (DD rmse only): 0.149728\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015594 - Val Loss (DD rmse only): 0.148045\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015529 - Val Loss (DD rmse only): 0.148283\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015079 - Val Loss (DD rmse only): 0.144823\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015388 - Val Loss (DD rmse only): 0.143545\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014488 - Val Loss (DD rmse only): 0.145427\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014892 - Val Loss (DD rmse only): 0.142902\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014572 - Val Loss (DD rmse only): 0.143296\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014690 - Val Loss (DD rmse only): 0.143523\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014825 - Val Loss (DD rmse only): 0.141675\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014626 - Val Loss (DD rmse only): 0.145286\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014727 - Val Loss (DD rmse only): 0.141636\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014463 - Val Loss (DD rmse only): 0.145932\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014887 - Val Loss (DD rmse only): 0.141717\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014217 - Val Loss (DD rmse only): 0.142076\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014695 - Val Loss (DD rmse only): 0.145401\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014079 - Val Loss (DD rmse only): 0.141156\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014217 - Val Loss (DD rmse only): 0.146535\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014258 - Val Loss (DD rmse only): 0.142129\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013878 - Val Loss (DD rmse only): 0.147746\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013671 - Val Loss (DD rmse only): 0.142050\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013634 - Val Loss (DD rmse only): 0.145812\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014183 - Val Loss (DD rmse only): 0.145155\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013655 - Val Loss (DD rmse only): 0.140843\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014114 - Val Loss (DD rmse only): 0.145128\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013731 - Val Loss (DD rmse only): 0.142739\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014004 - Val Loss (DD rmse only): 0.142784\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013680 - Val Loss (DD rmse only): 0.154392\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013939 - Val Loss (DD rmse only): 0.145031\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013831 - Val Loss (DD rmse only): 0.144837\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012979 - Val Loss (DD rmse only): 0.146193\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013377 - Val Loss (DD rmse only): 0.143333\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013937 - Val Loss (DD rmse only): 0.154804\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014711 - Val Loss (DD rmse only): 0.147468\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013137 - Val Loss (DD rmse only): 0.147226\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013402 - Val Loss (DD rmse only): 0.153083\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013916 - Val Loss (DD rmse only): 0.147398\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013007 - Val Loss (DD rmse only): 0.148236\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013166 - Val Loss (DD rmse only): 0.147113\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:44:03,545] Trial 15 finished with value: 0.14084292203187943 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 188, 'lr': 0.0004335537465239168, 'weight_decay': 2.89710107147184e-06, 'batch_size': 16}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013051 - Val Loss (DD rmse only): 0.154499\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.095848 - Val Loss (DD rmse only): 0.397033\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.083709 - Val Loss (DD rmse only): 0.378533\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.075335 - Val Loss (DD rmse only): 0.359744\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.065291 - Val Loss (DD rmse only): 0.339339\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.051588 - Val Loss (DD rmse only): 0.316283\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.044432 - Val Loss (DD rmse only): 0.290681\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.036676 - Val Loss (DD rmse only): 0.263893\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.031342 - Val Loss (DD rmse only): 0.238526\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.029592 - Val Loss (DD rmse only): 0.219857\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.030843 - Val Loss (DD rmse only): 0.211635\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.032656 - Val Loss (DD rmse only): 0.210145\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.031921 - Val Loss (DD rmse only): 0.213457\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.029044 - Val Loss (DD rmse only): 0.219819\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.027406 - Val Loss (DD rmse only): 0.227401\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.027690 - Val Loss (DD rmse only): 0.233372\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.027423 - Val Loss (DD rmse only): 0.235360\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.027360 - Val Loss (DD rmse only): 0.233357\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.027364 - Val Loss (DD rmse only): 0.227613\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.026182 - Val Loss (DD rmse only): 0.219290\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.025000 - Val Loss (DD rmse only): 0.210746\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.024778 - Val Loss (DD rmse only): 0.204378\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.024649 - Val Loss (DD rmse only): 0.200924\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.023807 - Val Loss (DD rmse only): 0.200229\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.023588 - Val Loss (DD rmse only): 0.200438\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.022860 - Val Loss (DD rmse only): 0.200563\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.022785 - Val Loss (DD rmse only): 0.198371\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.022040 - Val Loss (DD rmse only): 0.193903\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.022251 - Val Loss (DD rmse only): 0.188016\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.019909 - Val Loss (DD rmse only): 0.181499\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.019718 - Val Loss (DD rmse only): 0.177299\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.019388 - Val Loss (DD rmse only): 0.173728\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.019160 - Val Loss (DD rmse only): 0.170948\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.018348 - Val Loss (DD rmse only): 0.167984\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.017521 - Val Loss (DD rmse only): 0.164294\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.017693 - Val Loss (DD rmse only): 0.161689\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.017345 - Val Loss (DD rmse only): 0.159980\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.017009 - Val Loss (DD rmse only): 0.158683\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.016669 - Val Loss (DD rmse only): 0.157368\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.016713 - Val Loss (DD rmse only): 0.156280\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015869 - Val Loss (DD rmse only): 0.155226\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015459 - Val Loss (DD rmse only): 0.154171\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015867 - Val Loss (DD rmse only): 0.153098\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.016101 - Val Loss (DD rmse only): 0.152117\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015777 - Val Loss (DD rmse only): 0.151465\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015581 - Val Loss (DD rmse only): 0.150785\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015267 - Val Loss (DD rmse only): 0.149779\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014876 - Val Loss (DD rmse only): 0.149333\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015130 - Val Loss (DD rmse only): 0.148250\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.015132 - Val Loss (DD rmse only): 0.147865\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:44:23,984] Trial 16 finished with value: 0.1467011719942093 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 256, 'lr': 0.00022293559840653378, 'weight_decay': 2.413041909785649e-06, 'batch_size': 64}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.015267 - Val Loss (DD rmse only): 0.146701\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.098946 - Val Loss (DD rmse only): 0.323801\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.038454 - Val Loss (DD rmse only): 0.216488\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.040808 - Val Loss (DD rmse only): 0.206721\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.033509 - Val Loss (DD rmse only): 0.230049\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.032571 - Val Loss (DD rmse only): 0.247958\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.032894 - Val Loss (DD rmse only): 0.235581\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.025051 - Val Loss (DD rmse only): 0.209540\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.025463 - Val Loss (DD rmse only): 0.196206\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.027851 - Val Loss (DD rmse only): 0.194253\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.023665 - Val Loss (DD rmse only): 0.197886\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.024222 - Val Loss (DD rmse only): 0.191394\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.021136 - Val Loss (DD rmse only): 0.175961\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.018662 - Val Loss (DD rmse only): 0.171534\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.018606 - Val Loss (DD rmse only): 0.169405\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.018269 - Val Loss (DD rmse only): 0.161029\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.017625 - Val Loss (DD rmse only): 0.159465\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016352 - Val Loss (DD rmse only): 0.160610\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016817 - Val Loss (DD rmse only): 0.158853\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016835 - Val Loss (DD rmse only): 0.153698\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014757 - Val Loss (DD rmse only): 0.152641\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015843 - Val Loss (DD rmse only): 0.150048\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.016584 - Val Loss (DD rmse only): 0.148244\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.017223 - Val Loss (DD rmse only): 0.147075\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015899 - Val Loss (DD rmse only): 0.147063\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015291 - Val Loss (DD rmse only): 0.144469\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.016390 - Val Loss (DD rmse only): 0.146353\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.016173 - Val Loss (DD rmse only): 0.144727\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015907 - Val Loss (DD rmse only): 0.142414\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014753 - Val Loss (DD rmse only): 0.142730\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014181 - Val Loss (DD rmse only): 0.141963\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014926 - Val Loss (DD rmse only): 0.142183\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014239 - Val Loss (DD rmse only): 0.141745\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.016568 - Val Loss (DD rmse only): 0.141984\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015282 - Val Loss (DD rmse only): 0.143274\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015245 - Val Loss (DD rmse only): 0.142400\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.016199 - Val Loss (DD rmse only): 0.141847\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.016328 - Val Loss (DD rmse only): 0.142375\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014712 - Val Loss (DD rmse only): 0.143037\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014833 - Val Loss (DD rmse only): 0.143151\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013335 - Val Loss (DD rmse only): 0.144450\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014477 - Val Loss (DD rmse only): 0.144134\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015361 - Val Loss (DD rmse only): 0.144021\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014654 - Val Loss (DD rmse only): 0.145506\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014214 - Val Loss (DD rmse only): 0.145120\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013930 - Val Loss (DD rmse only): 0.144452\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015506 - Val Loss (DD rmse only): 0.145118\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014347 - Val Loss (DD rmse only): 0.144031\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013512 - Val Loss (DD rmse only): 0.143445\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014305 - Val Loss (DD rmse only): 0.142999\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:44:46,851] Trial 17 finished with value: 0.14174477756023407 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 227, 'lr': 0.0005070479117046343, 'weight_decay': 0.00014030179481191034, 'batch_size': 32}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014611 - Val Loss (DD rmse only): 0.143188\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.120153 - Val Loss (DD rmse only): 0.424259\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.098350 - Val Loss (DD rmse only): 0.397985\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.084710 - Val Loss (DD rmse only): 0.372767\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.071758 - Val Loss (DD rmse only): 0.348342\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.059730 - Val Loss (DD rmse only): 0.324381\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.051093 - Val Loss (DD rmse only): 0.301276\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.042915 - Val Loss (DD rmse only): 0.279619\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.036579 - Val Loss (DD rmse only): 0.259846\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.032170 - Val Loss (DD rmse only): 0.243768\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.030434 - Val Loss (DD rmse only): 0.231757\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.028872 - Val Loss (DD rmse only): 0.223023\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.027365 - Val Loss (DD rmse only): 0.217411\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.026902 - Val Loss (DD rmse only): 0.213670\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.026488 - Val Loss (DD rmse only): 0.211241\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.025537 - Val Loss (DD rmse only): 0.210098\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.025252 - Val Loss (DD rmse only): 0.209531\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.024841 - Val Loss (DD rmse only): 0.208928\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.024980 - Val Loss (DD rmse only): 0.206856\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.024320 - Val Loss (DD rmse only): 0.205159\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.024046 - Val Loss (DD rmse only): 0.202622\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.022595 - Val Loss (DD rmse only): 0.199939\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.022838 - Val Loss (DD rmse only): 0.197158\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.021875 - Val Loss (DD rmse only): 0.194959\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.021832 - Val Loss (DD rmse only): 0.192993\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.020758 - Val Loss (DD rmse only): 0.189656\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.021481 - Val Loss (DD rmse only): 0.187746\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.020412 - Val Loss (DD rmse only): 0.185320\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.019324 - Val Loss (DD rmse only): 0.182786\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.019027 - Val Loss (DD rmse only): 0.179303\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.018716 - Val Loss (DD rmse only): 0.178302\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.018256 - Val Loss (DD rmse only): 0.176241\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.018446 - Val Loss (DD rmse only): 0.173138\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.018130 - Val Loss (DD rmse only): 0.170286\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.016991 - Val Loss (DD rmse only): 0.167991\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.018039 - Val Loss (DD rmse only): 0.167096\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.016767 - Val Loss (DD rmse only): 0.165171\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.016458 - Val Loss (DD rmse only): 0.163114\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.016399 - Val Loss (DD rmse only): 0.161979\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.016594 - Val Loss (DD rmse only): 0.160365\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015868 - Val Loss (DD rmse only): 0.159378\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.016819 - Val Loss (DD rmse only): 0.157922\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015873 - Val Loss (DD rmse only): 0.156683\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.015670 - Val Loss (DD rmse only): 0.155876\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015351 - Val Loss (DD rmse only): 0.155120\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015708 - Val Loss (DD rmse only): 0.154232\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015468 - Val Loss (DD rmse only): 0.153462\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015343 - Val Loss (DD rmse only): 0.152785\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015561 - Val Loss (DD rmse only): 0.152061\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.016384 - Val Loss (DD rmse only): 0.151436\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:45:10,827] Trial 18 finished with value: 0.15103796124458313 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 178, 'lr': 5.3657102527289343e-05, 'weight_decay': 3.940491458332612e-06, 'batch_size': 16}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.015548 - Val Loss (DD rmse only): 0.151038\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.142022 - Val Loss (DD rmse only): 0.425130\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.095148 - Val Loss (DD rmse only): 0.349231\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.051724 - Val Loss (DD rmse only): 0.247500\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.029328 - Val Loss (DD rmse only): 0.204838\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.045506 - Val Loss (DD rmse only): 0.201549\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.034257 - Val Loss (DD rmse only): 0.218116\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.025830 - Val Loss (DD rmse only): 0.247891\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.032176 - Val Loss (DD rmse only): 0.263812\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.033439 - Val Loss (DD rmse only): 0.259000\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.031059 - Val Loss (DD rmse only): 0.240619\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.027790 - Val Loss (DD rmse only): 0.216956\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.026019 - Val Loss (DD rmse only): 0.198843\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.026228 - Val Loss (DD rmse only): 0.191143\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.026639 - Val Loss (DD rmse only): 0.192022\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.024998 - Val Loss (DD rmse only): 0.200968\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.023674 - Val Loss (DD rmse only): 0.209300\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.024012 - Val Loss (DD rmse only): 0.210391\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.023311 - Val Loss (DD rmse only): 0.201996\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.022060 - Val Loss (DD rmse only): 0.190073\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.021800 - Val Loss (DD rmse only): 0.180068\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.020423 - Val Loss (DD rmse only): 0.175224\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.019729 - Val Loss (DD rmse only): 0.175421\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.019950 - Val Loss (DD rmse only): 0.177138\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.018941 - Val Loss (DD rmse only): 0.171847\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.018216 - Val Loss (DD rmse only): 0.163485\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.017844 - Val Loss (DD rmse only): 0.158851\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.017125 - Val Loss (DD rmse only): 0.159075\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.017469 - Val Loss (DD rmse only): 0.159304\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.017180 - Val Loss (DD rmse only): 0.155177\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016556 - Val Loss (DD rmse only): 0.154078\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.017134 - Val Loss (DD rmse only): 0.152797\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.016209 - Val Loss (DD rmse only): 0.152780\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.016567 - Val Loss (DD rmse only): 0.151433\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.016567 - Val Loss (DD rmse only): 0.150511\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.016002 - Val Loss (DD rmse only): 0.149810\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015996 - Val Loss (DD rmse only): 0.149567\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015321 - Val Loss (DD rmse only): 0.148969\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.016484 - Val Loss (DD rmse only): 0.148518\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015844 - Val Loss (DD rmse only): 0.147889\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015700 - Val Loss (DD rmse only): 0.147460\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015613 - Val Loss (DD rmse only): 0.147785\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.016387 - Val Loss (DD rmse only): 0.146377\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.015728 - Val Loss (DD rmse only): 0.146510\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015205 - Val Loss (DD rmse only): 0.145407\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015033 - Val Loss (DD rmse only): 0.146530\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015278 - Val Loss (DD rmse only): 0.144908\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015151 - Val Loss (DD rmse only): 0.144880\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015198 - Val Loss (DD rmse only): 0.144663\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.015495 - Val Loss (DD rmse only): 0.144969\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:45:26,336] Trial 19 finished with value: 0.14447839558124542 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 168, 'lr': 0.0009604109729804366, 'weight_decay': 0.0006822204516710209, 'batch_size': 64}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014789 - Val Loss (DD rmse only): 0.144478\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.119837 - Val Loss (DD rmse only): 0.406933\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.083976 - Val Loss (DD rmse only): 0.352079\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.054192 - Val Loss (DD rmse only): 0.305096\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.039878 - Val Loss (DD rmse only): 0.264913\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.032750 - Val Loss (DD rmse only): 0.235782\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.032304 - Val Loss (DD rmse only): 0.221313\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.035638 - Val Loss (DD rmse only): 0.215599\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.035312 - Val Loss (DD rmse only): 0.213852\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.032414 - Val Loss (DD rmse only): 0.215982\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.029194 - Val Loss (DD rmse only): 0.221157\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.026148 - Val Loss (DD rmse only): 0.226192\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.026649 - Val Loss (DD rmse only): 0.228209\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.025301 - Val Loss (DD rmse only): 0.225005\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.023854 - Val Loss (DD rmse only): 0.216972\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.023220 - Val Loss (DD rmse only): 0.206715\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.021786 - Val Loss (DD rmse only): 0.196524\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.021090 - Val Loss (DD rmse only): 0.188727\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.020958 - Val Loss (DD rmse only): 0.184168\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019743 - Val Loss (DD rmse only): 0.181850\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.018477 - Val Loss (DD rmse only): 0.181164\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.018232 - Val Loss (DD rmse only): 0.180601\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.018076 - Val Loss (DD rmse only): 0.177686\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.017706 - Val Loss (DD rmse only): 0.172867\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.017673 - Val Loss (DD rmse only): 0.168152\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.017222 - Val Loss (DD rmse only): 0.164975\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.016848 - Val Loss (DD rmse only): 0.163098\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.016448 - Val Loss (DD rmse only): 0.162051\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015951 - Val Loss (DD rmse only): 0.161018\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.016044 - Val Loss (DD rmse only): 0.158917\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016313 - Val Loss (DD rmse only): 0.156746\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015910 - Val Loss (DD rmse only): 0.155176\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015549 - Val Loss (DD rmse only): 0.153567\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015082 - Val Loss (DD rmse only): 0.152064\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015178 - Val Loss (DD rmse only): 0.150798\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015770 - Val Loss (DD rmse only): 0.149312\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015113 - Val Loss (DD rmse only): 0.148190\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014982 - Val Loss (DD rmse only): 0.147124\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014600 - Val Loss (DD rmse only): 0.146213\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014597 - Val Loss (DD rmse only): 0.145460\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014467 - Val Loss (DD rmse only): 0.144967\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013946 - Val Loss (DD rmse only): 0.144761\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014247 - Val Loss (DD rmse only): 0.144110\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014541 - Val Loss (DD rmse only): 0.143760\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014845 - Val Loss (DD rmse only): 0.143622\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014001 - Val Loss (DD rmse only): 0.144415\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014760 - Val Loss (DD rmse only): 0.144197\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014621 - Val Loss (DD rmse only): 0.143746\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014667 - Val Loss (DD rmse only): 0.143855\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014584 - Val Loss (DD rmse only): 0.144394\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:45:39,603] Trial 20 finished with value: 0.1436222642660141 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 226, 'lr': 0.0003691171880253425, 'weight_decay': 1.6922627881979055e-07, 'batch_size': 64}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014161 - Val Loss (DD rmse only): 0.144240\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.127646 - Val Loss (DD rmse only): 0.398238\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.058765 - Val Loss (DD rmse only): 0.263066\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.034035 - Val Loss (DD rmse only): 0.209962\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.036066 - Val Loss (DD rmse only): 0.220093\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.027816 - Val Loss (DD rmse only): 0.251083\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.031189 - Val Loss (DD rmse only): 0.243344\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.026351 - Val Loss (DD rmse only): 0.211935\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.025949 - Val Loss (DD rmse only): 0.193939\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.023169 - Val Loss (DD rmse only): 0.195617\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.022975 - Val Loss (DD rmse only): 0.203840\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.022005 - Val Loss (DD rmse only): 0.189333\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.018851 - Val Loss (DD rmse only): 0.171206\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.018426 - Val Loss (DD rmse only): 0.167011\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016760 - Val Loss (DD rmse only): 0.171375\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.017770 - Val Loss (DD rmse only): 0.161080\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016457 - Val Loss (DD rmse only): 0.157037\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017094 - Val Loss (DD rmse only): 0.155105\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016960 - Val Loss (DD rmse only): 0.153485\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.017234 - Val Loss (DD rmse only): 0.151488\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016246 - Val Loss (DD rmse only): 0.149973\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014941 - Val Loss (DD rmse only): 0.148993\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.016421 - Val Loss (DD rmse only): 0.148802\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.016126 - Val Loss (DD rmse only): 0.146903\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014777 - Val Loss (DD rmse only): 0.146303\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014405 - Val Loss (DD rmse only): 0.145935\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014776 - Val Loss (DD rmse only): 0.145030\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014084 - Val Loss (DD rmse only): 0.144355\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015579 - Val Loss (DD rmse only): 0.144086\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014827 - Val Loss (DD rmse only): 0.143513\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014546 - Val Loss (DD rmse only): 0.143819\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014571 - Val Loss (DD rmse only): 0.143843\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014715 - Val Loss (DD rmse only): 0.144139\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014849 - Val Loss (DD rmse only): 0.144426\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014184 - Val Loss (DD rmse only): 0.144195\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013073 - Val Loss (DD rmse only): 0.143921\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015738 - Val Loss (DD rmse only): 0.144102\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014370 - Val Loss (DD rmse only): 0.145914\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015283 - Val Loss (DD rmse only): 0.145489\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014819 - Val Loss (DD rmse only): 0.145621\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013612 - Val Loss (DD rmse only): 0.147672\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014811 - Val Loss (DD rmse only): 0.146294\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013885 - Val Loss (DD rmse only): 0.145692\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.015718 - Val Loss (DD rmse only): 0.146430\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015666 - Val Loss (DD rmse only): 0.145704\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015724 - Val Loss (DD rmse only): 0.144809\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015170 - Val Loss (DD rmse only): 0.144713\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014511 - Val Loss (DD rmse only): 0.144459\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013500 - Val Loss (DD rmse only): 0.144298\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013868 - Val Loss (DD rmse only): 0.144721\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:46:01,526] Trial 21 finished with value: 0.1435127556324005 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 230, 'lr': 0.0005149126592504811, 'weight_decay': 0.00017366397059879983, 'batch_size': 32}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.015447 - Val Loss (DD rmse only): 0.144920\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.120365 - Val Loss (DD rmse only): 0.428968\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.098464 - Val Loss (DD rmse only): 0.388678\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.067911 - Val Loss (DD rmse only): 0.345921\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.048488 - Val Loss (DD rmse only): 0.301323\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.036756 - Val Loss (DD rmse only): 0.256897\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.031632 - Val Loss (DD rmse only): 0.224028\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.033611 - Val Loss (DD rmse only): 0.212145\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.032655 - Val Loss (DD rmse only): 0.212776\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.029310 - Val Loss (DD rmse only): 0.218950\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.027899 - Val Loss (DD rmse only): 0.224481\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.028740 - Val Loss (DD rmse only): 0.225321\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.026359 - Val Loss (DD rmse only): 0.222355\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.025515 - Val Loss (DD rmse only): 0.216409\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.023804 - Val Loss (DD rmse only): 0.214655\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.026116 - Val Loss (DD rmse only): 0.211789\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.022621 - Val Loss (DD rmse only): 0.205485\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.020860 - Val Loss (DD rmse only): 0.200615\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.022098 - Val Loss (DD rmse only): 0.196695\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019878 - Val Loss (DD rmse only): 0.187955\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.018292 - Val Loss (DD rmse only): 0.185546\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.018368 - Val Loss (DD rmse only): 0.184029\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.017528 - Val Loss (DD rmse only): 0.177810\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.017585 - Val Loss (DD rmse only): 0.173458\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016656 - Val Loss (DD rmse only): 0.170182\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016505 - Val Loss (DD rmse only): 0.166061\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.016005 - Val Loss (DD rmse only): 0.164042\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.016661 - Val Loss (DD rmse only): 0.162758\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.016913 - Val Loss (DD rmse only): 0.158810\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015297 - Val Loss (DD rmse only): 0.156903\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016088 - Val Loss (DD rmse only): 0.155005\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.016432 - Val Loss (DD rmse only): 0.154211\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014357 - Val Loss (DD rmse only): 0.152918\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014914 - Val Loss (DD rmse only): 0.151303\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014205 - Val Loss (DD rmse only): 0.149922\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014749 - Val Loss (DD rmse only): 0.148795\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.017155 - Val Loss (DD rmse only): 0.147946\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014879 - Val Loss (DD rmse only): 0.147960\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015443 - Val Loss (DD rmse only): 0.146736\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014458 - Val Loss (DD rmse only): 0.146259\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.018269 - Val Loss (DD rmse only): 0.146152\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014541 - Val Loss (DD rmse only): 0.145708\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015279 - Val Loss (DD rmse only): 0.146141\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014324 - Val Loss (DD rmse only): 0.145832\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014281 - Val Loss (DD rmse only): 0.145394\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013738 - Val Loss (DD rmse only): 0.145430\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014423 - Val Loss (DD rmse only): 0.145147\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.016233 - Val Loss (DD rmse only): 0.144907\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014712 - Val Loss (DD rmse only): 0.145250\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014972 - Val Loss (DD rmse only): 0.144995\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:46:21,950] Trial 22 finished with value: 0.14487454295158386 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 204, 'lr': 0.00019221034810349246, 'weight_decay': 8.53886037932311e-05, 'batch_size': 32}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013955 - Val Loss (DD rmse only): 0.144875\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.119508 - Val Loss (DD rmse only): 0.417258\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.092388 - Val Loss (DD rmse only): 0.391037\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.080133 - Val Loss (DD rmse only): 0.364369\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.058850 - Val Loss (DD rmse only): 0.336603\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.053708 - Val Loss (DD rmse only): 0.308154\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.040808 - Val Loss (DD rmse only): 0.279479\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.032382 - Val Loss (DD rmse only): 0.252120\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.030930 - Val Loss (DD rmse only): 0.230651\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.028066 - Val Loss (DD rmse only): 0.217499\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.027438 - Val Loss (DD rmse only): 0.212537\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.028526 - Val Loss (DD rmse only): 0.212480\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.026555 - Val Loss (DD rmse only): 0.215026\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.025357 - Val Loss (DD rmse only): 0.218995\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.023891 - Val Loss (DD rmse only): 0.220540\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.023978 - Val Loss (DD rmse only): 0.218578\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.023809 - Val Loss (DD rmse only): 0.213743\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.023244 - Val Loss (DD rmse only): 0.206630\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.022725 - Val Loss (DD rmse only): 0.199840\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.021464 - Val Loss (DD rmse only): 0.195239\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.020093 - Val Loss (DD rmse only): 0.196159\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.020639 - Val Loss (DD rmse only): 0.195489\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.018863 - Val Loss (DD rmse only): 0.188814\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.018558 - Val Loss (DD rmse only): 0.183982\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.019643 - Val Loss (DD rmse only): 0.177331\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.020089 - Val Loss (DD rmse only): 0.174175\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.020202 - Val Loss (DD rmse only): 0.172209\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.017087 - Val Loss (DD rmse only): 0.166845\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.018629 - Val Loss (DD rmse only): 0.163554\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.016806 - Val Loss (DD rmse only): 0.160806\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016233 - Val Loss (DD rmse only): 0.159713\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015258 - Val Loss (DD rmse only): 0.159015\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.016026 - Val Loss (DD rmse only): 0.157022\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014852 - Val Loss (DD rmse only): 0.155448\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.016521 - Val Loss (DD rmse only): 0.154559\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014620 - Val Loss (DD rmse only): 0.153442\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014729 - Val Loss (DD rmse only): 0.152494\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014453 - Val Loss (DD rmse only): 0.151933\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014176 - Val Loss (DD rmse only): 0.150987\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015057 - Val Loss (DD rmse only): 0.150419\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015740 - Val Loss (DD rmse only): 0.149702\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.016286 - Val Loss (DD rmse only): 0.149998\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015864 - Val Loss (DD rmse only): 0.148740\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014869 - Val Loss (DD rmse only): 0.147778\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014230 - Val Loss (DD rmse only): 0.147409\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014821 - Val Loss (DD rmse only): 0.146895\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014176 - Val Loss (DD rmse only): 0.146574\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015873 - Val Loss (DD rmse only): 0.146613\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015766 - Val Loss (DD rmse only): 0.146135\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.016356 - Val Loss (DD rmse only): 0.145665\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:46:44,010] Trial 23 finished with value: 0.145204097032547 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 229, 'lr': 0.00010848145558838075, 'weight_decay': 1.4827713337316835e-05, 'batch_size': 32}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014486 - Val Loss (DD rmse only): 0.145204\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.126852 - Val Loss (DD rmse only): 0.412484\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.081036 - Val Loss (DD rmse only): 0.323509\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.040796 - Val Loss (DD rmse only): 0.220853\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.035257 - Val Loss (DD rmse only): 0.202314\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.032481 - Val Loss (DD rmse only): 0.208835\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.031286 - Val Loss (DD rmse only): 0.229874\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.026515 - Val Loss (DD rmse only): 0.227582\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.024226 - Val Loss (DD rmse only): 0.210538\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.024354 - Val Loss (DD rmse only): 0.196960\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.024578 - Val Loss (DD rmse only): 0.192899\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.023491 - Val Loss (DD rmse only): 0.190720\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.021861 - Val Loss (DD rmse only): 0.186128\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.019477 - Val Loss (DD rmse only): 0.176239\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.018714 - Val Loss (DD rmse only): 0.170143\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.017885 - Val Loss (DD rmse only): 0.165899\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.018772 - Val Loss (DD rmse only): 0.164304\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016646 - Val Loss (DD rmse only): 0.159207\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017344 - Val Loss (DD rmse only): 0.157175\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.017971 - Val Loss (DD rmse only): 0.153390\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015392 - Val Loss (DD rmse only): 0.151429\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.016007 - Val Loss (DD rmse only): 0.150234\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014954 - Val Loss (DD rmse only): 0.147551\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013963 - Val Loss (DD rmse only): 0.146139\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014924 - Val Loss (DD rmse only): 0.145476\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016129 - Val Loss (DD rmse only): 0.143931\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.017138 - Val Loss (DD rmse only): 0.143345\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013863 - Val Loss (DD rmse only): 0.142772\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013486 - Val Loss (DD rmse only): 0.142190\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014636 - Val Loss (DD rmse only): 0.142043\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014003 - Val Loss (DD rmse only): 0.141961\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014842 - Val Loss (DD rmse only): 0.142120\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014082 - Val Loss (DD rmse only): 0.142311\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013676 - Val Loss (DD rmse only): 0.141982\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015378 - Val Loss (DD rmse only): 0.141734\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015645 - Val Loss (DD rmse only): 0.141718\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013963 - Val Loss (DD rmse only): 0.141388\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013897 - Val Loss (DD rmse only): 0.141505\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015612 - Val Loss (DD rmse only): 0.141817\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014080 - Val Loss (DD rmse only): 0.142621\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014452 - Val Loss (DD rmse only): 0.142350\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015076 - Val Loss (DD rmse only): 0.142257\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014417 - Val Loss (DD rmse only): 0.142243\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013666 - Val Loss (DD rmse only): 0.142510\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015153 - Val Loss (DD rmse only): 0.142642\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015099 - Val Loss (DD rmse only): 0.142547\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014126 - Val Loss (DD rmse only): 0.143396\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014447 - Val Loss (DD rmse only): 0.145534\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014046 - Val Loss (DD rmse only): 0.143958\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014889 - Val Loss (DD rmse only): 0.143476\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:47:07,865] Trial 24 finished with value: 0.1413876712322235 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 195, 'lr': 0.0004299637254313597, 'weight_decay': 0.0002871384519303311, 'batch_size': 32}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013916 - Val Loss (DD rmse only): 0.142506\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.141212 - Val Loss (DD rmse only): 0.431519\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.087765 - Val Loss (DD rmse only): 0.336041\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.042926 - Val Loss (DD rmse only): 0.232046\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.033632 - Val Loss (DD rmse only): 0.205348\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.034686 - Val Loss (DD rmse only): 0.215533\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.026803 - Val Loss (DD rmse only): 0.235545\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.026927 - Val Loss (DD rmse only): 0.237704\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.029276 - Val Loss (DD rmse only): 0.221825\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.024119 - Val Loss (DD rmse only): 0.197454\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.027102 - Val Loss (DD rmse only): 0.187094\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.022501 - Val Loss (DD rmse only): 0.192696\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.023685 - Val Loss (DD rmse only): 0.192741\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.019212 - Val Loss (DD rmse only): 0.172450\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.021214 - Val Loss (DD rmse only): 0.166570\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.019321 - Val Loss (DD rmse only): 0.170919\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.020399 - Val Loss (DD rmse only): 0.160093\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016856 - Val Loss (DD rmse only): 0.158134\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016068 - Val Loss (DD rmse only): 0.155728\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.017044 - Val Loss (DD rmse only): 0.153541\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016589 - Val Loss (DD rmse only): 0.158563\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.016178 - Val Loss (DD rmse only): 0.152125\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.018112 - Val Loss (DD rmse only): 0.154111\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015493 - Val Loss (DD rmse only): 0.152016\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016236 - Val Loss (DD rmse only): 0.149458\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015927 - Val Loss (DD rmse only): 0.152201\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.016770 - Val Loss (DD rmse only): 0.147765\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.017392 - Val Loss (DD rmse only): 0.147726\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015255 - Val Loss (DD rmse only): 0.147130\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014126 - Val Loss (DD rmse only): 0.147081\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014037 - Val Loss (DD rmse only): 0.146029\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.016541 - Val Loss (DD rmse only): 0.145393\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015100 - Val Loss (DD rmse only): 0.144802\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015811 - Val Loss (DD rmse only): 0.144263\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.017838 - Val Loss (DD rmse only): 0.145794\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015416 - Val Loss (DD rmse only): 0.143031\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015474 - Val Loss (DD rmse only): 0.143532\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014131 - Val Loss (DD rmse only): 0.142974\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014548 - Val Loss (DD rmse only): 0.142982\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014853 - Val Loss (DD rmse only): 0.142547\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.016703 - Val Loss (DD rmse only): 0.143297\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014530 - Val Loss (DD rmse only): 0.142453\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014227 - Val Loss (DD rmse only): 0.144244\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014579 - Val Loss (DD rmse only): 0.143094\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.016539 - Val Loss (DD rmse only): 0.143083\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015215 - Val Loss (DD rmse only): 0.143350\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015446 - Val Loss (DD rmse only): 0.143816\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015934 - Val Loss (DD rmse only): 0.143091\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014355 - Val Loss (DD rmse only): 0.143234\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013967 - Val Loss (DD rmse only): 0.143544\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:47:26,170] Trial 25 finished with value: 0.14245323836803436 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 113, 'lr': 0.0009410708849424372, 'weight_decay': 0.00032192071261422263, 'batch_size': 32}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.015480 - Val Loss (DD rmse only): 0.143347\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.048950 - Val Loss (DD rmse only): 0.249931\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.034008 - Val Loss (DD rmse only): 0.218662\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.030101 - Val Loss (DD rmse only): 0.202850\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028015 - Val Loss (DD rmse only): 0.217969\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.026016 - Val Loss (DD rmse only): 0.178260\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.019888 - Val Loss (DD rmse only): 0.165139\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.018592 - Val Loss (DD rmse only): 0.163642\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017487 - Val Loss (DD rmse only): 0.153538\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015546 - Val Loss (DD rmse only): 0.153925\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015949 - Val Loss (DD rmse only): 0.154451\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017229 - Val Loss (DD rmse only): 0.145685\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015645 - Val Loss (DD rmse only): 0.143853\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015452 - Val Loss (DD rmse only): 0.160317\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016551 - Val Loss (DD rmse only): 0.143896\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015842 - Val Loss (DD rmse only): 0.158909\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015508 - Val Loss (DD rmse only): 0.148684\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014711 - Val Loss (DD rmse only): 0.144785\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015295 - Val Loss (DD rmse only): 0.146713\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015191 - Val Loss (DD rmse only): 0.151073\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014281 - Val Loss (DD rmse only): 0.151689\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014359 - Val Loss (DD rmse only): 0.146028\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014291 - Val Loss (DD rmse only): 0.148737\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014684 - Val Loss (DD rmse only): 0.163751\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016107 - Val Loss (DD rmse only): 0.187661\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.017461 - Val Loss (DD rmse only): 0.155755\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015531 - Val Loss (DD rmse only): 0.147793\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014550 - Val Loss (DD rmse only): 0.154087\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014336 - Val Loss (DD rmse only): 0.154842\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014491 - Val Loss (DD rmse only): 0.157663\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014137 - Val Loss (DD rmse only): 0.154921\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013974 - Val Loss (DD rmse only): 0.148717\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014140 - Val Loss (DD rmse only): 0.148203\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013655 - Val Loss (DD rmse only): 0.150674\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013664 - Val Loss (DD rmse only): 0.153353\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013332 - Val Loss (DD rmse only): 0.153559\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013604 - Val Loss (DD rmse only): 0.148336\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013709 - Val Loss (DD rmse only): 0.150031\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013549 - Val Loss (DD rmse only): 0.151967\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013965 - Val Loss (DD rmse only): 0.157108\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014535 - Val Loss (DD rmse only): 0.153630\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014542 - Val Loss (DD rmse only): 0.181958\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015115 - Val Loss (DD rmse only): 0.184687\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014065 - Val Loss (DD rmse only): 0.151018\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013202 - Val Loss (DD rmse only): 0.152069\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013526 - Val Loss (DD rmse only): 0.155520\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014717 - Val Loss (DD rmse only): 0.183670\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013439 - Val Loss (DD rmse only): 0.166802\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012757 - Val Loss (DD rmse only): 0.155722\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013008 - Val Loss (DD rmse only): 0.151388\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:48:02,248] Trial 26 finished with value: 0.1438533142209053 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 164, 'lr': 0.0030117153711580074, 'weight_decay': 4.3636917576492415e-05, 'batch_size': 16}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014549 - Val Loss (DD rmse only): 0.150398\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.120081 - Val Loss (DD rmse only): 0.352603\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.039174 - Val Loss (DD rmse only): 0.194783\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.029566 - Val Loss (DD rmse only): 0.232281\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.026474 - Val Loss (DD rmse only): 0.189890\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.022830 - Val Loss (DD rmse only): 0.188579\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.020240 - Val Loss (DD rmse only): 0.171541\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.018355 - Val Loss (DD rmse only): 0.167885\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016845 - Val Loss (DD rmse only): 0.159225\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016819 - Val Loss (DD rmse only): 0.154620\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016455 - Val Loss (DD rmse only): 0.151385\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015445 - Val Loss (DD rmse only): 0.149068\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015219 - Val Loss (DD rmse only): 0.150660\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015273 - Val Loss (DD rmse only): 0.145376\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014910 - Val Loss (DD rmse only): 0.146682\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015275 - Val Loss (DD rmse only): 0.146842\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014884 - Val Loss (DD rmse only): 0.145515\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014671 - Val Loss (DD rmse only): 0.142983\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014562 - Val Loss (DD rmse only): 0.147188\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015788 - Val Loss (DD rmse only): 0.150980\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015455 - Val Loss (DD rmse only): 0.154520\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014893 - Val Loss (DD rmse only): 0.145893\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014331 - Val Loss (DD rmse only): 0.147759\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014513 - Val Loss (DD rmse only): 0.146216\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014622 - Val Loss (DD rmse only): 0.144329\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014576 - Val Loss (DD rmse only): 0.147105\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014171 - Val Loss (DD rmse only): 0.144587\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014017 - Val Loss (DD rmse only): 0.146264\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014331 - Val Loss (DD rmse only): 0.144225\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014595 - Val Loss (DD rmse only): 0.147898\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013997 - Val Loss (DD rmse only): 0.146629\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014268 - Val Loss (DD rmse only): 0.145685\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014651 - Val Loss (DD rmse only): 0.144443\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015030 - Val Loss (DD rmse only): 0.145530\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014671 - Val Loss (DD rmse only): 0.153899\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013678 - Val Loss (DD rmse only): 0.145610\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013857 - Val Loss (DD rmse only): 0.145643\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014264 - Val Loss (DD rmse only): 0.147167\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013509 - Val Loss (DD rmse only): 0.145910\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013980 - Val Loss (DD rmse only): 0.146262\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014324 - Val Loss (DD rmse only): 0.145490\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013971 - Val Loss (DD rmse only): 0.147800\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013761 - Val Loss (DD rmse only): 0.146288\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013754 - Val Loss (DD rmse only): 0.148906\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013478 - Val Loss (DD rmse only): 0.149813\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013575 - Val Loss (DD rmse only): 0.147947\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013161 - Val Loss (DD rmse only): 0.150369\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013904 - Val Loss (DD rmse only): 0.154406\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013571 - Val Loss (DD rmse only): 0.151516\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013149 - Val Loss (DD rmse only): 0.154229\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:48:56,816] Trial 27 finished with value: 0.14298343161741892 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 202, 'lr': 0.00029703976925170175, 'weight_decay': 7.935554460933664e-06, 'batch_size': 8}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013035 - Val Loss (DD rmse only): 0.149564\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.169040 - Val Loss (DD rmse only): 0.500983\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.157561 - Val Loss (DD rmse only): 0.490030\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.152094 - Val Loss (DD rmse only): 0.478591\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.144435 - Val Loss (DD rmse only): 0.466363\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.133590 - Val Loss (DD rmse only): 0.452574\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.124305 - Val Loss (DD rmse only): 0.437216\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.113474 - Val Loss (DD rmse only): 0.419899\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.101615 - Val Loss (DD rmse only): 0.400119\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.090045 - Val Loss (DD rmse only): 0.377107\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.075450 - Val Loss (DD rmse only): 0.350554\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.060370 - Val Loss (DD rmse only): 0.321157\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.050721 - Val Loss (DD rmse only): 0.291396\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.041251 - Val Loss (DD rmse only): 0.263299\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.035446 - Val Loss (DD rmse only): 0.239802\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.031719 - Val Loss (DD rmse only): 0.225902\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.030484 - Val Loss (DD rmse only): 0.216546\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.030233 - Val Loss (DD rmse only): 0.213659\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.030365 - Val Loss (DD rmse only): 0.214262\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.029526 - Val Loss (DD rmse only): 0.215215\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.029213 - Val Loss (DD rmse only): 0.215068\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.028252 - Val Loss (DD rmse only): 0.213979\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.027269 - Val Loss (DD rmse only): 0.212455\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.028598 - Val Loss (DD rmse only): 0.209428\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.027093 - Val Loss (DD rmse only): 0.206373\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.027001 - Val Loss (DD rmse only): 0.206834\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.026391 - Val Loss (DD rmse only): 0.206030\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.025060 - Val Loss (DD rmse only): 0.203910\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.024489 - Val Loss (DD rmse only): 0.201884\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.024540 - Val Loss (DD rmse only): 0.198183\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.024133 - Val Loss (DD rmse only): 0.194620\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.024202 - Val Loss (DD rmse only): 0.194045\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.023429 - Val Loss (DD rmse only): 0.190747\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.021680 - Val Loss (DD rmse only): 0.189281\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.021689 - Val Loss (DD rmse only): 0.189084\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.021355 - Val Loss (DD rmse only): 0.185749\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.021237 - Val Loss (DD rmse only): 0.184066\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.020831 - Val Loss (DD rmse only): 0.181443\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.020268 - Val Loss (DD rmse only): 0.179743\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.020410 - Val Loss (DD rmse only): 0.177225\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.018965 - Val Loss (DD rmse only): 0.176997\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.018495 - Val Loss (DD rmse only): 0.175631\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.018033 - Val Loss (DD rmse only): 0.173471\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.018506 - Val Loss (DD rmse only): 0.172218\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.017783 - Val Loss (DD rmse only): 0.170607\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.017653 - Val Loss (DD rmse only): 0.169347\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.017382 - Val Loss (DD rmse only): 0.168149\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.016715 - Val Loss (DD rmse only): 0.167127\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.016503 - Val Loss (DD rmse only): 0.166309\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.016656 - Val Loss (DD rmse only): 0.165707\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:49:36,666] Trial 28 finished with value: 0.16400381177663803 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 189, 'lr': 4.494195500389599e-05, 'weight_decay': 1.353241377819275e-06, 'batch_size': 16}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.016072 - Val Loss (DD rmse only): 0.164004\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.049622 - Val Loss (DD rmse only): 0.198770\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.029318 - Val Loss (DD rmse only): 0.226859\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.025909 - Val Loss (DD rmse only): 0.188870\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.021571 - Val Loss (DD rmse only): 0.167558\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.018809 - Val Loss (DD rmse only): 0.156083\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.017526 - Val Loss (DD rmse only): 0.160918\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.015969 - Val Loss (DD rmse only): 0.151119\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015509 - Val Loss (DD rmse only): 0.145704\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015383 - Val Loss (DD rmse only): 0.146810\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015308 - Val Loss (DD rmse only): 0.144805\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015733 - Val Loss (DD rmse only): 0.144603\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015279 - Val Loss (DD rmse only): 0.144238\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014784 - Val Loss (DD rmse only): 0.142298\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014841 - Val Loss (DD rmse only): 0.142387\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014620 - Val Loss (DD rmse only): 0.141125\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015561 - Val Loss (DD rmse only): 0.147255\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015257 - Val Loss (DD rmse only): 0.142865\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014979 - Val Loss (DD rmse only): 0.142577\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014603 - Val Loss (DD rmse only): 0.143513\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014878 - Val Loss (DD rmse only): 0.142413\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014408 - Val Loss (DD rmse only): 0.142046\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014410 - Val Loss (DD rmse only): 0.141645\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014477 - Val Loss (DD rmse only): 0.141987\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014223 - Val Loss (DD rmse only): 0.144927\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015025 - Val Loss (DD rmse only): 0.144201\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014290 - Val Loss (DD rmse only): 0.144451\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014218 - Val Loss (DD rmse only): 0.145174\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013999 - Val Loss (DD rmse only): 0.142922\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013804 - Val Loss (DD rmse only): 0.143756\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013712 - Val Loss (DD rmse only): 0.145445\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014411 - Val Loss (DD rmse only): 0.147828\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014183 - Val Loss (DD rmse only): 0.144799\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013828 - Val Loss (DD rmse only): 0.144348\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013601 - Val Loss (DD rmse only): 0.147040\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013759 - Val Loss (DD rmse only): 0.145095\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013936 - Val Loss (DD rmse only): 0.152317\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013869 - Val Loss (DD rmse only): 0.154086\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014317 - Val Loss (DD rmse only): 0.152878\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013538 - Val Loss (DD rmse only): 0.150232\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014632 - Val Loss (DD rmse only): 0.154161\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015109 - Val Loss (DD rmse only): 0.148187\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014121 - Val Loss (DD rmse only): 0.151950\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013370 - Val Loss (DD rmse only): 0.146280\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013544 - Val Loss (DD rmse only): 0.144453\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013454 - Val Loss (DD rmse only): 0.145104\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013263 - Val Loss (DD rmse only): 0.148640\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013191 - Val Loss (DD rmse only): 0.145470\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013072 - Val Loss (DD rmse only): 0.154953\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013777 - Val Loss (DD rmse only): 0.149391\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:50:38,629] Trial 29 finished with value: 0.14112543563048044 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 167, 'lr': 0.0007654505592186309, 'weight_decay': 2.142048596457221e-05, 'batch_size': 8}. Best is trial 15 with value: 0.14084292203187943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014424 - Val Loss (DD rmse only): 0.155409\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.071018 - Val Loss (DD rmse only): 0.292180\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.033818 - Val Loss (DD rmse only): 0.204467\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.030051 - Val Loss (DD rmse only): 0.225930\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028057 - Val Loss (DD rmse only): 0.209212\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.027876 - Val Loss (DD rmse only): 0.213422\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.026131 - Val Loss (DD rmse only): 0.197918\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.024522 - Val Loss (DD rmse only): 0.190272\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.021835 - Val Loss (DD rmse only): 0.186685\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.020013 - Val Loss (DD rmse only): 0.180668\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.018699 - Val Loss (DD rmse only): 0.160942\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016698 - Val Loss (DD rmse only): 0.153694\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016074 - Val Loss (DD rmse only): 0.149582\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016803 - Val Loss (DD rmse only): 0.149741\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015258 - Val Loss (DD rmse only): 0.144867\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014931 - Val Loss (DD rmse only): 0.142935\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015187 - Val Loss (DD rmse only): 0.141479\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015438 - Val Loss (DD rmse only): 0.142723\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015512 - Val Loss (DD rmse only): 0.147543\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015167 - Val Loss (DD rmse only): 0.142214\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015200 - Val Loss (DD rmse only): 0.141017\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015028 - Val Loss (DD rmse only): 0.141704\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014992 - Val Loss (DD rmse only): 0.142513\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014485 - Val Loss (DD rmse only): 0.140738\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014522 - Val Loss (DD rmse only): 0.143398\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014805 - Val Loss (DD rmse only): 0.141047\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014375 - Val Loss (DD rmse only): 0.141342\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014503 - Val Loss (DD rmse only): 0.142263\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014746 - Val Loss (DD rmse only): 0.142309\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014367 - Val Loss (DD rmse only): 0.141231\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014205 - Val Loss (DD rmse only): 0.141473\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014306 - Val Loss (DD rmse only): 0.143457\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014986 - Val Loss (DD rmse only): 0.142428\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014398 - Val Loss (DD rmse only): 0.143189\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014557 - Val Loss (DD rmse only): 0.143441\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014219 - Val Loss (DD rmse only): 0.142045\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014109 - Val Loss (DD rmse only): 0.143129\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014116 - Val Loss (DD rmse only): 0.145017\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014418 - Val Loss (DD rmse only): 0.142917\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014451 - Val Loss (DD rmse only): 0.142668\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014039 - Val Loss (DD rmse only): 0.142762\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013997 - Val Loss (DD rmse only): 0.142853\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013807 - Val Loss (DD rmse only): 0.141473\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013969 - Val Loss (DD rmse only): 0.142487\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013597 - Val Loss (DD rmse only): 0.144791\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013688 - Val Loss (DD rmse only): 0.143150\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013731 - Val Loss (DD rmse only): 0.142318\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013741 - Val Loss (DD rmse only): 0.144351\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013600 - Val Loss (DD rmse only): 0.145281\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013611 - Val Loss (DD rmse only): 0.145564\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:51:15,543] Trial 30 finished with value: 0.14073761800924936 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 124, 'lr': 0.0004932323530456757, 'weight_decay': 1.6702716837594655e-05, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014145 - Val Loss (DD rmse only): 0.147391\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.061835 - Val Loss (DD rmse only): 0.280511\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.032884 - Val Loss (DD rmse only): 0.202715\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.028460 - Val Loss (DD rmse only): 0.221456\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.027691 - Val Loss (DD rmse only): 0.200105\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.025520 - Val Loss (DD rmse only): 0.200730\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.023133 - Val Loss (DD rmse only): 0.175902\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.020673 - Val Loss (DD rmse only): 0.167289\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017980 - Val Loss (DD rmse only): 0.159124\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.017646 - Val Loss (DD rmse only): 0.156026\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016705 - Val Loss (DD rmse only): 0.156808\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016082 - Val Loss (DD rmse only): 0.153626\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015537 - Val Loss (DD rmse only): 0.151948\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015433 - Val Loss (DD rmse only): 0.147963\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015725 - Val Loss (DD rmse only): 0.146214\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015260 - Val Loss (DD rmse only): 0.146280\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014982 - Val Loss (DD rmse only): 0.145367\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014970 - Val Loss (DD rmse only): 0.143929\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014763 - Val Loss (DD rmse only): 0.143540\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014894 - Val Loss (DD rmse only): 0.144778\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015063 - Val Loss (DD rmse only): 0.145521\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014767 - Val Loss (DD rmse only): 0.145483\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014584 - Val Loss (DD rmse only): 0.144575\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014574 - Val Loss (DD rmse only): 0.147019\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014634 - Val Loss (DD rmse only): 0.147294\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014707 - Val Loss (DD rmse only): 0.147683\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014584 - Val Loss (DD rmse only): 0.146377\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014830 - Val Loss (DD rmse only): 0.148521\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014760 - Val Loss (DD rmse only): 0.145015\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014745 - Val Loss (DD rmse only): 0.145062\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014409 - Val Loss (DD rmse only): 0.147105\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015192 - Val Loss (DD rmse only): 0.147382\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015060 - Val Loss (DD rmse only): 0.149280\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014497 - Val Loss (DD rmse only): 0.147559\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014207 - Val Loss (DD rmse only): 0.148847\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014191 - Val Loss (DD rmse only): 0.145575\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014117 - Val Loss (DD rmse only): 0.143857\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014248 - Val Loss (DD rmse only): 0.147454\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014403 - Val Loss (DD rmse only): 0.145588\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014280 - Val Loss (DD rmse only): 0.149742\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013988 - Val Loss (DD rmse only): 0.149941\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014160 - Val Loss (DD rmse only): 0.149851\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014329 - Val Loss (DD rmse only): 0.155044\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014603 - Val Loss (DD rmse only): 0.151025\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014084 - Val Loss (DD rmse only): 0.162079\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015089 - Val Loss (DD rmse only): 0.157480\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014717 - Val Loss (DD rmse only): 0.148421\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014491 - Val Loss (DD rmse only): 0.148497\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013942 - Val Loss (DD rmse only): 0.150247\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013645 - Val Loss (DD rmse only): 0.151109\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:51:52,347] Trial 31 finished with value: 0.1435404121875763 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 112, 'lr': 0.0004891686136592471, 'weight_decay': 9.235209404751587e-06, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013842 - Val Loss (DD rmse only): 0.162634\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.070055 - Val Loss (DD rmse only): 0.202997\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.029047 - Val Loss (DD rmse only): 0.254732\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031356 - Val Loss (DD rmse only): 0.202366\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.026218 - Val Loss (DD rmse only): 0.209039\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.022405 - Val Loss (DD rmse only): 0.168995\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.019317 - Val Loss (DD rmse only): 0.166103\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016775 - Val Loss (DD rmse only): 0.154155\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016128 - Val Loss (DD rmse only): 0.151355\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015606 - Val Loss (DD rmse only): 0.149014\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015823 - Val Loss (DD rmse only): 0.149578\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015563 - Val Loss (DD rmse only): 0.150118\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015173 - Val Loss (DD rmse only): 0.146222\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015601 - Val Loss (DD rmse only): 0.150761\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014911 - Val Loss (DD rmse only): 0.143548\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014455 - Val Loss (DD rmse only): 0.144303\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014333 - Val Loss (DD rmse only): 0.144067\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014476 - Val Loss (DD rmse only): 0.144208\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015142 - Val Loss (DD rmse only): 0.146748\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014603 - Val Loss (DD rmse only): 0.149956\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014413 - Val Loss (DD rmse only): 0.146381\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.013830 - Val Loss (DD rmse only): 0.150809\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014096 - Val Loss (DD rmse only): 0.148317\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013927 - Val Loss (DD rmse only): 0.148789\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.013918 - Val Loss (DD rmse only): 0.146530\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014040 - Val Loss (DD rmse only): 0.148505\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013782 - Val Loss (DD rmse only): 0.153518\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014807 - Val Loss (DD rmse only): 0.155478\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014901 - Val Loss (DD rmse only): 0.170204\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014955 - Val Loss (DD rmse only): 0.154717\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014142 - Val Loss (DD rmse only): 0.151543\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013448 - Val Loss (DD rmse only): 0.157965\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013541 - Val Loss (DD rmse only): 0.149514\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013293 - Val Loss (DD rmse only): 0.152961\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013721 - Val Loss (DD rmse only): 0.147658\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013336 - Val Loss (DD rmse only): 0.152139\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013314 - Val Loss (DD rmse only): 0.149188\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013550 - Val Loss (DD rmse only): 0.155515\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013082 - Val Loss (DD rmse only): 0.151321\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.012965 - Val Loss (DD rmse only): 0.156344\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.012979 - Val Loss (DD rmse only): 0.154103\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012896 - Val Loss (DD rmse only): 0.160361\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013171 - Val Loss (DD rmse only): 0.165676\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014484 - Val Loss (DD rmse only): 0.158085\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014965 - Val Loss (DD rmse only): 0.164355\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015434 - Val Loss (DD rmse only): 0.156134\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013266 - Val Loss (DD rmse only): 0.166058\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012894 - Val Loss (DD rmse only): 0.155083\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013536 - Val Loss (DD rmse only): 0.156013\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013475 - Val Loss (DD rmse only): 0.154841\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:52:26,981] Trial 32 finished with value: 0.14354812602202097 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 126, 'lr': 0.0011421109295151326, 'weight_decay': 2.0482313610098236e-05, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013805 - Val Loss (DD rmse only): 0.155721\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.070011 - Val Loss (DD rmse only): 0.206731\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.031333 - Val Loss (DD rmse only): 0.237531\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.027192 - Val Loss (DD rmse only): 0.200864\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025747 - Val Loss (DD rmse only): 0.194848\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.022784 - Val Loss (DD rmse only): 0.192803\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.019307 - Val Loss (DD rmse only): 0.169219\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016488 - Val Loss (DD rmse only): 0.156271\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015939 - Val Loss (DD rmse only): 0.151012\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016162 - Val Loss (DD rmse only): 0.149474\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015125 - Val Loss (DD rmse only): 0.156458\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016514 - Val Loss (DD rmse only): 0.159236\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015496 - Val Loss (DD rmse only): 0.147996\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014746 - Val Loss (DD rmse only): 0.146752\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014644 - Val Loss (DD rmse only): 0.146438\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015167 - Val Loss (DD rmse only): 0.147156\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014725 - Val Loss (DD rmse only): 0.144898\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014477 - Val Loss (DD rmse only): 0.149194\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015700 - Val Loss (DD rmse only): 0.150798\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016334 - Val Loss (DD rmse only): 0.148255\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014424 - Val Loss (DD rmse only): 0.149564\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014228 - Val Loss (DD rmse only): 0.149453\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014300 - Val Loss (DD rmse only): 0.148611\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013983 - Val Loss (DD rmse only): 0.147315\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014170 - Val Loss (DD rmse only): 0.146990\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014458 - Val Loss (DD rmse only): 0.148151\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014116 - Val Loss (DD rmse only): 0.147751\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014132 - Val Loss (DD rmse only): 0.148061\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013819 - Val Loss (DD rmse only): 0.148617\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014065 - Val Loss (DD rmse only): 0.154510\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013802 - Val Loss (DD rmse only): 0.149578\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014393 - Val Loss (DD rmse only): 0.148773\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014248 - Val Loss (DD rmse only): 0.154987\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014775 - Val Loss (DD rmse only): 0.150924\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.016760 - Val Loss (DD rmse only): 0.183634\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014989 - Val Loss (DD rmse only): 0.151952\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014176 - Val Loss (DD rmse only): 0.158003\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014386 - Val Loss (DD rmse only): 0.155678\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013945 - Val Loss (DD rmse only): 0.162907\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013566 - Val Loss (DD rmse only): 0.149332\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013534 - Val Loss (DD rmse only): 0.148365\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014092 - Val Loss (DD rmse only): 0.149453\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013568 - Val Loss (DD rmse only): 0.149683\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013419 - Val Loss (DD rmse only): 0.150558\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013336 - Val Loss (DD rmse only): 0.151051\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013517 - Val Loss (DD rmse only): 0.155007\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013177 - Val Loss (DD rmse only): 0.157112\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013710 - Val Loss (DD rmse only): 0.156343\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013866 - Val Loss (DD rmse only): 0.161914\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013035 - Val Loss (DD rmse only): 0.152859\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:53:05,975] Trial 33 finished with value: 0.14489774405956268 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 164, 'lr': 0.0006204252237646521, 'weight_decay': 2.5669948108754295e-06, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013712 - Val Loss (DD rmse only): 0.153298\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.064787 - Val Loss (DD rmse only): 0.305097\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.037043 - Val Loss (DD rmse only): 0.236830\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.029004 - Val Loss (DD rmse only): 0.206616\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028553 - Val Loss (DD rmse only): 0.217216\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028644 - Val Loss (DD rmse only): 0.223688\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.027698 - Val Loss (DD rmse only): 0.211228\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.027364 - Val Loss (DD rmse only): 0.206600\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.026581 - Val Loss (DD rmse only): 0.211444\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.025362 - Val Loss (DD rmse only): 0.195884\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.023655 - Val Loss (DD rmse only): 0.191565\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.021328 - Val Loss (DD rmse only): 0.174766\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.019232 - Val Loss (DD rmse only): 0.171833\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.018522 - Val Loss (DD rmse only): 0.162896\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.017727 - Val Loss (DD rmse only): 0.161184\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.017494 - Val Loss (DD rmse only): 0.158434\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016796 - Val Loss (DD rmse only): 0.156521\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016529 - Val Loss (DD rmse only): 0.153506\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016149 - Val Loss (DD rmse only): 0.150534\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015609 - Val Loss (DD rmse only): 0.150564\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016024 - Val Loss (DD rmse only): 0.148744\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015491 - Val Loss (DD rmse only): 0.145312\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015561 - Val Loss (DD rmse only): 0.145655\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015891 - Val Loss (DD rmse only): 0.145876\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015463 - Val Loss (DD rmse only): 0.144460\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015190 - Val Loss (DD rmse only): 0.143587\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015253 - Val Loss (DD rmse only): 0.143230\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015114 - Val Loss (DD rmse only): 0.144530\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015061 - Val Loss (DD rmse only): 0.143214\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015218 - Val Loss (DD rmse only): 0.142367\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014940 - Val Loss (DD rmse only): 0.142410\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015050 - Val Loss (DD rmse only): 0.143076\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015123 - Val Loss (DD rmse only): 0.142644\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014955 - Val Loss (DD rmse only): 0.143189\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015025 - Val Loss (DD rmse only): 0.143464\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014977 - Val Loss (DD rmse only): 0.142467\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014984 - Val Loss (DD rmse only): 0.142120\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015440 - Val Loss (DD rmse only): 0.143139\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015329 - Val Loss (DD rmse only): 0.148547\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015622 - Val Loss (DD rmse only): 0.144368\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015100 - Val Loss (DD rmse only): 0.146020\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015023 - Val Loss (DD rmse only): 0.144340\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014963 - Val Loss (DD rmse only): 0.142937\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.015150 - Val Loss (DD rmse only): 0.150095\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015607 - Val Loss (DD rmse only): 0.144173\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015093 - Val Loss (DD rmse only): 0.143250\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015352 - Val Loss (DD rmse only): 0.143246\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015128 - Val Loss (DD rmse only): 0.143309\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014823 - Val Loss (DD rmse only): 0.144019\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.015137 - Val Loss (DD rmse only): 0.147502\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:53:35,246] Trial 34 finished with value: 0.1421202669541041 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 81, 'lr': 0.00037529785402893684, 'weight_decay': 0.00042523432972266055, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.015036 - Val Loss (DD rmse only): 0.143180\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.069181 - Val Loss (DD rmse only): 0.325234\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.047489 - Val Loss (DD rmse only): 0.271642\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031385 - Val Loss (DD rmse only): 0.217419\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.029599 - Val Loss (DD rmse only): 0.210744\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.027590 - Val Loss (DD rmse only): 0.218290\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.027424 - Val Loss (DD rmse only): 0.213936\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.026429 - Val Loss (DD rmse only): 0.203971\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.024669 - Val Loss (DD rmse only): 0.197968\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.022225 - Val Loss (DD rmse only): 0.183168\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.019378 - Val Loss (DD rmse only): 0.165335\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.018071 - Val Loss (DD rmse only): 0.159664\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016616 - Val Loss (DD rmse only): 0.154563\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016285 - Val Loss (DD rmse only): 0.155325\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015979 - Val Loss (DD rmse only): 0.148542\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015422 - Val Loss (DD rmse only): 0.147339\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015412 - Val Loss (DD rmse only): 0.146697\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015404 - Val Loss (DD rmse only): 0.145127\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015202 - Val Loss (DD rmse only): 0.145626\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015188 - Val Loss (DD rmse only): 0.149131\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015348 - Val Loss (DD rmse only): 0.143808\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014928 - Val Loss (DD rmse only): 0.145425\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015048 - Val Loss (DD rmse only): 0.144802\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015281 - Val Loss (DD rmse only): 0.144210\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015255 - Val Loss (DD rmse only): 0.148881\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015197 - Val Loss (DD rmse only): 0.145466\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014713 - Val Loss (DD rmse only): 0.144832\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014932 - Val Loss (DD rmse only): 0.143691\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014668 - Val Loss (DD rmse only): 0.143213\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015195 - Val Loss (DD rmse only): 0.147261\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014901 - Val Loss (DD rmse only): 0.143683\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015027 - Val Loss (DD rmse only): 0.144051\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014762 - Val Loss (DD rmse only): 0.143170\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014601 - Val Loss (DD rmse only): 0.143073\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014519 - Val Loss (DD rmse only): 0.146335\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014461 - Val Loss (DD rmse only): 0.143485\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014567 - Val Loss (DD rmse only): 0.143627\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014330 - Val Loss (DD rmse only): 0.144392\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014406 - Val Loss (DD rmse only): 0.142885\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014357 - Val Loss (DD rmse only): 0.142940\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014456 - Val Loss (DD rmse only): 0.142685\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014449 - Val Loss (DD rmse only): 0.143452\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014379 - Val Loss (DD rmse only): 0.147679\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014407 - Val Loss (DD rmse only): 0.144724\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015365 - Val Loss (DD rmse only): 0.144134\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014612 - Val Loss (DD rmse only): 0.146025\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014425 - Val Loss (DD rmse only): 0.144152\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014072 - Val Loss (DD rmse only): 0.144689\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014045 - Val Loss (DD rmse only): 0.143617\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014141 - Val Loss (DD rmse only): 0.143336\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:54:11,945] Trial 35 finished with value: 0.14268471797307333 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 130, 'lr': 0.0002163242803053108, 'weight_decay': 2.9672250656356328e-05, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014034 - Val Loss (DD rmse only): 0.143231\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.036086 - Val Loss (DD rmse only): 0.219163\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.025349 - Val Loss (DD rmse only): 0.174568\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.020291 - Val Loss (DD rmse only): 0.170623\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.018674 - Val Loss (DD rmse only): 0.152401\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.018162 - Val Loss (DD rmse only): 0.148442\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.016329 - Val Loss (DD rmse only): 0.153911\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016524 - Val Loss (DD rmse only): 0.147296\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016137 - Val Loss (DD rmse only): 0.146951\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015696 - Val Loss (DD rmse only): 0.153826\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015410 - Val Loss (DD rmse only): 0.149278\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015813 - Val Loss (DD rmse only): 0.153042\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015341 - Val Loss (DD rmse only): 0.170615\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015387 - Val Loss (DD rmse only): 0.144012\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014585 - Val Loss (DD rmse only): 0.151288\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015934 - Val Loss (DD rmse only): 0.161736\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014688 - Val Loss (DD rmse only): 0.145319\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014294 - Val Loss (DD rmse only): 0.148765\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.013723 - Val Loss (DD rmse only): 0.148923\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014539 - Val Loss (DD rmse only): 0.149153\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.013949 - Val Loss (DD rmse only): 0.149595\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.013839 - Val Loss (DD rmse only): 0.152735\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014074 - Val Loss (DD rmse only): 0.144730\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013847 - Val Loss (DD rmse only): 0.157979\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014322 - Val Loss (DD rmse only): 0.158174\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013698 - Val Loss (DD rmse only): 0.151486\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013639 - Val Loss (DD rmse only): 0.164534\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013295 - Val Loss (DD rmse only): 0.160263\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013463 - Val Loss (DD rmse only): 0.165928\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014816 - Val Loss (DD rmse only): 0.158718\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016209 - Val Loss (DD rmse only): 0.163577\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014913 - Val Loss (DD rmse only): 0.163892\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014254 - Val Loss (DD rmse only): 0.155290\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013231 - Val Loss (DD rmse only): 0.158601\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013150 - Val Loss (DD rmse only): 0.165421\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.012893 - Val Loss (DD rmse only): 0.158969\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014378 - Val Loss (DD rmse only): 0.157828\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013316 - Val Loss (DD rmse only): 0.221699\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013088 - Val Loss (DD rmse only): 0.150935\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013117 - Val Loss (DD rmse only): 0.181984\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.012786 - Val Loss (DD rmse only): 0.154486\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013731 - Val Loss (DD rmse only): 0.157302\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012902 - Val Loss (DD rmse only): 0.168109\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012452 - Val Loss (DD rmse only): 0.161342\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012531 - Val Loss (DD rmse only): 0.178198\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012437 - Val Loss (DD rmse only): 0.171118\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012637 - Val Loss (DD rmse only): 0.161639\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012447 - Val Loss (DD rmse only): 0.195090\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013219 - Val Loss (DD rmse only): 0.165727\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013518 - Val Loss (DD rmse only): 0.164179\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:54:38,047] Trial 36 finished with value: 0.14401216308275858 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 88, 'lr': 0.0056920689588228724, 'weight_decay': 5.290112191135019e-06, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012384 - Val Loss (DD rmse only): 0.172205\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.057875 - Val Loss (DD rmse only): 0.262864\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.029524 - Val Loss (DD rmse only): 0.204507\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.026473 - Val Loss (DD rmse only): 0.194411\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.020641 - Val Loss (DD rmse only): 0.162828\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.018668 - Val Loss (DD rmse only): 0.161798\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.015912 - Val Loss (DD rmse only): 0.141350\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016262 - Val Loss (DD rmse only): 0.144695\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016717 - Val Loss (DD rmse only): 0.175831\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016885 - Val Loss (DD rmse only): 0.151810\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015410 - Val Loss (DD rmse only): 0.152208\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016128 - Val Loss (DD rmse only): 0.142446\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015019 - Val Loss (DD rmse only): 0.161168\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016347 - Val Loss (DD rmse only): 0.144980\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014760 - Val Loss (DD rmse only): 0.143634\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014933 - Val Loss (DD rmse only): 0.145280\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015479 - Val Loss (DD rmse only): 0.160134\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015039 - Val Loss (DD rmse only): 0.145441\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014446 - Val Loss (DD rmse only): 0.145895\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014261 - Val Loss (DD rmse only): 0.144261\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014025 - Val Loss (DD rmse only): 0.150017\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014370 - Val Loss (DD rmse only): 0.148349\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014345 - Val Loss (DD rmse only): 0.150656\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014138 - Val Loss (DD rmse only): 0.147041\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.013955 - Val Loss (DD rmse only): 0.150952\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014500 - Val Loss (DD rmse only): 0.156027\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014503 - Val Loss (DD rmse only): 0.153095\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015629 - Val Loss (DD rmse only): 0.153505\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014977 - Val Loss (DD rmse only): 0.150308\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014068 - Val Loss (DD rmse only): 0.157516\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014529 - Val Loss (DD rmse only): 0.150372\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015853 - Val Loss (DD rmse only): 0.159983\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014466 - Val Loss (DD rmse only): 0.154443\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013626 - Val Loss (DD rmse only): 0.150915\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013433 - Val Loss (DD rmse only): 0.157466\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013709 - Val Loss (DD rmse only): 0.153427\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013652 - Val Loss (DD rmse only): 0.157739\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013087 - Val Loss (DD rmse only): 0.154373\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013674 - Val Loss (DD rmse only): 0.160939\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.012922 - Val Loss (DD rmse only): 0.161332\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.012810 - Val Loss (DD rmse only): 0.164622\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012824 - Val Loss (DD rmse only): 0.160269\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012755 - Val Loss (DD rmse only): 0.159875\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013344 - Val Loss (DD rmse only): 0.162311\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012919 - Val Loss (DD rmse only): 0.156308\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012579 - Val Loss (DD rmse only): 0.159370\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013435 - Val Loss (DD rmse only): 0.160281\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012600 - Val Loss (DD rmse only): 0.165367\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013187 - Val Loss (DD rmse only): 0.165122\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013427 - Val Loss (DD rmse only): 0.159391\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:55:17,036] Trial 37 finished with value: 0.14135009547074637 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 153, 'lr': 0.0018640027043272276, 'weight_decay': 1.1771224526570465e-05, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013156 - Val Loss (DD rmse only): 0.156449\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.039308 - Val Loss (DD rmse only): 0.222570\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.027391 - Val Loss (DD rmse only): 0.210672\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.020180 - Val Loss (DD rmse only): 0.156224\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.018412 - Val Loss (DD rmse only): 0.151471\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.017163 - Val Loss (DD rmse only): 0.170688\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.017929 - Val Loss (DD rmse only): 0.146855\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016063 - Val Loss (DD rmse only): 0.146511\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015142 - Val Loss (DD rmse only): 0.149708\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.014943 - Val Loss (DD rmse only): 0.147353\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015230 - Val Loss (DD rmse only): 0.155281\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015614 - Val Loss (DD rmse only): 0.147749\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.014627 - Val Loss (DD rmse only): 0.150400\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016916 - Val Loss (DD rmse only): 0.144187\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014438 - Val Loss (DD rmse only): 0.143923\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014568 - Val Loss (DD rmse only): 0.146316\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014330 - Val Loss (DD rmse only): 0.176524\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015816 - Val Loss (DD rmse only): 0.154038\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015413 - Val Loss (DD rmse only): 0.148571\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014500 - Val Loss (DD rmse only): 0.150246\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015318 - Val Loss (DD rmse only): 0.168429\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014974 - Val Loss (DD rmse only): 0.168193\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.013967 - Val Loss (DD rmse only): 0.153541\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013711 - Val Loss (DD rmse only): 0.165385\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014194 - Val Loss (DD rmse only): 0.149791\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013849 - Val Loss (DD rmse only): 0.148855\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014797 - Val Loss (DD rmse only): 0.154053\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014622 - Val Loss (DD rmse only): 0.153545\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014118 - Val Loss (DD rmse only): 0.159899\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014712 - Val Loss (DD rmse only): 0.167064\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015252 - Val Loss (DD rmse only): 0.153615\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014155 - Val Loss (DD rmse only): 0.161539\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014208 - Val Loss (DD rmse only): 0.156693\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013796 - Val Loss (DD rmse only): 0.151328\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013570 - Val Loss (DD rmse only): 0.168187\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013753 - Val Loss (DD rmse only): 0.148935\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014196 - Val Loss (DD rmse only): 0.155723\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013882 - Val Loss (DD rmse only): 0.149352\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.012881 - Val Loss (DD rmse only): 0.160973\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.012837 - Val Loss (DD rmse only): 0.157700\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013269 - Val Loss (DD rmse only): 0.161344\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012721 - Val Loss (DD rmse only): 0.156396\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012743 - Val Loss (DD rmse only): 0.162101\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012259 - Val Loss (DD rmse only): 0.184622\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012920 - Val Loss (DD rmse only): 0.177664\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013038 - Val Loss (DD rmse only): 0.153959\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012342 - Val Loss (DD rmse only): 0.187509\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013012 - Val Loss (DD rmse only): 0.163054\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012255 - Val Loss (DD rmse only): 0.173704\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013008 - Val Loss (DD rmse only): 0.157040\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:56:02,280] Trial 38 finished with value: 0.1439228355884552 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 154, 'lr': 0.0025101537852131733, 'weight_decay': 1.548737835056369e-05, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012350 - Val Loss (DD rmse only): 0.171934\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.074681 - Val Loss (DD rmse only): 0.250838\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.030058 - Val Loss (DD rmse only): 0.197264\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.026693 - Val Loss (DD rmse only): 0.202985\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.023352 - Val Loss (DD rmse only): 0.171152\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.020289 - Val Loss (DD rmse only): 0.160765\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.017305 - Val Loss (DD rmse only): 0.148997\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016018 - Val Loss (DD rmse only): 0.144067\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015304 - Val Loss (DD rmse only): 0.144158\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015248 - Val Loss (DD rmse only): 0.141923\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.014952 - Val Loss (DD rmse only): 0.144613\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015671 - Val Loss (DD rmse only): 0.150545\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015800 - Val Loss (DD rmse only): 0.143068\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015343 - Val Loss (DD rmse only): 0.145170\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015340 - Val Loss (DD rmse only): 0.146729\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014567 - Val Loss (DD rmse only): 0.148896\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014372 - Val Loss (DD rmse only): 0.143424\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014437 - Val Loss (DD rmse only): 0.151852\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014602 - Val Loss (DD rmse only): 0.143468\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014060 - Val Loss (DD rmse only): 0.142426\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014006 - Val Loss (DD rmse only): 0.144847\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014097 - Val Loss (DD rmse only): 0.149567\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014564 - Val Loss (DD rmse only): 0.145214\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014084 - Val Loss (DD rmse only): 0.143067\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.013772 - Val Loss (DD rmse only): 0.149132\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013827 - Val Loss (DD rmse only): 0.146391\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014461 - Val Loss (DD rmse only): 0.147479\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014326 - Val Loss (DD rmse only): 0.177370\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014718 - Val Loss (DD rmse only): 0.145911\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013782 - Val Loss (DD rmse only): 0.157144\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013668 - Val Loss (DD rmse only): 0.155176\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013400 - Val Loss (DD rmse only): 0.152183\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013405 - Val Loss (DD rmse only): 0.149736\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014560 - Val Loss (DD rmse only): 0.156201\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013917 - Val Loss (DD rmse only): 0.152716\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013848 - Val Loss (DD rmse only): 0.155600\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013395 - Val Loss (DD rmse only): 0.150455\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013953 - Val Loss (DD rmse only): 0.165510\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013536 - Val Loss (DD rmse only): 0.146646\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013118 - Val Loss (DD rmse only): 0.169972\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013290 - Val Loss (DD rmse only): 0.154900\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013364 - Val Loss (DD rmse only): 0.163433\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013357 - Val Loss (DD rmse only): 0.155628\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014005 - Val Loss (DD rmse only): 0.157919\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014413 - Val Loss (DD rmse only): 0.151406\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014952 - Val Loss (DD rmse only): 0.163062\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015794 - Val Loss (DD rmse only): 0.156227\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013137 - Val Loss (DD rmse only): 0.159894\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012921 - Val Loss (DD rmse only): 0.158909\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012953 - Val Loss (DD rmse only): 0.153817\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:56:43,141] Trial 39 finished with value: 0.141922856370608 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 135, 'lr': 0.0016330318196792649, 'weight_decay': 9.793933003491681e-06, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013037 - Val Loss (DD rmse only): 0.169805\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.050630 - Val Loss (DD rmse only): 0.182590\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.022161 - Val Loss (DD rmse only): 0.166988\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.016806 - Val Loss (DD rmse only): 0.159461\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.016103 - Val Loss (DD rmse only): 0.151223\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.015452 - Val Loss (DD rmse only): 0.148726\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.015402 - Val Loss (DD rmse only): 0.145970\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.015161 - Val Loss (DD rmse only): 0.143584\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015193 - Val Loss (DD rmse only): 0.144362\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015084 - Val Loss (DD rmse only): 0.147169\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015645 - Val Loss (DD rmse only): 0.152803\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015551 - Val Loss (DD rmse only): 0.144907\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015198 - Val Loss (DD rmse only): 0.147917\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015267 - Val Loss (DD rmse only): 0.144904\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015275 - Val Loss (DD rmse only): 0.145833\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015441 - Val Loss (DD rmse only): 0.154213\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015578 - Val Loss (DD rmse only): 0.143272\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014852 - Val Loss (DD rmse only): 0.145050\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015368 - Val Loss (DD rmse only): 0.148451\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016807 - Val Loss (DD rmse only): 0.157389\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015602 - Val Loss (DD rmse only): 0.154097\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015142 - Val Loss (DD rmse only): 0.148143\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015648 - Val Loss (DD rmse only): 0.143444\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.016410 - Val Loss (DD rmse only): 0.152945\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015250 - Val Loss (DD rmse only): 0.143626\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014619 - Val Loss (DD rmse only): 0.141862\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014571 - Val Loss (DD rmse only): 0.153961\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014676 - Val Loss (DD rmse only): 0.144991\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014538 - Val Loss (DD rmse only): 0.152297\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015929 - Val Loss (DD rmse only): 0.157394\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015641 - Val Loss (DD rmse only): 0.154394\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014577 - Val Loss (DD rmse only): 0.148172\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014253 - Val Loss (DD rmse only): 0.146538\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014265 - Val Loss (DD rmse only): 0.152330\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014030 - Val Loss (DD rmse only): 0.150428\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014313 - Val Loss (DD rmse only): 0.150355\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.016091 - Val Loss (DD rmse only): 0.176624\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015442 - Val Loss (DD rmse only): 0.158343\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015009 - Val Loss (DD rmse only): 0.163087\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014055 - Val Loss (DD rmse only): 0.157814\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013851 - Val Loss (DD rmse only): 0.152795\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013882 - Val Loss (DD rmse only): 0.160667\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014458 - Val Loss (DD rmse only): 0.149790\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014179 - Val Loss (DD rmse only): 0.163130\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014326 - Val Loss (DD rmse only): 0.156945\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013800 - Val Loss (DD rmse only): 0.163701\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014897 - Val Loss (DD rmse only): 0.153993\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013778 - Val Loss (DD rmse only): 0.164976\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014351 - Val Loss (DD rmse only): 0.162909\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013735 - Val Loss (DD rmse only): 0.171590\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:57:05,007] Trial 40 finished with value: 0.14186241726080576 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 60, 'lr': 0.00869164174631107, 'weight_decay': 1.334865631826853e-06, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014815 - Val Loss (DD rmse only): 0.153375\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.073172 - Val Loss (DD rmse only): 0.194549\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.030831 - Val Loss (DD rmse only): 0.221815\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.027545 - Val Loss (DD rmse only): 0.206765\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.024995 - Val Loss (DD rmse only): 0.196102\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.020734 - Val Loss (DD rmse only): 0.161861\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.017894 - Val Loss (DD rmse only): 0.155705\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.017307 - Val Loss (DD rmse only): 0.155104\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016933 - Val Loss (DD rmse only): 0.162162\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015715 - Val Loss (DD rmse only): 0.149041\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015311 - Val Loss (DD rmse only): 0.145773\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015249 - Val Loss (DD rmse only): 0.148386\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016413 - Val Loss (DD rmse only): 0.154032\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015633 - Val Loss (DD rmse only): 0.145909\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015181 - Val Loss (DD rmse only): 0.146309\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014978 - Val Loss (DD rmse only): 0.146220\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014670 - Val Loss (DD rmse only): 0.144681\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014542 - Val Loss (DD rmse only): 0.146811\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014934 - Val Loss (DD rmse only): 0.153155\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015081 - Val Loss (DD rmse only): 0.147927\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014800 - Val Loss (DD rmse only): 0.145994\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014568 - Val Loss (DD rmse only): 0.147008\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014594 - Val Loss (DD rmse only): 0.149555\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014711 - Val Loss (DD rmse only): 0.146067\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014119 - Val Loss (DD rmse only): 0.155929\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014846 - Val Loss (DD rmse only): 0.151432\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015722 - Val Loss (DD rmse only): 0.150157\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014447 - Val Loss (DD rmse only): 0.147934\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014268 - Val Loss (DD rmse only): 0.146806\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014346 - Val Loss (DD rmse only): 0.146008\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014331 - Val Loss (DD rmse only): 0.151150\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014030 - Val Loss (DD rmse only): 0.146284\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013834 - Val Loss (DD rmse only): 0.146164\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013858 - Val Loss (DD rmse only): 0.147034\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013903 - Val Loss (DD rmse only): 0.152606\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014195 - Val Loss (DD rmse only): 0.148646\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013945 - Val Loss (DD rmse only): 0.147150\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013810 - Val Loss (DD rmse only): 0.149420\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013678 - Val Loss (DD rmse only): 0.148335\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013754 - Val Loss (DD rmse only): 0.146969\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014230 - Val Loss (DD rmse only): 0.152079\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014446 - Val Loss (DD rmse only): 0.150253\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014389 - Val Loss (DD rmse only): 0.155674\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014467 - Val Loss (DD rmse only): 0.150673\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014034 - Val Loss (DD rmse only): 0.149684\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013625 - Val Loss (DD rmse only): 0.151390\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013313 - Val Loss (DD rmse only): 0.157753\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013546 - Val Loss (DD rmse only): 0.165339\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013710 - Val Loss (DD rmse only): 0.152599\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013728 - Val Loss (DD rmse only): 0.151552\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:58:10,911] Trial 41 finished with value: 0.14468072851498923 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 157, 'lr': 0.0006980423805237828, 'weight_decay': 7.997333306098656e-05, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013873 - Val Loss (DD rmse only): 0.156910\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.093434 - Val Loss (DD rmse only): 0.204601\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.031362 - Val Loss (DD rmse only): 0.226222\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.025987 - Val Loss (DD rmse only): 0.187760\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.022208 - Val Loss (DD rmse only): 0.169269\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.019223 - Val Loss (DD rmse only): 0.161017\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.017415 - Val Loss (DD rmse only): 0.153740\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016727 - Val Loss (DD rmse only): 0.151509\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016598 - Val Loss (DD rmse only): 0.146414\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015691 - Val Loss (DD rmse only): 0.145443\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015203 - Val Loss (DD rmse only): 0.143431\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015549 - Val Loss (DD rmse only): 0.145375\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015818 - Val Loss (DD rmse only): 0.146140\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016429 - Val Loss (DD rmse only): 0.143776\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015780 - Val Loss (DD rmse only): 0.142636\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015300 - Val Loss (DD rmse only): 0.155717\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016480 - Val Loss (DD rmse only): 0.146155\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014924 - Val Loss (DD rmse only): 0.141852\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015163 - Val Loss (DD rmse only): 0.143604\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014359 - Val Loss (DD rmse only): 0.142985\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014244 - Val Loss (DD rmse only): 0.146181\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014254 - Val Loss (DD rmse only): 0.146055\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014710 - Val Loss (DD rmse only): 0.157315\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014553 - Val Loss (DD rmse only): 0.147243\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014623 - Val Loss (DD rmse only): 0.149780\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014362 - Val Loss (DD rmse only): 0.144885\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014139 - Val Loss (DD rmse only): 0.145523\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013985 - Val Loss (DD rmse only): 0.152442\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014076 - Val Loss (DD rmse only): 0.151341\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014358 - Val Loss (DD rmse only): 0.147405\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013668 - Val Loss (DD rmse only): 0.146890\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013852 - Val Loss (DD rmse only): 0.148537\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013859 - Val Loss (DD rmse only): 0.153206\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014217 - Val Loss (DD rmse only): 0.154565\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013551 - Val Loss (DD rmse only): 0.153865\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013560 - Val Loss (DD rmse only): 0.146470\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013158 - Val Loss (DD rmse only): 0.149663\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013108 - Val Loss (DD rmse only): 0.160419\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013278 - Val Loss (DD rmse only): 0.150073\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013380 - Val Loss (DD rmse only): 0.162654\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013681 - Val Loss (DD rmse only): 0.150139\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013608 - Val Loss (DD rmse only): 0.150589\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012959 - Val Loss (DD rmse only): 0.151244\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012830 - Val Loss (DD rmse only): 0.152256\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012878 - Val Loss (DD rmse only): 0.156581\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013056 - Val Loss (DD rmse only): 0.153242\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012895 - Val Loss (DD rmse only): 0.157988\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013641 - Val Loss (DD rmse only): 0.174641\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013779 - Val Loss (DD rmse only): 0.158670\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012739 - Val Loss (DD rmse only): 0.151411\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:59:14,720] Trial 42 finished with value: 0.14185194174448648 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 174, 'lr': 0.0007958411113913587, 'weight_decay': 3.4268206208129325e-06, 'batch_size': 8}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012721 - Val Loss (DD rmse only): 0.151929\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.104739 - Val Loss (DD rmse only): 0.362869\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.045434 - Val Loss (DD rmse only): 0.218663\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.034337 - Val Loss (DD rmse only): 0.209380\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.027820 - Val Loss (DD rmse only): 0.249336\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.032621 - Val Loss (DD rmse only): 0.245374\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.028219 - Val Loss (DD rmse only): 0.212311\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.028531 - Val Loss (DD rmse only): 0.197485\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.024860 - Val Loss (DD rmse only): 0.212610\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.026354 - Val Loss (DD rmse only): 0.212176\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.025865 - Val Loss (DD rmse only): 0.188197\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.020761 - Val Loss (DD rmse only): 0.183114\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.019042 - Val Loss (DD rmse only): 0.187528\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.018760 - Val Loss (DD rmse only): 0.174792\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016973 - Val Loss (DD rmse only): 0.166684\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.017501 - Val Loss (DD rmse only): 0.165089\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.017753 - Val Loss (DD rmse only): 0.162249\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015171 - Val Loss (DD rmse only): 0.160122\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015999 - Val Loss (DD rmse only): 0.158116\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015997 - Val Loss (DD rmse only): 0.154522\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014295 - Val Loss (DD rmse only): 0.152383\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014111 - Val Loss (DD rmse only): 0.151245\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015333 - Val Loss (DD rmse only): 0.151526\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013903 - Val Loss (DD rmse only): 0.154921\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015877 - Val Loss (DD rmse only): 0.150367\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014743 - Val Loss (DD rmse only): 0.150606\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014477 - Val Loss (DD rmse only): 0.150419\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013751 - Val Loss (DD rmse only): 0.150798\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013981 - Val Loss (DD rmse only): 0.150322\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015595 - Val Loss (DD rmse only): 0.151671\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015780 - Val Loss (DD rmse only): 0.150882\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014718 - Val Loss (DD rmse only): 0.148430\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014481 - Val Loss (DD rmse only): 0.150069\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014730 - Val Loss (DD rmse only): 0.150063\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015144 - Val Loss (DD rmse only): 0.149082\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014152 - Val Loss (DD rmse only): 0.150195\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014571 - Val Loss (DD rmse only): 0.149800\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015345 - Val Loss (DD rmse only): 0.150396\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013881 - Val Loss (DD rmse only): 0.149130\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013043 - Val Loss (DD rmse only): 0.148266\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013781 - Val Loss (DD rmse only): 0.148248\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013666 - Val Loss (DD rmse only): 0.148460\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015775 - Val Loss (DD rmse only): 0.149038\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013594 - Val Loss (DD rmse only): 0.149694\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013253 - Val Loss (DD rmse only): 0.148198\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014228 - Val Loss (DD rmse only): 0.147621\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013291 - Val Loss (DD rmse only): 0.148462\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014599 - Val Loss (DD rmse only): 0.153531\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014779 - Val Loss (DD rmse only): 0.153547\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013777 - Val Loss (DD rmse only): 0.153214\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 18:59:37,478] Trial 43 finished with value: 0.1476205736398697 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 142, 'lr': 0.00119151899859715, 'weight_decay': 3.194249697909796e-05, 'batch_size': 32}. Best is trial 30 with value: 0.14073761800924936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013599 - Val Loss (DD rmse only): 0.153272\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.080375 - Val Loss (DD rmse only): 0.339248\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.049891 - Val Loss (DD rmse only): 0.267251\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031362 - Val Loss (DD rmse only): 0.210472\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028251 - Val Loss (DD rmse only): 0.208586\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.026765 - Val Loss (DD rmse only): 0.210107\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025776 - Val Loss (DD rmse only): 0.205211\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.024200 - Val Loss (DD rmse only): 0.195794\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.022546 - Val Loss (DD rmse only): 0.185603\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.020820 - Val Loss (DD rmse only): 0.175562\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.018724 - Val Loss (DD rmse only): 0.167148\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017236 - Val Loss (DD rmse only): 0.156591\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016399 - Val Loss (DD rmse only): 0.151349\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015626 - Val Loss (DD rmse only): 0.146941\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015015 - Val Loss (DD rmse only): 0.148860\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015780 - Val Loss (DD rmse only): 0.144024\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015475 - Val Loss (DD rmse only): 0.145022\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014966 - Val Loss (DD rmse only): 0.143112\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014713 - Val Loss (DD rmse only): 0.143045\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014583 - Val Loss (DD rmse only): 0.141644\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014584 - Val Loss (DD rmse only): 0.141085\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014575 - Val Loss (DD rmse only): 0.141927\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014451 - Val Loss (DD rmse only): 0.142244\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014559 - Val Loss (DD rmse only): 0.141917\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014370 - Val Loss (DD rmse only): 0.141224\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014358 - Val Loss (DD rmse only): 0.142393\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014230 - Val Loss (DD rmse only): 0.141491\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014195 - Val Loss (DD rmse only): 0.142294\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014152 - Val Loss (DD rmse only): 0.140043\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014198 - Val Loss (DD rmse only): 0.141185\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014015 - Val Loss (DD rmse only): 0.141046\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014207 - Val Loss (DD rmse only): 0.143827\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013992 - Val Loss (DD rmse only): 0.141608\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013911 - Val Loss (DD rmse only): 0.142330\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013817 - Val Loss (DD rmse only): 0.142375\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013934 - Val Loss (DD rmse only): 0.143985\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013949 - Val Loss (DD rmse only): 0.141717\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014206 - Val Loss (DD rmse only): 0.146747\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013756 - Val Loss (DD rmse only): 0.141931\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013817 - Val Loss (DD rmse only): 0.141300\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013592 - Val Loss (DD rmse only): 0.148166\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014140 - Val Loss (DD rmse only): 0.146642\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014050 - Val Loss (DD rmse only): 0.144069\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013743 - Val Loss (DD rmse only): 0.143199\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013750 - Val Loss (DD rmse only): 0.143004\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013840 - Val Loss (DD rmse only): 0.145462\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013481 - Val Loss (DD rmse only): 0.144454\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013472 - Val Loss (DD rmse only): 0.142503\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013561 - Val Loss (DD rmse only): 0.142958\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013553 - Val Loss (DD rmse only): 0.144826\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:00:28,835] Trial 44 finished with value: 0.14004295070966086 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 184, 'lr': 0.00013022485650778092, 'weight_decay': 1.762147613805231e-06, 'batch_size': 8}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013414 - Val Loss (DD rmse only): 0.147424\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.178607 - Val Loss (DD rmse only): 0.503129\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.155045 - Val Loss (DD rmse only): 0.472779\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.130526 - Val Loss (DD rmse only): 0.436637\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.102380 - Val Loss (DD rmse only): 0.386641\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.070204 - Val Loss (DD rmse only): 0.315416\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.040072 - Val Loss (DD rmse only): 0.237129\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.031168 - Val Loss (DD rmse only): 0.219976\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.030926 - Val Loss (DD rmse only): 0.226220\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.029915 - Val Loss (DD rmse only): 0.226601\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.029240 - Val Loss (DD rmse only): 0.217859\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.028715 - Val Loss (DD rmse only): 0.214310\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.027900 - Val Loss (DD rmse only): 0.218316\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.027046 - Val Loss (DD rmse only): 0.206926\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.026238 - Val Loss (DD rmse only): 0.204951\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.025409 - Val Loss (DD rmse only): 0.205355\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.024375 - Val Loss (DD rmse only): 0.197249\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.023254 - Val Loss (DD rmse only): 0.190003\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.022305 - Val Loss (DD rmse only): 0.186623\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.021007 - Val Loss (DD rmse only): 0.177392\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.019753 - Val Loss (DD rmse only): 0.176577\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.018671 - Val Loss (DD rmse only): 0.168583\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.017982 - Val Loss (DD rmse only): 0.166463\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.017268 - Val Loss (DD rmse only): 0.162116\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016995 - Val Loss (DD rmse only): 0.160297\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016641 - Val Loss (DD rmse only): 0.158453\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.016357 - Val Loss (DD rmse only): 0.156906\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.016180 - Val Loss (DD rmse only): 0.155523\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.016087 - Val Loss (DD rmse only): 0.154711\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015906 - Val Loss (DD rmse only): 0.153246\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015740 - Val Loss (DD rmse only): 0.151989\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015717 - Val Loss (DD rmse only): 0.152835\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015736 - Val Loss (DD rmse only): 0.149980\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015484 - Val Loss (DD rmse only): 0.149858\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015294 - Val Loss (DD rmse only): 0.148785\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015342 - Val Loss (DD rmse only): 0.147630\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015347 - Val Loss (DD rmse only): 0.147770\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015070 - Val Loss (DD rmse only): 0.146311\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015114 - Val Loss (DD rmse only): 0.147731\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015336 - Val Loss (DD rmse only): 0.145849\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015082 - Val Loss (DD rmse only): 0.145325\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015027 - Val Loss (DD rmse only): 0.145263\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014961 - Val Loss (DD rmse only): 0.144367\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014894 - Val Loss (DD rmse only): 0.144847\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014872 - Val Loss (DD rmse only): 0.143569\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014844 - Val Loss (DD rmse only): 0.143296\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014840 - Val Loss (DD rmse only): 0.143879\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014691 - Val Loss (DD rmse only): 0.143145\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015144 - Val Loss (DD rmse only): 0.147240\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.015485 - Val Loss (DD rmse only): 0.143284\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:01:22,799] Trial 45 finished with value: 0.14291327695051828 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 145, 'lr': 9.12541237556612e-05, 'weight_decay': 1.8543498359723674e-06, 'batch_size': 8}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014954 - Val Loss (DD rmse only): 0.142913\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.113225 - Val Loss (DD rmse only): 0.402647\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.081721 - Val Loss (DD rmse only): 0.342963\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.047887 - Val Loss (DD rmse only): 0.250269\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.030569 - Val Loss (DD rmse only): 0.201869\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028626 - Val Loss (DD rmse only): 0.217653\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.026914 - Val Loss (DD rmse only): 0.205069\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.025569 - Val Loss (DD rmse only): 0.199985\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.024254 - Val Loss (DD rmse only): 0.194761\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.022758 - Val Loss (DD rmse only): 0.187704\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.020910 - Val Loss (DD rmse only): 0.174104\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.019347 - Val Loss (DD rmse only): 0.168283\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.017799 - Val Loss (DD rmse only): 0.162006\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016946 - Val Loss (DD rmse only): 0.157610\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016318 - Val Loss (DD rmse only): 0.154812\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015945 - Val Loss (DD rmse only): 0.152764\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015520 - Val Loss (DD rmse only): 0.150487\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015601 - Val Loss (DD rmse only): 0.150477\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015357 - Val Loss (DD rmse only): 0.146667\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015030 - Val Loss (DD rmse only): 0.145229\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015104 - Val Loss (DD rmse only): 0.144421\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014833 - Val Loss (DD rmse only): 0.148645\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014924 - Val Loss (DD rmse only): 0.143808\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014700 - Val Loss (DD rmse only): 0.143725\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014641 - Val Loss (DD rmse only): 0.142309\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014691 - Val Loss (DD rmse only): 0.143050\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014631 - Val Loss (DD rmse only): 0.143160\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014562 - Val Loss (DD rmse only): 0.142469\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014473 - Val Loss (DD rmse only): 0.145254\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014585 - Val Loss (DD rmse only): 0.142160\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014548 - Val Loss (DD rmse only): 0.142055\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014308 - Val Loss (DD rmse only): 0.146277\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014559 - Val Loss (DD rmse only): 0.142538\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014533 - Val Loss (DD rmse only): 0.142718\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014369 - Val Loss (DD rmse only): 0.142334\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014345 - Val Loss (DD rmse only): 0.142598\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014842 - Val Loss (DD rmse only): 0.143920\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014194 - Val Loss (DD rmse only): 0.143346\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014246 - Val Loss (DD rmse only): 0.148212\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014357 - Val Loss (DD rmse only): 0.143105\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014170 - Val Loss (DD rmse only): 0.143630\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014736 - Val Loss (DD rmse only): 0.144223\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014412 - Val Loss (DD rmse only): 0.143985\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014201 - Val Loss (DD rmse only): 0.144314\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013950 - Val Loss (DD rmse only): 0.143836\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013904 - Val Loss (DD rmse only): 0.145300\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013850 - Val Loss (DD rmse only): 0.143215\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014208 - Val Loss (DD rmse only): 0.144208\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014007 - Val Loss (DD rmse only): 0.147019\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014042 - Val Loss (DD rmse only): 0.144982\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:02:24,211] Trial 46 finished with value: 0.1420548011859258 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 182, 'lr': 0.00012109670495201485, 'weight_decay': 2.3422697309800505e-07, 'batch_size': 8}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014040 - Val Loss (DD rmse only): 0.146240\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.054690 - Val Loss (DD rmse only): 0.227425\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.025885 - Val Loss (DD rmse only): 0.180685\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.020150 - Val Loss (DD rmse only): 0.161968\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.016990 - Val Loss (DD rmse only): 0.156743\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.015938 - Val Loss (DD rmse only): 0.152845\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.015431 - Val Loss (DD rmse only): 0.150274\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.015077 - Val Loss (DD rmse only): 0.143924\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.014667 - Val Loss (DD rmse only): 0.144081\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015168 - Val Loss (DD rmse only): 0.145720\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015293 - Val Loss (DD rmse only): 0.155138\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.014235 - Val Loss (DD rmse only): 0.147100\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.014274 - Val Loss (DD rmse only): 0.147358\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014102 - Val Loss (DD rmse only): 0.144787\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014289 - Val Loss (DD rmse only): 0.168816\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015850 - Val Loss (DD rmse only): 0.147489\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014863 - Val Loss (DD rmse only): 0.147358\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014482 - Val Loss (DD rmse only): 0.147541\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014423 - Val Loss (DD rmse only): 0.148525\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014236 - Val Loss (DD rmse only): 0.168475\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014026 - Val Loss (DD rmse only): 0.156401\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.013502 - Val Loss (DD rmse only): 0.157497\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.013408 - Val Loss (DD rmse only): 0.157188\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.013486 - Val Loss (DD rmse only): 0.155291\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.013232 - Val Loss (DD rmse only): 0.164498\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013439 - Val Loss (DD rmse only): 0.168504\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013174 - Val Loss (DD rmse only): 0.159408\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.012942 - Val Loss (DD rmse only): 0.161640\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013275 - Val Loss (DD rmse only): 0.178741\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015027 - Val Loss (DD rmse only): 0.157016\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013779 - Val Loss (DD rmse only): 0.165037\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013092 - Val Loss (DD rmse only): 0.152455\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013297 - Val Loss (DD rmse only): 0.158760\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013420 - Val Loss (DD rmse only): 0.166663\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.012889 - Val Loss (DD rmse only): 0.163856\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013599 - Val Loss (DD rmse only): 0.162416\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.012847 - Val Loss (DD rmse only): 0.171306\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.012425 - Val Loss (DD rmse only): 0.166592\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.012499 - Val Loss (DD rmse only): 0.184421\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.012632 - Val Loss (DD rmse only): 0.176896\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.012672 - Val Loss (DD rmse only): 0.168116\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012372 - Val Loss (DD rmse only): 0.175723\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012506 - Val Loss (DD rmse only): 0.171050\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012162 - Val Loss (DD rmse only): 0.167097\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013506 - Val Loss (DD rmse only): 0.188027\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012307 - Val Loss (DD rmse only): 0.170046\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.011881 - Val Loss (DD rmse only): 0.165646\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.011858 - Val Loss (DD rmse only): 0.171923\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012202 - Val Loss (DD rmse only): 0.171056\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012235 - Val Loss (DD rmse only): 0.195865\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:02:45,879] Trial 47 finished with value: 0.14392443497975668 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 122, 'lr': 0.002122518230468237, 'weight_decay': 5.351917415226848e-06, 'batch_size': 8}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012367 - Val Loss (DD rmse only): 0.162903\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.217045 - Val Loss (DD rmse only): 0.533877\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.159782 - Val Loss (DD rmse only): 0.471272\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.114618 - Val Loss (DD rmse only): 0.413371\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.080932 - Val Loss (DD rmse only): 0.361016\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.057350 - Val Loss (DD rmse only): 0.314499\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.042364 - Val Loss (DD rmse only): 0.275891\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.033433 - Val Loss (DD rmse only): 0.250146\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.029930 - Val Loss (DD rmse only): 0.232434\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.028197 - Val Loss (DD rmse only): 0.221582\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.027038 - Val Loss (DD rmse only): 0.215914\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.026082 - Val Loss (DD rmse only): 0.211291\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.025033 - Val Loss (DD rmse only): 0.209671\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.024200 - Val Loss (DD rmse only): 0.204669\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.023272 - Val Loss (DD rmse only): 0.202146\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.022389 - Val Loss (DD rmse only): 0.198171\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.021650 - Val Loss (DD rmse only): 0.194545\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.020952 - Val Loss (DD rmse only): 0.191380\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.020375 - Val Loss (DD rmse only): 0.189764\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019815 - Val Loss (DD rmse only): 0.185734\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.019317 - Val Loss (DD rmse only): 0.181342\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.018819 - Val Loss (DD rmse only): 0.180109\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.018421 - Val Loss (DD rmse only): 0.177714\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.018028 - Val Loss (DD rmse only): 0.175101\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.017691 - Val Loss (DD rmse only): 0.173027\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.017420 - Val Loss (DD rmse only): 0.170122\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.017147 - Val Loss (DD rmse only): 0.168272\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.016885 - Val Loss (DD rmse only): 0.166839\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.016676 - Val Loss (DD rmse only): 0.164767\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.016472 - Val Loss (DD rmse only): 0.162896\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016311 - Val Loss (DD rmse only): 0.160654\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.016232 - Val Loss (DD rmse only): 0.159978\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.016172 - Val Loss (DD rmse only): 0.157096\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015888 - Val Loss (DD rmse only): 0.156924\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015805 - Val Loss (DD rmse only): 0.156546\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015701 - Val Loss (DD rmse only): 0.154292\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015632 - Val Loss (DD rmse only): 0.153508\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015566 - Val Loss (DD rmse only): 0.152336\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015446 - Val Loss (DD rmse only): 0.152258\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015388 - Val Loss (DD rmse only): 0.151000\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015324 - Val Loss (DD rmse only): 0.149972\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015260 - Val Loss (DD rmse only): 0.149508\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.015217 - Val Loss (DD rmse only): 0.149037\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.015184 - Val Loss (DD rmse only): 0.148409\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015130 - Val Loss (DD rmse only): 0.147839\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015119 - Val Loss (DD rmse only): 0.147305\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.015091 - Val Loss (DD rmse only): 0.146804\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015095 - Val Loss (DD rmse only): 0.146552\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015028 - Val Loss (DD rmse only): 0.146062\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014980 - Val Loss (DD rmse only): 0.145740\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:03:12,355] Trial 48 finished with value: 0.14542731642723083 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 157, 'lr': 6.968220015192749e-05, 'weight_decay': 1.2325968960552246e-05, 'batch_size': 8}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014984 - Val Loss (DD rmse only): 0.145427\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.091510 - Val Loss (DD rmse only): 0.371661\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.070396 - Val Loss (DD rmse only): 0.333142\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.054098 - Val Loss (DD rmse only): 0.287854\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.038384 - Val Loss (DD rmse only): 0.235642\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.030209 - Val Loss (DD rmse only): 0.201466\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.030298 - Val Loss (DD rmse only): 0.200532\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.029076 - Val Loss (DD rmse only): 0.209830\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.027915 - Val Loss (DD rmse only): 0.213225\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.026782 - Val Loss (DD rmse only): 0.204818\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.026135 - Val Loss (DD rmse only): 0.196606\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.025510 - Val Loss (DD rmse only): 0.196647\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.024158 - Val Loss (DD rmse only): 0.191311\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.023619 - Val Loss (DD rmse only): 0.184812\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.022463 - Val Loss (DD rmse only): 0.183300\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.020346 - Val Loss (DD rmse only): 0.176525\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.019353 - Val Loss (DD rmse only): 0.173727\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.019488 - Val Loss (DD rmse only): 0.171721\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017893 - Val Loss (DD rmse only): 0.170010\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.017397 - Val Loss (DD rmse only): 0.168666\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016945 - Val Loss (DD rmse only): 0.167303\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.017811 - Val Loss (DD rmse only): 0.163861\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.017109 - Val Loss (DD rmse only): 0.162605\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.016091 - Val Loss (DD rmse only): 0.156924\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015614 - Val Loss (DD rmse only): 0.157304\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015945 - Val Loss (DD rmse only): 0.154704\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015289 - Val Loss (DD rmse only): 0.151634\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015318 - Val Loss (DD rmse only): 0.151428\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014861 - Val Loss (DD rmse only): 0.148974\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014977 - Val Loss (DD rmse only): 0.148294\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014856 - Val Loss (DD rmse only): 0.148457\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014625 - Val Loss (DD rmse only): 0.145049\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014474 - Val Loss (DD rmse only): 0.146916\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014470 - Val Loss (DD rmse only): 0.144010\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014824 - Val Loss (DD rmse only): 0.146835\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014388 - Val Loss (DD rmse only): 0.145060\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014526 - Val Loss (DD rmse only): 0.144476\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014549 - Val Loss (DD rmse only): 0.146676\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014648 - Val Loss (DD rmse only): 0.146123\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014064 - Val Loss (DD rmse only): 0.143069\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014772 - Val Loss (DD rmse only): 0.145892\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014867 - Val Loss (DD rmse only): 0.147325\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014945 - Val Loss (DD rmse only): 0.144504\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014943 - Val Loss (DD rmse only): 0.146414\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014590 - Val Loss (DD rmse only): 0.147611\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014662 - Val Loss (DD rmse only): 0.144782\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013894 - Val Loss (DD rmse only): 0.146778\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013968 - Val Loss (DD rmse only): 0.143826\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014523 - Val Loss (DD rmse only): 0.147429\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014702 - Val Loss (DD rmse only): 0.147237\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:03:42,979] Trial 49 finished with value: 0.14306914806365967 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 172, 'lr': 0.00015856175819519024, 'weight_decay': 5.632004744765957e-07, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014129 - Val Loss (DD rmse only): 0.145561\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.172560 - Val Loss (DD rmse only): 0.508860\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.168998 - Val Loss (DD rmse only): 0.504495\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.165438 - Val Loss (DD rmse only): 0.500250\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.162058 - Val Loss (DD rmse only): 0.496086\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.158792 - Val Loss (DD rmse only): 0.491979\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.155561 - Val Loss (DD rmse only): 0.487874\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.152313 - Val Loss (DD rmse only): 0.483547\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.148955 - Val Loss (DD rmse only): 0.479224\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.145666 - Val Loss (DD rmse only): 0.474927\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.142408 - Val Loss (DD rmse only): 0.470675\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.139326 - Val Loss (DD rmse only): 0.466160\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.135975 - Val Loss (DD rmse only): 0.461634\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.132612 - Val Loss (DD rmse only): 0.456905\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.129250 - Val Loss (DD rmse only): 0.451863\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.125692 - Val Loss (DD rmse only): 0.446708\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.122127 - Val Loss (DD rmse only): 0.441340\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.118370 - Val Loss (DD rmse only): 0.435763\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.114582 - Val Loss (DD rmse only): 0.429960\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.110805 - Val Loss (DD rmse only): 0.423801\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.106799 - Val Loss (DD rmse only): 0.417453\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.102677 - Val Loss (DD rmse only): 0.411025\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.098670 - Val Loss (DD rmse only): 0.404269\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.094585 - Val Loss (DD rmse only): 0.397279\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.090468 - Val Loss (DD rmse only): 0.389875\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.086052 - Val Loss (DD rmse only): 0.382138\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.081573 - Val Loss (DD rmse only): 0.373720\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.076903 - Val Loss (DD rmse only): 0.364501\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.072007 - Val Loss (DD rmse only): 0.354912\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.067104 - Val Loss (DD rmse only): 0.344772\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.062363 - Val Loss (DD rmse only): 0.334122\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.057507 - Val Loss (DD rmse only): 0.323623\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.053125 - Val Loss (DD rmse only): 0.312405\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.048746 - Val Loss (DD rmse only): 0.301185\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.044708 - Val Loss (DD rmse only): 0.289862\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.041119 - Val Loss (DD rmse only): 0.278484\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.037811 - Val Loss (DD rmse only): 0.268268\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.035323 - Val Loss (DD rmse only): 0.258402\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.033311 - Val Loss (DD rmse only): 0.249869\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.031704 - Val Loss (DD rmse only): 0.243570\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.030912 - Val Loss (DD rmse only): 0.237059\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.030087 - Val Loss (DD rmse only): 0.233130\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.029660 - Val Loss (DD rmse only): 0.230135\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.029371 - Val Loss (DD rmse only): 0.227517\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.029155 - Val Loss (DD rmse only): 0.225165\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.028962 - Val Loss (DD rmse only): 0.223925\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.028796 - Val Loss (DD rmse only): 0.222225\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.028627 - Val Loss (DD rmse only): 0.221674\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.028489 - Val Loss (DD rmse only): 0.220607\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.028361 - Val Loss (DD rmse only): 0.219646\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:04:14,717] Trial 50 finished with value: 0.21952072282632193 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 96, 'lr': 1.8241750664925843e-05, 'weight_decay': 9.180644852872224e-07, 'batch_size': 8}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028232 - Val Loss (DD rmse only): 0.219521\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.109532 - Val Loss (DD rmse only): 0.408156\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.091264 - Val Loss (DD rmse only): 0.363257\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.058900 - Val Loss (DD rmse only): 0.310957\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.038712 - Val Loss (DD rmse only): 0.252766\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.029343 - Val Loss (DD rmse only): 0.215396\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.035520 - Val Loss (DD rmse only): 0.211444\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.031080 - Val Loss (DD rmse only): 0.225562\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.030320 - Val Loss (DD rmse only): 0.241735\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.031671 - Val Loss (DD rmse only): 0.243064\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.029805 - Val Loss (DD rmse only): 0.230776\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.028160 - Val Loss (DD rmse only): 0.213997\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.027900 - Val Loss (DD rmse only): 0.204788\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.028435 - Val Loss (DD rmse only): 0.204098\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.025191 - Val Loss (DD rmse only): 0.206791\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.024172 - Val Loss (DD rmse only): 0.202280\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.024219 - Val Loss (DD rmse only): 0.194963\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.021203 - Val Loss (DD rmse only): 0.189467\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.020644 - Val Loss (DD rmse only): 0.187554\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019232 - Val Loss (DD rmse only): 0.179868\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.019441 - Val Loss (DD rmse only): 0.173433\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.017537 - Val Loss (DD rmse only): 0.172673\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.017291 - Val Loss (DD rmse only): 0.167696\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.017068 - Val Loss (DD rmse only): 0.163294\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.017548 - Val Loss (DD rmse only): 0.160958\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016363 - Val Loss (DD rmse only): 0.159064\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.016195 - Val Loss (DD rmse only): 0.156981\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015574 - Val Loss (DD rmse only): 0.155003\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014624 - Val Loss (DD rmse only): 0.153727\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015104 - Val Loss (DD rmse only): 0.152192\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016503 - Val Loss (DD rmse only): 0.151366\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.017211 - Val Loss (DD rmse only): 0.150198\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014238 - Val Loss (DD rmse only): 0.148896\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014408 - Val Loss (DD rmse only): 0.148582\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014829 - Val Loss (DD rmse only): 0.147074\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014041 - Val Loss (DD rmse only): 0.146602\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015304 - Val Loss (DD rmse only): 0.147338\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013327 - Val Loss (DD rmse only): 0.146861\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014083 - Val Loss (DD rmse only): 0.146485\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013782 - Val Loss (DD rmse only): 0.146115\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014261 - Val Loss (DD rmse only): 0.146428\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014193 - Val Loss (DD rmse only): 0.145813\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013851 - Val Loss (DD rmse only): 0.145821\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014149 - Val Loss (DD rmse only): 0.146029\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013883 - Val Loss (DD rmse only): 0.146002\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.015571 - Val Loss (DD rmse only): 0.145532\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014145 - Val Loss (DD rmse only): 0.144875\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013268 - Val Loss (DD rmse only): 0.144948\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014227 - Val Loss (DD rmse only): 0.145150\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014755 - Val Loss (DD rmse only): 0.146116\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:04:42,138] Trial 51 finished with value: 0.14487525820732117 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 211, 'lr': 0.000276617980081748, 'weight_decay': 4.428112770149152e-06, 'batch_size': 32}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013369 - Val Loss (DD rmse only): 0.146615\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.091507 - Val Loss (DD rmse only): 0.317703\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.038923 - Val Loss (DD rmse only): 0.194033\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031781 - Val Loss (DD rmse only): 0.216065\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.032003 - Val Loss (DD rmse only): 0.235702\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.027436 - Val Loss (DD rmse only): 0.194616\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025903 - Val Loss (DD rmse only): 0.191474\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.023610 - Val Loss (DD rmse only): 0.189958\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.022090 - Val Loss (DD rmse only): 0.176631\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.020484 - Val Loss (DD rmse only): 0.168542\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.018596 - Val Loss (DD rmse only): 0.162601\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017784 - Val Loss (DD rmse only): 0.160528\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016243 - Val Loss (DD rmse only): 0.159075\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.017355 - Val Loss (DD rmse only): 0.155348\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015537 - Val Loss (DD rmse only): 0.152393\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015716 - Val Loss (DD rmse only): 0.150176\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015517 - Val Loss (DD rmse only): 0.146275\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015005 - Val Loss (DD rmse only): 0.143459\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014994 - Val Loss (DD rmse only): 0.146817\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015369 - Val Loss (DD rmse only): 0.142982\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014825 - Val Loss (DD rmse only): 0.141639\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014991 - Val Loss (DD rmse only): 0.146525\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.016378 - Val Loss (DD rmse only): 0.142541\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014719 - Val Loss (DD rmse only): 0.141468\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014320 - Val Loss (DD rmse only): 0.140838\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015168 - Val Loss (DD rmse only): 0.141125\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014601 - Val Loss (DD rmse only): 0.140929\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014741 - Val Loss (DD rmse only): 0.141744\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015300 - Val Loss (DD rmse only): 0.142557\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015110 - Val Loss (DD rmse only): 0.141924\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014972 - Val Loss (DD rmse only): 0.141655\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014466 - Val Loss (DD rmse only): 0.143695\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015077 - Val Loss (DD rmse only): 0.140332\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014614 - Val Loss (DD rmse only): 0.146400\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014791 - Val Loss (DD rmse only): 0.142119\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014154 - Val Loss (DD rmse only): 0.142568\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014181 - Val Loss (DD rmse only): 0.142343\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015165 - Val Loss (DD rmse only): 0.142354\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013953 - Val Loss (DD rmse only): 0.141149\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014040 - Val Loss (DD rmse only): 0.143026\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015209 - Val Loss (DD rmse only): 0.142006\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015478 - Val Loss (DD rmse only): 0.142369\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014566 - Val Loss (DD rmse only): 0.142819\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014085 - Val Loss (DD rmse only): 0.143760\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013932 - Val Loss (DD rmse only): 0.142683\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014010 - Val Loss (DD rmse only): 0.141778\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014400 - Val Loss (DD rmse only): 0.147031\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013862 - Val Loss (DD rmse only): 0.142312\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013984 - Val Loss (DD rmse only): 0.142830\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013767 - Val Loss (DD rmse only): 0.142540\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:05:14,882] Trial 52 finished with value: 0.1403324231505394 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 191, 'lr': 0.0005207743378764189, 'weight_decay': 6.44728542188089e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013457 - Val Loss (DD rmse only): 0.144000\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.070571 - Val Loss (DD rmse only): 0.191619\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.031191 - Val Loss (DD rmse only): 0.230132\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.029053 - Val Loss (DD rmse only): 0.194450\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.026762 - Val Loss (DD rmse only): 0.203508\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.023956 - Val Loss (DD rmse only): 0.170660\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.019918 - Val Loss (DD rmse only): 0.161423\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.017701 - Val Loss (DD rmse only): 0.156838\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017317 - Val Loss (DD rmse only): 0.162041\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016229 - Val Loss (DD rmse only): 0.150305\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015867 - Val Loss (DD rmse only): 0.147408\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015340 - Val Loss (DD rmse only): 0.146249\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015119 - Val Loss (DD rmse only): 0.150334\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015832 - Val Loss (DD rmse only): 0.145752\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015189 - Val Loss (DD rmse only): 0.145381\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014784 - Val Loss (DD rmse only): 0.144793\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015058 - Val Loss (DD rmse only): 0.154282\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015354 - Val Loss (DD rmse only): 0.145648\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014433 - Val Loss (DD rmse only): 0.144373\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014703 - Val Loss (DD rmse only): 0.159348\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015091 - Val Loss (DD rmse only): 0.144250\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014850 - Val Loss (DD rmse only): 0.145370\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015086 - Val Loss (DD rmse only): 0.144043\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014648 - Val Loss (DD rmse only): 0.148824\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014300 - Val Loss (DD rmse only): 0.146223\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013988 - Val Loss (DD rmse only): 0.145839\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014878 - Val Loss (DD rmse only): 0.146268\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015389 - Val Loss (DD rmse only): 0.149292\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015704 - Val Loss (DD rmse only): 0.158885\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014680 - Val Loss (DD rmse only): 0.174464\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016099 - Val Loss (DD rmse only): 0.150410\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015562 - Val Loss (DD rmse only): 0.151037\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015425 - Val Loss (DD rmse only): 0.155290\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014230 - Val Loss (DD rmse only): 0.146081\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014831 - Val Loss (DD rmse only): 0.145989\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013784 - Val Loss (DD rmse only): 0.154899\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013834 - Val Loss (DD rmse only): 0.147950\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014184 - Val Loss (DD rmse only): 0.151952\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015076 - Val Loss (DD rmse only): 0.149984\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015287 - Val Loss (DD rmse only): 0.162694\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015622 - Val Loss (DD rmse only): 0.151493\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014546 - Val Loss (DD rmse only): 0.151588\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014232 - Val Loss (DD rmse only): 0.158883\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013689 - Val Loss (DD rmse only): 0.149724\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013615 - Val Loss (DD rmse only): 0.147667\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013288 - Val Loss (DD rmse only): 0.164465\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014259 - Val Loss (DD rmse only): 0.163363\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013710 - Val Loss (DD rmse only): 0.153637\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013792 - Val Loss (DD rmse only): 0.149563\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013419 - Val Loss (DD rmse only): 0.150264\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:05:51,174] Trial 53 finished with value: 0.14404262602329254 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 189, 'lr': 0.0014327783431071356, 'weight_decay': 0.00010056826384502997, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013368 - Val Loss (DD rmse only): 0.153230\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.113886 - Val Loss (DD rmse only): 0.351150\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.045476 - Val Loss (DD rmse only): 0.200155\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.033706 - Val Loss (DD rmse only): 0.202808\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028338 - Val Loss (DD rmse only): 0.224282\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.026948 - Val Loss (DD rmse only): 0.198330\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025018 - Val Loss (DD rmse only): 0.186319\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.023290 - Val Loss (DD rmse only): 0.182958\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.021066 - Val Loss (DD rmse only): 0.173510\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.019698 - Val Loss (DD rmse only): 0.167161\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.018023 - Val Loss (DD rmse only): 0.164571\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017040 - Val Loss (DD rmse only): 0.159509\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.017744 - Val Loss (DD rmse only): 0.168392\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016820 - Val Loss (DD rmse only): 0.153325\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016137 - Val Loss (DD rmse only): 0.153219\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015130 - Val Loss (DD rmse only): 0.149456\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015103 - Val Loss (DD rmse only): 0.148262\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015192 - Val Loss (DD rmse only): 0.147553\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014981 - Val Loss (DD rmse only): 0.147559\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014962 - Val Loss (DD rmse only): 0.146997\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015335 - Val Loss (DD rmse only): 0.148439\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015250 - Val Loss (DD rmse only): 0.145028\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014885 - Val Loss (DD rmse only): 0.145084\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014714 - Val Loss (DD rmse only): 0.146477\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014317 - Val Loss (DD rmse only): 0.145622\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014861 - Val Loss (DD rmse only): 0.147149\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014514 - Val Loss (DD rmse only): 0.143015\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014166 - Val Loss (DD rmse only): 0.142966\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014342 - Val Loss (DD rmse only): 0.142200\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014248 - Val Loss (DD rmse only): 0.141626\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014406 - Val Loss (DD rmse only): 0.144017\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014126 - Val Loss (DD rmse only): 0.141260\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014337 - Val Loss (DD rmse only): 0.143097\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013990 - Val Loss (DD rmse only): 0.141611\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014162 - Val Loss (DD rmse only): 0.140969\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014099 - Val Loss (DD rmse only): 0.142547\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014821 - Val Loss (DD rmse only): 0.143680\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013553 - Val Loss (DD rmse only): 0.140211\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014847 - Val Loss (DD rmse only): 0.153868\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015036 - Val Loss (DD rmse only): 0.145726\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015155 - Val Loss (DD rmse only): 0.145873\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014840 - Val Loss (DD rmse only): 0.143608\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014716 - Val Loss (DD rmse only): 0.142134\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013589 - Val Loss (DD rmse only): 0.142845\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014234 - Val Loss (DD rmse only): 0.140948\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014018 - Val Loss (DD rmse only): 0.140740\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014008 - Val Loss (DD rmse only): 0.140849\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013475 - Val Loss (DD rmse only): 0.140501\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014056 - Val Loss (DD rmse only): 0.143437\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014127 - Val Loss (DD rmse only): 0.140088\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:06:19,873] Trial 54 finished with value: 0.14008762687444687 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 138, 'lr': 0.0006023234456453093, 'weight_decay': 1.9872343633462172e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014016 - Val Loss (DD rmse only): 0.140299\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.064853 - Val Loss (DD rmse only): 0.253815\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.033226 - Val Loss (DD rmse only): 0.191293\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.028574 - Val Loss (DD rmse only): 0.221534\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028260 - Val Loss (DD rmse only): 0.213884\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.026863 - Val Loss (DD rmse only): 0.190855\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025539 - Val Loss (DD rmse only): 0.189184\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.022472 - Val Loss (DD rmse only): 0.175798\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.020190 - Val Loss (DD rmse only): 0.165871\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.017831 - Val Loss (DD rmse only): 0.162146\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016796 - Val Loss (DD rmse only): 0.152539\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015837 - Val Loss (DD rmse only): 0.173496\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.018396 - Val Loss (DD rmse only): 0.147789\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015996 - Val Loss (DD rmse only): 0.148631\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016754 - Val Loss (DD rmse only): 0.148324\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015695 - Val Loss (DD rmse only): 0.146843\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016190 - Val Loss (DD rmse only): 0.147398\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016229 - Val Loss (DD rmse only): 0.145581\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015361 - Val Loss (DD rmse only): 0.146456\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014965 - Val Loss (DD rmse only): 0.145456\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014807 - Val Loss (DD rmse only): 0.144707\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014786 - Val Loss (DD rmse only): 0.145372\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014446 - Val Loss (DD rmse only): 0.142978\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015095 - Val Loss (DD rmse only): 0.145038\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014711 - Val Loss (DD rmse only): 0.143077\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014857 - Val Loss (DD rmse only): 0.145550\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014714 - Val Loss (DD rmse only): 0.145226\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015037 - Val Loss (DD rmse only): 0.148676\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015151 - Val Loss (DD rmse only): 0.143540\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015499 - Val Loss (DD rmse only): 0.148771\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014380 - Val Loss (DD rmse only): 0.145245\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014191 - Val Loss (DD rmse only): 0.144083\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014401 - Val Loss (DD rmse only): 0.150116\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014421 - Val Loss (DD rmse only): 0.144390\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014584 - Val Loss (DD rmse only): 0.146862\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014617 - Val Loss (DD rmse only): 0.144976\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014452 - Val Loss (DD rmse only): 0.151347\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014320 - Val Loss (DD rmse only): 0.145622\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013765 - Val Loss (DD rmse only): 0.150788\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014427 - Val Loss (DD rmse only): 0.150340\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014595 - Val Loss (DD rmse only): 0.150843\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013633 - Val Loss (DD rmse only): 0.147694\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014056 - Val Loss (DD rmse only): 0.149400\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013973 - Val Loss (DD rmse only): 0.148444\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014442 - Val Loss (DD rmse only): 0.154677\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014658 - Val Loss (DD rmse only): 0.154739\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014460 - Val Loss (DD rmse only): 0.148962\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014021 - Val Loss (DD rmse only): 0.151482\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013579 - Val Loss (DD rmse only): 0.149327\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013345 - Val Loss (DD rmse only): 0.157220\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:06:48,588] Trial 55 finished with value: 0.1429775208234787 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 139, 'lr': 0.0006805233733376468, 'weight_decay': 4.155114272543872e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013347 - Val Loss (DD rmse only): 0.153934\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.121060 - Val Loss (DD rmse only): 0.391955\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.071167 - Val Loss (DD rmse only): 0.297407\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.037109 - Val Loss (DD rmse only): 0.212746\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.033224 - Val Loss (DD rmse only): 0.193292\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.033554 - Val Loss (DD rmse only): 0.202722\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.028246 - Val Loss (DD rmse only): 0.214928\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.026561 - Val Loss (DD rmse only): 0.202001\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.025316 - Val Loss (DD rmse only): 0.192295\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.024360 - Val Loss (DD rmse only): 0.182390\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.021466 - Val Loss (DD rmse only): 0.183219\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.021466 - Val Loss (DD rmse only): 0.175977\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.019595 - Val Loss (DD rmse only): 0.167738\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.018394 - Val Loss (DD rmse only): 0.167230\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016986 - Val Loss (DD rmse only): 0.162529\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.016459 - Val Loss (DD rmse only): 0.159980\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016861 - Val Loss (DD rmse only): 0.156884\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015515 - Val Loss (DD rmse only): 0.157085\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015601 - Val Loss (DD rmse only): 0.151987\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015159 - Val Loss (DD rmse only): 0.152362\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015585 - Val Loss (DD rmse only): 0.150051\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015171 - Val Loss (DD rmse only): 0.147766\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015739 - Val Loss (DD rmse only): 0.148535\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015616 - Val Loss (DD rmse only): 0.146200\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015152 - Val Loss (DD rmse only): 0.145508\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014969 - Val Loss (DD rmse only): 0.146425\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014685 - Val Loss (DD rmse only): 0.146624\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015126 - Val Loss (DD rmse only): 0.145257\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014800 - Val Loss (DD rmse only): 0.146365\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015234 - Val Loss (DD rmse only): 0.146497\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015541 - Val Loss (DD rmse only): 0.145840\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014936 - Val Loss (DD rmse only): 0.148661\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014706 - Val Loss (DD rmse only): 0.145763\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014274 - Val Loss (DD rmse only): 0.146813\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014146 - Val Loss (DD rmse only): 0.145707\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014235 - Val Loss (DD rmse only): 0.147232\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014587 - Val Loss (DD rmse only): 0.146505\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014045 - Val Loss (DD rmse only): 0.147176\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014954 - Val Loss (DD rmse only): 0.147070\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013788 - Val Loss (DD rmse only): 0.148878\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014208 - Val Loss (DD rmse only): 0.147157\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014267 - Val Loss (DD rmse only): 0.149165\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013933 - Val Loss (DD rmse only): 0.147125\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013728 - Val Loss (DD rmse only): 0.149252\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014492 - Val Loss (DD rmse only): 0.147586\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013882 - Val Loss (DD rmse only): 0.148675\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014080 - Val Loss (DD rmse only): 0.150301\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014595 - Val Loss (DD rmse only): 0.150067\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013536 - Val Loss (DD rmse only): 0.149940\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.015051 - Val Loss (DD rmse only): 0.150865\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:07:15,237] Trial 56 finished with value: 0.14525719732046127 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 179, 'lr': 0.0002616919777359765, 'weight_decay': 1.876566327651574e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014090 - Val Loss (DD rmse only): 0.148955\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.054820 - Val Loss (DD rmse only): 0.213279\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.029882 - Val Loss (DD rmse only): 0.202625\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.027817 - Val Loss (DD rmse only): 0.211462\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025519 - Val Loss (DD rmse only): 0.187510\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.022855 - Val Loss (DD rmse only): 0.185423\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.020159 - Val Loss (DD rmse only): 0.172669\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.017744 - Val Loss (DD rmse only): 0.158147\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017028 - Val Loss (DD rmse only): 0.156807\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016279 - Val Loss (DD rmse only): 0.150043\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015214 - Val Loss (DD rmse only): 0.148407\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015108 - Val Loss (DD rmse only): 0.142817\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015778 - Val Loss (DD rmse only): 0.141051\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015804 - Val Loss (DD rmse only): 0.145940\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014580 - Val Loss (DD rmse only): 0.144300\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015401 - Val Loss (DD rmse only): 0.142761\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015324 - Val Loss (DD rmse only): 0.141923\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015336 - Val Loss (DD rmse only): 0.143688\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014665 - Val Loss (DD rmse only): 0.143919\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014781 - Val Loss (DD rmse only): 0.147272\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014636 - Val Loss (DD rmse only): 0.146311\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014622 - Val Loss (DD rmse only): 0.142240\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015477 - Val Loss (DD rmse only): 0.143560\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014412 - Val Loss (DD rmse only): 0.143086\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014537 - Val Loss (DD rmse only): 0.145675\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014759 - Val Loss (DD rmse only): 0.152794\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015074 - Val Loss (DD rmse only): 0.141332\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014707 - Val Loss (DD rmse only): 0.143657\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014295 - Val Loss (DD rmse only): 0.141527\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015249 - Val Loss (DD rmse only): 0.141808\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013486 - Val Loss (DD rmse only): 0.143563\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013988 - Val Loss (DD rmse only): 0.145622\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014125 - Val Loss (DD rmse only): 0.147526\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013551 - Val Loss (DD rmse only): 0.142571\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014312 - Val Loss (DD rmse only): 0.147478\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013839 - Val Loss (DD rmse only): 0.142882\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013712 - Val Loss (DD rmse only): 0.151627\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014490 - Val Loss (DD rmse only): 0.158670\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014185 - Val Loss (DD rmse only): 0.143748\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013593 - Val Loss (DD rmse only): 0.146739\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013896 - Val Loss (DD rmse only): 0.145064\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014212 - Val Loss (DD rmse only): 0.144159\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013713 - Val Loss (DD rmse only): 0.145692\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013930 - Val Loss (DD rmse only): 0.159537\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013877 - Val Loss (DD rmse only): 0.149591\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013321 - Val Loss (DD rmse only): 0.153199\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013324 - Val Loss (DD rmse only): 0.152028\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013379 - Val Loss (DD rmse only): 0.145163\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013290 - Val Loss (DD rmse only): 0.151603\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013088 - Val Loss (DD rmse only): 0.151174\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:07:47,528] Trial 57 finished with value: 0.14105064421892166 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 195, 'lr': 0.0005668882116704378, 'weight_decay': 6.104803595556624e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013265 - Val Loss (DD rmse only): 0.153799\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.062492 - Val Loss (DD rmse only): 0.200481\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.030873 - Val Loss (DD rmse only): 0.199722\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.025650 - Val Loss (DD rmse only): 0.193488\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.021949 - Val Loss (DD rmse only): 0.172981\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.019399 - Val Loss (DD rmse only): 0.168389\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.018890 - Val Loss (DD rmse only): 0.160298\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016699 - Val Loss (DD rmse only): 0.157537\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015812 - Val Loss (DD rmse only): 0.150112\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015981 - Val Loss (DD rmse only): 0.148633\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015068 - Val Loss (DD rmse only): 0.143194\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.014864 - Val Loss (DD rmse only): 0.141719\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015548 - Val Loss (DD rmse only): 0.143210\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015292 - Val Loss (DD rmse only): 0.143910\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015009 - Val Loss (DD rmse only): 0.142850\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014811 - Val Loss (DD rmse only): 0.142800\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015407 - Val Loss (DD rmse only): 0.150205\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014916 - Val Loss (DD rmse only): 0.144457\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014344 - Val Loss (DD rmse only): 0.153842\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015495 - Val Loss (DD rmse only): 0.143091\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014485 - Val Loss (DD rmse only): 0.141192\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014718 - Val Loss (DD rmse only): 0.142319\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014287 - Val Loss (DD rmse only): 0.145594\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014240 - Val Loss (DD rmse only): 0.144678\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014366 - Val Loss (DD rmse only): 0.149676\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014672 - Val Loss (DD rmse only): 0.144200\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014140 - Val Loss (DD rmse only): 0.148584\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014557 - Val Loss (DD rmse only): 0.143850\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014191 - Val Loss (DD rmse only): 0.151937\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013702 - Val Loss (DD rmse only): 0.147794\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013980 - Val Loss (DD rmse only): 0.146925\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013651 - Val Loss (DD rmse only): 0.147921\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014432 - Val Loss (DD rmse only): 0.146804\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015506 - Val Loss (DD rmse only): 0.151770\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014640 - Val Loss (DD rmse only): 0.161022\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015321 - Val Loss (DD rmse only): 0.151320\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014546 - Val Loss (DD rmse only): 0.160920\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014341 - Val Loss (DD rmse only): 0.149769\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013806 - Val Loss (DD rmse only): 0.147613\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013893 - Val Loss (DD rmse only): 0.158377\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013735 - Val Loss (DD rmse only): 0.153100\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013419 - Val Loss (DD rmse only): 0.151021\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013215 - Val Loss (DD rmse only): 0.154064\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013545 - Val Loss (DD rmse only): 0.148886\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013226 - Val Loss (DD rmse only): 0.155788\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013125 - Val Loss (DD rmse only): 0.151465\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013322 - Val Loss (DD rmse only): 0.159343\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013461 - Val Loss (DD rmse only): 0.169881\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013958 - Val Loss (DD rmse only): 0.154595\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013211 - Val Loss (DD rmse only): 0.154313\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:08:15,946] Trial 58 finished with value: 0.14119195193052292 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 219, 'lr': 0.0005569074808158025, 'weight_decay': 7.069674353177923e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013302 - Val Loss (DD rmse only): 0.162370\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.094470 - Val Loss (DD rmse only): 0.352144\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.051270 - Val Loss (DD rmse only): 0.248314\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.030288 - Val Loss (DD rmse only): 0.194307\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.031128 - Val Loss (DD rmse only): 0.210976\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.027862 - Val Loss (DD rmse only): 0.219964\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.026938 - Val Loss (DD rmse only): 0.206246\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.025453 - Val Loss (DD rmse only): 0.185621\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.023489 - Val Loss (DD rmse only): 0.193478\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.022073 - Val Loss (DD rmse only): 0.179972\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.020579 - Val Loss (DD rmse only): 0.167438\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.018724 - Val Loss (DD rmse only): 0.163143\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.017030 - Val Loss (DD rmse only): 0.157974\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016181 - Val Loss (DD rmse only): 0.155679\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016015 - Val Loss (DD rmse only): 0.152261\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015564 - Val Loss (DD rmse only): 0.149562\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016026 - Val Loss (DD rmse only): 0.146808\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015276 - Val Loss (DD rmse only): 0.145610\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015513 - Val Loss (DD rmse only): 0.143597\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014771 - Val Loss (DD rmse only): 0.143281\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015498 - Val Loss (DD rmse only): 0.144333\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015591 - Val Loss (DD rmse only): 0.141941\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015164 - Val Loss (DD rmse only): 0.144590\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014907 - Val Loss (DD rmse only): 0.141099\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014459 - Val Loss (DD rmse only): 0.142885\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014434 - Val Loss (DD rmse only): 0.141699\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014098 - Val Loss (DD rmse only): 0.141703\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014559 - Val Loss (DD rmse only): 0.143187\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014752 - Val Loss (DD rmse only): 0.141842\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014363 - Val Loss (DD rmse only): 0.142452\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014606 - Val Loss (DD rmse only): 0.143866\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014074 - Val Loss (DD rmse only): 0.142421\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014514 - Val Loss (DD rmse only): 0.142685\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014492 - Val Loss (DD rmse only): 0.146955\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014332 - Val Loss (DD rmse only): 0.142265\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015230 - Val Loss (DD rmse only): 0.145635\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013532 - Val Loss (DD rmse only): 0.142405\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014490 - Val Loss (DD rmse only): 0.148246\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014182 - Val Loss (DD rmse only): 0.142780\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014500 - Val Loss (DD rmse only): 0.147763\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014610 - Val Loss (DD rmse only): 0.143576\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013530 - Val Loss (DD rmse only): 0.142173\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013772 - Val Loss (DD rmse only): 0.143252\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014055 - Val Loss (DD rmse only): 0.145448\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013927 - Val Loss (DD rmse only): 0.146201\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013625 - Val Loss (DD rmse only): 0.143223\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014670 - Val Loss (DD rmse only): 0.152319\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014565 - Val Loss (DD rmse only): 0.143608\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014379 - Val Loss (DD rmse only): 0.144081\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013789 - Val Loss (DD rmse only): 0.144010\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:08:49,680] Trial 59 finished with value: 0.14109892398118973 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 199, 'lr': 0.0003439609940414243, 'weight_decay': 0.00012142718795273066, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013381 - Val Loss (DD rmse only): 0.146625\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.075669 - Val Loss (DD rmse only): 0.348451\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.061628 - Val Loss (DD rmse only): 0.315017\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.046097 - Val Loss (DD rmse only): 0.280407\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.037812 - Val Loss (DD rmse only): 0.240082\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028841 - Val Loss (DD rmse only): 0.204992\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.028943 - Val Loss (DD rmse only): 0.197658\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.028211 - Val Loss (DD rmse only): 0.203863\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.026012 - Val Loss (DD rmse only): 0.209260\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.025697 - Val Loss (DD rmse only): 0.203930\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.024306 - Val Loss (DD rmse only): 0.194327\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.023180 - Val Loss (DD rmse only): 0.190821\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.022422 - Val Loss (DD rmse only): 0.181986\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.020507 - Val Loss (DD rmse only): 0.179050\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.020772 - Val Loss (DD rmse only): 0.174726\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.018257 - Val Loss (DD rmse only): 0.168608\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.017755 - Val Loss (DD rmse only): 0.166732\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017308 - Val Loss (DD rmse only): 0.164377\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017150 - Val Loss (DD rmse only): 0.162463\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016536 - Val Loss (DD rmse only): 0.159677\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016383 - Val Loss (DD rmse only): 0.158442\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015771 - Val Loss (DD rmse only): 0.155404\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015735 - Val Loss (DD rmse only): 0.153981\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015278 - Val Loss (DD rmse only): 0.151858\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015498 - Val Loss (DD rmse only): 0.150893\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014972 - Val Loss (DD rmse only): 0.150164\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015003 - Val Loss (DD rmse only): 0.147166\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015222 - Val Loss (DD rmse only): 0.150358\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015177 - Val Loss (DD rmse only): 0.146804\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015152 - Val Loss (DD rmse only): 0.149112\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014844 - Val Loss (DD rmse only): 0.148056\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014829 - Val Loss (DD rmse only): 0.147342\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014523 - Val Loss (DD rmse only): 0.147093\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014537 - Val Loss (DD rmse only): 0.148375\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014982 - Val Loss (DD rmse only): 0.145333\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015505 - Val Loss (DD rmse only): 0.150384\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015861 - Val Loss (DD rmse only): 0.147169\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014431 - Val Loss (DD rmse only): 0.147201\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014180 - Val Loss (DD rmse only): 0.147899\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015110 - Val Loss (DD rmse only): 0.147340\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014144 - Val Loss (DD rmse only): 0.148687\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014169 - Val Loss (DD rmse only): 0.148249\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014876 - Val Loss (DD rmse only): 0.148667\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014235 - Val Loss (DD rmse only): 0.147246\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014199 - Val Loss (DD rmse only): 0.146915\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014308 - Val Loss (DD rmse only): 0.148443\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014779 - Val Loss (DD rmse only): 0.147550\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013987 - Val Loss (DD rmse only): 0.149407\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014006 - Val Loss (DD rmse only): 0.148295\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014135 - Val Loss (DD rmse only): 0.149096\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:09:21,442] Trial 60 finished with value: 0.145332969725132 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 193, 'lr': 0.00016643263048093392, 'weight_decay': 4.860033315481685e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014179 - Val Loss (DD rmse only): 0.150993\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.129253 - Val Loss (DD rmse only): 0.388077\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.061702 - Val Loss (DD rmse only): 0.227865\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.034271 - Val Loss (DD rmse only): 0.194195\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.030593 - Val Loss (DD rmse only): 0.226578\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.029040 - Val Loss (DD rmse only): 0.205801\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025704 - Val Loss (DD rmse only): 0.186312\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.023887 - Val Loss (DD rmse only): 0.191698\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.023590 - Val Loss (DD rmse only): 0.181623\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.021859 - Val Loss (DD rmse only): 0.173960\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.019739 - Val Loss (DD rmse only): 0.173533\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.019817 - Val Loss (DD rmse only): 0.168266\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.018327 - Val Loss (DD rmse only): 0.165106\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.017309 - Val Loss (DD rmse only): 0.162698\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.017276 - Val Loss (DD rmse only): 0.160262\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.016126 - Val Loss (DD rmse only): 0.156811\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015459 - Val Loss (DD rmse only): 0.155553\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015092 - Val Loss (DD rmse only): 0.152376\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016042 - Val Loss (DD rmse only): 0.151562\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015434 - Val Loss (DD rmse only): 0.149692\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015876 - Val Loss (DD rmse only): 0.148691\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015231 - Val Loss (DD rmse only): 0.147038\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015459 - Val Loss (DD rmse only): 0.147161\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015068 - Val Loss (DD rmse only): 0.144963\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014701 - Val Loss (DD rmse only): 0.145315\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014627 - Val Loss (DD rmse only): 0.144812\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015061 - Val Loss (DD rmse only): 0.150113\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015846 - Val Loss (DD rmse only): 0.144992\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014211 - Val Loss (DD rmse only): 0.147366\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014680 - Val Loss (DD rmse only): 0.147221\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014725 - Val Loss (DD rmse only): 0.145069\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015453 - Val Loss (DD rmse only): 0.149735\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014318 - Val Loss (DD rmse only): 0.144945\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014479 - Val Loss (DD rmse only): 0.145873\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014743 - Val Loss (DD rmse only): 0.146300\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014210 - Val Loss (DD rmse only): 0.143992\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015148 - Val Loss (DD rmse only): 0.148630\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015397 - Val Loss (DD rmse only): 0.144122\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014646 - Val Loss (DD rmse only): 0.144348\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014642 - Val Loss (DD rmse only): 0.152356\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014533 - Val Loss (DD rmse only): 0.144844\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014148 - Val Loss (DD rmse only): 0.146024\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013755 - Val Loss (DD rmse only): 0.143691\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014037 - Val Loss (DD rmse only): 0.145393\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013920 - Val Loss (DD rmse only): 0.144765\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014341 - Val Loss (DD rmse only): 0.147949\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014523 - Val Loss (DD rmse only): 0.143928\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014311 - Val Loss (DD rmse only): 0.144395\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014394 - Val Loss (DD rmse only): 0.146366\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014210 - Val Loss (DD rmse only): 0.147739\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:09:54,742] Trial 61 finished with value: 0.1436910554766655 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 209, 'lr': 0.0003507469692347734, 'weight_decay': 0.00013217555523955716, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013869 - Val Loss (DD rmse only): 0.148220\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.094842 - Val Loss (DD rmse only): 0.313563\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.033331 - Val Loss (DD rmse only): 0.192495\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.038785 - Val Loss (DD rmse only): 0.201164\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028705 - Val Loss (DD rmse only): 0.234953\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028499 - Val Loss (DD rmse only): 0.212596\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.026391 - Val Loss (DD rmse only): 0.191183\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.024382 - Val Loss (DD rmse only): 0.191665\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.023464 - Val Loss (DD rmse only): 0.205409\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.021310 - Val Loss (DD rmse only): 0.178159\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.021116 - Val Loss (DD rmse only): 0.178619\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.020073 - Val Loss (DD rmse only): 0.174689\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.019140 - Val Loss (DD rmse only): 0.166346\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.017205 - Val Loss (DD rmse only): 0.165083\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016571 - Val Loss (DD rmse only): 0.159951\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015600 - Val Loss (DD rmse only): 0.156965\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015537 - Val Loss (DD rmse only): 0.158060\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014811 - Val Loss (DD rmse only): 0.150516\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015477 - Val Loss (DD rmse only): 0.156456\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015383 - Val Loss (DD rmse only): 0.149180\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014966 - Val Loss (DD rmse only): 0.147293\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015819 - Val Loss (DD rmse only): 0.152320\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014905 - Val Loss (DD rmse only): 0.148537\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015189 - Val Loss (DD rmse only): 0.151947\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014950 - Val Loss (DD rmse only): 0.147535\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014681 - Val Loss (DD rmse only): 0.148597\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014331 - Val Loss (DD rmse only): 0.148167\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014858 - Val Loss (DD rmse only): 0.146124\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014217 - Val Loss (DD rmse only): 0.150674\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014251 - Val Loss (DD rmse only): 0.146421\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014305 - Val Loss (DD rmse only): 0.147636\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013818 - Val Loss (DD rmse only): 0.146787\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013934 - Val Loss (DD rmse only): 0.157790\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014679 - Val Loss (DD rmse only): 0.149865\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014125 - Val Loss (DD rmse only): 0.161435\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015416 - Val Loss (DD rmse only): 0.149831\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014352 - Val Loss (DD rmse only): 0.158355\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014081 - Val Loss (DD rmse only): 0.150271\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014007 - Val Loss (DD rmse only): 0.154243\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013622 - Val Loss (DD rmse only): 0.150197\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013687 - Val Loss (DD rmse only): 0.154214\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013731 - Val Loss (DD rmse only): 0.151133\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013474 - Val Loss (DD rmse only): 0.156034\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013831 - Val Loss (DD rmse only): 0.150110\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013582 - Val Loss (DD rmse only): 0.152399\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013684 - Val Loss (DD rmse only): 0.156351\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014093 - Val Loss (DD rmse only): 0.154578\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013888 - Val Loss (DD rmse only): 0.153830\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012949 - Val Loss (DD rmse only): 0.153461\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013005 - Val Loss (DD rmse only): 0.156490\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:10:27,962] Trial 62 finished with value: 0.1461244449019432 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 200, 'lr': 0.00043435678571337246, 'weight_decay': 5.7565835463873195e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013088 - Val Loss (DD rmse only): 0.158567\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.066293 - Val Loss (DD rmse only): 0.211562\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.036592 - Val Loss (DD rmse only): 0.269252\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031858 - Val Loss (DD rmse only): 0.189539\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028525 - Val Loss (DD rmse only): 0.201191\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.025842 - Val Loss (DD rmse only): 0.190877\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.021025 - Val Loss (DD rmse only): 0.170709\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.019114 - Val Loss (DD rmse only): 0.166188\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017205 - Val Loss (DD rmse only): 0.159956\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016759 - Val Loss (DD rmse only): 0.164445\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016888 - Val Loss (DD rmse only): 0.152329\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015884 - Val Loss (DD rmse only): 0.150242\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015715 - Val Loss (DD rmse only): 0.147535\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015059 - Val Loss (DD rmse only): 0.144432\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015505 - Val Loss (DD rmse only): 0.146629\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014936 - Val Loss (DD rmse only): 0.147373\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015384 - Val Loss (DD rmse only): 0.143722\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015007 - Val Loss (DD rmse only): 0.144354\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014882 - Val Loss (DD rmse only): 0.145012\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014549 - Val Loss (DD rmse only): 0.145140\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014607 - Val Loss (DD rmse only): 0.151586\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.016029 - Val Loss (DD rmse only): 0.153189\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015447 - Val Loss (DD rmse only): 0.144290\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014813 - Val Loss (DD rmse only): 0.146555\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014811 - Val Loss (DD rmse only): 0.149851\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015279 - Val Loss (DD rmse only): 0.151833\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014860 - Val Loss (DD rmse only): 0.147239\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014375 - Val Loss (DD rmse only): 0.145917\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015549 - Val Loss (DD rmse only): 0.149057\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014198 - Val Loss (DD rmse only): 0.143027\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014342 - Val Loss (DD rmse only): 0.145629\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014415 - Val Loss (DD rmse only): 0.146895\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014508 - Val Loss (DD rmse only): 0.144222\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014206 - Val Loss (DD rmse only): 0.144731\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014401 - Val Loss (DD rmse only): 0.146234\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014256 - Val Loss (DD rmse only): 0.154704\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013840 - Val Loss (DD rmse only): 0.144518\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014461 - Val Loss (DD rmse only): 0.148300\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014671 - Val Loss (DD rmse only): 0.148853\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014455 - Val Loss (DD rmse only): 0.155962\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014951 - Val Loss (DD rmse only): 0.152026\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014882 - Val Loss (DD rmse only): 0.145240\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014489 - Val Loss (DD rmse only): 0.148137\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013977 - Val Loss (DD rmse only): 0.149596\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014154 - Val Loss (DD rmse only): 0.146377\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014574 - Val Loss (DD rmse only): 0.146178\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014838 - Val Loss (DD rmse only): 0.147417\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.015032 - Val Loss (DD rmse only): 0.150291\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014124 - Val Loss (DD rmse only): 0.152496\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014033 - Val Loss (DD rmse only): 0.158779\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:11:04,538] Trial 63 finished with value: 0.14302684366703033 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 218, 'lr': 0.0009583540214803513, 'weight_decay': 0.0002160338104453431, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014438 - Val Loss (DD rmse only): 0.150491\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.104669 - Val Loss (DD rmse only): 0.364401\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.058709 - Val Loss (DD rmse only): 0.267942\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.030798 - Val Loss (DD rmse only): 0.198263\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.032011 - Val Loss (DD rmse only): 0.195572\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.027585 - Val Loss (DD rmse only): 0.212497\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025428 - Val Loss (DD rmse only): 0.202838\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.023360 - Val Loss (DD rmse only): 0.188768\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.022702 - Val Loss (DD rmse only): 0.181747\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.020395 - Val Loss (DD rmse only): 0.182334\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.019404 - Val Loss (DD rmse only): 0.168998\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017814 - Val Loss (DD rmse only): 0.165166\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016929 - Val Loss (DD rmse only): 0.161617\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015836 - Val Loss (DD rmse only): 0.158682\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016240 - Val Loss (DD rmse only): 0.155878\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015603 - Val Loss (DD rmse only): 0.153314\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015483 - Val Loss (DD rmse only): 0.151108\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015177 - Val Loss (DD rmse only): 0.149425\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015210 - Val Loss (DD rmse only): 0.148002\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015810 - Val Loss (DD rmse only): 0.148415\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014932 - Val Loss (DD rmse only): 0.146303\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015265 - Val Loss (DD rmse only): 0.147846\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015489 - Val Loss (DD rmse only): 0.145096\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014785 - Val Loss (DD rmse only): 0.146481\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014592 - Val Loss (DD rmse only): 0.145042\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015402 - Val Loss (DD rmse only): 0.144699\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014875 - Val Loss (DD rmse only): 0.145707\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014473 - Val Loss (DD rmse only): 0.144818\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015077 - Val Loss (DD rmse only): 0.147088\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014620 - Val Loss (DD rmse only): 0.144223\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015207 - Val Loss (DD rmse only): 0.146164\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014821 - Val Loss (DD rmse only): 0.144639\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014429 - Val Loss (DD rmse only): 0.146911\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014377 - Val Loss (DD rmse only): 0.144722\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014266 - Val Loss (DD rmse only): 0.146406\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014288 - Val Loss (DD rmse only): 0.143698\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014492 - Val Loss (DD rmse only): 0.146935\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014930 - Val Loss (DD rmse only): 0.144790\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014398 - Val Loss (DD rmse only): 0.144631\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014938 - Val Loss (DD rmse only): 0.146634\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014453 - Val Loss (DD rmse only): 0.145034\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014298 - Val Loss (DD rmse only): 0.148532\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014190 - Val Loss (DD rmse only): 0.146467\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014318 - Val Loss (DD rmse only): 0.146157\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013779 - Val Loss (DD rmse only): 0.146489\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014007 - Val Loss (DD rmse only): 0.145749\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014179 - Val Loss (DD rmse only): 0.148559\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013887 - Val Loss (DD rmse only): 0.145635\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013984 - Val Loss (DD rmse only): 0.151328\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014066 - Val Loss (DD rmse only): 0.146890\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:11:35,835] Trial 64 finished with value: 0.14369846880435944 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 240, 'lr': 0.00022487165975245996, 'weight_decay': 0.0001317841029624226, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013829 - Val Loss (DD rmse only): 0.145490\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.122026 - Val Loss (DD rmse only): 0.415984\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.092963 - Val Loss (DD rmse only): 0.366078\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.062907 - Val Loss (DD rmse only): 0.302013\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.037975 - Val Loss (DD rmse only): 0.221984\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.032319 - Val Loss (DD rmse only): 0.203607\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.031067 - Val Loss (DD rmse only): 0.223977\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.028484 - Val Loss (DD rmse only): 0.220328\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.028688 - Val Loss (DD rmse only): 0.211869\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.026944 - Val Loss (DD rmse only): 0.204955\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.026405 - Val Loss (DD rmse only): 0.203012\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.025368 - Val Loss (DD rmse only): 0.203414\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.024536 - Val Loss (DD rmse only): 0.192484\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.023148 - Val Loss (DD rmse only): 0.185146\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.021961 - Val Loss (DD rmse only): 0.181220\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.020368 - Val Loss (DD rmse only): 0.175395\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.019521 - Val Loss (DD rmse only): 0.173789\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.019138 - Val Loss (DD rmse only): 0.170175\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017373 - Val Loss (DD rmse only): 0.167576\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016739 - Val Loss (DD rmse only): 0.167601\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016465 - Val Loss (DD rmse only): 0.161707\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.016687 - Val Loss (DD rmse only): 0.161475\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015506 - Val Loss (DD rmse only): 0.155128\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015432 - Val Loss (DD rmse only): 0.153472\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015254 - Val Loss (DD rmse only): 0.149847\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015177 - Val Loss (DD rmse only): 0.149746\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014881 - Val Loss (DD rmse only): 0.147129\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015474 - Val Loss (DD rmse only): 0.152473\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014478 - Val Loss (DD rmse only): 0.145991\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014878 - Val Loss (DD rmse only): 0.148361\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014982 - Val Loss (DD rmse only): 0.148428\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015251 - Val Loss (DD rmse only): 0.146202\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014596 - Val Loss (DD rmse only): 0.149977\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014438 - Val Loss (DD rmse only): 0.147451\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014520 - Val Loss (DD rmse only): 0.148660\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014171 - Val Loss (DD rmse only): 0.148408\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014519 - Val Loss (DD rmse only): 0.146269\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014606 - Val Loss (DD rmse only): 0.152347\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014924 - Val Loss (DD rmse only): 0.149201\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014603 - Val Loss (DD rmse only): 0.147042\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014923 - Val Loss (DD rmse only): 0.153991\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014003 - Val Loss (DD rmse only): 0.148320\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014031 - Val Loss (DD rmse only): 0.152548\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014348 - Val Loss (DD rmse only): 0.149927\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013897 - Val Loss (DD rmse only): 0.153447\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013827 - Val Loss (DD rmse only): 0.147793\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014573 - Val Loss (DD rmse only): 0.152876\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014076 - Val Loss (DD rmse only): 0.147883\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014094 - Val Loss (DD rmse only): 0.160807\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014440 - Val Loss (DD rmse only): 0.147258\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:12:02,644] Trial 65 finished with value: 0.14599120616912842 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 119, 'lr': 0.00039266425389035143, 'weight_decay': 5.9717993818101145e-06, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014882 - Val Loss (DD rmse only): 0.154978\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.104103 - Val Loss (DD rmse only): 0.358382\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.051729 - Val Loss (DD rmse only): 0.230428\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.033480 - Val Loss (DD rmse only): 0.202758\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.031943 - Val Loss (DD rmse only): 0.233205\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028360 - Val Loss (DD rmse only): 0.208183\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.027289 - Val Loss (DD rmse only): 0.201489\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.025191 - Val Loss (DD rmse only): 0.200936\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.023715 - Val Loss (DD rmse only): 0.189331\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.022202 - Val Loss (DD rmse only): 0.178853\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.020314 - Val Loss (DD rmse only): 0.174264\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017543 - Val Loss (DD rmse only): 0.164864\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.017451 - Val Loss (DD rmse only): 0.160702\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016673 - Val Loss (DD rmse only): 0.156421\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016993 - Val Loss (DD rmse only): 0.154530\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015456 - Val Loss (DD rmse only): 0.150798\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016190 - Val Loss (DD rmse only): 0.150848\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015172 - Val Loss (DD rmse only): 0.146928\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015208 - Val Loss (DD rmse only): 0.145724\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015091 - Val Loss (DD rmse only): 0.146091\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014832 - Val Loss (DD rmse only): 0.144661\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014817 - Val Loss (DD rmse only): 0.143990\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014739 - Val Loss (DD rmse only): 0.143657\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015082 - Val Loss (DD rmse only): 0.143855\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014904 - Val Loss (DD rmse only): 0.144344\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015251 - Val Loss (DD rmse only): 0.146503\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015775 - Val Loss (DD rmse only): 0.145385\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015378 - Val Loss (DD rmse only): 0.145578\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015120 - Val Loss (DD rmse only): 0.147173\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014944 - Val Loss (DD rmse only): 0.148893\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015879 - Val Loss (DD rmse only): 0.144497\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015135 - Val Loss (DD rmse only): 0.145973\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015648 - Val Loss (DD rmse only): 0.152070\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015816 - Val Loss (DD rmse only): 0.145033\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.015205 - Val Loss (DD rmse only): 0.144951\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014474 - Val Loss (DD rmse only): 0.145860\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.015126 - Val Loss (DD rmse only): 0.145863\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015380 - Val Loss (DD rmse only): 0.144586\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014626 - Val Loss (DD rmse only): 0.143902\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015610 - Val Loss (DD rmse only): 0.147455\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.015195 - Val Loss (DD rmse only): 0.147075\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015690 - Val Loss (DD rmse only): 0.147607\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014773 - Val Loss (DD rmse only): 0.145460\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014516 - Val Loss (DD rmse only): 0.145837\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015967 - Val Loss (DD rmse only): 0.147989\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014759 - Val Loss (DD rmse only): 0.144202\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014699 - Val Loss (DD rmse only): 0.146677\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014567 - Val Loss (DD rmse only): 0.144650\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.015290 - Val Loss (DD rmse only): 0.144733\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014356 - Val Loss (DD rmse only): 0.144108\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:12:33,773] Trial 66 finished with value: 0.14365672320127487 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 185, 'lr': 0.0004961070304004298, 'weight_decay': 0.000498512771879744, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014496 - Val Loss (DD rmse only): 0.146532\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.126842 - Val Loss (DD rmse only): 0.363945\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.055431 - Val Loss (DD rmse only): 0.232396\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031530 - Val Loss (DD rmse only): 0.192986\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.031293 - Val Loss (DD rmse only): 0.189087\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.024894 - Val Loss (DD rmse only): 0.202231\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.024478 - Val Loss (DD rmse only): 0.197707\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.022386 - Val Loss (DD rmse only): 0.178769\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.021719 - Val Loss (DD rmse only): 0.173155\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.019216 - Val Loss (DD rmse only): 0.173428\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.018247 - Val Loss (DD rmse only): 0.166308\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017372 - Val Loss (DD rmse only): 0.162061\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016727 - Val Loss (DD rmse only): 0.160634\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016652 - Val Loss (DD rmse only): 0.157490\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015819 - Val Loss (DD rmse only): 0.155780\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015646 - Val Loss (DD rmse only): 0.154886\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015373 - Val Loss (DD rmse only): 0.152292\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015582 - Val Loss (DD rmse only): 0.151104\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015127 - Val Loss (DD rmse only): 0.150348\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015971 - Val Loss (DD rmse only): 0.149235\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015086 - Val Loss (DD rmse only): 0.149229\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014534 - Val Loss (DD rmse only): 0.148440\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015174 - Val Loss (DD rmse only): 0.148044\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014793 - Val Loss (DD rmse only): 0.146764\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014728 - Val Loss (DD rmse only): 0.148626\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014781 - Val Loss (DD rmse only): 0.146864\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014497 - Val Loss (DD rmse only): 0.148317\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015441 - Val Loss (DD rmse only): 0.147315\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014367 - Val Loss (DD rmse only): 0.147252\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015744 - Val Loss (DD rmse only): 0.146727\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015243 - Val Loss (DD rmse only): 0.147712\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014380 - Val Loss (DD rmse only): 0.146833\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015089 - Val Loss (DD rmse only): 0.147566\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014141 - Val Loss (DD rmse only): 0.147368\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014191 - Val Loss (DD rmse only): 0.148821\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013684 - Val Loss (DD rmse only): 0.148717\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013917 - Val Loss (DD rmse only): 0.147714\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014004 - Val Loss (DD rmse only): 0.148561\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014032 - Val Loss (DD rmse only): 0.147151\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014382 - Val Loss (DD rmse only): 0.148854\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013685 - Val Loss (DD rmse only): 0.148970\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014262 - Val Loss (DD rmse only): 0.149816\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013580 - Val Loss (DD rmse only): 0.151166\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014711 - Val Loss (DD rmse only): 0.149444\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014139 - Val Loss (DD rmse only): 0.149870\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014217 - Val Loss (DD rmse only): 0.150627\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013785 - Val Loss (DD rmse only): 0.151348\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013611 - Val Loss (DD rmse only): 0.152058\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013459 - Val Loss (DD rmse only): 0.151111\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013187 - Val Loss (DD rmse only): 0.152494\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:12:56,329] Trial 67 finished with value: 0.14672738313674927 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 194, 'lr': 0.00031184684648549293, 'weight_decay': 8.131049108689601e-08, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013837 - Val Loss (DD rmse only): 0.152468\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.116566 - Val Loss (DD rmse only): 0.331509\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.040324 - Val Loss (DD rmse only): 0.193500\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.033348 - Val Loss (DD rmse only): 0.245846\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028636 - Val Loss (DD rmse only): 0.206024\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.025301 - Val Loss (DD rmse only): 0.190430\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.023547 - Val Loss (DD rmse only): 0.197329\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.022054 - Val Loss (DD rmse only): 0.177954\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.019415 - Val Loss (DD rmse only): 0.174539\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.018410 - Val Loss (DD rmse only): 0.163459\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.017508 - Val Loss (DD rmse only): 0.159548\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016010 - Val Loss (DD rmse only): 0.155702\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015798 - Val Loss (DD rmse only): 0.151812\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015939 - Val Loss (DD rmse only): 0.150382\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015496 - Val Loss (DD rmse only): 0.145040\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015213 - Val Loss (DD rmse only): 0.143594\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014997 - Val Loss (DD rmse only): 0.151329\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015462 - Val Loss (DD rmse only): 0.144521\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014345 - Val Loss (DD rmse only): 0.145251\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014827 - Val Loss (DD rmse only): 0.144828\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014686 - Val Loss (DD rmse only): 0.144221\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014612 - Val Loss (DD rmse only): 0.144198\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015100 - Val Loss (DD rmse only): 0.146401\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014031 - Val Loss (DD rmse only): 0.144263\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014483 - Val Loss (DD rmse only): 0.150750\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014347 - Val Loss (DD rmse only): 0.145422\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014078 - Val Loss (DD rmse only): 0.146253\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014327 - Val Loss (DD rmse only): 0.145385\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013746 - Val Loss (DD rmse only): 0.145908\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014247 - Val Loss (DD rmse only): 0.147442\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013960 - Val Loss (DD rmse only): 0.150402\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014475 - Val Loss (DD rmse only): 0.149478\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014380 - Val Loss (DD rmse only): 0.147899\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013632 - Val Loss (DD rmse only): 0.149561\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013768 - Val Loss (DD rmse only): 0.146110\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014698 - Val Loss (DD rmse only): 0.146619\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014467 - Val Loss (DD rmse only): 0.157249\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013264 - Val Loss (DD rmse only): 0.149152\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013665 - Val Loss (DD rmse only): 0.147253\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013779 - Val Loss (DD rmse only): 0.148798\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013478 - Val Loss (DD rmse only): 0.150201\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014031 - Val Loss (DD rmse only): 0.161135\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013761 - Val Loss (DD rmse only): 0.154494\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013223 - Val Loss (DD rmse only): 0.157441\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014089 - Val Loss (DD rmse only): 0.148698\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012802 - Val Loss (DD rmse only): 0.148260\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013471 - Val Loss (DD rmse only): 0.155575\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013753 - Val Loss (DD rmse only): 0.166422\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013497 - Val Loss (DD rmse only): 0.150589\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014417 - Val Loss (DD rmse only): 0.148001\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:13:29,630] Trial 68 finished with value: 0.1435939073562622 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 205, 'lr': 0.0006082787820287118, 'weight_decay': 2.7702342980166793e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013588 - Val Loss (DD rmse only): 0.151593\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.133540 - Val Loss (DD rmse only): 0.462711\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.127562 - Val Loss (DD rmse only): 0.454342\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.125177 - Val Loss (DD rmse only): 0.446020\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.116314 - Val Loss (DD rmse only): 0.437773\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.110197 - Val Loss (DD rmse only): 0.429681\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.103775 - Val Loss (DD rmse only): 0.421732\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.101991 - Val Loss (DD rmse only): 0.413906\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.096507 - Val Loss (DD rmse only): 0.406179\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.092942 - Val Loss (DD rmse only): 0.398458\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.084743 - Val Loss (DD rmse only): 0.390677\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.083080 - Val Loss (DD rmse only): 0.382849\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.076923 - Val Loss (DD rmse only): 0.374975\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.071916 - Val Loss (DD rmse only): 0.367051\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.068863 - Val Loss (DD rmse only): 0.359045\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.066524 - Val Loss (DD rmse only): 0.350952\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.061401 - Val Loss (DD rmse only): 0.342722\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.058404 - Val Loss (DD rmse only): 0.334342\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.054699 - Val Loss (DD rmse only): 0.325830\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.049138 - Val Loss (DD rmse only): 0.317261\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.046330 - Val Loss (DD rmse only): 0.308710\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.043370 - Val Loss (DD rmse only): 0.300280\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.040562 - Val Loss (DD rmse only): 0.291960\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.039208 - Val Loss (DD rmse only): 0.283838\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.036168 - Val Loss (DD rmse only): 0.275983\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.035859 - Val Loss (DD rmse only): 0.268510\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.032877 - Val Loss (DD rmse only): 0.261427\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.032319 - Val Loss (DD rmse only): 0.254942\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.029998 - Val Loss (DD rmse only): 0.249034\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.029853 - Val Loss (DD rmse only): 0.243800\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.029331 - Val Loss (DD rmse only): 0.239179\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.030025 - Val Loss (DD rmse only): 0.235295\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.028182 - Val Loss (DD rmse only): 0.232040\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.028154 - Val Loss (DD rmse only): 0.229572\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.028745 - Val Loss (DD rmse only): 0.227887\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.029170 - Val Loss (DD rmse only): 0.226776\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.029122 - Val Loss (DD rmse only): 0.225988\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.028994 - Val Loss (DD rmse only): 0.225529\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.027392 - Val Loss (DD rmse only): 0.225320\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.027711 - Val Loss (DD rmse only): 0.225281\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.027060 - Val Loss (DD rmse only): 0.225386\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.027560 - Val Loss (DD rmse only): 0.225554\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.026961 - Val Loss (DD rmse only): 0.225732\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.027188 - Val Loss (DD rmse only): 0.225840\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.026364 - Val Loss (DD rmse only): 0.225655\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.026248 - Val Loss (DD rmse only): 0.225373\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.026195 - Val Loss (DD rmse only): 0.224755\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.025795 - Val Loss (DD rmse only): 0.223861\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.026274 - Val Loss (DD rmse only): 0.222875\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.025662 - Val Loss (DD rmse only): 0.221543\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:13:40,344] Trial 69 finished with value: 0.2199426144361496 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 112, 'lr': 0.00013579055075463322, 'weight_decay': 1.9051940948948936e-06, 'batch_size': 64}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.025378 - Val Loss (DD rmse only): 0.219943\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.151105 - Val Loss (DD rmse only): 0.474555\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.138362 - Val Loss (DD rmse only): 0.460820\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.128840 - Val Loss (DD rmse only): 0.447532\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.122510 - Val Loss (DD rmse only): 0.434180\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.113259 - Val Loss (DD rmse only): 0.420296\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.101890 - Val Loss (DD rmse only): 0.405131\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.094421 - Val Loss (DD rmse only): 0.388238\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.084992 - Val Loss (DD rmse only): 0.369620\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.073434 - Val Loss (DD rmse only): 0.349276\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.064158 - Val Loss (DD rmse only): 0.329396\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.056683 - Val Loss (DD rmse only): 0.310508\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.047319 - Val Loss (DD rmse only): 0.292181\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.043034 - Val Loss (DD rmse only): 0.275284\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.037395 - Val Loss (DD rmse only): 0.258859\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.033247 - Val Loss (DD rmse only): 0.243723\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.031576 - Val Loss (DD rmse only): 0.232364\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.030264 - Val Loss (DD rmse only): 0.223099\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.030201 - Val Loss (DD rmse only): 0.217997\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.029220 - Val Loss (DD rmse only): 0.215386\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.028580 - Val Loss (DD rmse only): 0.214220\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.029663 - Val Loss (DD rmse only): 0.214715\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.028913 - Val Loss (DD rmse only): 0.213791\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.027995 - Val Loss (DD rmse only): 0.212751\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.027548 - Val Loss (DD rmse only): 0.213184\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.028304 - Val Loss (DD rmse only): 0.212714\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.027630 - Val Loss (DD rmse only): 0.211337\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.027977 - Val Loss (DD rmse only): 0.211018\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.027447 - Val Loss (DD rmse only): 0.209481\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.026274 - Val Loss (DD rmse only): 0.207651\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.026321 - Val Loss (DD rmse only): 0.206864\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.025705 - Val Loss (DD rmse only): 0.206274\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.026238 - Val Loss (DD rmse only): 0.206117\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.025291 - Val Loss (DD rmse only): 0.204882\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.025026 - Val Loss (DD rmse only): 0.203379\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.024930 - Val Loss (DD rmse only): 0.201892\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.024982 - Val Loss (DD rmse only): 0.199809\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.024667 - Val Loss (DD rmse only): 0.197300\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.023557 - Val Loss (DD rmse only): 0.194561\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.023734 - Val Loss (DD rmse only): 0.194997\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.022803 - Val Loss (DD rmse only): 0.193867\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.022440 - Val Loss (DD rmse only): 0.191881\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.022851 - Val Loss (DD rmse only): 0.190221\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.021785 - Val Loss (DD rmse only): 0.186585\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.021064 - Val Loss (DD rmse only): 0.185339\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.020490 - Val Loss (DD rmse only): 0.184815\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.020289 - Val Loss (DD rmse only): 0.182129\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.019787 - Val Loss (DD rmse only): 0.179251\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.019777 - Val Loss (DD rmse only): 0.177309\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.020127 - Val Loss (DD rmse only): 0.176704\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:14:05,146] Trial 70 finished with value: 0.1747148185968399 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 104, 'lr': 8.610747140694676e-05, 'weight_decay': 3.773276136813173e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.019358 - Val Loss (DD rmse only): 0.174715\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.051729 - Val Loss (DD rmse only): 0.199003\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.031933 - Val Loss (DD rmse only): 0.209956\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.030167 - Val Loss (DD rmse only): 0.228306\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.027769 - Val Loss (DD rmse only): 0.192263\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.025717 - Val Loss (DD rmse only): 0.199539\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.023570 - Val Loss (DD rmse only): 0.183232\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.020139 - Val Loss (DD rmse only): 0.169533\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017837 - Val Loss (DD rmse only): 0.160027\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.018053 - Val Loss (DD rmse only): 0.172850\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.017880 - Val Loss (DD rmse only): 0.153626\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016219 - Val Loss (DD rmse only): 0.153757\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015549 - Val Loss (DD rmse only): 0.149868\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014983 - Val Loss (DD rmse only): 0.148927\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015515 - Val Loss (DD rmse only): 0.151396\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014975 - Val Loss (DD rmse only): 0.145178\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015410 - Val Loss (DD rmse only): 0.144139\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015229 - Val Loss (DD rmse only): 0.149617\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015064 - Val Loss (DD rmse only): 0.145218\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014299 - Val Loss (DD rmse only): 0.147646\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014770 - Val Loss (DD rmse only): 0.146020\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014357 - Val Loss (DD rmse only): 0.146257\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014389 - Val Loss (DD rmse only): 0.149847\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.016250 - Val Loss (DD rmse only): 0.147063\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016047 - Val Loss (DD rmse only): 0.146441\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015447 - Val Loss (DD rmse only): 0.144917\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014059 - Val Loss (DD rmse only): 0.144455\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014169 - Val Loss (DD rmse only): 0.143862\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014176 - Val Loss (DD rmse only): 0.145220\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014270 - Val Loss (DD rmse only): 0.147027\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015547 - Val Loss (DD rmse only): 0.147562\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015840 - Val Loss (DD rmse only): 0.160434\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015052 - Val Loss (DD rmse only): 0.155565\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015635 - Val Loss (DD rmse only): 0.157988\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014667 - Val Loss (DD rmse only): 0.149377\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014426 - Val Loss (DD rmse only): 0.147591\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014375 - Val Loss (DD rmse only): 0.157017\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014191 - Val Loss (DD rmse only): 0.147287\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014030 - Val Loss (DD rmse only): 0.147310\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014708 - Val Loss (DD rmse only): 0.156857\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014231 - Val Loss (DD rmse only): 0.147878\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014424 - Val Loss (DD rmse only): 0.147889\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014260 - Val Loss (DD rmse only): 0.151379\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013376 - Val Loss (DD rmse only): 0.149616\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013144 - Val Loss (DD rmse only): 0.148323\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014425 - Val Loss (DD rmse only): 0.153998\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013169 - Val Loss (DD rmse only): 0.150079\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013522 - Val Loss (DD rmse only): 0.151295\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013910 - Val Loss (DD rmse only): 0.151112\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013248 - Val Loss (DD rmse only): 0.156719\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:14:35,167] Trial 71 finished with value: 0.14386191219091415 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 168, 'lr': 0.000932153679170572, 'weight_decay': 1.7689419548168704e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014894 - Val Loss (DD rmse only): 0.155289\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.052104 - Val Loss (DD rmse only): 0.194639\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.034667 - Val Loss (DD rmse only): 0.219842\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.029650 - Val Loss (DD rmse only): 0.221895\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025882 - Val Loss (DD rmse only): 0.180271\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.023081 - Val Loss (DD rmse only): 0.183338\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.019702 - Val Loss (DD rmse only): 0.166199\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.017744 - Val Loss (DD rmse only): 0.160710\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016990 - Val Loss (DD rmse only): 0.155856\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.015879 - Val Loss (DD rmse only): 0.152184\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015879 - Val Loss (DD rmse only): 0.154397\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.014935 - Val Loss (DD rmse only): 0.146838\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015497 - Val Loss (DD rmse only): 0.143851\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014775 - Val Loss (DD rmse only): 0.142902\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014714 - Val Loss (DD rmse only): 0.145183\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014494 - Val Loss (DD rmse only): 0.141926\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015029 - Val Loss (DD rmse only): 0.144526\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014678 - Val Loss (DD rmse only): 0.144066\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014781 - Val Loss (DD rmse only): 0.143953\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014818 - Val Loss (DD rmse only): 0.152283\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015151 - Val Loss (DD rmse only): 0.146155\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014882 - Val Loss (DD rmse only): 0.144227\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015063 - Val Loss (DD rmse only): 0.154020\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015462 - Val Loss (DD rmse only): 0.145807\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014757 - Val Loss (DD rmse only): 0.144949\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014821 - Val Loss (DD rmse only): 0.146794\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014294 - Val Loss (DD rmse only): 0.145409\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014435 - Val Loss (DD rmse only): 0.144977\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014263 - Val Loss (DD rmse only): 0.147904\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013692 - Val Loss (DD rmse only): 0.143975\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014532 - Val Loss (DD rmse only): 0.149753\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014298 - Val Loss (DD rmse only): 0.148560\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013553 - Val Loss (DD rmse only): 0.147347\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014170 - Val Loss (DD rmse only): 0.146648\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013812 - Val Loss (DD rmse only): 0.147577\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014513 - Val Loss (DD rmse only): 0.150548\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013732 - Val Loss (DD rmse only): 0.145794\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014173 - Val Loss (DD rmse only): 0.149061\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014365 - Val Loss (DD rmse only): 0.149091\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014581 - Val Loss (DD rmse only): 0.157416\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014226 - Val Loss (DD rmse only): 0.149516\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014169 - Val Loss (DD rmse only): 0.148519\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014255 - Val Loss (DD rmse only): 0.150834\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013609 - Val Loss (DD rmse only): 0.159495\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013936 - Val Loss (DD rmse only): 0.147903\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013469 - Val Loss (DD rmse only): 0.151925\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013368 - Val Loss (DD rmse only): 0.152285\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013029 - Val Loss (DD rmse only): 0.151566\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013273 - Val Loss (DD rmse only): 0.154078\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013224 - Val Loss (DD rmse only): 0.148841\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:15:03,968] Trial 72 finished with value: 0.14192555099725723 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 178, 'lr': 0.0007419450961583926, 'weight_decay': 0.00010414684011731483, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013080 - Val Loss (DD rmse only): 0.152492\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.066309 - Val Loss (DD rmse only): 0.205199\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.034549 - Val Loss (DD rmse only): 0.190227\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.026265 - Val Loss (DD rmse only): 0.206973\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.023481 - Val Loss (DD rmse only): 0.176198\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.019991 - Val Loss (DD rmse only): 0.169396\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.018371 - Val Loss (DD rmse only): 0.165086\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.017345 - Val Loss (DD rmse only): 0.162890\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016258 - Val Loss (DD rmse only): 0.158068\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016477 - Val Loss (DD rmse only): 0.156479\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015819 - Val Loss (DD rmse only): 0.151557\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015296 - Val Loss (DD rmse only): 0.147926\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015404 - Val Loss (DD rmse only): 0.149893\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014984 - Val Loss (DD rmse only): 0.145427\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014954 - Val Loss (DD rmse only): 0.151141\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014812 - Val Loss (DD rmse only): 0.144131\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015332 - Val Loss (DD rmse only): 0.149375\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014921 - Val Loss (DD rmse only): 0.145061\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014541 - Val Loss (DD rmse only): 0.146126\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014659 - Val Loss (DD rmse only): 0.148855\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014620 - Val Loss (DD rmse only): 0.145224\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014460 - Val Loss (DD rmse only): 0.147707\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014664 - Val Loss (DD rmse only): 0.148969\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014726 - Val Loss (DD rmse only): 0.145302\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014537 - Val Loss (DD rmse only): 0.154276\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014496 - Val Loss (DD rmse only): 0.143221\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014569 - Val Loss (DD rmse only): 0.144668\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014205 - Val Loss (DD rmse only): 0.151274\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013777 - Val Loss (DD rmse only): 0.146632\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013620 - Val Loss (DD rmse only): 0.145249\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014747 - Val Loss (DD rmse only): 0.154646\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014073 - Val Loss (DD rmse only): 0.147103\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014028 - Val Loss (DD rmse only): 0.143992\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013900 - Val Loss (DD rmse only): 0.158615\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014507 - Val Loss (DD rmse only): 0.147392\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014552 - Val Loss (DD rmse only): 0.143828\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014261 - Val Loss (DD rmse only): 0.157981\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013529 - Val Loss (DD rmse only): 0.147447\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013728 - Val Loss (DD rmse only): 0.156986\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013001 - Val Loss (DD rmse only): 0.151459\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013362 - Val Loss (DD rmse only): 0.146409\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013871 - Val Loss (DD rmse only): 0.159595\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014569 - Val Loss (DD rmse only): 0.155573\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014511 - Val Loss (DD rmse only): 0.150804\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013687 - Val Loss (DD rmse only): 0.151528\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013814 - Val Loss (DD rmse only): 0.156020\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013141 - Val Loss (DD rmse only): 0.151878\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013055 - Val Loss (DD rmse only): 0.165018\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013105 - Val Loss (DD rmse only): 0.150460\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013466 - Val Loss (DD rmse only): 0.149593\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:15:26,614] Trial 73 finished with value: 0.14322128891944885 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 198, 'lr': 0.00047280800396730296, 'weight_decay': 2.141776755827766e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013163 - Val Loss (DD rmse only): 0.163300\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.143268 - Val Loss (DD rmse only): 0.468912\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.131817 - Val Loss (DD rmse only): 0.450363\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.112774 - Val Loss (DD rmse only): 0.430835\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.101987 - Val Loss (DD rmse only): 0.410426\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.092491 - Val Loss (DD rmse only): 0.388218\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.079793 - Val Loss (DD rmse only): 0.364615\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.067125 - Val Loss (DD rmse only): 0.339434\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.055430 - Val Loss (DD rmse only): 0.311591\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.043218 - Val Loss (DD rmse only): 0.281113\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.035583 - Val Loss (DD rmse only): 0.251656\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.031067 - Val Loss (DD rmse only): 0.229470\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.032684 - Val Loss (DD rmse only): 0.218619\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.037174 - Val Loss (DD rmse only): 0.214882\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.035801 - Val Loss (DD rmse only): 0.215368\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.033286 - Val Loss (DD rmse only): 0.219833\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.030205 - Val Loss (DD rmse only): 0.227647\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.029894 - Val Loss (DD rmse only): 0.236477\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.030192 - Val Loss (DD rmse only): 0.243278\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.030125 - Val Loss (DD rmse only): 0.245650\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.029181 - Val Loss (DD rmse only): 0.244069\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.028846 - Val Loss (DD rmse only): 0.239083\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.028442 - Val Loss (DD rmse only): 0.231875\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.027670 - Val Loss (DD rmse only): 0.223473\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.026586 - Val Loss (DD rmse only): 0.215919\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.026374 - Val Loss (DD rmse only): 0.210646\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.026673 - Val Loss (DD rmse only): 0.207494\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.025375 - Val Loss (DD rmse only): 0.206927\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.024751 - Val Loss (DD rmse only): 0.207802\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.024478 - Val Loss (DD rmse only): 0.209934\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.023645 - Val Loss (DD rmse only): 0.210566\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.024512 - Val Loss (DD rmse only): 0.209724\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.023275 - Val Loss (DD rmse only): 0.205980\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.022498 - Val Loss (DD rmse only): 0.200987\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.022035 - Val Loss (DD rmse only): 0.195267\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.021848 - Val Loss (DD rmse only): 0.190274\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.020375 - Val Loss (DD rmse only): 0.186255\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.021373 - Val Loss (DD rmse only): 0.183187\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.019903 - Val Loss (DD rmse only): 0.181619\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.019742 - Val Loss (DD rmse only): 0.180876\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.018554 - Val Loss (DD rmse only): 0.178715\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.019198 - Val Loss (DD rmse only): 0.175482\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.018510 - Val Loss (DD rmse only): 0.170457\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.017364 - Val Loss (DD rmse only): 0.166510\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.017966 - Val Loss (DD rmse only): 0.165051\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.017662 - Val Loss (DD rmse only): 0.165145\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.017604 - Val Loss (DD rmse only): 0.163994\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.016974 - Val Loss (DD rmse only): 0.161597\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.016468 - Val Loss (DD rmse only): 0.159486\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.016823 - Val Loss (DD rmse only): 0.157992\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:15:42,342] Trial 74 finished with value: 0.1569771021604538 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 188, 'lr': 0.00026235455797361275, 'weight_decay': 0.0002097313520729477, 'batch_size': 64}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.016222 - Val Loss (DD rmse only): 0.156977\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.085274 - Val Loss (DD rmse only): 0.355187\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.063046 - Val Loss (DD rmse only): 0.310641\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.045915 - Val Loss (DD rmse only): 0.260571\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.030987 - Val Loss (DD rmse only): 0.210751\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028847 - Val Loss (DD rmse only): 0.195577\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.028514 - Val Loss (DD rmse only): 0.197400\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.027214 - Val Loss (DD rmse only): 0.204492\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.025247 - Val Loss (DD rmse only): 0.198683\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.025177 - Val Loss (DD rmse only): 0.189797\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.024676 - Val Loss (DD rmse only): 0.189178\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.022444 - Val Loss (DD rmse only): 0.176184\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.021070 - Val Loss (DD rmse only): 0.172017\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.019780 - Val Loss (DD rmse only): 0.165235\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.018713 - Val Loss (DD rmse only): 0.160738\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.018637 - Val Loss (DD rmse only): 0.158027\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.017371 - Val Loss (DD rmse only): 0.157049\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017809 - Val Loss (DD rmse only): 0.154543\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016246 - Val Loss (DD rmse only): 0.154239\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016311 - Val Loss (DD rmse only): 0.152328\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015964 - Val Loss (DD rmse only): 0.152400\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014998 - Val Loss (DD rmse only): 0.148637\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015865 - Val Loss (DD rmse only): 0.154263\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015906 - Val Loss (DD rmse only): 0.147280\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015146 - Val Loss (DD rmse only): 0.146241\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014914 - Val Loss (DD rmse only): 0.145320\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014921 - Val Loss (DD rmse only): 0.144834\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015050 - Val Loss (DD rmse only): 0.146427\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014762 - Val Loss (DD rmse only): 0.143954\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015027 - Val Loss (DD rmse only): 0.144870\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015285 - Val Loss (DD rmse only): 0.144276\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014964 - Val Loss (DD rmse only): 0.142334\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014540 - Val Loss (DD rmse only): 0.147339\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014791 - Val Loss (DD rmse only): 0.142925\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013957 - Val Loss (DD rmse only): 0.145479\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014380 - Val Loss (DD rmse only): 0.142466\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014345 - Val Loss (DD rmse only): 0.147586\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014709 - Val Loss (DD rmse only): 0.142534\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014225 - Val Loss (DD rmse only): 0.143843\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014093 - Val Loss (DD rmse only): 0.142434\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014848 - Val Loss (DD rmse only): 0.152522\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015025 - Val Loss (DD rmse only): 0.142615\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014699 - Val Loss (DD rmse only): 0.149298\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014431 - Val Loss (DD rmse only): 0.142389\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.015007 - Val Loss (DD rmse only): 0.147400\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013951 - Val Loss (DD rmse only): 0.142417\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014246 - Val Loss (DD rmse only): 0.149028\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014112 - Val Loss (DD rmse only): 0.142898\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013596 - Val Loss (DD rmse only): 0.146021\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013892 - Val Loss (DD rmse only): 0.142818\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:16:12,443] Trial 75 finished with value: 0.14233370125293732 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 164, 'lr': 0.00019730831396427893, 'weight_decay': 7.66482824111035e-06, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014249 - Val Loss (DD rmse only): 0.148804\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.094896 - Val Loss (DD rmse only): 0.250738\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.033968 - Val Loss (DD rmse only): 0.204520\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.028340 - Val Loss (DD rmse only): 0.225811\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.026814 - Val Loss (DD rmse only): 0.187869\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.025769 - Val Loss (DD rmse only): 0.195581\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.022768 - Val Loss (DD rmse only): 0.162902\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.021080 - Val Loss (DD rmse only): 0.166684\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017662 - Val Loss (DD rmse only): 0.156265\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016986 - Val Loss (DD rmse only): 0.150317\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016042 - Val Loss (DD rmse only): 0.149510\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015453 - Val Loss (DD rmse only): 0.149496\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015959 - Val Loss (DD rmse only): 0.144281\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015374 - Val Loss (DD rmse only): 0.144227\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014746 - Val Loss (DD rmse only): 0.142617\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014985 - Val Loss (DD rmse only): 0.141312\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015091 - Val Loss (DD rmse only): 0.140974\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015358 - Val Loss (DD rmse only): 0.141091\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016035 - Val Loss (DD rmse only): 0.153789\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014961 - Val Loss (DD rmse only): 0.142056\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014642 - Val Loss (DD rmse only): 0.142433\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015265 - Val Loss (DD rmse only): 0.150774\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015407 - Val Loss (DD rmse only): 0.142966\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015158 - Val Loss (DD rmse only): 0.146033\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015486 - Val Loss (DD rmse only): 0.150025\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014999 - Val Loss (DD rmse only): 0.142942\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014548 - Val Loss (DD rmse only): 0.144512\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014487 - Val Loss (DD rmse only): 0.143459\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015091 - Val Loss (DD rmse only): 0.142875\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014195 - Val Loss (DD rmse only): 0.143189\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014766 - Val Loss (DD rmse only): 0.144324\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014990 - Val Loss (DD rmse only): 0.145267\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014239 - Val Loss (DD rmse only): 0.143324\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014790 - Val Loss (DD rmse only): 0.142805\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014255 - Val Loss (DD rmse only): 0.143670\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014916 - Val Loss (DD rmse only): 0.145243\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014826 - Val Loss (DD rmse only): 0.145750\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014499 - Val Loss (DD rmse only): 0.151951\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013536 - Val Loss (DD rmse only): 0.144708\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013814 - Val Loss (DD rmse only): 0.147069\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013764 - Val Loss (DD rmse only): 0.146511\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013834 - Val Loss (DD rmse only): 0.149880\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013658 - Val Loss (DD rmse only): 0.145112\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014045 - Val Loss (DD rmse only): 0.149349\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014026 - Val Loss (DD rmse only): 0.156215\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013762 - Val Loss (DD rmse only): 0.171378\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013996 - Val Loss (DD rmse only): 0.148905\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013620 - Val Loss (DD rmse only): 0.145994\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013920 - Val Loss (DD rmse only): 0.149170\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014756 - Val Loss (DD rmse only): 0.169418\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:16:45,717] Trial 76 finished with value: 0.14097414165735245 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 150, 'lr': 0.001153840188451868, 'weight_decay': 2.269133072844383e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013763 - Val Loss (DD rmse only): 0.159075\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.058417 - Val Loss (DD rmse only): 0.192299\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.034550 - Val Loss (DD rmse only): 0.217288\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.029290 - Val Loss (DD rmse only): 0.216483\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025815 - Val Loss (DD rmse only): 0.180063\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.023242 - Val Loss (DD rmse only): 0.185663\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.021573 - Val Loss (DD rmse only): 0.167040\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.018328 - Val Loss (DD rmse only): 0.162603\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017130 - Val Loss (DD rmse only): 0.158465\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016275 - Val Loss (DD rmse only): 0.156865\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.015617 - Val Loss (DD rmse only): 0.151915\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015275 - Val Loss (DD rmse only): 0.148556\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015085 - Val Loss (DD rmse only): 0.146641\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014672 - Val Loss (DD rmse only): 0.147197\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015788 - Val Loss (DD rmse only): 0.144602\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015165 - Val Loss (DD rmse only): 0.145328\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015050 - Val Loss (DD rmse only): 0.146024\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014722 - Val Loss (DD rmse only): 0.144924\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014545 - Val Loss (DD rmse only): 0.145439\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014768 - Val Loss (DD rmse only): 0.144648\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015217 - Val Loss (DD rmse only): 0.163552\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015656 - Val Loss (DD rmse only): 0.147270\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015148 - Val Loss (DD rmse only): 0.146826\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015861 - Val Loss (DD rmse only): 0.159355\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015328 - Val Loss (DD rmse only): 0.150582\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014736 - Val Loss (DD rmse only): 0.149532\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013925 - Val Loss (DD rmse only): 0.145633\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014731 - Val Loss (DD rmse only): 0.149187\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014367 - Val Loss (DD rmse only): 0.144501\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014838 - Val Loss (DD rmse only): 0.145857\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014148 - Val Loss (DD rmse only): 0.143528\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014024 - Val Loss (DD rmse only): 0.144307\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013791 - Val Loss (DD rmse only): 0.144730\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014409 - Val Loss (DD rmse only): 0.145927\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013768 - Val Loss (DD rmse only): 0.146460\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014371 - Val Loss (DD rmse only): 0.157009\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014219 - Val Loss (DD rmse only): 0.147200\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014599 - Val Loss (DD rmse only): 0.146563\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014212 - Val Loss (DD rmse only): 0.156513\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.015108 - Val Loss (DD rmse only): 0.149596\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014644 - Val Loss (DD rmse only): 0.145304\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013803 - Val Loss (DD rmse only): 0.153852\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013661 - Val Loss (DD rmse only): 0.145887\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013280 - Val Loss (DD rmse only): 0.150128\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014396 - Val Loss (DD rmse only): 0.149913\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014370 - Val Loss (DD rmse only): 0.145782\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013883 - Val Loss (DD rmse only): 0.150900\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013321 - Val Loss (DD rmse only): 0.145235\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013758 - Val Loss (DD rmse only): 0.144989\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013636 - Val Loss (DD rmse only): 0.144307\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:17:12,254] Trial 77 finished with value: 0.14352791756391525 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 130, 'lr': 0.0011243898607688396, 'weight_decay': 6.713260408325409e-05, 'batch_size': 16}. Best is trial 44 with value: 0.14004295070966086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013473 - Val Loss (DD rmse only): 0.145548\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.074626 - Val Loss (DD rmse only): 0.300670\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.035355 - Val Loss (DD rmse only): 0.195372\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031619 - Val Loss (DD rmse only): 0.203580\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.029457 - Val Loss (DD rmse only): 0.217413\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.026797 - Val Loss (DD rmse only): 0.196490\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025543 - Val Loss (DD rmse only): 0.189844\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.023084 - Val Loss (DD rmse only): 0.184080\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.020723 - Val Loss (DD rmse only): 0.167222\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.018837 - Val Loss (DD rmse only): 0.161609\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016982 - Val Loss (DD rmse only): 0.156720\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016648 - Val Loss (DD rmse only): 0.158310\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016994 - Val Loss (DD rmse only): 0.152524\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016133 - Val Loss (DD rmse only): 0.148018\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016196 - Val Loss (DD rmse only): 0.149079\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.016345 - Val Loss (DD rmse only): 0.144893\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015059 - Val Loss (DD rmse only): 0.142838\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015760 - Val Loss (DD rmse only): 0.143660\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014864 - Val Loss (DD rmse only): 0.141619\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015262 - Val Loss (DD rmse only): 0.140237\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015227 - Val Loss (DD rmse only): 0.139486\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014397 - Val Loss (DD rmse only): 0.139529\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015413 - Val Loss (DD rmse only): 0.138849\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015003 - Val Loss (DD rmse only): 0.138330\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014330 - Val Loss (DD rmse only): 0.140107\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015118 - Val Loss (DD rmse only): 0.141006\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015725 - Val Loss (DD rmse only): 0.142891\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015243 - Val Loss (DD rmse only): 0.143520\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015154 - Val Loss (DD rmse only): 0.145482\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014676 - Val Loss (DD rmse only): 0.140940\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014196 - Val Loss (DD rmse only): 0.141329\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014067 - Val Loss (DD rmse only): 0.141008\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014764 - Val Loss (DD rmse only): 0.140278\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013965 - Val Loss (DD rmse only): 0.140146\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014072 - Val Loss (DD rmse only): 0.140862\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014150 - Val Loss (DD rmse only): 0.140515\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014065 - Val Loss (DD rmse only): 0.142061\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013613 - Val Loss (DD rmse only): 0.143199\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013576 - Val Loss (DD rmse only): 0.142241\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014253 - Val Loss (DD rmse only): 0.147340\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014561 - Val Loss (DD rmse only): 0.141352\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014121 - Val Loss (DD rmse only): 0.141952\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013848 - Val Loss (DD rmse only): 0.147963\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013653 - Val Loss (DD rmse only): 0.141707\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013510 - Val Loss (DD rmse only): 0.144443\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013781 - Val Loss (DD rmse only): 0.147461\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013807 - Val Loss (DD rmse only): 0.147425\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013164 - Val Loss (DD rmse only): 0.143852\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013698 - Val Loss (DD rmse only): 0.149845\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013706 - Val Loss (DD rmse only): 0.148833\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:18:09,561] Trial 78 finished with value: 0.1383296102285385 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 222, 'lr': 0.00033336842296011085, 'weight_decay': 2.5793536155792996e-06, 'batch_size': 16}. Best is trial 78 with value: 0.1383296102285385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013317 - Val Loss (DD rmse only): 0.150258\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.093745 - Val Loss (DD rmse only): 0.342168\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.044764 - Val Loss (DD rmse only): 0.194913\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.032887 - Val Loss (DD rmse only): 0.202770\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.027807 - Val Loss (DD rmse only): 0.225732\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028080 - Val Loss (DD rmse only): 0.198323\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025677 - Val Loss (DD rmse only): 0.187890\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.024459 - Val Loss (DD rmse only): 0.190838\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.024078 - Val Loss (DD rmse only): 0.180623\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.021699 - Val Loss (DD rmse only): 0.172074\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.019987 - Val Loss (DD rmse only): 0.169027\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017489 - Val Loss (DD rmse only): 0.166863\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016300 - Val Loss (DD rmse only): 0.161055\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016853 - Val Loss (DD rmse only): 0.173122\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015972 - Val Loss (DD rmse only): 0.156017\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015893 - Val Loss (DD rmse only): 0.162902\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016160 - Val Loss (DD rmse only): 0.151248\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015545 - Val Loss (DD rmse only): 0.150319\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014820 - Val Loss (DD rmse only): 0.148449\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015887 - Val Loss (DD rmse only): 0.145922\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014707 - Val Loss (DD rmse only): 0.144040\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014710 - Val Loss (DD rmse only): 0.144075\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015181 - Val Loss (DD rmse only): 0.144896\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014769 - Val Loss (DD rmse only): 0.145529\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014470 - Val Loss (DD rmse only): 0.142512\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014296 - Val Loss (DD rmse only): 0.144011\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014118 - Val Loss (DD rmse only): 0.144838\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015365 - Val Loss (DD rmse only): 0.152713\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014979 - Val Loss (DD rmse only): 0.146810\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014543 - Val Loss (DD rmse only): 0.157099\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014172 - Val Loss (DD rmse only): 0.144004\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014624 - Val Loss (DD rmse only): 0.147026\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014500 - Val Loss (DD rmse only): 0.145579\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013957 - Val Loss (DD rmse only): 0.144551\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013997 - Val Loss (DD rmse only): 0.143018\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014017 - Val Loss (DD rmse only): 0.145842\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014304 - Val Loss (DD rmse only): 0.144051\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014199 - Val Loss (DD rmse only): 0.146228\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013611 - Val Loss (DD rmse only): 0.143606\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014849 - Val Loss (DD rmse only): 0.145610\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014022 - Val Loss (DD rmse only): 0.147597\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013443 - Val Loss (DD rmse only): 0.144121\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013698 - Val Loss (DD rmse only): 0.143068\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013614 - Val Loss (DD rmse only): 0.145827\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013646 - Val Loss (DD rmse only): 0.152648\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014014 - Val Loss (DD rmse only): 0.143123\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013843 - Val Loss (DD rmse only): 0.145834\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013845 - Val Loss (DD rmse only): 0.147668\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013332 - Val Loss (DD rmse only): 0.148210\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013259 - Val Loss (DD rmse only): 0.144340\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:18:49,858] Trial 79 finished with value: 0.14251165091991425 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 150, 'lr': 0.0005678680138827735, 'weight_decay': 3.320764810839436e-06, 'batch_size': 16}. Best is trial 78 with value: 0.1383296102285385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013185 - Val Loss (DD rmse only): 0.146682\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.055964 - Val Loss (DD rmse only): 0.214500\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.029467 - Val Loss (DD rmse only): 0.209939\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.026633 - Val Loss (DD rmse only): 0.190353\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.022457 - Val Loss (DD rmse only): 0.168343\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.018631 - Val Loss (DD rmse only): 0.174435\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.017280 - Val Loss (DD rmse only): 0.157502\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.015639 - Val Loss (DD rmse only): 0.142027\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.015234 - Val Loss (DD rmse only): 0.137961\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016508 - Val Loss (DD rmse only): 0.156764\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016592 - Val Loss (DD rmse only): 0.144930\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.014960 - Val Loss (DD rmse only): 0.140438\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.014941 - Val Loss (DD rmse only): 0.140727\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.014552 - Val Loss (DD rmse only): 0.145347\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016078 - Val Loss (DD rmse only): 0.142215\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014971 - Val Loss (DD rmse only): 0.171312\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016542 - Val Loss (DD rmse only): 0.142117\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014583 - Val Loss (DD rmse only): 0.145771\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015342 - Val Loss (DD rmse only): 0.142800\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015990 - Val Loss (DD rmse only): 0.158748\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015075 - Val Loss (DD rmse only): 0.142550\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014270 - Val Loss (DD rmse only): 0.140528\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.013963 - Val Loss (DD rmse only): 0.140409\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014481 - Val Loss (DD rmse only): 0.148406\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014707 - Val Loss (DD rmse only): 0.149050\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014908 - Val Loss (DD rmse only): 0.143337\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014633 - Val Loss (DD rmse only): 0.147127\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015659 - Val Loss (DD rmse only): 0.147545\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014221 - Val Loss (DD rmse only): 0.146339\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013814 - Val Loss (DD rmse only): 0.148709\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014666 - Val Loss (DD rmse only): 0.143065\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013913 - Val Loss (DD rmse only): 0.142486\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013385 - Val Loss (DD rmse only): 0.143807\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014046 - Val Loss (DD rmse only): 0.149434\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013604 - Val Loss (DD rmse only): 0.147818\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.012912 - Val Loss (DD rmse only): 0.143358\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013498 - Val Loss (DD rmse only): 0.143193\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013350 - Val Loss (DD rmse only): 0.145248\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014029 - Val Loss (DD rmse only): 0.148282\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013065 - Val Loss (DD rmse only): 0.146409\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013409 - Val Loss (DD rmse only): 0.148269\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013271 - Val Loss (DD rmse only): 0.153931\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013846 - Val Loss (DD rmse only): 0.162225\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013516 - Val Loss (DD rmse only): 0.161325\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012777 - Val Loss (DD rmse only): 0.147761\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012514 - Val Loss (DD rmse only): 0.150541\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013259 - Val Loss (DD rmse only): 0.150597\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012903 - Val Loss (DD rmse only): 0.150662\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012666 - Val Loss (DD rmse only): 0.150749\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013353 - Val Loss (DD rmse only): 0.159069\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:19:34,923] Trial 80 finished with value: 0.137961283326149 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 219, 'lr': 0.001400724556706755, 'weight_decay': 1.4564281375374214e-08, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013127 - Val Loss (DD rmse only): 0.156699\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.058577 - Val Loss (DD rmse only): 0.207385\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.031592 - Val Loss (DD rmse only): 0.228340\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.028880 - Val Loss (DD rmse only): 0.191790\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.026247 - Val Loss (DD rmse only): 0.197239\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.024617 - Val Loss (DD rmse only): 0.175163\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.019253 - Val Loss (DD rmse only): 0.165741\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.018034 - Val Loss (DD rmse only): 0.161913\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016435 - Val Loss (DD rmse only): 0.163314\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016682 - Val Loss (DD rmse only): 0.155385\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016290 - Val Loss (DD rmse only): 0.159840\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015458 - Val Loss (DD rmse only): 0.154631\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.014695 - Val Loss (DD rmse only): 0.150354\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015552 - Val Loss (DD rmse only): 0.151534\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015879 - Val Loss (DD rmse only): 0.156613\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015703 - Val Loss (DD rmse only): 0.147242\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015668 - Val Loss (DD rmse only): 0.155343\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014708 - Val Loss (DD rmse only): 0.146715\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014382 - Val Loss (DD rmse only): 0.146265\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014109 - Val Loss (DD rmse only): 0.149190\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.013965 - Val Loss (DD rmse only): 0.146340\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014727 - Val Loss (DD rmse only): 0.148526\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.013841 - Val Loss (DD rmse only): 0.166849\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014530 - Val Loss (DD rmse only): 0.147841\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014286 - Val Loss (DD rmse only): 0.151730\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014525 - Val Loss (DD rmse only): 0.151210\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014552 - Val Loss (DD rmse only): 0.154825\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013637 - Val Loss (DD rmse only): 0.167013\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014180 - Val Loss (DD rmse only): 0.156560\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013210 - Val Loss (DD rmse only): 0.155511\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014002 - Val Loss (DD rmse only): 0.154868\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014060 - Val Loss (DD rmse only): 0.156909\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015512 - Val Loss (DD rmse only): 0.155597\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015134 - Val Loss (DD rmse only): 0.167487\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013719 - Val Loss (DD rmse only): 0.152967\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013096 - Val Loss (DD rmse only): 0.154207\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.012732 - Val Loss (DD rmse only): 0.159750\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013196 - Val Loss (DD rmse only): 0.159907\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.012877 - Val Loss (DD rmse only): 0.155870\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013209 - Val Loss (DD rmse only): 0.161229\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013139 - Val Loss (DD rmse only): 0.167614\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012737 - Val Loss (DD rmse only): 0.156750\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012479 - Val Loss (DD rmse only): 0.163349\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012615 - Val Loss (DD rmse only): 0.165867\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012680 - Val Loss (DD rmse only): 0.172829\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012410 - Val Loss (DD rmse only): 0.156591\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012908 - Val Loss (DD rmse only): 0.158982\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012325 - Val Loss (DD rmse only): 0.164426\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012257 - Val Loss (DD rmse only): 0.168764\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012297 - Val Loss (DD rmse only): 0.174179\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:20:21,479] Trial 81 finished with value: 0.14626486599445343 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 220, 'lr': 0.0013865970723776929, 'weight_decay': 2.7603795141571003e-08, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012371 - Val Loss (DD rmse only): 0.184797\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.103414 - Val Loss (DD rmse only): 0.347475\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.050586 - Val Loss (DD rmse only): 0.202196\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.032342 - Val Loss (DD rmse only): 0.231892\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.031126 - Val Loss (DD rmse only): 0.226292\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028014 - Val Loss (DD rmse only): 0.195218\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.028918 - Val Loss (DD rmse only): 0.213676\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.024733 - Val Loss (DD rmse only): 0.168975\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.018960 - Val Loss (DD rmse only): 0.158855\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016810 - Val Loss (DD rmse only): 0.154175\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016175 - Val Loss (DD rmse only): 0.163255\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.018098 - Val Loss (DD rmse only): 0.179199\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.017997 - Val Loss (DD rmse only): 0.152902\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.017717 - Val Loss (DD rmse only): 0.167606\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016574 - Val Loss (DD rmse only): 0.148336\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014998 - Val Loss (DD rmse only): 0.154529\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015979 - Val Loss (DD rmse only): 0.145655\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015327 - Val Loss (DD rmse only): 0.145454\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014529 - Val Loss (DD rmse only): 0.154766\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014482 - Val Loss (DD rmse only): 0.143342\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014539 - Val Loss (DD rmse only): 0.142171\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014109 - Val Loss (DD rmse only): 0.143163\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014187 - Val Loss (DD rmse only): 0.150865\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014688 - Val Loss (DD rmse only): 0.149999\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014049 - Val Loss (DD rmse only): 0.146811\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013754 - Val Loss (DD rmse only): 0.153856\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013371 - Val Loss (DD rmse only): 0.161447\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013576 - Val Loss (DD rmse only): 0.149106\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014346 - Val Loss (DD rmse only): 0.155018\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014146 - Val Loss (DD rmse only): 0.157709\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.013265 - Val Loss (DD rmse only): 0.152830\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013148 - Val Loss (DD rmse only): 0.151924\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013517 - Val Loss (DD rmse only): 0.164867\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013094 - Val Loss (DD rmse only): 0.170338\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013133 - Val Loss (DD rmse only): 0.161373\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013192 - Val Loss (DD rmse only): 0.174513\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014082 - Val Loss (DD rmse only): 0.158008\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013721 - Val Loss (DD rmse only): 0.164272\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013597 - Val Loss (DD rmse only): 0.157950\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014412 - Val Loss (DD rmse only): 0.153761\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.012929 - Val Loss (DD rmse only): 0.154797\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013294 - Val Loss (DD rmse only): 0.169925\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012560 - Val Loss (DD rmse only): 0.159431\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012941 - Val Loss (DD rmse only): 0.164426\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013196 - Val Loss (DD rmse only): 0.159086\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013811 - Val Loss (DD rmse only): 0.161634\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012923 - Val Loss (DD rmse only): 0.166185\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012958 - Val Loss (DD rmse only): 0.172915\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013616 - Val Loss (DD rmse only): 0.186488\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013249 - Val Loss (DD rmse only): 0.181297\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:21:12,221] Trial 82 finished with value: 0.1421714946627617 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 238, 'lr': 0.004038636392322368, 'weight_decay': 1.785022852850889e-08, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012500 - Val Loss (DD rmse only): 0.176168\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.048843 - Val Loss (DD rmse only): 0.191630\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.030702 - Val Loss (DD rmse only): 0.238517\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.028486 - Val Loss (DD rmse only): 0.193157\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025543 - Val Loss (DD rmse only): 0.193296\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.021446 - Val Loss (DD rmse only): 0.164069\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.018563 - Val Loss (DD rmse only): 0.158783\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.016624 - Val Loss (DD rmse only): 0.156302\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016334 - Val Loss (DD rmse only): 0.168264\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.017467 - Val Loss (DD rmse only): 0.152016\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016066 - Val Loss (DD rmse only): 0.150023\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016586 - Val Loss (DD rmse only): 0.154340\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015120 - Val Loss (DD rmse only): 0.145404\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015148 - Val Loss (DD rmse only): 0.145110\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014217 - Val Loss (DD rmse only): 0.144669\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014910 - Val Loss (DD rmse only): 0.149614\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014087 - Val Loss (DD rmse only): 0.143996\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014506 - Val Loss (DD rmse only): 0.144619\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014374 - Val Loss (DD rmse only): 0.143996\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014783 - Val Loss (DD rmse only): 0.147584\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014966 - Val Loss (DD rmse only): 0.145913\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014363 - Val Loss (DD rmse only): 0.154887\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014008 - Val Loss (DD rmse only): 0.146088\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014078 - Val Loss (DD rmse only): 0.145593\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014279 - Val Loss (DD rmse only): 0.155174\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014135 - Val Loss (DD rmse only): 0.151532\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014063 - Val Loss (DD rmse only): 0.147185\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014256 - Val Loss (DD rmse only): 0.145703\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013760 - Val Loss (DD rmse only): 0.149204\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013354 - Val Loss (DD rmse only): 0.149152\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014107 - Val Loss (DD rmse only): 0.153653\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013651 - Val Loss (DD rmse only): 0.159720\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013701 - Val Loss (DD rmse only): 0.159935\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013810 - Val Loss (DD rmse only): 0.148849\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013210 - Val Loss (DD rmse only): 0.150898\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013244 - Val Loss (DD rmse only): 0.169653\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014441 - Val Loss (DD rmse only): 0.171308\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014693 - Val Loss (DD rmse only): 0.149956\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014677 - Val Loss (DD rmse only): 0.147684\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013705 - Val Loss (DD rmse only): 0.164751\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.012871 - Val Loss (DD rmse only): 0.158439\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.012834 - Val Loss (DD rmse only): 0.155092\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.012815 - Val Loss (DD rmse only): 0.158950\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.012769 - Val Loss (DD rmse only): 0.170173\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.012837 - Val Loss (DD rmse only): 0.163629\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012803 - Val Loss (DD rmse only): 0.159016\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013080 - Val Loss (DD rmse only): 0.167587\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012908 - Val Loss (DD rmse only): 0.174525\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012748 - Val Loss (DD rmse only): 0.158466\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012595 - Val Loss (DD rmse only): 0.157368\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:21:57,205] Trial 83 finished with value: 0.14399608969688416 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 232, 'lr': 0.0008790136797394409, 'weight_decay': 1.396645334016192e-07, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012522 - Val Loss (DD rmse only): 0.163899\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.129249 - Val Loss (DD rmse only): 0.387742\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.056186 - Val Loss (DD rmse only): 0.196733\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.035729 - Val Loss (DD rmse only): 0.222126\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.032477 - Val Loss (DD rmse only): 0.247493\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028449 - Val Loss (DD rmse only): 0.193625\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.029572 - Val Loss (DD rmse only): 0.193545\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.027427 - Val Loss (DD rmse only): 0.216269\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.024503 - Val Loss (DD rmse only): 0.174163\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.022109 - Val Loss (DD rmse only): 0.179258\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.019870 - Val Loss (DD rmse only): 0.163581\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.019470 - Val Loss (DD rmse only): 0.159796\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016531 - Val Loss (DD rmse only): 0.159550\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016237 - Val Loss (DD rmse only): 0.153663\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016302 - Val Loss (DD rmse only): 0.154793\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015639 - Val Loss (DD rmse only): 0.151026\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015350 - Val Loss (DD rmse only): 0.148102\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014480 - Val Loss (DD rmse only): 0.149232\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014982 - Val Loss (DD rmse only): 0.144398\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014244 - Val Loss (DD rmse only): 0.143920\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015156 - Val Loss (DD rmse only): 0.143111\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014612 - Val Loss (DD rmse only): 0.143195\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014973 - Val Loss (DD rmse only): 0.144599\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014850 - Val Loss (DD rmse only): 0.146114\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015440 - Val Loss (DD rmse only): 0.144640\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013983 - Val Loss (DD rmse only): 0.145950\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014627 - Val Loss (DD rmse only): 0.144718\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014873 - Val Loss (DD rmse only): 0.143381\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013696 - Val Loss (DD rmse only): 0.144627\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014424 - Val Loss (DD rmse only): 0.149390\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014587 - Val Loss (DD rmse only): 0.146153\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014530 - Val Loss (DD rmse only): 0.142957\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014370 - Val Loss (DD rmse only): 0.146830\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014201 - Val Loss (DD rmse only): 0.149380\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013933 - Val Loss (DD rmse only): 0.147642\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014384 - Val Loss (DD rmse only): 0.147282\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013542 - Val Loss (DD rmse only): 0.146496\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013635 - Val Loss (DD rmse only): 0.148150\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013922 - Val Loss (DD rmse only): 0.149389\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013193 - Val Loss (DD rmse only): 0.144463\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013731 - Val Loss (DD rmse only): 0.146963\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013409 - Val Loss (DD rmse only): 0.147803\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013915 - Val Loss (DD rmse only): 0.155249\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014243 - Val Loss (DD rmse only): 0.152122\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013901 - Val Loss (DD rmse only): 0.146462\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013995 - Val Loss (DD rmse only): 0.151770\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013123 - Val Loss (DD rmse only): 0.156052\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013955 - Val Loss (DD rmse only): 0.146700\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013533 - Val Loss (DD rmse only): 0.146462\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013831 - Val Loss (DD rmse only): 0.161260\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:22:46,275] Trial 84 finished with value: 0.1429574191570282 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 247, 'lr': 0.00043022405982102784, 'weight_decay': 1.0541617416043406e-08, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013487 - Val Loss (DD rmse only): 0.148761\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.173190 - Val Loss (DD rmse only): 0.501883\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.162180 - Val Loss (DD rmse only): 0.497432\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.156423 - Val Loss (DD rmse only): 0.492980\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.158302 - Val Loss (DD rmse only): 0.488630\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.153409 - Val Loss (DD rmse only): 0.484305\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.151182 - Val Loss (DD rmse only): 0.479975\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.146724 - Val Loss (DD rmse only): 0.475618\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.142118 - Val Loss (DD rmse only): 0.471218\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.139711 - Val Loss (DD rmse only): 0.466793\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.138614 - Val Loss (DD rmse only): 0.462299\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.132449 - Val Loss (DD rmse only): 0.457614\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.131148 - Val Loss (DD rmse only): 0.452807\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.126866 - Val Loss (DD rmse only): 0.447721\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.124867 - Val Loss (DD rmse only): 0.442370\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.120383 - Val Loss (DD rmse only): 0.436549\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.114868 - Val Loss (DD rmse only): 0.430068\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.110447 - Val Loss (DD rmse only): 0.423057\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.106647 - Val Loss (DD rmse only): 0.415535\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.104744 - Val Loss (DD rmse only): 0.407519\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.096095 - Val Loss (DD rmse only): 0.398969\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.091117 - Val Loss (DD rmse only): 0.390109\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.086532 - Val Loss (DD rmse only): 0.380750\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.079748 - Val Loss (DD rmse only): 0.370911\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.076269 - Val Loss (DD rmse only): 0.360790\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.069993 - Val Loss (DD rmse only): 0.349968\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.064179 - Val Loss (DD rmse only): 0.338402\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.058251 - Val Loss (DD rmse only): 0.326050\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.054645 - Val Loss (DD rmse only): 0.313404\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.050442 - Val Loss (DD rmse only): 0.300710\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.047272 - Val Loss (DD rmse only): 0.287475\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.042019 - Val Loss (DD rmse only): 0.274330\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.036818 - Val Loss (DD rmse only): 0.262180\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.035252 - Val Loss (DD rmse only): 0.251256\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.032246 - Val Loss (DD rmse only): 0.241573\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.031702 - Val Loss (DD rmse only): 0.234010\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.030639 - Val Loss (DD rmse only): 0.227784\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.031001 - Val Loss (DD rmse only): 0.222766\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.030363 - Val Loss (DD rmse only): 0.219634\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.029083 - Val Loss (DD rmse only): 0.216960\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.030721 - Val Loss (DD rmse only): 0.216183\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.029778 - Val Loss (DD rmse only): 0.214815\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.028678 - Val Loss (DD rmse only): 0.214441\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.028962 - Val Loss (DD rmse only): 0.214793\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.028732 - Val Loss (DD rmse only): 0.215027\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.029521 - Val Loss (DD rmse only): 0.215506\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.028125 - Val Loss (DD rmse only): 0.214957\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.028660 - Val Loss (DD rmse only): 0.213865\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.028121 - Val Loss (DD rmse only): 0.213016\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.028902 - Val Loss (DD rmse only): 0.212346\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:23:21,051] Trial 85 finished with value: 0.21186283230781555 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 136, 'lr': 2.4065605012838244e-05, 'weight_decay': 4.919572776613846e-07, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028581 - Val Loss (DD rmse only): 0.211863\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.086510 - Val Loss (DD rmse only): 0.204675\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.039425 - Val Loss (DD rmse only): 0.272400\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.032174 - Val Loss (DD rmse only): 0.189543\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.026866 - Val Loss (DD rmse only): 0.216859\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.025287 - Val Loss (DD rmse only): 0.189667\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.022164 - Val Loss (DD rmse only): 0.187090\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.019337 - Val Loss (DD rmse only): 0.162448\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017883 - Val Loss (DD rmse only): 0.156493\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016601 - Val Loss (DD rmse only): 0.154881\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016096 - Val Loss (DD rmse only): 0.149906\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015574 - Val Loss (DD rmse only): 0.149167\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.014776 - Val Loss (DD rmse only): 0.147220\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016086 - Val Loss (DD rmse only): 0.160697\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015380 - Val Loss (DD rmse only): 0.146668\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.014845 - Val Loss (DD rmse only): 0.144607\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.014812 - Val Loss (DD rmse only): 0.148173\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014314 - Val Loss (DD rmse only): 0.143703\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.014770 - Val Loss (DD rmse only): 0.144672\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016134 - Val Loss (DD rmse only): 0.153053\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015073 - Val Loss (DD rmse only): 0.160152\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014660 - Val Loss (DD rmse only): 0.146673\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014680 - Val Loss (DD rmse only): 0.147071\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014529 - Val Loss (DD rmse only): 0.168240\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014772 - Val Loss (DD rmse only): 0.153387\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014195 - Val Loss (DD rmse only): 0.149577\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014198 - Val Loss (DD rmse only): 0.154271\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014544 - Val Loss (DD rmse only): 0.147794\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.013895 - Val Loss (DD rmse only): 0.146820\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015404 - Val Loss (DD rmse only): 0.167916\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014425 - Val Loss (DD rmse only): 0.149149\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014290 - Val Loss (DD rmse only): 0.148653\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014071 - Val Loss (DD rmse only): 0.152822\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013575 - Val Loss (DD rmse only): 0.150854\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013790 - Val Loss (DD rmse only): 0.149738\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014147 - Val Loss (DD rmse only): 0.151721\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014525 - Val Loss (DD rmse only): 0.153949\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013676 - Val Loss (DD rmse only): 0.170867\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014093 - Val Loss (DD rmse only): 0.161150\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014913 - Val Loss (DD rmse only): 0.151302\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014046 - Val Loss (DD rmse only): 0.156286\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013463 - Val Loss (DD rmse only): 0.165337\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014072 - Val Loss (DD rmse only): 0.159704\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013396 - Val Loss (DD rmse only): 0.167430\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013789 - Val Loss (DD rmse only): 0.166564\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014000 - Val Loss (DD rmse only): 0.166643\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013173 - Val Loss (DD rmse only): 0.162292\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013501 - Val Loss (DD rmse only): 0.162537\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013158 - Val Loss (DD rmse only): 0.182593\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014720 - Val Loss (DD rmse only): 0.157393\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:24:04,999] Trial 86 finished with value: 0.14370322227478027 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 223, 'lr': 0.0010625558844933485, 'weight_decay': 1.0444058458296798e-06, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014028 - Val Loss (DD rmse only): 0.153264\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.065873 - Val Loss (DD rmse only): 0.220954\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.031692 - Val Loss (DD rmse only): 0.207172\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.027822 - Val Loss (DD rmse only): 0.215834\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025843 - Val Loss (DD rmse only): 0.187855\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.023635 - Val Loss (DD rmse only): 0.191116\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.021814 - Val Loss (DD rmse only): 0.173769\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.019363 - Val Loss (DD rmse only): 0.176587\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.017715 - Val Loss (DD rmse only): 0.167836\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.017470 - Val Loss (DD rmse only): 0.155922\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.016188 - Val Loss (DD rmse only): 0.155416\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.015914 - Val Loss (DD rmse only): 0.152633\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016333 - Val Loss (DD rmse only): 0.148519\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016006 - Val Loss (DD rmse only): 0.156161\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016986 - Val Loss (DD rmse only): 0.145640\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.016138 - Val Loss (DD rmse only): 0.143558\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016971 - Val Loss (DD rmse only): 0.151653\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015843 - Val Loss (DD rmse only): 0.145553\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.016016 - Val Loss (DD rmse only): 0.144469\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014356 - Val Loss (DD rmse only): 0.141919\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014394 - Val Loss (DD rmse only): 0.143571\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015087 - Val Loss (DD rmse only): 0.141073\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014361 - Val Loss (DD rmse only): 0.142155\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015067 - Val Loss (DD rmse only): 0.141955\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014589 - Val Loss (DD rmse only): 0.146174\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015313 - Val Loss (DD rmse only): 0.143334\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014551 - Val Loss (DD rmse only): 0.143545\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014462 - Val Loss (DD rmse only): 0.142580\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014415 - Val Loss (DD rmse only): 0.151121\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014509 - Val Loss (DD rmse only): 0.144107\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014945 - Val Loss (DD rmse only): 0.142322\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014676 - Val Loss (DD rmse only): 0.156698\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015479 - Val Loss (DD rmse only): 0.144298\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014125 - Val Loss (DD rmse only): 0.143213\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014861 - Val Loss (DD rmse only): 0.152052\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015025 - Val Loss (DD rmse only): 0.143425\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014158 - Val Loss (DD rmse only): 0.144039\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015627 - Val Loss (DD rmse only): 0.152270\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014243 - Val Loss (DD rmse only): 0.144410\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013819 - Val Loss (DD rmse only): 0.145446\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014433 - Val Loss (DD rmse only): 0.150198\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014115 - Val Loss (DD rmse only): 0.146781\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013725 - Val Loss (DD rmse only): 0.148959\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013544 - Val Loss (DD rmse only): 0.146442\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013329 - Val Loss (DD rmse only): 0.156318\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013775 - Val Loss (DD rmse only): 0.146888\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013945 - Val Loss (DD rmse only): 0.147529\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013481 - Val Loss (DD rmse only): 0.148762\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013325 - Val Loss (DD rmse only): 0.158042\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013504 - Val Loss (DD rmse only): 0.157748\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:24:50,998] Trial 87 finished with value: 0.14107344299554825 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 206, 'lr': 0.0006553889589737904, 'weight_decay': 1.2564746660188995e-05, 'batch_size': 16}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013481 - Val Loss (DD rmse only): 0.151162\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.075603 - Val Loss (DD rmse only): 0.287409\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.036945 - Val Loss (DD rmse only): 0.206841\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.032071 - Val Loss (DD rmse only): 0.230383\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.029542 - Val Loss (DD rmse only): 0.245704\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.029124 - Val Loss (DD rmse only): 0.228402\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.027866 - Val Loss (DD rmse only): 0.206351\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.026931 - Val Loss (DD rmse only): 0.200555\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.025786 - Val Loss (DD rmse only): 0.208293\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.024628 - Val Loss (DD rmse only): 0.207589\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.022911 - Val Loss (DD rmse only): 0.187592\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.021317 - Val Loss (DD rmse only): 0.169183\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.020218 - Val Loss (DD rmse only): 0.168975\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.020244 - Val Loss (DD rmse only): 0.160408\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.018602 - Val Loss (DD rmse only): 0.159263\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.018380 - Val Loss (DD rmse only): 0.158875\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.017198 - Val Loss (DD rmse only): 0.153505\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017658 - Val Loss (DD rmse only): 0.155332\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017832 - Val Loss (DD rmse only): 0.162234\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016922 - Val Loss (DD rmse only): 0.151115\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016438 - Val Loss (DD rmse only): 0.151263\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.017617 - Val Loss (DD rmse only): 0.149858\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015880 - Val Loss (DD rmse only): 0.156799\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.016371 - Val Loss (DD rmse only): 0.144194\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015382 - Val Loss (DD rmse only): 0.144105\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015818 - Val Loss (DD rmse only): 0.148492\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015294 - Val Loss (DD rmse only): 0.142699\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015664 - Val Loss (DD rmse only): 0.144631\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015076 - Val Loss (DD rmse only): 0.142974\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015188 - Val Loss (DD rmse only): 0.140984\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014994 - Val Loss (DD rmse only): 0.141289\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014523 - Val Loss (DD rmse only): 0.140792\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014406 - Val Loss (DD rmse only): 0.142145\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015002 - Val Loss (DD rmse only): 0.141036\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014596 - Val Loss (DD rmse only): 0.140183\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015681 - Val Loss (DD rmse only): 0.140328\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014785 - Val Loss (DD rmse only): 0.139539\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014683 - Val Loss (DD rmse only): 0.139278\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013891 - Val Loss (DD rmse only): 0.139359\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014767 - Val Loss (DD rmse only): 0.139353\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014339 - Val Loss (DD rmse only): 0.139550\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014035 - Val Loss (DD rmse only): 0.139987\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014197 - Val Loss (DD rmse only): 0.140001\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014289 - Val Loss (DD rmse only): 0.140393\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014180 - Val Loss (DD rmse only): 0.140078\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014280 - Val Loss (DD rmse only): 0.140230\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014624 - Val Loss (DD rmse only): 0.140423\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014321 - Val Loss (DD rmse only): 0.141037\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013883 - Val Loss (DD rmse only): 0.141435\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014313 - Val Loss (DD rmse only): 0.141307\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:25:18,491] Trial 88 finished with value: 0.13927829265594482 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 212, 'lr': 0.0017628959911195493, 'weight_decay': 9.299788599203486e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014175 - Val Loss (DD rmse only): 0.141103\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.079978 - Val Loss (DD rmse only): 0.203218\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.033547 - Val Loss (DD rmse only): 0.252457\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.031423 - Val Loss (DD rmse only): 0.239877\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028145 - Val Loss (DD rmse only): 0.201732\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028023 - Val Loss (DD rmse only): 0.193101\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.027669 - Val Loss (DD rmse only): 0.207375\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.024714 - Val Loss (DD rmse only): 0.203616\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.023503 - Val Loss (DD rmse only): 0.177189\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.020582 - Val Loss (DD rmse only): 0.161104\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.019380 - Val Loss (DD rmse only): 0.156663\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017945 - Val Loss (DD rmse only): 0.156582\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.018198 - Val Loss (DD rmse only): 0.158561\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016645 - Val Loss (DD rmse only): 0.156408\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016748 - Val Loss (DD rmse only): 0.152339\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.017101 - Val Loss (DD rmse only): 0.156386\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016567 - Val Loss (DD rmse only): 0.148258\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016939 - Val Loss (DD rmse only): 0.145469\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017073 - Val Loss (DD rmse only): 0.146645\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.015188 - Val Loss (DD rmse only): 0.159971\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015725 - Val Loss (DD rmse only): 0.148591\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.016335 - Val Loss (DD rmse only): 0.142145\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015466 - Val Loss (DD rmse only): 0.144348\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014739 - Val Loss (DD rmse only): 0.146735\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015654 - Val Loss (DD rmse only): 0.140738\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014676 - Val Loss (DD rmse only): 0.142128\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015061 - Val Loss (DD rmse only): 0.145284\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015474 - Val Loss (DD rmse only): 0.140463\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014992 - Val Loss (DD rmse only): 0.142719\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014185 - Val Loss (DD rmse only): 0.146191\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.015107 - Val Loss (DD rmse only): 0.140771\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014453 - Val Loss (DD rmse only): 0.143816\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014290 - Val Loss (DD rmse only): 0.143198\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014846 - Val Loss (DD rmse only): 0.141397\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014169 - Val Loss (DD rmse only): 0.144714\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014411 - Val Loss (DD rmse only): 0.142191\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013719 - Val Loss (DD rmse only): 0.141939\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014197 - Val Loss (DD rmse only): 0.142256\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013654 - Val Loss (DD rmse only): 0.142398\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014060 - Val Loss (DD rmse only): 0.143552\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013395 - Val Loss (DD rmse only): 0.143340\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013872 - Val Loss (DD rmse only): 0.142609\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013272 - Val Loss (DD rmse only): 0.146733\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014075 - Val Loss (DD rmse only): 0.144343\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013642 - Val Loss (DD rmse only): 0.144567\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014066 - Val Loss (DD rmse only): 0.144918\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014655 - Val Loss (DD rmse only): 0.146441\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013826 - Val Loss (DD rmse only): 0.149530\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013283 - Val Loss (DD rmse only): 0.149177\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013543 - Val Loss (DD rmse only): 0.150855\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:25:38,724] Trial 89 finished with value: 0.14046324789524078 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 230, 'lr': 0.0033585365404333388, 'weight_decay': 2.589214672350875e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013443 - Val Loss (DD rmse only): 0.146729\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.083868 - Val Loss (DD rmse only): 0.255977\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.063888 - Val Loss (DD rmse only): 0.287103\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.042417 - Val Loss (DD rmse only): 0.299025\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.042698 - Val Loss (DD rmse only): 0.277091\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.035515 - Val Loss (DD rmse only): 0.241589\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.028402 - Val Loss (DD rmse only): 0.207549\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.031141 - Val Loss (DD rmse only): 0.200415\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.030900 - Val Loss (DD rmse only): 0.208300\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.027280 - Val Loss (DD rmse only): 0.224961\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.027471 - Val Loss (DD rmse only): 0.231171\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.029139 - Val Loss (DD rmse only): 0.221137\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.026610 - Val Loss (DD rmse only): 0.196214\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.024784 - Val Loss (DD rmse only): 0.177404\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.021507 - Val Loss (DD rmse only): 0.159974\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.019102 - Val Loss (DD rmse only): 0.161966\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.019616 - Val Loss (DD rmse only): 0.153534\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.016362 - Val Loss (DD rmse only): 0.157962\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017037 - Val Loss (DD rmse only): 0.162691\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016768 - Val Loss (DD rmse only): 0.157860\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016100 - Val Loss (DD rmse only): 0.150467\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015276 - Val Loss (DD rmse only): 0.145742\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.016019 - Val Loss (DD rmse only): 0.145844\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015072 - Val Loss (DD rmse only): 0.146075\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014899 - Val Loss (DD rmse only): 0.150968\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014809 - Val Loss (DD rmse only): 0.146902\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015019 - Val Loss (DD rmse only): 0.147303\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015426 - Val Loss (DD rmse only): 0.142962\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014513 - Val Loss (DD rmse only): 0.141837\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014145 - Val Loss (DD rmse only): 0.145817\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014335 - Val Loss (DD rmse only): 0.143043\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014454 - Val Loss (DD rmse only): 0.145608\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014854 - Val Loss (DD rmse only): 0.142824\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014474 - Val Loss (DD rmse only): 0.141870\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014483 - Val Loss (DD rmse only): 0.141705\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014175 - Val Loss (DD rmse only): 0.141932\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013745 - Val Loss (DD rmse only): 0.144399\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013782 - Val Loss (DD rmse only): 0.141970\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013909 - Val Loss (DD rmse only): 0.141294\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014428 - Val Loss (DD rmse only): 0.141193\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013263 - Val Loss (DD rmse only): 0.142100\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013677 - Val Loss (DD rmse only): 0.143760\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014140 - Val Loss (DD rmse only): 0.144405\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014358 - Val Loss (DD rmse only): 0.145823\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014545 - Val Loss (DD rmse only): 0.143283\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013852 - Val Loss (DD rmse only): 0.146016\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013925 - Val Loss (DD rmse only): 0.145912\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013494 - Val Loss (DD rmse only): 0.144400\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013810 - Val Loss (DD rmse only): 0.145448\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013758 - Val Loss (DD rmse only): 0.144526\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:26:00,826] Trial 90 finished with value: 0.1411927491426468 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 232, 'lr': 0.0033964690988424755, 'weight_decay': 2.4095139151782244e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012885 - Val Loss (DD rmse only): 0.145431\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.075837 - Val Loss (DD rmse only): 0.200283\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.049782 - Val Loss (DD rmse only): 0.336594\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.050842 - Val Loss (DD rmse only): 0.274453\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.032871 - Val Loss (DD rmse only): 0.202119\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.033616 - Val Loss (DD rmse only): 0.213732\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.029315 - Val Loss (DD rmse only): 0.235159\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.027956 - Val Loss (DD rmse only): 0.225993\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.029985 - Val Loss (DD rmse only): 0.220979\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.028985 - Val Loss (DD rmse only): 0.215212\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.028160 - Val Loss (DD rmse only): 0.220052\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.027431 - Val Loss (DD rmse only): 0.217644\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.025197 - Val Loss (DD rmse only): 0.193245\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.019951 - Val Loss (DD rmse only): 0.165690\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.021899 - Val Loss (DD rmse only): 0.161579\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.019523 - Val Loss (DD rmse only): 0.171518\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.018489 - Val Loss (DD rmse only): 0.176734\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.019917 - Val Loss (DD rmse only): 0.172859\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017090 - Val Loss (DD rmse only): 0.156795\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016312 - Val Loss (DD rmse only): 0.164561\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016385 - Val Loss (DD rmse only): 0.148353\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015576 - Val Loss (DD rmse only): 0.152291\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015452 - Val Loss (DD rmse only): 0.155126\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015681 - Val Loss (DD rmse only): 0.154257\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015517 - Val Loss (DD rmse only): 0.147563\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014934 - Val Loss (DD rmse only): 0.146706\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014570 - Val Loss (DD rmse only): 0.149316\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014091 - Val Loss (DD rmse only): 0.150255\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014677 - Val Loss (DD rmse only): 0.149990\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014759 - Val Loss (DD rmse only): 0.153670\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014754 - Val Loss (DD rmse only): 0.149517\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015248 - Val Loss (DD rmse only): 0.148768\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014112 - Val Loss (DD rmse only): 0.149301\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014148 - Val Loss (DD rmse only): 0.149882\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013974 - Val Loss (DD rmse only): 0.149352\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014220 - Val Loss (DD rmse only): 0.149012\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014673 - Val Loss (DD rmse only): 0.149585\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014644 - Val Loss (DD rmse only): 0.154122\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014149 - Val Loss (DD rmse only): 0.149925\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013433 - Val Loss (DD rmse only): 0.150276\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014005 - Val Loss (DD rmse only): 0.150427\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014053 - Val Loss (DD rmse only): 0.149845\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013566 - Val Loss (DD rmse only): 0.156900\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013483 - Val Loss (DD rmse only): 0.151573\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013236 - Val Loss (DD rmse only): 0.152637\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013462 - Val Loss (DD rmse only): 0.151544\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013001 - Val Loss (DD rmse only): 0.151695\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012976 - Val Loss (DD rmse only): 0.155568\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012990 - Val Loss (DD rmse only): 0.154066\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012668 - Val Loss (DD rmse only): 0.156931\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:26:24,292] Trial 91 finished with value: 0.14670635759830475 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 255, 'lr': 0.005540139354890149, 'weight_decay': 6.406466865668791e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013107 - Val Loss (DD rmse only): 0.154445\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.158545 - Val Loss (DD rmse only): 0.408315\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.073074 - Val Loss (DD rmse only): 0.222739\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.057439 - Val Loss (DD rmse only): 0.205008\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.033486 - Val Loss (DD rmse only): 0.269007\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.038606 - Val Loss (DD rmse only): 0.295226\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.042095 - Val Loss (DD rmse only): 0.285124\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.036574 - Val Loss (DD rmse only): 0.255827\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.029316 - Val Loss (DD rmse only): 0.220107\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.026505 - Val Loss (DD rmse only): 0.197777\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.029816 - Val Loss (DD rmse only): 0.192953\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.028961 - Val Loss (DD rmse only): 0.199180\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.025243 - Val Loss (DD rmse only): 0.214375\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.025625 - Val Loss (DD rmse only): 0.222845\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.026226 - Val Loss (DD rmse only): 0.217775\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.025038 - Val Loss (DD rmse only): 0.200514\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.022228 - Val Loss (DD rmse only): 0.181143\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.021448 - Val Loss (DD rmse only): 0.172573\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.021017 - Val Loss (DD rmse only): 0.171660\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019203 - Val Loss (DD rmse only): 0.170832\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.017643 - Val Loss (DD rmse only): 0.161576\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.017461 - Val Loss (DD rmse only): 0.158259\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.016136 - Val Loss (DD rmse only): 0.157716\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015918 - Val Loss (DD rmse only): 0.156716\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016316 - Val Loss (DD rmse only): 0.155440\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016251 - Val Loss (DD rmse only): 0.153752\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015951 - Val Loss (DD rmse only): 0.152718\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015340 - Val Loss (DD rmse only): 0.152863\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015266 - Val Loss (DD rmse only): 0.149330\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014570 - Val Loss (DD rmse only): 0.147373\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014407 - Val Loss (DD rmse only): 0.145372\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015867 - Val Loss (DD rmse only): 0.144569\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015483 - Val Loss (DD rmse only): 0.144196\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015285 - Val Loss (DD rmse only): 0.146624\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014972 - Val Loss (DD rmse only): 0.142161\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015034 - Val Loss (DD rmse only): 0.141536\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014420 - Val Loss (DD rmse only): 0.144277\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.015162 - Val Loss (DD rmse only): 0.141681\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014852 - Val Loss (DD rmse only): 0.142791\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014446 - Val Loss (DD rmse only): 0.144682\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014714 - Val Loss (DD rmse only): 0.142983\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014611 - Val Loss (DD rmse only): 0.142684\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014190 - Val Loss (DD rmse only): 0.142764\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014072 - Val Loss (DD rmse only): 0.143381\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014742 - Val Loss (DD rmse only): 0.144359\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.014187 - Val Loss (DD rmse only): 0.143573\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014135 - Val Loss (DD rmse only): 0.143223\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014370 - Val Loss (DD rmse only): 0.144218\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014029 - Val Loss (DD rmse only): 0.143234\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.014022 - Val Loss (DD rmse only): 0.142881\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:26:44,155] Trial 92 finished with value: 0.14153560996055603 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 211, 'lr': 0.0019626844566169336, 'weight_decay': 9.956417009181415e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013843 - Val Loss (DD rmse only): 0.143839\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.161470 - Val Loss (DD rmse only): 0.373059\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.051718 - Val Loss (DD rmse only): 0.262562\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.073386 - Val Loss (DD rmse only): 0.270804\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.039583 - Val Loss (DD rmse only): 0.323593\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.052112 - Val Loss (DD rmse only): 0.320537\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.049040 - Val Loss (DD rmse only): 0.297038\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.038707 - Val Loss (DD rmse only): 0.257937\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.030386 - Val Loss (DD rmse only): 0.212860\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.029852 - Val Loss (DD rmse only): 0.196446\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.033295 - Val Loss (DD rmse only): 0.198651\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.029068 - Val Loss (DD rmse only): 0.215018\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.026169 - Val Loss (DD rmse only): 0.230665\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.029319 - Val Loss (DD rmse only): 0.230009\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.026887 - Val Loss (DD rmse only): 0.214012\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.025665 - Val Loss (DD rmse only): 0.192814\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.025761 - Val Loss (DD rmse only): 0.185588\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.023723 - Val Loss (DD rmse only): 0.185249\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.021468 - Val Loss (DD rmse only): 0.176672\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019617 - Val Loss (DD rmse only): 0.166545\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.017833 - Val Loss (DD rmse only): 0.165242\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.017447 - Val Loss (DD rmse only): 0.163206\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.016799 - Val Loss (DD rmse only): 0.162542\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015885 - Val Loss (DD rmse only): 0.160354\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.016112 - Val Loss (DD rmse only): 0.160301\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014984 - Val Loss (DD rmse only): 0.156026\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015386 - Val Loss (DD rmse only): 0.152966\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015515 - Val Loss (DD rmse only): 0.150071\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015071 - Val Loss (DD rmse only): 0.153047\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015828 - Val Loss (DD rmse only): 0.148609\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.016139 - Val Loss (DD rmse only): 0.146250\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015310 - Val Loss (DD rmse only): 0.146287\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014700 - Val Loss (DD rmse only): 0.153688\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014847 - Val Loss (DD rmse only): 0.146584\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014858 - Val Loss (DD rmse only): 0.146503\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014712 - Val Loss (DD rmse only): 0.147445\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014683 - Val Loss (DD rmse only): 0.145607\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014068 - Val Loss (DD rmse only): 0.146223\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014607 - Val Loss (DD rmse only): 0.145936\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013446 - Val Loss (DD rmse only): 0.145374\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014360 - Val Loss (DD rmse only): 0.144930\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013665 - Val Loss (DD rmse only): 0.144632\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014296 - Val Loss (DD rmse only): 0.145429\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013750 - Val Loss (DD rmse only): 0.146095\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014037 - Val Loss (DD rmse only): 0.146366\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013646 - Val Loss (DD rmse only): 0.146177\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013573 - Val Loss (DD rmse only): 0.146598\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014047 - Val Loss (DD rmse only): 0.146674\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013536 - Val Loss (DD rmse only): 0.146827\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013569 - Val Loss (DD rmse only): 0.147287\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:27:11,704] Trial 93 finished with value: 0.14463160932064056 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 214, 'lr': 0.0025460451841037707, 'weight_decay': 4.844907843269387e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013627 - Val Loss (DD rmse only): 0.147449\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.120547 - Val Loss (DD rmse only): 0.277613\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.043660 - Val Loss (DD rmse only): 0.221321\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.056782 - Val Loss (DD rmse only): 0.210903\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.038285 - Val Loss (DD rmse only): 0.228370\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.028538 - Val Loss (DD rmse only): 0.254182\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.030866 - Val Loss (DD rmse only): 0.257133\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.030000 - Val Loss (DD rmse only): 0.234354\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.025925 - Val Loss (DD rmse only): 0.201452\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.021071 - Val Loss (DD rmse only): 0.177474\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.021316 - Val Loss (DD rmse only): 0.169871\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.022813 - Val Loss (DD rmse only): 0.168143\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.019259 - Val Loss (DD rmse only): 0.170931\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.017851 - Val Loss (DD rmse only): 0.177365\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.018937 - Val Loss (DD rmse only): 0.177040\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.019338 - Val Loss (DD rmse only): 0.168977\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.017943 - Val Loss (DD rmse only): 0.163722\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.017708 - Val Loss (DD rmse only): 0.164746\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017328 - Val Loss (DD rmse only): 0.162081\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.016660 - Val Loss (DD rmse only): 0.158624\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015553 - Val Loss (DD rmse only): 0.157188\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.015818 - Val Loss (DD rmse only): 0.155135\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015889 - Val Loss (DD rmse only): 0.152670\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015565 - Val Loss (DD rmse only): 0.151687\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015764 - Val Loss (DD rmse only): 0.150544\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015459 - Val Loss (DD rmse only): 0.148221\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015486 - Val Loss (DD rmse only): 0.146324\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015081 - Val Loss (DD rmse only): 0.145316\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.015173 - Val Loss (DD rmse only): 0.144811\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015030 - Val Loss (DD rmse only): 0.145281\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014712 - Val Loss (DD rmse only): 0.145380\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014485 - Val Loss (DD rmse only): 0.144201\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014768 - Val Loss (DD rmse only): 0.143798\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014286 - Val Loss (DD rmse only): 0.144255\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014144 - Val Loss (DD rmse only): 0.145162\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014261 - Val Loss (DD rmse only): 0.144566\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014545 - Val Loss (DD rmse only): 0.144202\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014009 - Val Loss (DD rmse only): 0.144508\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014657 - Val Loss (DD rmse only): 0.145180\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014106 - Val Loss (DD rmse only): 0.144790\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014348 - Val Loss (DD rmse only): 0.144502\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014347 - Val Loss (DD rmse only): 0.144308\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013910 - Val Loss (DD rmse only): 0.144918\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013918 - Val Loss (DD rmse only): 0.145215\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013450 - Val Loss (DD rmse only): 0.144092\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013718 - Val Loss (DD rmse only): 0.143973\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013635 - Val Loss (DD rmse only): 0.144595\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013459 - Val Loss (DD rmse only): 0.146065\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013223 - Val Loss (DD rmse only): 0.145479\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013238 - Val Loss (DD rmse only): 0.144950\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:27:23,648] Trial 94 finished with value: 0.14379793405532837 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 227, 'lr': 0.001489952811648948, 'weight_decay': 1.406042446301731e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013148 - Val Loss (DD rmse only): 0.145500\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.073960 - Val Loss (DD rmse only): 0.598157\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.266820 - Val Loss (DD rmse only): 0.313231\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.053427 - Val Loss (DD rmse only): 0.351009\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.062778 - Val Loss (DD rmse only): 0.344340\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.058808 - Val Loss (DD rmse only): 0.311378\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.041598 - Val Loss (DD rmse only): 0.235994\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.034166 - Val Loss (DD rmse only): 0.201371\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.030718 - Val Loss (DD rmse only): 0.233513\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.029844 - Val Loss (DD rmse only): 0.243085\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.029840 - Val Loss (DD rmse only): 0.227488\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.028696 - Val Loss (DD rmse only): 0.208899\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.027295 - Val Loss (DD rmse only): 0.206823\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.027418 - Val Loss (DD rmse only): 0.210306\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.026105 - Val Loss (DD rmse only): 0.193367\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.022791 - Val Loss (DD rmse only): 0.170811\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.020818 - Val Loss (DD rmse only): 0.160597\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.018466 - Val Loss (DD rmse only): 0.160228\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.018714 - Val Loss (DD rmse only): 0.167374\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.017266 - Val Loss (DD rmse only): 0.155135\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016968 - Val Loss (DD rmse only): 0.153514\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.016683 - Val Loss (DD rmse only): 0.156658\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015637 - Val Loss (DD rmse only): 0.157226\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015470 - Val Loss (DD rmse only): 0.152152\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015871 - Val Loss (DD rmse only): 0.144622\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014888 - Val Loss (DD rmse only): 0.142334\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014515 - Val Loss (DD rmse only): 0.142986\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014652 - Val Loss (DD rmse only): 0.142890\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014780 - Val Loss (DD rmse only): 0.139365\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015106 - Val Loss (DD rmse only): 0.138751\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014183 - Val Loss (DD rmse only): 0.138655\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014260 - Val Loss (DD rmse only): 0.140110\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014294 - Val Loss (DD rmse only): 0.141006\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014457 - Val Loss (DD rmse only): 0.141294\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013765 - Val Loss (DD rmse only): 0.140472\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014205 - Val Loss (DD rmse only): 0.141550\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013856 - Val Loss (DD rmse only): 0.139865\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014146 - Val Loss (DD rmse only): 0.140003\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013452 - Val Loss (DD rmse only): 0.142667\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014511 - Val Loss (DD rmse only): 0.140789\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013821 - Val Loss (DD rmse only): 0.140284\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013172 - Val Loss (DD rmse only): 0.140782\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013448 - Val Loss (DD rmse only): 0.143632\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014230 - Val Loss (DD rmse only): 0.141323\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013710 - Val Loss (DD rmse only): 0.142773\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012775 - Val Loss (DD rmse only): 0.142037\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.013302 - Val Loss (DD rmse only): 0.145019\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013303 - Val Loss (DD rmse only): 0.142807\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013584 - Val Loss (DD rmse only): 0.143579\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013395 - Val Loss (DD rmse only): 0.143198\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:27:50,058] Trial 95 finished with value: 0.13865463435649872 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 237, 'lr': 0.0049670264567959555, 'weight_decay': 2.6964327635105204e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013330 - Val Loss (DD rmse only): 0.144444\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.083346 - Val Loss (DD rmse only): 0.216677\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.030551 - Val Loss (DD rmse only): 0.221161\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.027243 - Val Loss (DD rmse only): 0.220009\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.026615 - Val Loss (DD rmse only): 0.195768\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.024515 - Val Loss (DD rmse only): 0.213295\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025464 - Val Loss (DD rmse only): 0.165869\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.022676 - Val Loss (DD rmse only): 0.181254\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.020591 - Val Loss (DD rmse only): 0.165267\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.018004 - Val Loss (DD rmse only): 0.165148\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.020229 - Val Loss (DD rmse only): 0.164758\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017930 - Val Loss (DD rmse only): 0.156573\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016992 - Val Loss (DD rmse only): 0.153537\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015787 - Val Loss (DD rmse only): 0.151576\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.015853 - Val Loss (DD rmse only): 0.152564\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015428 - Val Loss (DD rmse only): 0.159305\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015774 - Val Loss (DD rmse only): 0.146536\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015291 - Val Loss (DD rmse only): 0.145581\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015396 - Val Loss (DD rmse only): 0.147232\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014703 - Val Loss (DD rmse only): 0.144094\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014737 - Val Loss (DD rmse only): 0.148700\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014753 - Val Loss (DD rmse only): 0.143216\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.015102 - Val Loss (DD rmse only): 0.142561\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014518 - Val Loss (DD rmse only): 0.142690\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014243 - Val Loss (DD rmse only): 0.143428\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014429 - Val Loss (DD rmse only): 0.145265\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013936 - Val Loss (DD rmse only): 0.146520\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014406 - Val Loss (DD rmse only): 0.148225\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014050 - Val Loss (DD rmse only): 0.145528\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.013892 - Val Loss (DD rmse only): 0.145177\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014207 - Val Loss (DD rmse only): 0.145293\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.013932 - Val Loss (DD rmse only): 0.145285\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.013720 - Val Loss (DD rmse only): 0.146098\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013621 - Val Loss (DD rmse only): 0.147430\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014047 - Val Loss (DD rmse only): 0.148093\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.013672 - Val Loss (DD rmse only): 0.148766\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013603 - Val Loss (DD rmse only): 0.149388\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013214 - Val Loss (DD rmse only): 0.148844\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013997 - Val Loss (DD rmse only): 0.149177\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013471 - Val Loss (DD rmse only): 0.149359\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013590 - Val Loss (DD rmse only): 0.148414\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013235 - Val Loss (DD rmse only): 0.148502\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013220 - Val Loss (DD rmse only): 0.148039\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013266 - Val Loss (DD rmse only): 0.148659\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013106 - Val Loss (DD rmse only): 0.149213\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012693 - Val Loss (DD rmse only): 0.150718\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012498 - Val Loss (DD rmse only): 0.152662\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013268 - Val Loss (DD rmse only): 0.154669\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012608 - Val Loss (DD rmse only): 0.155435\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.012661 - Val Loss (DD rmse only): 0.151236\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:28:16,775] Trial 96 finished with value: 0.14256104826927185 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 235, 'lr': 0.003274134075237952, 'weight_decay': 2.6251376012540226e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012881 - Val Loss (DD rmse only): 0.153895\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.103314 - Val Loss (DD rmse only): 0.795828\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.445912 - Val Loss (DD rmse only): 0.359620\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.076735 - Val Loss (DD rmse only): 0.412188\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.095487 - Val Loss (DD rmse only): 0.414048\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.097380 - Val Loss (DD rmse only): 0.391484\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.079131 - Val Loss (DD rmse only): 0.328496\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.044152 - Val Loss (DD rmse only): 0.208615\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.041048 - Val Loss (DD rmse only): 0.221767\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.029492 - Val Loss (DD rmse only): 0.255314\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.031759 - Val Loss (DD rmse only): 0.249407\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.029696 - Val Loss (DD rmse only): 0.221702\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.028030 - Val Loss (DD rmse only): 0.203832\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.029018 - Val Loss (DD rmse only): 0.211062\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.026771 - Val Loss (DD rmse only): 0.219209\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.027846 - Val Loss (DD rmse only): 0.220201\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.026067 - Val Loss (DD rmse only): 0.203062\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.024123 - Val Loss (DD rmse only): 0.179958\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.022546 - Val Loss (DD rmse only): 0.175321\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.019454 - Val Loss (DD rmse only): 0.175811\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.019601 - Val Loss (DD rmse only): 0.161409\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.018162 - Val Loss (DD rmse only): 0.161935\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.017661 - Val Loss (DD rmse only): 0.157696\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.017003 - Val Loss (DD rmse only): 0.155270\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015500 - Val Loss (DD rmse only): 0.162495\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.016579 - Val Loss (DD rmse only): 0.151988\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.015575 - Val Loss (DD rmse only): 0.150531\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.015031 - Val Loss (DD rmse only): 0.149709\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014911 - Val Loss (DD rmse only): 0.148990\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015907 - Val Loss (DD rmse only): 0.148204\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014757 - Val Loss (DD rmse only): 0.144805\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014957 - Val Loss (DD rmse only): 0.143369\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.015104 - Val Loss (DD rmse only): 0.144538\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014894 - Val Loss (DD rmse only): 0.144739\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014592 - Val Loss (DD rmse only): 0.143878\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.015090 - Val Loss (DD rmse only): 0.143869\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014574 - Val Loss (DD rmse only): 0.144792\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014454 - Val Loss (DD rmse only): 0.146943\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.015080 - Val Loss (DD rmse only): 0.143731\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.014381 - Val Loss (DD rmse only): 0.143954\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014581 - Val Loss (DD rmse only): 0.144719\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.015125 - Val Loss (DD rmse only): 0.145518\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014030 - Val Loss (DD rmse only): 0.149518\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014293 - Val Loss (DD rmse only): 0.147777\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.014429 - Val Loss (DD rmse only): 0.145493\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013909 - Val Loss (DD rmse only): 0.146055\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014055 - Val Loss (DD rmse only): 0.148541\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013746 - Val Loss (DD rmse only): 0.147470\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013782 - Val Loss (DD rmse only): 0.147323\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013865 - Val Loss (DD rmse only): 0.146855\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:28:47,244] Trial 97 finished with value: 0.14336885511875153 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 224, 'lr': 0.005176619673928122, 'weight_decay': 1.8994620607552711e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.013372 - Val Loss (DD rmse only): 0.148008\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.119702 - Val Loss (DD rmse only): 0.201086\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.037685 - Val Loss (DD rmse only): 0.269277\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.032372 - Val Loss (DD rmse only): 0.207802\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.028341 - Val Loss (DD rmse only): 0.193092\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.027433 - Val Loss (DD rmse only): 0.217561\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.025886 - Val Loss (DD rmse only): 0.208184\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.023696 - Val Loss (DD rmse only): 0.172911\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.023396 - Val Loss (DD rmse only): 0.172769\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.020053 - Val Loss (DD rmse only): 0.161461\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.017848 - Val Loss (DD rmse only): 0.163105\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.017695 - Val Loss (DD rmse only): 0.172254\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.016890 - Val Loss (DD rmse only): 0.165690\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.016691 - Val Loss (DD rmse only): 0.162523\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.016702 - Val Loss (DD rmse only): 0.161704\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.016055 - Val Loss (DD rmse only): 0.157073\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.016473 - Val Loss (DD rmse only): 0.151537\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.014864 - Val Loss (DD rmse only): 0.146890\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015334 - Val Loss (DD rmse only): 0.146220\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014789 - Val Loss (DD rmse only): 0.146310\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.015044 - Val Loss (DD rmse only): 0.145953\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014742 - Val Loss (DD rmse only): 0.144660\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014909 - Val Loss (DD rmse only): 0.145126\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.014690 - Val Loss (DD rmse only): 0.144489\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014639 - Val Loss (DD rmse only): 0.144833\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.014583 - Val Loss (DD rmse only): 0.145483\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014229 - Val Loss (DD rmse only): 0.145254\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013999 - Val Loss (DD rmse only): 0.144994\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014451 - Val Loss (DD rmse only): 0.145197\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.014068 - Val Loss (DD rmse only): 0.144373\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014532 - Val Loss (DD rmse only): 0.144053\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014298 - Val Loss (DD rmse only): 0.144667\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014685 - Val Loss (DD rmse only): 0.148887\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.015372 - Val Loss (DD rmse only): 0.145243\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014812 - Val Loss (DD rmse only): 0.145067\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014990 - Val Loss (DD rmse only): 0.148076\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014662 - Val Loss (DD rmse only): 0.146887\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014633 - Val Loss (DD rmse only): 0.148261\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.014091 - Val Loss (DD rmse only): 0.147862\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013938 - Val Loss (DD rmse only): 0.147890\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013976 - Val Loss (DD rmse only): 0.146281\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013893 - Val Loss (DD rmse only): 0.145191\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.013842 - Val Loss (DD rmse only): 0.144598\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013744 - Val Loss (DD rmse only): 0.146680\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013519 - Val Loss (DD rmse only): 0.146030\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013263 - Val Loss (DD rmse only): 0.147255\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012851 - Val Loss (DD rmse only): 0.146645\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.013195 - Val Loss (DD rmse only): 0.151474\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.013451 - Val Loss (DD rmse only): 0.146834\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013163 - Val Loss (DD rmse only): 0.148443\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:29:11,512] Trial 98 finished with value: 0.1440531462430954 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 217, 'lr': 0.004440709440373806, 'weight_decay': 4.266821884433857e-06, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.012423 - Val Loss (DD rmse only): 0.149113\n",
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.114119 - Val Loss (DD rmse only): 0.350012\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.048357 - Val Loss (DD rmse only): 0.216847\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.047475 - Val Loss (DD rmse only): 0.227474\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.029971 - Val Loss (DD rmse only): 0.272166\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.037849 - Val Loss (DD rmse only): 0.271965\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.034218 - Val Loss (DD rmse only): 0.245489\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.027588 - Val Loss (DD rmse only): 0.208519\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.026281 - Val Loss (DD rmse only): 0.189362\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.028870 - Val Loss (DD rmse only): 0.188046\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.025813 - Val Loss (DD rmse only): 0.200489\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.023992 - Val Loss (DD rmse only): 0.210112\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.024493 - Val Loss (DD rmse only): 0.204016\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.022916 - Val Loss (DD rmse only): 0.183078\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.021119 - Val Loss (DD rmse only): 0.166248\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.021015 - Val Loss (DD rmse only): 0.161891\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.018448 - Val Loss (DD rmse only): 0.164988\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.018153 - Val Loss (DD rmse only): 0.160049\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.017193 - Val Loss (DD rmse only): 0.155654\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.017107 - Val Loss (DD rmse only): 0.160155\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.016583 - Val Loss (DD rmse only): 0.151913\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.016778 - Val Loss (DD rmse only): 0.150723\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.016246 - Val Loss (DD rmse only): 0.150902\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015393 - Val Loss (DD rmse only): 0.148541\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.015508 - Val Loss (DD rmse only): 0.148617\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.015542 - Val Loss (DD rmse only): 0.147008\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.014880 - Val Loss (DD rmse only): 0.146058\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.014938 - Val Loss (DD rmse only): 0.144825\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014539 - Val Loss (DD rmse only): 0.143671\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015136 - Val Loss (DD rmse only): 0.141725\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014657 - Val Loss (DD rmse only): 0.140499\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.015229 - Val Loss (DD rmse only): 0.139961\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014535 - Val Loss (DD rmse only): 0.139204\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.014048 - Val Loss (DD rmse only): 0.141051\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.014695 - Val Loss (DD rmse only): 0.139103\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.014189 - Val Loss (DD rmse only): 0.138689\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.014278 - Val Loss (DD rmse only): 0.139139\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.014042 - Val Loss (DD rmse only): 0.139234\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013767 - Val Loss (DD rmse only): 0.139921\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.013640 - Val Loss (DD rmse only): 0.140893\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.014037 - Val Loss (DD rmse only): 0.140244\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.013711 - Val Loss (DD rmse only): 0.140895\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014222 - Val Loss (DD rmse only): 0.140153\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.014105 - Val Loss (DD rmse only): 0.140325\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013841 - Val Loss (DD rmse only): 0.140605\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.013977 - Val Loss (DD rmse only): 0.141741\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.014469 - Val Loss (DD rmse only): 0.140758\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.014173 - Val Loss (DD rmse only): 0.142435\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.014822 - Val Loss (DD rmse only): 0.142777\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013928 - Val Loss (DD rmse only): 0.144449\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 19:29:37,617] Trial 99 finished with value: 0.13868948817253113 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 246, 'lr': 0.0016553764663103626, 'weight_decay': 7.364389700421432e-07, 'batch_size': 64}. Best is trial 80 with value: 0.137961283326149.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.014072 - Val Loss (DD rmse only): 0.143747\n",
      "Best Hyperparameters: {'n_hidden_layers': 5, 'n_hidden_units': 219, 'lr': 0.001400724556706755, 'weight_decay': 1.4564281375374214e-08, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to search over\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 5)\n",
    "    n_hidden_units = trial.suggest_int(\"n_hidden_units\", 32, 256)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])  # Match the original hp['batch_sz']\n",
    "\n",
    "    # Create train & validation loaders (following the original code)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize MLP model\n",
    "    model = BasicMLP(\n",
    "        N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "        N_HIDDEN_LAYERS=n_hidden_layers,\n",
    "        N_HIDDEN_UNITS=n_hidden_units,\n",
    "        N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "        loss_function=LOSS_FUNC,\n",
    "    )\n",
    "\n",
    "    # Train and return validation loss\n",
    "    val_loss = model.train_model(train_loader, val_loader, epochs=50, lr=lr, weight_decay=weight_decay, device=device)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"mlp_hyperparameter_optimization_phy_newvalloss\", storage=\"sqlite:///mlp_hyperparameter_optimization_phy.db\", load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_hidden_layers': 5,\n",
       " 'n_hidden_units': 219,\n",
       " 'lr': 0.001400724556706755,\n",
       " 'weight_decay': 1.4564281375374214e-08,\n",
       " 'batch_size': 16}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best params for the physics model in previous studies\n",
    "# remove if retune\n",
    "best_params = {'n_hidden_layers': 5, 'n_hidden_units': 219, 'lr': 0.001400724556706755, 'weight_decay': 1.4564281375374214e-08, 'batch_size': 16}\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50 - Train Loss: 0.076395 - Val Loss (DD rmse only): 0.195735\n",
      "Epoch 2/50\n",
      "Epoch 2/50 - Train Loss: 0.035114 - Val Loss (DD rmse only): 0.219104\n",
      "Epoch 3/50\n",
      "Epoch 3/50 - Train Loss: 0.028916 - Val Loss (DD rmse only): 0.196710\n",
      "Epoch 4/50\n",
      "Epoch 4/50 - Train Loss: 0.025263 - Val Loss (DD rmse only): 0.201397\n",
      "Epoch 5/50\n",
      "Epoch 5/50 - Train Loss: 0.023486 - Val Loss (DD rmse only): 0.167498\n",
      "Epoch 6/50\n",
      "Epoch 6/50 - Train Loss: 0.019068 - Val Loss (DD rmse only): 0.164679\n",
      "Epoch 7/50\n",
      "Epoch 7/50 - Train Loss: 0.017388 - Val Loss (DD rmse only): 0.155734\n",
      "Epoch 8/50\n",
      "Epoch 8/50 - Train Loss: 0.016486 - Val Loss (DD rmse only): 0.153477\n",
      "Epoch 9/50\n",
      "Epoch 9/50 - Train Loss: 0.016158 - Val Loss (DD rmse only): 0.148664\n",
      "Epoch 10/50\n",
      "Epoch 10/50 - Train Loss: 0.017104 - Val Loss (DD rmse only): 0.158203\n",
      "Epoch 11/50\n",
      "Epoch 11/50 - Train Loss: 0.016049 - Val Loss (DD rmse only): 0.151517\n",
      "Epoch 12/50\n",
      "Epoch 12/50 - Train Loss: 0.015886 - Val Loss (DD rmse only): 0.145542\n",
      "Epoch 13/50\n",
      "Epoch 13/50 - Train Loss: 0.015538 - Val Loss (DD rmse only): 0.144335\n",
      "Epoch 14/50\n",
      "Epoch 14/50 - Train Loss: 0.014218 - Val Loss (DD rmse only): 0.144127\n",
      "Epoch 15/50\n",
      "Epoch 15/50 - Train Loss: 0.015481 - Val Loss (DD rmse only): 0.154776\n",
      "Epoch 16/50\n",
      "Epoch 16/50 - Train Loss: 0.015085 - Val Loss (DD rmse only): 0.146352\n",
      "Epoch 17/50\n",
      "Epoch 17/50 - Train Loss: 0.015473 - Val Loss (DD rmse only): 0.143813\n",
      "Epoch 18/50\n",
      "Epoch 18/50 - Train Loss: 0.015411 - Val Loss (DD rmse only): 0.144189\n",
      "Epoch 19/50\n",
      "Epoch 19/50 - Train Loss: 0.014534 - Val Loss (DD rmse only): 0.149814\n",
      "Epoch 20/50\n",
      "Epoch 20/50 - Train Loss: 0.014215 - Val Loss (DD rmse only): 0.155441\n",
      "Epoch 21/50\n",
      "Epoch 21/50 - Train Loss: 0.014751 - Val Loss (DD rmse only): 0.148292\n",
      "Epoch 22/50\n",
      "Epoch 22/50 - Train Loss: 0.014023 - Val Loss (DD rmse only): 0.146721\n",
      "Epoch 23/50\n",
      "Epoch 23/50 - Train Loss: 0.015255 - Val Loss (DD rmse only): 0.169448\n",
      "Epoch 24/50\n",
      "Epoch 24/50 - Train Loss: 0.014832 - Val Loss (DD rmse only): 0.155721\n",
      "Epoch 25/50\n",
      "Epoch 25/50 - Train Loss: 0.013489 - Val Loss (DD rmse only): 0.147236\n",
      "Epoch 26/50\n",
      "Epoch 26/50 - Train Loss: 0.013917 - Val Loss (DD rmse only): 0.149945\n",
      "Epoch 27/50\n",
      "Epoch 27/50 - Train Loss: 0.013538 - Val Loss (DD rmse only): 0.169175\n",
      "Epoch 28/50\n",
      "Epoch 28/50 - Train Loss: 0.014409 - Val Loss (DD rmse only): 0.146961\n",
      "Epoch 29/50\n",
      "Epoch 29/50 - Train Loss: 0.015896 - Val Loss (DD rmse only): 0.147784\n",
      "Epoch 30/50\n",
      "Epoch 30/50 - Train Loss: 0.014595 - Val Loss (DD rmse only): 0.150039\n",
      "Epoch 31/50\n",
      "Epoch 31/50 - Train Loss: 0.014822 - Val Loss (DD rmse only): 0.152923\n",
      "Epoch 32/50\n",
      "Epoch 32/50 - Train Loss: 0.014131 - Val Loss (DD rmse only): 0.157904\n",
      "Epoch 33/50\n",
      "Epoch 33/50 - Train Loss: 0.013115 - Val Loss (DD rmse only): 0.153821\n",
      "Epoch 34/50\n",
      "Epoch 34/50 - Train Loss: 0.013108 - Val Loss (DD rmse only): 0.153591\n",
      "Epoch 35/50\n",
      "Epoch 35/50 - Train Loss: 0.012942 - Val Loss (DD rmse only): 0.150766\n",
      "Epoch 36/50\n",
      "Epoch 36/50 - Train Loss: 0.013786 - Val Loss (DD rmse only): 0.159832\n",
      "Epoch 37/50\n",
      "Epoch 37/50 - Train Loss: 0.013506 - Val Loss (DD rmse only): 0.167118\n",
      "Epoch 38/50\n",
      "Epoch 38/50 - Train Loss: 0.013002 - Val Loss (DD rmse only): 0.161410\n",
      "Epoch 39/50\n",
      "Epoch 39/50 - Train Loss: 0.012759 - Val Loss (DD rmse only): 0.151819\n",
      "Epoch 40/50\n",
      "Epoch 40/50 - Train Loss: 0.013189 - Val Loss (DD rmse only): 0.152658\n",
      "Epoch 41/50\n",
      "Epoch 41/50 - Train Loss: 0.014840 - Val Loss (DD rmse only): 0.168233\n",
      "Epoch 42/50\n",
      "Epoch 42/50 - Train Loss: 0.014719 - Val Loss (DD rmse only): 0.171128\n",
      "Epoch 43/50\n",
      "Epoch 43/50 - Train Loss: 0.013757 - Val Loss (DD rmse only): 0.158764\n",
      "Epoch 44/50\n",
      "Epoch 44/50 - Train Loss: 0.013046 - Val Loss (DD rmse only): 0.160766\n",
      "Epoch 45/50\n",
      "Epoch 45/50 - Train Loss: 0.012681 - Val Loss (DD rmse only): 0.159801\n",
      "Epoch 46/50\n",
      "Epoch 46/50 - Train Loss: 0.012820 - Val Loss (DD rmse only): 0.158917\n",
      "Epoch 47/50\n",
      "Epoch 47/50 - Train Loss: 0.012847 - Val Loss (DD rmse only): 0.159863\n",
      "Epoch 48/50\n",
      "Epoch 48/50 - Train Loss: 0.012454 - Val Loss (DD rmse only): 0.167219\n",
      "Epoch 49/50\n",
      "Epoch 49/50 - Train Loss: 0.013056 - Val Loss (DD rmse only): 0.175754\n",
      "Epoch 50/50\n",
      "Epoch 50/50 - Train Loss: 0.013285 - Val Loss (DD rmse only): 0.191301\n",
      "Model saved as best_mlp_no2.pth in Model folder\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "torch.manual_seed(42) \n",
    "best_model = BasicMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=best_params[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=best_params[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "    loss_function=LOSS_FUNC,\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "best_model.train_model(train_loader, val_loader, epochs=50, lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"], device=device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(best_model.state_dict(), f\"{MODEL_PATH}/best_mlp_no2_linear.pth\")\n",
    "print(\"Model saved as best_mlp_no2.pth in Model folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO2</td>\n",
       "      <td>1.37</td>\n",
       "      <td>93.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0   min    max\n",
       "0        NO2  1.37  93.95"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "df_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 201.944328\n",
      "Test RMSE Loss: 14.363733\n",
      "Test SMAPE Loss: 35.930710%\n"
     ]
    }
   ],
   "source": [
    "best_model.load_state_dict(torch.load(f\"{MODEL_PATH}/best_mlp_no2_linear.pth\"))\n",
    "best_model.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "mse, rmse_val, smape_val = best_model.test_model(test_loader, min_value=min_value, max_value=max_value, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 201.94432754744366, Test RMSE: 14.363732621172364, Test SMAPE: 35.93070992717036\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test MSE: {mse}, Test RMSE: {rmse_val}, Test SMAPE: {smape_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzsnXeY1NT6x7+zvdJ7rwtIF66zFAWUIkUFrl1BENSrYuN6WdCrsGLbvfafCl5BKTYsWC4IiiiCwA6CoILA0nvvZdma3x9nz+Qkk2SSTKbs7vt5nn1mNpNJzmSSzPme7/u+xyVJkgSCIAiCIAiCIAhCl6hwN4AgCIIgCIIgCCLSIeFEEARBEARBEAThBxJOBEEQBEEQBEEQfiDhRBAEQRAEQRAE4QcSTgRBEARBEARBEH4g4UQQBEEQBEEQBOEHEk4EQRAEQRAEQRB+IOFEEARBEARBEAThBxJOBEEQBEEQBEEQfiDhRBAEUcZo0qQJRo0a5f1/2bJlcLlcWLZsmWP7cLlcmDJlimPbIyKbYJxDwaBJkyYYMmRIuJtBEEQFhYQTQRCEBWbNmgWXy+X9S0hIQFpaGsaNG4cjR46Eu3mW+Pbbb0kcWWTFihW4+eabUb9+fcTFxaFy5cpwu9145plnytz3bxXxvDf6C1R8/fXXX5gyZQp2797tSLsJgiCcIibcDSAIgiiLPPPMM2jatCkuXbqEX375BdOmTcO3336LjRs3IikpKaRtueqqq5CXl4e4uDhL7/v222/x1ltvaYqnvLw8xMTQT4TI008/jalTp6JZs2YYNWoUmjVrhkuXLmHdunV4+eWXMXv2bOzYsSPczQwac+fOVfw/Z84cLFmyxGd5mzZtAtrPX3/9hczMTPTu3RtNmjQJaFsEQRBOQr+KBEEQNhg4cCC6du0KABg7diyqV6+OV155BV9//TVuu+02zfdcuHABycnJjrclKioKCQkJjm7T6e2VdebNm4epU6fi5ptvxty5c31E6quvvopXX33VcBuSJOHSpUtITEwMZlODxp133qn4PycnB0uWLPFZrubixYshH0wgCIIIBhSqRxAE4QBXX301AGDXrl0AgFGjRiElJQU7duzAoEGDkJqaijvuuAMAUFJSgtdeew1t27ZFQkICateujfvuuw+nTp1SbFOSJDz77LNo0KABkpKS0KdPH2zatMln33r5KR6PB4MGDULVqlWRnJyMDh064PXXX/e276233gKgDMHiaOU4rV+/HgMHDkSlSpWQkpKCa665Bjk5OYp1eCjjypUrMX78eNSsWRPJyckYNmwYjh07plh37dq1GDBgAGrUqIHExEQ0bdoUd999t+FxHjJkCJo1a6b5Wrdu3bxiFgCWLFmCnj17okqVKkhJSUGrVq3wxBNPGG5fj6effho1atTAzJkzNZ29ypUr+xwvno/z3XffoWvXrkhMTMQ777wDANi5cyduuukmVKtWDUlJSUhPT8fChQsV7+fHUh2ypvV99+7dG+3atcNff/2FPn36ICkpCfXr10d2drZPW/fv34+hQ4ciOTkZtWrVwmOPPYb8/Hxbx0UNb8e6detw1VVXISkpyXvM9fLmxJy9WbNm4aabbgIA9OnTRzf875dffsEVV1yBhIQENGvWDHPmzHGk/QRBEEaQ40QQBOEAPESrevXq3mVFRUUYMGAAevbsiZdeesk76n7fffdh1qxZGD16NB5++GHs2rULb775JtavX4+VK1ciNjYWAOusP/vssxg0aBAGDRqE3377Df3790dBQYHf9ixZsgRDhgxB3bp18cgjj6BOnTrYvHkzFixYgEceeQT33XcfDh48qBlqpcWmTZtw5ZVXolKlSpgwYQJiY2PxzjvvoHfv3vj555/hdrsV6z/00EOoWrUqJk+ejN27d+O1117DuHHjMG/ePADA0aNH0b9/f9SsWRMTJ05ElSpVsHv3bsyfP9+wHbfccgtGjhyJX3/9FX/729+8y/fs2YOcnBz85z//8bZ3yJAh6NChA5555hnEx8dj+/btWLlypd/PqiY3Nxe5ubkYO3YsUlJSLL1369atuO2223DffffhnnvuQatWrXDkyBF0794dFy9exMMPP4zq1atj9uzZuP766/H5559j2LBhltsIAKdOncK1116L4cOH4+abb8bnn3+OjIwMtG/fHgMHDgTAQjCvueYa7N27Fw8//DDq1auHuXPn4scff7S1Ty1OnDiBgQMH4tZbb8Wdd96J2rVrm37vVVddhYcffhhvvPEGnnjiCW/Ynxj+t337dtx4440YM2YM7rrrLrz33nsYNWoUunTpgrZt2zr2OQiCIHyQCIIgCNO8//77EgDphx9+kI4dOybt27dP+uSTT6Tq1atLiYmJ0v79+yVJkqS77rpLAiBNnDhR8f4VK1ZIAKQPP/xQsXzx4sWK5UePHpXi4uKkwYMHSyUlJd71nnjiCQmAdNddd3mX/fTTTxIA6aeffpIkSZKKioqkpk2bSo0bN5ZOnTql2I+4rQcffFDS+xkAIE2ePNn7/9ChQ6W4uDhpx44d3mUHDx6UUlNTpauuusrn+PTt21exr8cee0yKjo6WTp8+LUmSJH355ZcSAOnXX3/V3L8eZ86ckeLj46V//vOfiuXZ2dmSy+WS9uzZI0mSJL366qsSAOnYsWOWtq/F119/LQGQXnvtNcXykpIS6dixY4q/wsJC7+uNGzeWAEiLFy9WvO/RRx+VAEgrVqzwLjt37pzUtGlTqUmTJlJxcbEkSfKx3LVrl+L96u9bkiSpV69eEgBpzpw53mX5+flSnTp1pL///e/eZa+99poEQPr000+9yy5cuCC1aNHCZ5v+0Dp/eDumT5/us776nOI0btxYcT5/9tlnum3hx3T58uXeZUePHtU8JwiCIJyGQvUIgiBs0LdvX9SsWRMNGzbErbfeipSUFHz55ZeoX7++Yr37779f8f9nn32GypUro1+/fjh+/Lj3r0uXLkhJScFPP/0EAPjhhx9QUFCAhx56SBFC9+ijj/pt2/r167Fr1y48+uijqFKliuI1cVtmKS4uxvfff4+hQ4cqwuTq1q2L22+/Hb/88gvOnj2reM+9996r2NeVV16J4uJi7NmzBwC87VqwYAEKCwtNt6VSpUoYOHAgPv30U0iS5F0+b948pKeno1GjRortf/311ygpKbH0edXwz6Z2m86cOYOaNWsq/jZs2KBYp2nTphgwYIBi2bfffosrrrgCPXv29C5LSUnBvffei927d+Ovv/6y1c6UlBRFvlFcXByuuOIK7Ny5U7HvunXr4sYbb/QuS0pKwr333mtrn1rEx8dj9OjRjm1PzWWXXYYrr7zS+3/NmjXRqlUrxeckCIIIBiScCIIgbPDWW29hyZIl+Omnn/DXX39h586dPh3kmJgYNGjQQLFs27ZtOHPmDGrVquXT6T5//jyOHj0KAF6B0bJlS8X7a9asiapVqxq2jYcNtmvXLqDPyDl27BguXryIVq1a+bzWpk0blJSUYN++fYrlXMBweJt5HlevXr3w97//HZmZmahRowZuuOEGvP/++6ZybW655Rbs27cPq1evBsA+77p163DLLbco1unRowfGjh2L2rVr49Zbb8Wnn35qS0SlpqYCAM6fP69YnpKSgiVLlmDJkiX417/+pfnepk2b+izbs2eP7rHkr9uhQYMGPsK4atWqity5PXv2oEWLFj7rabXHLrxUe7BQn1uA7+ckCIIIBpTjRBAEYYMrrrhCUYhAi/j4eERFKcenSkpKUKtWLXz44Yea76lZs6ZjbQwn0dHRmsu5S+RyufD5558jJycH//vf//Ddd9/h7rvvxssvv4ycnBzDXKLrrrsOSUlJ+PTTT9G9e3d8+umniIqK8hYVAIDExEQsX74cP/30ExYuXIjFixdj3rx5uPrqq/H999/rtk+L1q1bAwA2btyoWB4TE4O+ffsCYAUXtAikgp6eO1hcXKy53N8xDxVWP7Pe59EjUj4nQRAVD3KcCIIgQkjz5s1x4sQJ9OjRA3379vX569ixIwCgcePGAJhDJXLs2DG/I+vNmzcH4NvRV2M2bK9mzZpISkrC1q1bfV7bsmULoqKi0LBhQ1PbUpOeno7nnnsOa9euxYcffohNmzbhk08+MXxPcnIyhgwZgs8++wwlJSWYN28errzyStSrV0+xXlRUFK655hq88sor+Ouvv/Dcc8/hxx9/9IZDmqVVq1Zo2bIlvvrqK1y4cMHyZ1TTuHFj3WPJXwdkl+706dOK9ew6UnzbO3bs8BEZWu1xmqpVq/p8loKCAhw6dEixzE44KUEQRCgg4UQQBBFCbr75ZhQXF2Pq1Kk+rxUVFXk7ln379kVsbCz+7//+T9HJfe211/zu4/LLL0fTpk3x2muv+XRUxW3xOaXU66iJjo5G//798fXXXytKYx85cgQfffQRevbsiUqVKvltl8ipU6d8Ou+dOnUCANPhegcPHsSMGTPw+++/K8L0AODkyZM+79Ha/pYtW7B3716/+5syZQqOHz+Oe+65RzMny4rbMWjQIKxZs8YbagiwOb7++9//okmTJrjssssAyAJ4+fLl3vWKi4vx3//+1/S+tPZ98OBBfP75595lFy9eDGibZmnevLniswDAf//7Xx/Hyex5SRAEEWooVI8gCCKE9OrVC/fddx9eeOEFbNiwAf3790dsbCy2bduGzz77DK+//jpuvPFG1KxZE48//jheeOEFDBkyBIMGDcL69euxaNEi1KhRw3AfUVFRmDZtGq677jp06tQJo0ePRt26dbFlyxZs2rQJ3333HQCgS5cuAICHH34YAwYMQHR0NG699VbNbT777LPeeZEeeOABxMTE4J133kF+fr7mXEH+mD17Nt5++20MGzYMzZs3x7lz5/Duu++iUqVKGDRokN/387mxHn/8cURHR+Pvf/+74vVnnnkGy5cvx+DBg9G4cWMcPXoUb7/9Nho0aKAoytCmTRv06tXLZ54gNbfffjs2btyIF154AWvWrMGtt96Kpk2b4sKFC9i4cSM+/vhjpKam+s0/A4CJEyfi448/xsCBA/Hwww+jWrVqmD17Nnbt2oUvvvjCG97Ztm1bpKenY9KkSTh58iSqVauGTz75BEVFRX73occ999yDN998EyNHjsS6detQt25dzJ07NyQT1I4dOxb/+Mc/8Pe//x39+vXD77//ju+++87nfO7UqROio6ORlZWFM2fOID4+HldffTVq1aoV9DYSBEEYErZ6fgRBEGUQXiLaXxntu+66S0pOTtZ9/b///a/UpUsXKTExUUpNTZXat28vTZgwQTp48KB3neLiYikzM1OqW7eulJiYKPXu3VvauHGjT/lmrfLUkiRJv/zyi9SvXz8pNTVVSk5Oljp06CD93//9n/f1oqIi6aGHHpJq1qwpuVwuRWlpaJSO/u2336QBAwZIKSkpUlJSktSnTx9p1apVpo6Puo2//fabdNttt0mNGjWS4uPjpVq1aklDhgyR1q5da3RYFdxxxx3e0udqli5dKt1www1SvXr1pLi4OKlevXrSbbfdJuXm5irWAyD16tXL9D6XLVsm3XjjjVLdunWl2NhYqVKlSlLXrl2lyZMnS4cOHVKs27hxY2nw4MGa29mxY4d04403SlWqVJESEhKkK664QlqwYIHmen379pXi4+Ol2rVrS0888YS0ZMkSzXLkbdu29Xn/XXfdJTVu3FixbM+ePdL1118vJSUlSTVq1JAeeeQRbzl8J8qRa7VDktj5nJGRIdWoUUNKSkqSBgwYIG3fvt3nfJYkSXr33XelZs2aSdHR0Yp26R3TXr16WfoeCYIg7OCSJMqmJAiCIAiCIAiCMIJynAiCIAiCIAiCIPxAwokgCIIgCIIgCMIPJJwIgiAIgiAIgiD8QMKJIAiCIAiCIAjCDyScCIIgCIIgCIIg/EDCiSAIgiAIgiAIwg8VbgLckpISHDx4EKmpqXC5XOFuDkEQBEEQBEEQYUKSJJw7dw716tXzTkCuR4UTTgcPHkTDhg3D3QyCIAiCIAiCICKEffv2oUGDBobrVDjhlJqaCoAdnEqVKoW5NQRBEARBEARBhIuzZ8+iYcOGXo1gRIUTTjw8r1KlSiScCIIgCIIgCIIwlcJDxSEIgiAIgiAIgiD8QMKJIAiCIAiCIAjCDyScCIIgCIIgCIIg/FDhcpwIgiAIgiAIY4qLi1FYWBjuZhCEI8TGxiI6Ojrg7ZBwIgiCIAiCILycP38e+/fvhyRJ4W4KQTiCy+VCgwYNkJKSEtB2SDgRBEEQBEEQAJjTtH//fiQlJaFmzZqmKo0RRCQjSRKOHTuG/fv3o2XLlgE5TyScCIIgCIIgCABAYWEhJElCzZo1kZiYGO7mEIQj1KxZE7t370ZhYWFAwimsxSGWL1+O6667DvXq1YPL5cJXX33l9z3Lli3D5Zdfjvj4eLRo0QKzZs0KejsJgiAIgiAqEuQ0EeUJp87nsAqnCxcuoGPHjnjrrbdMrb9r1y4MHjwYffr0wYYNG/Doo49i7Nix+O6774LcUoIgCIIgCIIgKjJhDdUbOHAgBg4caHr96dOno2nTpnj55ZcBAG3atMEvv/yCV199FQMGDAhWMwmCIAiCIAiCqOCUqXmcVq9ejb59+yqWDRgwAKtXr9Z9T35+Ps6ePav4IwiCIAiCIIhIYMqUKejUqVO4mwEA6N27Nx599FHL7ysoKECLFi2watUq5xvlh8WLF6NTp04oKSkJ+r7KlHA6fPgwateurVhWu3ZtnD17Fnl5eZrveeGFF1C5cmXvX8OGDUPRVIIgCIIgCCKEHD58GI888ghatGiBhIQE1K5dGz169MC0adNw8eLFcDfPFlOmTIHL5TL8s8OyZcvgcrlw+vRpR9rJo8K6d+/uXeZyuZCQkIA9e/Yo1h06dChGjRqlWLZv3z7cfffdqFevHuLi4tC4cWM88sgjOHHihN99X3vttYiNjcWHH37oyGcxokwJJztMmjQJZ86c8f7t27cv3E0iCIIgCIIgHGTnzp3o3Lkzvv/+ezz//PNYv349Vq9ejQkTJmDBggX44YcfdN8byRP9Pv744zh06JD3r0GDBnjmmWcUy0QKCgpC3kZJkvDmm29izJgxPq+5XC48/fTThu/fuXMnunbtim3btuHjjz/G9u3bMX36dCxduhTdunXDyZMn/bZh1KhReOONN2x/BrOUKeFUp04dHDlyRLHsyJEjqFSpkm7JzPj4eFSqVEnxRxAEQUQuK1cCAwYAW7aEuyUEQUgScOFCeP6szL/7wAMPICYmBmvXrsXNN9+MNm3aoFmzZrjhhhuwcOFCXHfddd51XS4Xpk2bhuuvvx7Jycl47rnnAADTpk1D8+bNERcXh1atWmHu3Lne9+zevRsulwsbNmzwLjt9+jRcLheWLVsGQHZxli5diq5duyIpKQndu3fH1q1bFW198cUXUbt2baSmpmLMmDG4dOmS7udKSUlBnTp1vH/R0dFITU31/n/rrbdi3LhxePTRR1GjRg0MGDDAb1t3796NPn36AACqVq0Kl8ulcIBKSkowYcIEVKtWDXXq1MGUKVMMj/26deuwY8cODB482Oe1cePG4YMPPsDGjRt13//ggw8iLi4O33//PXr16oVGjRph4MCB+OGHH3DgwAE8+eSThvsHgOuuuw5r167Fjh07/K4bCGVKOHXr1g1Lly5VLFuyZAm6desWphYRBEEQTvP++8D33wOffRbulhAEcfEikJISnj+z0XUnTpzA999/jwcffBDJycma66hD2qZMmYJhw4bhzz//xN13340vv/wSjzzyCP75z39i48aNuO+++zB69Gj89NNPlo/Zk08+iZdffhlr165FTEwM7r77bu9rn376KaZMmYLnn38ea9euRd26dfH2229b3ofI7NmzERcXh5UrV2L69Ol+12/YsCG++OILAMDWrVtx6NAhvP7664rtJScnw+PxIDs7G8888wyWLFmiu70VK1YgLS0NqampPq/16NEDQ4YMwcSJEzXfe/LkSXz33Xd44IEHfEyQOnXq4I477sC8efMgSRK++eYbuN1upKen48Ybb0R+fr533UaNGqF27dpYsWKF388fCGEVTufPn8eGDRu8injXrl3YsGED9u7dC4CF2Y0cOdK7/j/+8Q/s3LkTEyZMwJYtW/D222/j008/xWOPPRaO5hMEQRBB4Px59njuXHjbQRBE2WD79u2QJAmtWrVSLK9RowZSUlKQkpKCjIwMxWu33347Ro8ejWbNmqFRo0Z46aWXMGrUKDzwwANIS0vD+PHjMXz4cLz00kuW2/Pcc8+hV69euOyyyzBx4kSsWrXK6yq99tprGDNmDMaMGYNWrVrh2WefxWWXXWb/wwNo2bIlsrOz0apVK59joEV0dDSqVasGAKhVqxbq1KmDypUre1/v0KEDJk+ejJYtW2LkyJHo2rWrj3EhsmfPHtSrV0/39RdeeAGLFy/WFDXbtm2DJElo06aN5nvbtGmDU6dO4dixY7j88suxcuVK5OTkIDo6WuEIAkC9evV88qmcJqzCae3atejcuTM6d+4MABg/fjw6d+7sjYU8dOiQV0QBQNOmTbFw4UIsWbIEHTt2xMsvv4wZM2ZQKXKCIIhyBB9lvnAhvO0gCAJISmKDGeH4S0oKrO1r1qzBhg0b0LZtW4U7AQBdu3ZV/L9582b06NFDsaxHjx7YvHmz5f126NDB+7xu3boAgKNHj3r343a7FesHGjnVpUuXgN6vRmw/wD4Db78WeXl5SEhI0H39sssuw8iRI3VdJ4DlSfmjQYMGiIlhMym5XC5ERSllTGJiYtCLgIR1HqfevXsbHqhZs2Zpvmf9+vVBbBVBEAQRTrhgIuFEEOHH5QJ0ot8ihhYtWsDlcvnkEjVr1gwANPPg9UL69OCddLHfqldUIjY21vuchwgGs1S2+rNYaasWYvsB9hmM2l+jRg38+eefhtvMzMxEWloavvrqK8Vy/t1t3rwZw4YN83nf5s2bUbVqVdSsWdO7jLtO7733nmLdkydPKtYLBmUqx4kgCIIo/5DjRBCEFapXr45+/frhzTffxAWbN442bdpg5cqVimUrV670htHxDrlYxU4svmBlPx6PR7EsJyfH8naMMNPWuLg4AEBxcXHA++vcuTO2bNliaIY0bNgQ48aNwxNPPKHYJ//u3n77bZ+phQ4fPowPP/wQt9xyi1eAHj16FCNHjsScOXOQJFiSly5dwo4dO7xRbMGChBNBEAQRUZBwIgjCKm+//TaKiorQtWtXzJs3D5s3b8bWrVvxwQcfYMuWLYiOjjZ8/7/+9S/MmjUL06ZNw7Zt2/DKK69g/vz5ePzxxwEw1yo9PR0vvvgiNm/ejJ9//hn//ve/LbfzkUcewXvvvYf3338fubm5mDx5MjZt2mTrM+thpq2NGzeGy+XCggULcOzYMZznyaU26NOnD86fP+/3c0yaNAkHDx70KQ3/5ptvIj8/HwMGDMDy5cuxb98+LF68GP369UP9+vW9VQ8vXbqEYcOG4cknn8RVV12l2EZOTg7i4+ODXjCOhBNBEAQRUXDhFMDvOEEQFYzmzZtj/fr16Nu3LyZNmoSOHTuia9eu+L//+z88/vjjmDp1quH7hw4ditdffx0vvfQS2rZti3feeQfvv/8+evfu7V3nvffeQ1FREbp06YJHH30Uzz77rOV23nLLLXjqqacwYcIEdOnSBXv27MH9999veTv+8NfW+vXrIzMzExMnTkTt2rUxbtw42/uqXr06hg0b5ncC2mrVqiEjI8On/HrLli2xdu1aNGvWDDfffDOaN2+Oe++9F3369MHq1au9hSzeeust/PHHH5gzZw569+6NadOmebfx8ccf44477lC4UMHAJZnJxipHnD17FpUrV8aZM2doTieCIIgIpF494NAh4PLLgXXrwt0agqhYXLp0Cbt27ULTpk0NE/4JQuSPP/5Av379sGPHDqSkpIR038ePH0erVq2wdu1aNG3aVHMdo/PaijYgx4kgCIKIKChUjyAIomzRoUMHZGVlYdeuXSHf9+7du/H222/riiYnCWtVPYIgCIJQQ8KJIAii7DFq1Kiw7Ldr164+5eWDBTlOBEEQRMRQWMj+ABJOBEEQRGRBwokgCIKIGMS5C0k4EQRBEJEECSeCIAgiYhCFU0GB7D4RBEEQRLgh4UQQBEFEDKJwAsh1IgiCICIHEk4EQRBExEDCiSAIgohUSDgRBEEQEQMJJ4IgCCJSIeFEEARBRAxqoUTCiSAIgogUSDgRBEEQEYPacTp/PjztIAiC0GLUqFEYOnSo9//evXvj0UcfDWibTmyDCA0knAiCIIiIgUL1CIKww6hRo+ByueByuRAXF4cWLVrgmWeeQVFRUVD3O3/+fEydOtXUusuWLYPL5cLp06dtb4MILzHhbgBBEARBcEg4EQRhl2uvvRbvv/8+8vPz8e233+LBBx9EbGwsJk2apFivoKAAcXFxjuyzWrVqEbENIjSQ40QQBEFEDJTjRBCEXeLj41GnTh00btwY999/P/r27YtvvvnGG1733HPPoV69emjVqhUAYN++fbj55ptRpUoVVKtWDTfccAN2797t3V5xcTHGjx+PKlWqoHr16pgwYQIkSVLsUx1ml5+fj4yMDDRs2BDx8fFo0aIFZs6cid27d6NPnz4AgKpVq8LlcmHUqFGa2zh16hRGjhyJqlWrIikpCQMHDsS2bdu8r8+aNQtVqlTBd999hzZt2iAlJQXXXnstDh065F1n2bJluOKKK5CcnIwqVaqgR48e2LNnj0NHuuJCwokgCIKIGMhxIojyg8fjwdy5c+HxeMKy/8TERBQUFAAAli5diq1bt2LJkiVYsGABCgsLMWDAAKSmpmLFihVYuXKlV4Dw97z88suYNWsW3nvvPfzyyy84efIkvvzyS8N9jhw5Eh9//DHeeOMNbN68Ge+88w5SUlLQsGFDfPHFFwCArVu34tChQ3j99dc1tzFq1CisXbsW33zzDVavXg1JkjBo0CAUCjOCX7x4ES+99BLmzp2L5cuXY+/evXj88ccBAEVFRRg6dCh69eqFP/74A6tXr8a9994Ll8sV8DGt6FCoHkEQRJDweDzIzc1FWloa3G53uJtTJiDhRBDlg4yMDGRnZ3v/nzBhArKyskKyb0mSsHTpUnz33Xd46KGHcOzYMSQnJ2PGjBneEL0PPvgAJSUlmDFjhldQvP/++6hSpQqWLVuG/v3747XXXsOkSZMwfPhwAMD06dPx3Xff6e43NzcXn376KZYsWYK+ffsCAJo1a+Z9nYfk1apVC1WqVNHcxrZt2/DNN99g5cqV6N69OwDgww8/RMOGDfHVV1/hpptuAgAUFhZi+vTpaN68OQBg3LhxeOaZZwAAZ8+exZkzZzBkyBDv623atLF+IAkfyHEiCIIIAhkZGUhPT8fIkSORnp6OjIyMcDepTEBV9Qii7OPxeBSiCQCys7OD7jwtWLAAKSkpSEhIwMCBA3HLLbdgypQpAID27dsr8pp+//13bN++HampqUhJSUFKSgqqVauGS5cuYceOHThz5gwOHTqkGPSKiYlB165ddfe/YcMGREdHo1evXrY/w+bNmxETE6PYb/Xq1dGqVSts3rzZuywpKckrigCgbt26OHr0KAAm0EaNGoUBAwbguuuuw+uvv64I4yPsQ8KJIAjCYcLVaSgPUI4TQZR9cnNzLS13ij59+mDDhg3Ytm0b8vLyMHv2bCQnJwOA95Fz/vx5dOnSBRs2bFD85ebm4vbbb7e1/8TExIA/g1liY2MV/7tcLkX+1fvvv4/Vq1eje/fumDdvHtLS0pCTkxOy9pVXSDgRBEE4TLg6DeUB7jjxgWESTgRR9khLS7O03CmSk5PRokULNGrUCDExxtkol19+ObZt24ZatWqhRYsWir/KlSujcuXKqFu3rmLAq6ioCOvWrdPdZvv27VFSUoKff/5Z83XueBUXF+tuo02bNigqKlLs98SJE9i6dSsuu+wyw8+kpnPnzpg0aRJWrVqFdu3a4aOPPrL0fsIXEk4EQRAOE65OQ3mAC6eaNdkjCSeCKHu43W5MmDBBsSwjIyOicj3vuOMO1KhRAzfccANWrFiBXbt2YdmyZXj44Yexf/9+AMAjjzyCF198EV999RW2bNmCBx54wGcOJpEmTZrgrrvuwt13342vvvrKu81PP/0UANC4cWO4XC4sWLAAx44dw3mNWOSWLVvihhtuwD333INffvkFv//+O+68807Ur18fN9xwg6nPtmvXLkyaNAmrV6/Gnj178P3332Pbtm2U5+QAJJwIgiAcpix0GiIVLpxq1WKPJJwIomySlZWFnJwczJkzBzk5OXjxxRfD3SQFSUlJWL58ORo1aoThw4ejTZs2GDNmDC5duoRKlSoBAP75z39ixIgRuOuuu9CtWzekpqZi2LBhhtudNm0abrzxRjzwwANo3bo17rnnHlwovZHVr18fmZmZmDhxImrXro1x48ZpbuP9999Hly5dMGTIEHTr1g2SJOHbb7/1Cc8z+mxbtmzB3//+d6SlpeHee+/Fgw8+iPvuu8/CESK0cEnqgvTlnLNnz6Jy5co4c+aM98IgCIIIBlRVzzrXXAP8+CMwYADw3XfscfHicLeKICoOly5dwq5du9C0aVMkJCSEuzkE4QhG57UVbUDlyAmCIIKE2+0mwWQRCtUjCIIgIhUK1SMIgiAiBhJOBEEQRKRCwokgCIKIGCjHiSAIgohUSDgRBEEQEQM5TgRBEESkQsKJIAiCiBi4UCLHiSDCSwWrHUaUc5w6n0k4EQRBEBGD2nHSmOaEIIggEh0dDQAoKCgIc0sIwjn4+czPb7tQVT2CIAgiIigsZH+ALJyKioCCAiAuLnztIoiKRExMDJKSknDs2DHExsYiKorG2ImyTUlJCY4dO4akpCTExAQmfUg4EQRBEBFBXp78nIfqASxcj4QTQYQGl8uFunXrYteuXdizZ0+4m0MQjhAVFYVGjRrB5XIFtB0STgRBKJAkIMD7CkHYguczRUUBKSlATAxznC5cAKpWDW/bCKIiERcXh5YtW1K4HlFuiIuLc8Q9JeFEEISXnBxg4EDgxReB++4Ld2uIigbPb0pKYuI9KQk4e1ZeThBE6IiKikJCQkK4m0EQEQUFrhIE4WX8eOD0aeAf/wh3S8oXHo8Hc+fOhcfjCXdTIhpROAFAYiJ7FEP4CIIgCCJckHAiCMJLpUrhbkH5IyMjA+np6Rg5ciTS09ORkZER7iZFLCScCIIgiEiGhBNBEF4aNJCfl5SErx3lBY/Hg+zsbMWy7Oxscp504AKJCyYuoEg4EQRBEJEACSeCILzUqSM/P3w4fO0oL+Tm5lpaXtG5dIk9xsezR3KcCIIgiEiChBNBEJrs2hXuFpR90tLSLC2v6OTns0e1cKLiEARBEEQkQMKJIAgvfPJRANi9O2zNKDe43W5MmDBBsSwjIwNutztMLYps9IQTOU4EQRBEJEDlyImI4uOPAY8HeOUVNpcLEVpE4USOkzNkZWVh+PDhWLRoEQBg4MCBYW5R5MKFE6+ATMKJIAiCiCSoa0pEFBMmAK+/DqxfH+6WVEyKiuTnJJycY/78+cjMzERmZiZV1jNA7ThRcQiCIAgikiDhREQMkgQcOcKenz4d1qZUWChUz3mosp55KFSPIAiCiGRIOBERw7lzcsf93LnwtqWscf48kJ0NbN8e2HbIcXIeqqxnHhJOBEEQRCRDwomIGI4fl5+TcLLGZ58BGRnA1KmBbUd0nPbuVQopwh5UWc88euXIqaoeQRAEEQmQcCIiBhJO9jl5UvloF1EoFRcDBw4Etj2CKutZgRwngiAIIpIh4UREDCdOyM/Pnw9fO8oifKSedzztIjpOALBzZ2DbIxhZWVnIycnBnDlzkJOTgxdffDHcTQqYDRuAK64Ali51bptUVY8gCIKIZKgcORExkONkH97hdFo4bdkC9OkT2DYJhtvtLlcu09dfA7/+CnzyCXDNNc5sk6rqEQRBEJEMOU5ExEDCyT7cceKPduGhetWqscdNmwLbHlF+4SKnoMD5bVKoHkEQBBGJkHAiIgYSTvZxOlSvY0f2SMKJ0MMpl1Nrm04Xh9i8GVi7NrBtEARBEAQJJyJioBwn+zglnLjj1KkTeyThROjBnaZId5wkCejdG+jZEzhzJqDmEQRBEBUcEk5ExECOk32cCtXjjlP79uzx2DH2RxBqgiGc9MqRByKcTp0Cjh5lomzfvsDaRxAEQVRsSDgREQMJJ/s47ThVrgw0bcqek+vkLB6PB3PnzoXH4wl3UwIiFKF6ThSHOHxYfn70qP3tEARBEAQJJyJiIOFkH97hdMpxio0F2rZlz0k4OUdGRgbS09MxcuRIpKenIyMjI9xNsk0wQ/WcLEd+5Ij2c4IgCIKwCgknImIQhRPlOFnDaceJhJPzeDweZGdnK5ZlZ2eXWecplDlOgRSHIMeJIAiCcAoSTkREIEnK4hDkOFnD6ap6MTEknJwmNzfX0vJIJ5RV9ShUjyAIgogESDgREcHZs7LbAZBwsgoXTsXFyuNoFTFUr00b9ryM9usjjrS0NEvLI52yUlWPhBNBEAThFCSciIhADNMDWEcpEAFQ0RBzmwJxAPgxj4kBUlPZc5p81BncbjcmTJigWJaRkQG32x2mFgVGKB2nggI2KGAHMa+JhBNBEAQRCDHhbgBBALJwql1b7uhcuMCquxH+ETuv+flAcrK97YiOU2wse04C1jmysrIwfPhw5ObmIi0trcyKJiA05ch5VT3+mp3zWnScqDgEQRAEEQgknIiIgOc31avHnhcVsXA9Ek7mEB2nQCrriY4TF05cTBHO4Ha7y7Rg4oQyVA9gzmegwokcJ4IgCCIQKFSPiAi441SzphwiRnlO5nEqVE/LcSLhRGgRzFA9Xo48KgqIi2PP7VbWo1A9giAIwinCLpzeeustNGnSBAkJCXC73VizZo3h+q+99hpatWqFxMRENGzYEI899hguBTp5DRF2uHCqUYOEkx2C4TjFlPrRkmQ/v0Tk2DHgtdd889mIskkoHCcgsAIRxcVKsXThAvsjCIIgCDuEVTjNmzcP48ePx+TJk/Hbb7+hY8eOGDBgAI7qDAt+9NFHmDhxIiZPnozNmzdj5syZmDdvHp544okQt5xwGh6qV60akJLCntNcTuYJpuMkLg+EN98EHnuMPRJln7IgnI4fB0pKAJdL3uaxY4G1kSAIgqi4hFU4vfLKK7jnnnswevRoXHbZZZg+fTqSkpLw3nvvaa6/atUq9OjRA7fffjuaNGmC/v3747bbbvPrUhGRD3eXKlcmx8kqkuRbHMIuesLJiQIRp06xx717A98WEX74eVZYyMRJoJSUyOefKJx4gQg7wonnN9WsCdSpw55TgQiCIAjCLmETTgUFBVi3bh369u0rNyYqCn379sXq1as139O9e3esW7fOK5R27tyJb7/9FoMGDdLdT35+Ps6ePav4IyIPLpJSU0k4WUU94u90cQjAGceJt5NC9coH4nnnxPkhCn6nHCcukurUAWrVYs8pz4kgCIKwS9iq6h0/fhzFxcWoXbu2Ynnt2rWxZcsWzffcfvvtOH78OHr27AlJklBUVIR//OMfhqF6L7zwAjIzMx1tO+E8JJzsoxZKdh0nSZKFU2ysnOMEONMx5tsg4VQ+EIVTQYFS7NjBn3CyUxyCO061a8tFJkg4EQRBEHYJe3EIKyxbtgzPP/883n77bfz222+YP38+Fi5ciKlTp+q+Z9KkSThz5oz3b9++fSFsMWEWbgSmpkZWjtOvvwJffBHuVhjjlHASC0DExLC8kOho9r+TjhPlmJQPnAoP1doGFzmAdcfp99+B//2PPefCiRwngiAIwgnC5jjVqFED0dHROKIKOD9y5Ajq8GB0FU899RRGjBiBsWPHAgDat2+PCxcu4N5778WTTz6JqChfHRgfH4/4QIdCiaATqY7TFVewx40bgbZtw9sWPdTCyW6oniiOeJhebCwTVBSqR4hIkq/jFChiYQiXS15uRTgtXgwMHcq29dtvSuHEfx5IOBEEQRB2CZvjFBcXhy5dumDp0qXeZSUlJVi6dCm6deum+Z6LFy/6iKPo0iFxSZKC11gi6ESicBIdmAMHwtcOfzjlOIkFIHiYHhdQdotDnDgBrF3LnnPxdfp0+Z8byuPxYO7cufB4POFuSlBQnw9OCyeOx+PB8eMsSsCfcPJ4gBtukLezaBHwxx/seaNGsuNExSEIgiAIu4Q1VG/8+PF49913MXv2bGzevBn3338/Lly4gNGjRwMARo4ciUmTJnnXv+666zBt2jR88skn2LVrF5YsWYKnnnoK1113nVdAEWUTUThFSqjeyZPycy7mIoGiImDmTCA3l/2vFkpOO07q16xw223A3/4GbNqk7Fzz8vPlkYyMDKSnp2PkyJFIT09HRkZGuJvkOOpzzslQPS6c+HH87bcVAIBPP/2f4fs/+ICdY1WqsP/nzwd+/pk9HzBArqpH0doEQRCEXcIWqgcAt9xyC44dO4ann34ahw8fRqdOnbB48WJvwYi9e/cqHKZ///vfcLlc+Pe//40DBw6gZs2auO666/Dcc8+F6yMQDhFJjpMksVAhMRcnkgzNuXOB0mhVSJJzjpMojvg4RKDCaetW9rh3r3Ibx4/LHdnyhMfjQXZ2tmJZdnY2hg8fDrfbHaZWOY/aYXLacVIeR2Y1/fjjang8tXSP486d7HHsWOCll4B169j/rVsDLVvKbVy/nrnJNNZGEARBWCXsxSHGjRuHPXv2ID8/Hx6PR/GjuGzZMsyaNcv7f0xMDCZPnozt27cjLy8Pe/fuxVtvvYUqfIiRKJNIUuQIpwsXWEfrnnuUuTiRFFr222/K/50O1eOFIfhzwP7n565dQYGyc11e85xyuQ1ocnlZxUnhJEnAnj1yKF58vPp48Ri9RMPjuGsXe+zfn4Xmca67jj22bs3c7AsXgL/+st9egiAIouISduFEEPn5cqddDNXjwmn4cKB7d2fCgfyxeTMLgfvkE6XjFEnCqWZN+bmW4xRoqJ44f1MgjlNBgRxuWVCg3EZ5rayXlpZmaXlZxclQvQ8+AJo0AbjBFB+vPl6ycNI7jpIE7N7NnjdtCgjTA+L669ljdDTQtSt7TnOmEwRBEHYg4USEHdFZSkmRHafz54H9+4EvvwRWrwY2bAh+W7joOH9eDv0BnAlFcgpROJ05ExzHiRNIcQgxR6yiOE5utxsTJkxQLMvIyChXYXqAs47T5s3sMSeHPSYkqI8jE06dO/fQPY5HjjDHKiqKuU39+rHl1asDYq0h/nYSTgRBEIQdSDgRAfHHH8BVVwErVtjfBhdOSUlsVLhePfb/9u3AqlXyeqEQTqLo+P13+XkkOU6iI3TokHOj/047ThVROAFAVlYWcnJyMGfOHOTk5ODFF18Md5Mcx0nhxM8t7kLy4hD8ON500xAAQKdO2tVWATlMr0EDNgfU8OHAI48A776rzGXi0wuQcCIIgiDsENbiEETZ54svmGj66CPgyiv117t4kYmsK66Q51PhiPlNANCuHauMdfo08Pbb8noknBhiWw4edC5Uz8hxckI4VYRQPY7b7S53LpOIk6F66nNLLEfudrvRowfw2WfG5ci5O9y0KXuMiwNee813PS6c/vyT3ZOSkmw3myAIgqiAkONEBATvzPjrrF9/PQuZmTHD9zW1cIqOBnr1Ys95OWEg9MJpyxb5eSSF6olhc4cOOV9VT3ScAikOUVEdJ7OU5bmeguE4cdTzlfMJcC9e1N8Gd5y4cNKjfn2gbl1WVU9dZIUgCIIg/EHCiQgI3kn3J5z4PMdao8Bq4QQAffr4rvfHH8pJaYOB+DlEgVIRHCe+3VA4ThVdOJX1uZ6cFE7q9/Jy5FxUcuFk5DiZFU4uF3O0AWDHDnvtJQiCICouFKpHBAQXTmZdjtOnfZdpCaerr5afV6rEOt0XL7K8p1atbDXVFHqfI5KEk9px4jlhnECLQ2jlODldHKK8h+oZUR7megpmqN7WrX8gPT3d+//QobMBjMSFC/rb4MKpWTP/++NCLBRVOgmCIIjyBTlOREAESzi1bQvUqMGeu91Ahw7sebDD9cqCcPLnOAUaqueU43TihPycHCeZ8jDXUzBD9bZu/V3x/1dfzQbAKkjqoc5xMoKHAkZS+C1BEARRNiDhRASEVeGkFW6jJZyiooBrrmHPu3cHOndmz8MlnCKpk6V2nHib+fxXgRaHCFVVPUmyvs3yQHmY6ymYwglQn8BMMekJp8JCYN8+9tyMcIqLY4+RdE0TBEEQZQMSTkRA8E56IGEvWsIJYBNi/vvfwPjxQKdObNnvysFox9ETHWXBcapcmT1GYnGI/HzlNi5dgmHoVXmmPMz15GSonq+AUW/MWDjt2weUlDAnqU4d//sj4UQQBEHYhXKciICw6jhpoSecGjUCpk6VnwPA4cP292OGshCqp3acuItXuTJw4EBwJsB12nECmOvEXbKKRlZWFoYPH45FixYBAAYOHBjmFlnDiuPk8XiQm5uLtLQ0TXHoe24pT+Abb+yPzz9n94niYuW8TACbJBsAGjb0nepACzvCKS8P2LgR6NqVFZggCIIgKibkOBEBYVY4JSTov6YnnER4vlOwc2PKQqie2NG8eBE4epQ9r1KFPQZaVS8YxSEuXZIrIvJzobzmOZktMz5//nxkZmYiMzOzzFXWU18PeteNmeqB/oTTwIHdvc/PnvXdBxdODRqYO/Z2hNMTT7A5oN580/x7CIIgiPIHCSciIMwKJ96pB3xzW3hnyEg41azJHsMlnCLVcQLkimJOheoFw3ESw/J4FcDyWFnPbJlxvcp6ZWVOJ/U5piVCzH5Gf8KpbdsWXrGtFa7HhdPRo7+ZOvZ2hNM337DHF16wPzBBEARBlH1IOBEBYVY48U49IDtM6v/NOE55ecYTYQZKWchx8iecIrE4xPnz8vO6ddljeXOcrIihsl5Zz0yontnPaJTjxHO/+Ln94YcLfI7ngQPs8a+/vlcs1zv2vKqe2QGGvXvlqn2HDgGzZ5t7H0EQBFH+IOFEBITZ4hBiZ1wsUw2YE07JyXKHJ5gd7rIWqgfIAiUYjpPd4hCFhcqwKi3HqbwJJytiqKxX1jNTHELvsxSoLib1ufXII/djzpw5yMnJwYsvvlj6HmZP/vvf2T5uEnecgAM++9I69lYdp2XL2CPPrcrKYsUoCIIgiIoHCSciIMw6Tjy/BbAnnFwu2XUKZohXWQzV4wQqnJx0nE6dUv4vOk688ll5C9WzIobKemU9M46T1mcEgLFjxyqEj/rcatWqMUaMGOE9Fh6PB6dOldqqYCe56CYd8Oql/VCjdeztCqcHHmDiadeu4BepIQiCICITEk5EQJgVTmJnX+00mBFOQGgKRPDPceutQI8ewHXXsf8jSTjxtvDjwXGqOIQTOU5imB4gO05xcaHLVws1VsVQVlYWcnJyfNyVYCFJwKhRwKRJgW/LbFW94cOHY8yYMT7LReGjPre4s8xhrhFPbqqiWg5s2sRf83Wc5s+f77PMrnAaNAioVIk9N5qMlyAIgii/kHAiAoILDX+d9UAdJyA0wol/jt69gV9+YeIJiKxQPS5CL7tMuTyYjpPVqnpq4cQdp9jY0FVIDAdWxZDb7Va4K8Fk3z6Wn+NEqBk/x3j4mtY5xwtlzJw5U3MbXPiory21cGKuEVcqlRXLV6704Pz55NIlvo6TVp6TFeG0Zw9zmKKj2b2AD06QcCIIgqiYkHAiAoJ3mIqLleJITVkRTvzz8M5bIMURggVvi5FwUlcutLJdJ0L1zDhO5S1UjxNKMWQFfp1JUuAdfy46+DWrFiFahTLU8DA6f44Tc42Uwok7eWvX7gObjrAIwBHN/ajznKwIpy1b2GObNuyz8muMhBNBEETFhIQTERCi02TkdIiOhSicSkpkN4KEkzn4sWzdWrmcd+okyV57tSbAtVscQs9xiosr345TJCPmmalz0KziTzj5qw4ohjAaCSdZgJ0uXcJO8mHDhpXun18EhwBo22jqPCcrwonfD5KSSvdeeo2dPq25OkEQBFHOIeFEBIQoloyEk57jJFZbiyThxOeNsTPnS7DhHc1KlYCGDeXlYsl3O+F6wXCceGhTXp68PRJO4UGcBiDQjj8/v/g1qz7f9AplTJ48GTNmzEDbtm1N5TjJAkzpOPHlVaq0K13um98EaOeYWSlHrr4myHEiCIKo2MT4X4UgtCkqUuZKmHWcxA4z78xFRcmjunqEMsepLDhOMTFAy5YsdwVQCqdLl/wLUTVOFofgQqlaNWUnXQzVO3GCCWqeJ1PW8Xg8yM3NRVpaWsSF6QHBcZxSUpT/c3ihDDFcLyMjA3l5eRg7dqx32YQJE1BQkKV4Lx+0AEQBphROfDkvRd6nTxpGj57jXW70PVgZDCHhRBAEQYiQcCJsY2YuF46e48Tn+klNZSXHjQhlOfJIFk5iZy4tDfjxR/Z/UhJbVlhoz3FysjiEumMtbq9aNfZcklgHXl0d0CqRIFgyMjIUImHChAnIysoyeEfoCWWoHsAKZQwfPtz73QBAenq6Yp3s7GwkJj4PIBrR0ew+kZwsvy4LMJ6/VFnhIvFS5B06VMOIESMU79PDjnDi7yHhRBAEUbGhUD3CNlaEk16OEw/p4p1pI0JRxlotnHiHKZKEk9px4iQkWAtDUuOk46QnnOLi2DZ5CF+g3yWv3DZy5EifiVFDhVYhBK1qbuHGSeHkL1SPIxbK0Mt74udWdjbw6KNAp07K17OysvDii08AANq166moVMgdp/r1zbfdinDi65DjRBAEQQAknIgAUJcgt+M4WRFO4chx4h2mSMxxio0FWrSQlyckyO22M5eTkxPgqjvW6u054R5GimDREwT+CiSEGjHHKdihelro5T0VFbFYzTvuAF59VTt0829/Y+8tLlaeUJs3M8v60qXtJlrNoFA9giAIwi4knAjb6DlOHo8Hc+fOVXRg9Rwn/rx6df/7E4WTnXLbZihrOU6NGsnL4+Odd5zsVtUzcpwAZ9zDSBEseoJAb3m4EB2nQItDmAnVU6M1QfC//iXPxisKdjVa8ydlZGRg/fpdAICnn37AtNtIwokgCIKwCwknwjZawkkvdEp0nC5ckN9rRTjxdYqLg9dxKUuherGxQNu2LEypdWuWGxKIcHLScTLKcQKccQ8jRbBoCQKtam7hJhyhemrUEwRnZj7vfc1IOKkFi+w28ooyF0y7jfwaoRwngiAIwioknAjbqDtLGzZs1gydysnx+EyOy4tCWAnVS0iQO+L+OtySZL2gAaBfHCISQ/ViYlj7du4ENmxglQkDCdUzKkfuVHEI3gF1IlQvkgSLWhCIeTiRQiir6hkh5j2JgtyMcLpwgZ2LsqvIhdNFAObcRn4OmhF76hwnLeeLIAiCqDhQVT3CNuqOx65dBzXX27p1GwBlZ/bcORauZcVxAliH+/x5JpzE/B41t98OrFgBbN5sviy3JOnnOEWq4wTIHUEg8otD8O05VehDXbktnC6P2+2OOJdJJBg5TlZC9Yy2A5gTTgAbdJFdRS6cWP17M24jheoRBEEQdiHHibCN2tWoVauh5nrNm8udmcRE9sg7cXaEE+DfqfjxR1aq2Eq6i9iRiuQcJy2Bw+HtDndxCLOOkxOFPkQHg9DHyRwnu6F6avh5FRVlPJ9XbKx87zh9WnQbZcfJrNvohHAK9PgRBEEQZRMSToRt1J2lRo3SNEOnLr/8Cu//VauyRy6crITqAbLAEgtMaMFFgJUOnbiuOscpkkL1tAQOhztlkV4cIhRzchFKIiVUD5ALyKxZsx6AsdvEUbs9zz2XBYBdqIsXf2k6PNKJeZzOng1egRqCIAgicqFQPcI2WsUhtEKnxBChKlWAgwflHCerjhPvqF28aLweFxdWnBct4VRWHadIKQ6hV468Vi32eFA7urPCsXw5m5Orbt3g7SPYE+BKkv9JrAH1ZMHNAOwwLZwOH5aFU16e/NqVV3Yx23SFcPLXZr15nIqLWb6VemCAIAiCKN+QcCJso1eOXJ3rIRYWUDtOVoVTUmlkjlnhZMdxio1loUP8ORBZwkksR67GieIQWjlOdotDJCWxY1lSwv7nnVY+yenGjawjLOawVDSWLAH69wcaNgT27g3eftQ5TmaFjhbqUD1ejMWfAPKde4udEFFRhQCM36wuzCDeA/h5bwYxJ9Bfm9WheklJLKSQV/Yk4UQQBFGxoFA9wjZ6wkmNWFGPd37shuqZFU58n3aEE3dtgMgM1dOqfseJNMcpLk7ZUeXbq18faN6cCaqVK623VY3W3GFlhfffZ4/79gV3P6LjVFys/N8qWqGYZq4R36p37IRwufwrcy6u77oL+Ne/5HtAYqI80GEG8fr212b1teZyUYEIgiCIigwJJ8I2aldDr7POO+Rip+PcObb+hQvs/+rVWed3ypQpmDJlim4HOJiOk3ryW6DsOU7BrKq3bBkwbZq5bekJJ/F5r17s8eefrbdVRG/usFARqGjbudPhBumgFkqBFDjQCsU0I5x8q96xEywqqth3ZRUjR7L9nTkDvPSSPOjC7wlmEc9Bf9eJOscJIOFEEARRkSHhRNjGquMUHS13tM6elcP0oqKAF16YiPT0dGRmZiIzM1O3A2xGOEmSvE87OU56wilSksGNHCen53ESi0PcfTfwwAOsxLs/+LHUc5wAZ4STb+gXTE+E6gROiLZQCCdJkoUTD88LJM+Jf79JSfL2zIh137m3YkvbctTvsbvjDmVRmKNH5TaYxePx4KOP5sLlYhezP7GnznECSDgRBEFUZEg4EbYxK5xEh+T8eVYNIDf3kHfEuFKlQvznP1k+79PqAJsRTmJooJ1QPTFfgnf6RTEWboLlOPkL1eOlw81UwrPiOK1daz9sTG/CUzMToQaKE6KtpER5PHkumNMUFMjfb+3a7NGucCoultsZH+8bzurvM2RlZWHGjBml//ETogDZ2dmGbjPAzkd+DzhyhD2aFU5c5N5110hIUr6izXpoDSbQJLgEQRAVFxJOhG2sOk7Fxfn48EMW6/XRR9/gueemAwASEvRVkLoDbEY4iYUMAs1xEjtMkRCuJwo4oxwnp4tDFBbK4oZXRDTCX44TADRuzP6Ki4FVq6y3F9Cf8NTMRKiB4oRoU7tN/kJQ7SIWhmjQgD3aFU7q+c7495ufD3z8MXNkvvvOeBtx3pOCnxDs5DNymznctbYinGbOnKkSuexDrFmzwfB9FKpHEARBiJBwImxjVjitW7cBAFBYmAeA9+BS8ckn3wMAatbUPw3VHWCrwsmKgDDKcQIiQziJn82oqp7TjtO5c3KootgJ18OM4wQA6ens8Y8/rLcX0Ar9gumJUAPFCdGm/tyBFGwwgm83MVGeQ8uucBLPrbg4+XopKGB5cOfPs/LqRsjHSCmcOEbOHRdO/kL1eO7ZyJEjMXbsWNWr7ATdvt24jKGW40ST4Aaf7duB/fvD3QqCIAhfSDgRtjFTHCIjIwM333xb6X/FkIVTJQCslF7jxqk+nV/+XnUHOJyOUyRU1hM/m9NV9YwcJ/G7dspxAuROfCCj91lZWcjJycGcOXOQk5NjeiLUQHFCtIVaOKWkyFMC2O34i9dBbKwyVI+/Js6xpIV87LSFE6Dv3PFKfkaOk5h7NnfuXI2tsAukfv2mhu30l+MUKXmP5Ykff2RzmvXoEe6WEARB+ELzOBG28ec4yTkgbUuXFAHgve5UAGzypmrV5IlzFy1aBAAYOHCgZgeUd5J4NT4tAhVOYo5TdLQ8D1EkOE5iG8zO4/TMM6wzsnix8Xw3Wo6T1j6cdJx4vkigo/fqucNChdaEz1YIp3AKNFQvLo4VhhBD9cwKJ4AduypVcvHEEwB3gET0nDt/oXpauWe+sP21atXecC0jx+n554F33gFWrADatPGzO8IU+/cDgwez53v3snNKHMgiCIIIN+Q4EbZRT4KpFinyiDHvfYuOUyquuGIQAHnyW7fb7S1HDkCzxHMwQ/W0HCcgskqS23Gc3nmHVa7bsMF420aOk4gV4RQfrz0vFscoX6SszM3kdrsxYsQIy6Lp4kVg9WrlsmAJJ/6dicJp61Z7jon6PBFD9fj3bjZXq2lTJo5q11bOgG3k3KlD9RITla+byzFjDX3tNeP6+kY5TgCr8jdrlondBZGjR4HRo+3nCUYSTz6pvGebudcQBEGEEhJOhG1457xSJeX/kgQcPiyOGEeXPhaBC6cGDS5D+/asrFp1ZZ/Jp8TziBEjvB3oYIbqaeU4AXKnKRKEk9gGrUk/tYQTd+fMzlmjleMk4mSonp7jZLfMd1kQW8XF7Pg8/TS7Tho0APilYuSkBgIXZKmpwN/+xp5/+ilw223WxZM4vQCgHap34MAJU98DP+eOHJFn/73zzjsNwy39OU7mcsxYQz/++HPDNmpdE/x+x1m40MTugsgXXzDx9sor4W2HE+xVpZyZudcQBEGEEhJOhG3UwokLj9dfB+rWBXbv5nkMsuM0evRNAICCggTvnCzVqsnb1Aqz+eCDD7wd6Jkz3wAQuhwnQO40RVKOU0yMPH+OiFaoHu+M+3PftMqc+3OctISKJFkP1RMdJ7tlvsM9Ea4ZLlxglQQrVQJefZUtmz4dqFWLPQ9FqN711wNvvsm+23nzrDsVauEkinX+vS9dusrU97B1Ky8rKI8IfPDBB4bfNc9x4mXc1cJJK/fs2muvxX333Scs4RdznKFDpZXjNHgwcOWVwAsvsMGLTZuAPXt0NxF0+H20PLgz6sGp8vCZCIIoX5BwImzDO+I8dIULj99+Y49r17I8hv/+9z0AQIMGdfH0048CYD+I/AdfdJz8hdnMmcNCa4IpnNR5QJEUqmc0+S3g6ziJ8/c45TjxzoyeUBGPvz/HSatCmZ0y3+GeCNcs27cDBw6w76KkhE3qOniwLAZCIZxcLuDBB4ErrmDLDh9WrnvpEvDQQ8D332tvy8hxOn6cK2A5fs7oezhwoDTeTpXjZPRdc8eJzxelVRwiKysLI0aM8P6/ePFiXFDYebJwMnKotK6J+vVZ1cCJE4Hu3dmyb7/V3UTQ4YMOwSplH0pIOBEEEemQcCJsoxeqx3/IuTBq1YoVh0hOTvB2evLy5LCMOnXkbfoPs2G9g4qe46RVtAHwncdJ7Cv6E05mi0OcPWssVNTlqq06TnbKfIdzIlwr8O+lTh3gq6+AmTPZ/8EWTmKOE0evSMQPPzBHihVt8MVIOJ09yy84ZeKR3vdQrRq/+JUXl9F3ffassk71yZO+dau5EyrywQcfCGKKCacbbrjZMDdNK8dJZBBL0wxruB4fdAhWmGcoUd9jKVSPIIhIg4QTYRu1cDpw4Dg8Ho+3E3z8OHsUO1pcOAFyeMupU+u94V5aYTZKmGK6dEkecVYTrBynSAjV8+c4qedxEjtT/kSkleIQeh3hRYsWKY6THcfJTpnvcE6EawVeba5qVeCGG+RzLVSOk3j96QknHgJ38KD2toxC9WJikkvXUtpAet9D/fpNSp/JPWaj7zojIwPvvfe6YtmMGa/7hAPqnZ/9+vVDTk4O2rVrCQCoW7ex5RwnES6cfvopfKXJy5PjJN67AXKcCIKIPEg4EbbhnfOtW9cAAHbu3I/09HRs3nwAgOw4iS6JuiMdFVWMoUP/pgj3Gj58uMFe5d6BXsnj8hyqZ9Zx0hJOdhynqCjfIhRnz+p3hDMzM/HUU1MBsJCw6GhzjtPZs0ohbHVupnBOhGsFLl7V51hyqd4IRagehwunkyeV63IhdfSo9uAEX8bPC3FgIS6O70B2nIy+B35NDR48wO93Lbuc6oN00Scc0EhIu91uXLzI1Mb06TMN87C0cpxEWrYsbcHF8HXy+aBDeRBO/HzgAyrkOBEEEWmQcCJswzvif/75S+kS1ms/epS9wIWTeoRarEpVUrIHrEw5Izs72zuXkzayWlqxYp3mGlZC9dasATZuZM/LUqieP8dJK1TPjuOkta9z57SFCuftt98F4DvPj9a2eAdJknw7nlbLfIdrIlwrcLGvLqHNBU2wq+qJwokXZVE7Tvz/4mJfUcWXA8ZV9erUaWrqe+Dr169f0+93LbtIaoVyUfW6sZD2eDzYuXNz6VJ2sevlYflznJKS5Bwr7rCHmvLkOPHjzc9NcpwIgog0aAJcwjZyR5wPC3LFwZSROlSPd8hTU8VOxm6Le5XAxFMiNm/eg2uv7eKzhlnH6exZoFcv1p4jR/SFUySG6jntOEmSviiLjVW+l3dmsrKykJiYiMzMTNXW2AHjx81oHqeEBPZ6fj7rAIpz5NghXBPhmkXPcQpVjpOZUD3x/8OHgRo1lK8bherx86S4OF5RnEEPf8JERHaRtIWT2mXSm5yYCSy+Q/mEzM3N9Tl3/OU4Aez47N3L7mnNmvn/HE5THoVT9erArl0knAiCiDzIcSJsI3emeWY/7yGz3u+pUyysh3fIeUdL7LxpCaeBAweaynOqW7e55qtmhdP+/awje+wYe49ejlNZcpyMhJOR41Qsm34+okz9vxg+M3DgQI2tKYWTkeMEaOc5lVf8OU7hCNUzEk58riQRM46TXhitGivCSXaRfEP19MIBtVxLJrDkqnrK5dbbx4Ulzw0LNfy6ycvTz/ssK/D7Gz83xXtNuHLICIIgREg4EbbhnfNrr+1RuiQeLLeB9TJKStiPupbjxOnRo4Fim7wDpA67UgopJpxatuyo2S5RBBiJBbGjc+lS2chx8uc4GYXqGYlI8bNpOU7qdfm2tEKiRo1i8+VwEWeU4wRoV9Yrr4TLceLngVi6m4dDqcPxxP/tCqeLF811dK0IJ4C5SDNnKotD/N//ZVsKy3S73WjfvnXpf6zxesLLX44TIAsndajeqlXM0ebTMwQL8boxK1gjFb1QvXPngKZNgVGjwtIsgiAILyScCNvwzvPdd98IAEhIqIwFC35RrHPihK/jJOY43Xtvf928FHG0WBRSjRqxnopeaIpZx0ns6IjCKZJD9QJxnIyOhXjM/AknQBlCoxa599//CABtx0lLOHHH6csvfwrLvEtak/gGi3A5TlrnjdlQPTVmJsAtKTE30CBOlGyWbt3aKf7v0qWN+TeXcuWVbBKroUNvMczDMiPsatZkj2rh9MYbbL6njz6y3DzT5OcrB4eshut9+inQuzcLi4sExFA9QL7PLFzIqrDOnh2edhEEQXBIOBG24T/YXAgVF8fi/PloxTrHjxs7Tk2amC8C4Ha7S8NpWO8zUOEkOk55eWWjOITZHKeCAtZ5NRuqJ342f8UhAN9qV+J3qO4M+wvVO3aMJfW/+qpxhbNgoDeJr1msiq5wVdVTX4NA4KF6WlX1xMEFM+6HVccJUIf6ak+A6w/e5jZtmGut9R2KeX98fa3v28hxAmQHLxgCXe3SWhFOhYXAo48CP/8MTJ7sWJMCQi9UTwwxjYQBLIIgKi4knAjbqOdxKiwEbr31PsU6Wo6TWjiZhXdy9+5lFbGmT5+juZ7ZqnrqUL2ylOPkL1QPYB0MO6F6ZoSTUdK2kXBSOwsejwe7d/9e+l8VAPoVzpzGaBJfM9gRXeGqqqe+BgG5c3r6tDI3xp9w4usaheoBwRNOYica8D2WZuBt/uGH5brfodqF1fu+tYTTvn3sD2D3wEAFuh6BCKcvvwQOHWLPP/6Y5XyGG71QPfE7D1cuGUEQBEDCiQgA3hFXVkKrqVhnxowvkZu7A4Cv4xQTA9SrZ25fyk4u6x189dV3mp3cQEP11G5AMEL11BM9msVfR1MUffn51h2nmBhWQlxES6QZza/Cj6MZx4lVODtd+l8V1fLgorcPM/u2K7r0HKfdu1lN/JMnLUw8ZgEtwc2FkyTJHXBJsl4cgp9zly4pBbgZ4WQnVE8tnAJxnH799XfF8uzsbMycORNz587FypW/epdv2PCr7vetJZxWr5af79lzNiCBboS6oIoV4f3mm+wxOpqdH//3fwE3R5dvvgGuuQZYpz2DhBe9UD3xfknCiSCIcELCibCFJMmdHjFnCailWO/rr3/Bs8++AMA3x6lhQ33nRI2yM8uHVZM0O7l2QvWMcpycdpxee42JzZwc6+/15ziJHdBLl8w7TkYd2GA6Tiz0kg+bV1YtZwQrB8loklR/2BVdXDiJLklGRgbuvvsmAMDhw+eDEqqoFaoXHy+LDi6WLlxQXj9mcpz4d6rutJtxP+w4TjExyuMXiHASq+pxxo4di5EjR6JPn37eZbt26X/fWlX1Vq6Unx87pl3qzonBAbuO06ZNwIoV7DvkAuq//w1OVb6XXwZuuAH48UfjHKWSEnn/3HHiAzTivffoUefbSBBlhXnzgI4dgS1bwt2SigsJJ8IWovuizDmopVqzOgDWwzp79qRifSthesrOrCyctDq5auGkV91LHCEOZY7T8uWsgyOOSpvFX0fT5VIm61sN1dParriMu4t2hZN6+263Gz17ti/9rwoAZYUzdYiTmbmBzGI0Sao/7Iou7sJwx0l2rnhyU0pQQhW1QvUA3zwndYU9K1X1PJ6NivWyst7w2y47wglQ3nOcFk4ycqPatGmhucaOHTtw7NhfAJT3E57fBACXLmk30IxA94facTIrnP74gz127w7cdZe8LadDRffsAf71L/l/HhqohXjfVofqib835DgRFZlbb2XX70MPhbslFRcSToQtxLCv+HhRbGgJJzbMffEi+xVMT2cdpQEDzO9P2cllvYOrrhqo2clVh8HphdipHSe9/BPeyXJKOPH22SkE4M9xAvSFk5lQPX/CqW5d9mgUqqcWTkYT4ALAzTezE+GKK/opKpxphcN98MEHGDlypP7OLaKuCGi2rLVd0aV2nGTXgZ8M8QBiHQ9V1HKcAN+S5FxA8cIPR4/6uhB6oXrbtintqblzP/MrAO0KJx6uJw4UWEEuDtHJYC3eqCJ89dV8zbnlMjMz8cADzC3kwunCBWD9enmdc+fi8K9/2RPo/rDrOPH1KldWho06PYnunj3KgSt1IRIR8f5KjhNBGENFUsIHCSfCFqJ7IQqn/v25I8DVSw1wx6lyZdbb6dmT/SBajUjindy+fdm8UVddda3memrhpOe0qIUT7zSoR7B5p86pGxVvn5Fro4eZjqY4l5NVx8lfqF6dOuzRKccJkOdxqly5ibczKUnA009XAuDbWXU6bM9sVUc1dkSX2nGSXQdxqD/ZETdCRE9wqx0n/ti0KXssLvZ1ofQcJ0CVfIREvwLQTo4TIDtOSUm+OXlm4PerunUbGazFG1WI7OxsDB8+HDk5OZjsU4KOKaaTJyUUFwM//MCOES9TXlgIPPWUPYHuj0CFEz9+/J7ntHBSCyUjt0hLOOXns+Xia+Q4EYQ6t5wIJSScCFuIBQDEUd+SEhbwX7ky/0WXQ/Vq167ufb86Od4sbrcbl1/OJq80U45cbKuIJPkWh/AnnMqz42Q00ae4L+44OZXjBMg/ALwT6PF48OKL3+D779sAeA78/BEJRfEIM1gVXeriELJzVQiAHbj77/+XI26EiNlQPf5Yq5acoK/Oc+IOlLocuZZw8icAAw3VsxOmB8htPnXKSCnwRrFGLlq0CG63G82bN1etx5SlJLlw6hTw3nts6ahR8rV48qR9gW6E3eIQXMDz4xds4VS7Nns0covE+ys/LwF2r9FznDweD6ZMmYIpU6Z4B1NCOS8bQYQLEk7hg4QTYQu9cCw+Gti1K+t1NWjQEePGPQbAt9NmFz7njVnhpCUYzp9XCqq8vLIlnIw6mmKVM6dznKyE6vF2iGJJS/Bxx+n0aTmn6YknPuPvgG/4pzP5IeFAKxyUO1fJySym6eGHn3B8v3qhenrCqWpVubOrznPSC9UDlBMsXXfdrX5FQqCheoEKp/j4VIO1eKPYCZ2ZmYmMjAyNc68IADtwf/7JJmsFgLvvlsXniRP22umPQB0nfh4GWzjxQ3b8uH7OKb+3RUWx74cPLpw9qy2c+L0iMzMTmZmZSE9Ph9vtDkrZd4KIBMSoFxJO4SPswumtt95CkyZNkJCQALfbjTVr1hiuf/r0aTz44IOoW7cu4uPjkZaWhm+//TZErSU4aueDjwDv3s0emzXj61VF/fqNFOsGir8feTOOk3qyygsX5PXUnTGny5EHIpz8TYALyB0Op3OcEhJkkWPWcfJ4PFi+/AfvdrTCqvgPwPHjBUJOU2fv6926DVes71R+iBHBGrXWK0fudrtRrRpTIMGYBFfPcdLLcbIinHbv5u6f0nHq0MH/dxRuxykxsYpm7lJpq0of5V47Pz/V76lalR3gV19lx6d7d6B1a9/j6zR2i0OoB4mCLZxatmSPRUW+beaozwX+HZ8751scQiv/EYBP/yFUc8IRRCgQB2D4ADIResIqnObNm4fx48dj8uTJ+O2339CxY0cMGDAAR3X8/IKCAvTr1w+7d+/G559/jq1bt+Ldd99F/fr1Q9xyQi2cWrVij3wElAsnrQlwA4X/yOuFpZgRTuo4efHHXF0cIliOk50cJyuOk5WqembKkScny6XkjRynHTvYzJ+rVy9Deno6/vOfZ3kLNNeXxZh4giiFUzDyQ/QI1mSlgH4BEkB2UYIpnOw4TurbsVo4HTt2oPQVpXuzb5//ZBQncpzsIA6GZGVlYcaMGVprlT4qL/zc3Fyf/LbWrVlC0//+x9a5+272GCrHiQtxUfgYiX894WRm7i0r8POpTh353qGXo6QWTnx9rVA9K6G6kRLWSxCBIg748vswEXoc8gDs8corr+Cee+7B6NGjAQDTp0/HwoUL8d5772HixIk+67/33ns4efIkVq1ahdjSu2sTKzWtCcdQh/60bQt89ZX8OhdOhYWyKAmX4yQ6LZIE/Porq/YkIo4Ih0o4BdtxcjpULyVFOQqsRUZGBqZNSwCQie3b/ypdWlC6j4vweDb4uEXccSosjAarKpcPoJPweZrA7W7muMvk8XiQm5uLtLQ077b1JrZNS0tDu3btfNa3ip7jBARXOJkN1ePXQbVqckiVutOvFk5NmpTGb6pKe1euXMdvu8LtOMnCTUu5+TpOgBwm6na7vecBn8sJYAMBt9zCngfbceLCqV49YOdOdr17PB5MnToVC3nMIJhDlpWV5f1fLZz4PS9YjlO1aqxYxtmzTDhpRdqqB4X4d6wO1WPvNx+qW1bDeglCjTjoQFX1wkfYHKeCggKsW7cOffv2lRsTFYW+fftitc4EN9988w26deuGBx98ELVr10a7du3w/PPPo9hAeufn5+Ps2bOKPyJw1CPYbdsqX69bV3Y++Ii1046TnVC9FSsAtxu4+WblOrxjk5AgJ71zIilULxzlyPm+RMdJSzjJooN3Qgt8HrVGfytVkoXEmDFTATQBIGeHJyY202+4TfRcJb3R6bFjxzriQhk5Tjz0IhyhelqOE3dL1GGtauHUoUNrzX3WrNnYb7vClePErxF+TWt3rpU5ToB+mKgonO65R25fsB0nPijFcw9/+smD9PR0hWgCfEPWQl0compVucqgXoEI9aCQOEgjCqdz54COHX2nA9AiFGG9BBFsli4F5sxR3oudGsglrBM24XT8+HEUFxejNo8HKaV27do4rDVdPYCdO3fi888/R3FxMb799ls89dRTePnll/Hss89qrg8AL7zwAipXruz9a9iwoaOfo6LiTzhVrix3DnnnPRKE048/ar+Hd2y0OmKRFKpnpqMZyAS4RqF6ouOkNf4giw5eLYB3OHcCOAfgD80OalQU0LUre96z57/wwgvfKV7XuR3YRs9V8ng8pken7eZOmHGcnJ6EFPDvOGnlOOl1+vm2fKvqKTHTCY8Ux0lrXq5bbmFTKzRv3thvmKh4jooTU+o5Tk7l0HHHiQunP/7YpruuOCgQ6uIQonCyE6qnHrQ6dkwuqjJmzBjN7c2YMSPoYb0EEQpuv51NVJ2TIy8j4RQ+TAVPvfGG/xngOQ8//LDtxvijpKQEtWrVwn//+19ER0ejS5cuOHDgAP7zn/9ozK3BmDRpEsaPH+/9/+zZsySeHEArxyk6Wu5UVaokd+D5j3EkFIfQS4fjHZtQCqdgOU68Uy4WvADslyM3G6oniw6143QKQEM8/vgDcLuf19x/t27AL78Aq1cDtWqx7cTEsM976JB+u+2g5yrl5uZixIgRmDBhgmbiudb6Vkezy1KOE//e1cKJlyP3raqnxEy+jN0cp44d2WP79tbex9FykbOysjB8+HBvOOaJE27MmwdUrpyEESNGaG8ITATVr18MoDtatjyKgwd3oWFDdl5oic+MjAzF+aUOo7MCd5zq1eNL9JWkOCgQquIQ/L5qRzjpheoBzLVq2FAOl6xevbrimGZkZOgKKoIoS+Tnyy7t8uXychJO4cNUV/bVV181tTGXy2VaONWoUQPR0dE4oirZdOTIEdSpox0bX7duXcTGxiJasC7atGmDw4cPo6CgQDNOPT4+HvF2ppYnDFF3xOLjgRYtgK1b2f+VK8udk3A7TqJgUN9s4uPZjYl3GLU6tJEUqmfFcVKPchcUsLwVrcp2ZnKc/IXq8VH77GxZOI0YMQL9+vXzmxfUvTt7XLVKLkpw5ZXATz85L5z0XCW+PCsrC2lpaRg7dqyt7RgRrhwnvVA9nl/GHURROPHzyF+Ok57oMSOc7DpO114LHDwoT8hsFb1rWsxb+vpr/bbx/LglS5Zg7ty5YD+l/bBt249IT8/3iiG146Tndg4fPtyyCJck+Xv7/fdvAQyCnnBSh6yFuqpe1apsbjBAXzipf1P0ikMAvuF+atFL4XlEeUG8XjZskJ9TjlP4MCWcdu3a5fiO4+Li0KVLFyxduhRDhw4FwBylpUuXYty4cZrv6dGjBz766COUlJQgqjROJDc3F3Xr1tVJ7iWChdYIdtu2snAKp+OkTnkTXRe9UZpwOE75+WybVjqNVnKctBLS8/O1O+1OhOoBrAPz119HsWAB8MADY/HWW430GyrQrRt73LQJ2LiRibtRo5hwEsOgtAo6WMXtdmPo0Nn46qtvAXwP4JQ3Z2nu3LlIS0vDmDFjkJubq+s82cmdKC6Wj3MoHSdJ0g/VE6skSpKyo8vbGInCCZDD0+zA22wnfFXtGDGKACzy/sfFULVq7Bzhx/Cbb04D+AzAowAOeNe3617y7+Lnn+eBCSdljeLBgwfjqaee8tl2KHKc1OeT2RwnLceJu5wcLfElil6CKC+I57rYtyHHKXzYznEqKCjA1q1bUaQe3rfA+PHj8e6772L27NnYvHkz7r//fly4cMFbZW/kyJGYNGmSd/37778fJ0+exCOPPILc3FwsXLgQzz//PB588EHbbSDsoSecANYBjI72FU6bNv3hyJwagYTqia/VqgX84x/seThynADrnWQzHU0ujHinRVxXr6NoFKonFofgnZn8fP0Rr8qV2dBy8+bmRBPAXKamTeVKboMHM8cJYI6TJFkrE26UQ7JxI/DVVyMBfILo6GN4/fW/IEmSz7aHDx/uu2HYz50Qnc9QOk5ip1PPcSouZtcTD/2qUsV/jpMToXqBCKdAMOMia7VNb/4gLXJzc73HkA9iPP/8AAA3AlCeP3bcS2XbT5c+yjewyZMnY8GCBZpiIhSO08WL8jEMJFTPjONEEOUVdXEeDgmn8GFZOF28eBFjxoxBUlIS2rZti7179wIAHnroIcudiVtuuQUvvfQSnn76aXTq1AkbNmzA4sWLvQUj9u7di0NCnE7Dhg3x3Xff4ddff0WHDh3w8MMP45FHHtEsXU4EFyPhxDtjvEN14ADrwX/99eeOzIsj/shrzUJvJlRv9Gg2sWe7dsrlWsIpWKF6gPVOshXHiXd4+TxJgL5wMurA8mpvVarInRlAv7gFP07+ImTV4oaH6wHAgAGbvWFYly4BP/64Vreggxp/AmvnTvl5cXE0NmyopLntRYsWQQu77na4hJN4vqnPm6QkucjD4cPyupUry8IpL08pgsw6TmY64XZznAKF7+/cOWDzZu1OiNZggpU5gdLS0ryher5V9ap7n9mt/Ka8z3ELWL6BDRw4UPe9oSgOwQduoqPZuW1WOIkDNbxN6nvvgQMgiAqBv+uFCD2WhdOkSZPw+++/Y9myZUgQfv379u2LefPmWW7AuHHjsGfPHuTn58Pj8Sh+QJYtW4ZZs2Yp1u/WrRtycnJw6dIl7NixA0888YQi54kIDVod+KuuYh1r3gHmHeezZ/kvPOtxBTqbO/+xF0OftNrG0QrV450hdQc21I6T1cp6ZibA5Z+Jj3KnpMgdRb0CEUbCacwY5szddx97nW/fn3Ay6gxriZujR+eXvvonHnroMkyZkuEV4WvXaveU1B1Zo4p5HPUI3uHDp/UbqoHdeWG4+IiN1c734x1Fp6vqieEdauHkcsnhevvYvMWIipLDMvn6YsdfXVWvLDpOvM3FxcBllwEvvOC7jtg2LvILTI6ecDEkOk7//OeTwhp7MXjw4IAmdObti4oCRozg7miSYv96hMJxEudwcrn8Cyf1vU2sDso/K5+2ccsW59pJEJGM3vVCOU7hw3LWyVdffYV58+YhPT0dLiHLvG3bttixY4ejjSMiF61k87p1mYvDf/DkDhWPu5cVA+/w2slXEcVOfr5vB70sCSe7oXpWcpySk9nnLCjw7zhpiZ2mTYFp0+T/K1ViAsyucNITN6wa3+MA5nuXNW48BWfOJCI11bigA0fPEXj//fcBsDwI9Q9RcnJ1jXewEfu8vDyfal2BTn6rld8EhMZx0hJslSqxED0unCpXlguIVK/OrukTJ4AGDdgydVU99ffMC674E06SZG4gIBhUq8Y+GxeEv//uuw6/JnJz/0R6erp3+RVXXIE1a9Z4/+cFUHiBIvF+xh2nkhLglVd+ErZ+BgsXLsRTTz1l+zOIg1cZGQ9h7lwgNbU2lizJ8XuOhlI48cqNonDSKlKjvjeLeWj8nla16j7s3t2QhBNRYSDHKfKw7DgdO3YMtXh5HIELFy4ohBRRvtELGUtM1BqJ5mpEHvpesmSJ7QlFxRFuLQfFKFRP3W61cAplVT3Afqiemap6vFOYnCwv03OcjHKc1PgrEOFPOOmHOxUAeB6A3CtKSmIT1VSt2sZnnh0tEaPnBr3zzjve8+yLL35WvLZ16z7NyTTnz5/vnSvG3zw+ZuBCQitMDwiecDJynADZcSqNuva6fIB2npO/UD0eGupPOInXQaiFU0ICq1A1ZQr7X+tcloXTJsXyNWvWYMaMGd5zYs6cORgxYgTGjBmDtLQ05Obmeh3OhARxMKan2AIAwKJFi2zP5yQKDe5WFhUl+BVNkhSa4hB6wqmwUJ5/SkQtnETH6ddfNwAA1q+fBQDYvbvE8QqABBGJkHCKPCwLp65duypmJediacaMGejGS2MR5R4ruTbCuwCwEVpWwlfGSvheVJRx6JkVx0ktlIxynIwqcFkhkFA9M46TOlSPO06AvRwnNUZzOQH+hZOVULeGDdkHPXQIpkSM1kSmItnZ2fj1V57kxMr1/flnrmab+DnpdrsxYsSIgCt2RbLjBCgdJ44d4cTf7084iQMR4SiK2qAB0Lkze651Lsvt8+2hxMXF+ZwT6vDTIUOGwOPxoFEj/oXeJWyBnQSZmZm2Bo8A5SAKv2/l5flWoFMj3jNDKZwSE+VzXKszqDXFBQDs3n0QW7ZsL13rEIDjAKLw+ed/OtdYgohQSDhFHpaF0/PPP48nnngC999/P4qKivD666+jf//+eP/99/Hcc88Fo41EBKJX3lhELZzuvPN25OTkoF+/fprrW0m8NhIC/AeYG6CBhuqJI59OEKriEPw7Eh0nO6F6asQS1lrwfYjbEgtBaIkbPvGsSEZGBtq2rQFALkluRsRwgXXffffprFE69I1SpYBE/Prrr5prWjkn/REux4mfM1FR2nN4BSqcXC6l4Obv99cJF3/4Q+04cYzcU7l9vj0UtdDWCj9duHAh0tPTsWXLm6VL2gqv+qpnq7mf4iCKeN8ymugaUH4voSgOwYUTIB9vrXNcz3HasmUnlJNqbwYArF592rnGEkSEwoVT27bK5SScwodl4dSzZ09s2LABRUVFaN++Pb7//nvUqlULq1evRpcuXYLRRiICseM4det2Bdxut98JSM1gFHrG28Y7A1ZC9SJdOFmZAJcjOk5OhuqZdZy0CkFkZWVhxIgR3vdwB1LtKPHwHr2SrHq43W7vtAa+1Ch95MIpAX/7298017RbCEILo8lvgeCH6uldq1zo8FA9sQojF07i8VcLJ0Apks06TpEgnIwmdObt69jxMsVyrRBRY4G9xGdJmzaXa65pRaiL9wLRxfQnfvjrcXHA2rVsQGPPns2m3msFLeHEzxOtTp/63rZ7d+mkgIgHwE+QQnDhtGlTiSPTWxBEJMOFU48eyuVUHCJ82JqStHnz5nj33XedbgtRhrAjnPi63HEIJOneSAjwtiUnswplkeQ4lZQoS6jbrapnJlSPY8VxMtOBNepsAkrhpFcIIi0tTTNcc/jw4QpB5S/E0Ai98+ydd1qWzle0HwDQqFErjBkzyGfCW6Nz0s5EvFxI6IXqBauqnlYhF5FAc5wAdn7xdlsVTtHR2k5YKDDjOF15ZTreeSfH8Ps2FtgrAeRBdJmqVtWevdeKUBfvBWvXehAb2wWFhTGa4sfj8XjL61922TAAHeFyXRSKXrgB5JiqhGgWHiosCiejQjvqMOQjR/YBaAU94bRixTGkp9+ECRMmICsry7mGE0QEwYXT6NHsPly/PvDGG+Q4hRPLjlPfvn0xa9YsnNWL0yEqBHaEk9jRCjTp3qxwAqwJJ61OrZPCSZ1/FQrHqWpV/8Uh7OQ4mSkOoTeCbjY0LhDhBGifZyUlTA3ccgsbwqtXr7nuulpw58xqbooVx0lrfjK7+LtW1aGXonCqUWrOGZUjB/QdJ6PPEa45nETEQQB1W0UX1l+IqHFuXT6AFYol8fFVTRU7MYJfs+fOnUB6ejoKC1nFhWeeedlnu+np6cjMzERmZiZuuYXlWuXnnxLWYorpzBnnhrG1HCcj4aQufLN+fU7pK9rCCWgDIPDpLQgiUikqkq+jZs2Azz8H7riD/U/CKXxYFk5t27bFpEmTUKdOHdx00034+uuvUUjfYIUjEMeJE0jSvZkcJ94RFddRt9tMcYhIEk52HKf0dPPFIcx0Yq2E6umNoJsNjfMn+MwgnmcFBbJAGDqUhRaLo+z+zslACpv4c5z4+VpSEtjnVeMvVI+LB44/x0ldjhxQnjc81E+SjMNJwjWHkwg/lyXJ1+mzck0AsvAePHiwxqs/KP7Lywt88IjfC06f5nGUzGqaOfMj7/k4c+ZMH8dXrnIqWlPsebDmceKYcZz4vFlLlvyv9JU4cOHUunULyMKpJQB2EjqZi0gQ4UCSgD/+UF4bJ0/Kpfv5vdjp6VEI61gWTq+//joOHDiAr776CsnJyRg5ciRq166Ne++9Fz///LP/DRDlgkAdp0Cx4jiJ6wQaqheoE6AWTnar6llxnK66ytly5GZD9eLjtUfiMzIyMGbMGFMj7k7nl/FcnehoNu8YoH1MZs8GPvxQuczj8eCDDz7Q3K6Zjps/x0k895zMczIbqsfRynEyE6rHEYWXUUfc3/EIBUlJsnOmPp/tCDu3240FCxYowk0B4Oqrj6JBg0vgkXFcRAcyeORbvIIf7CTk5uYiIyMDY8eO1XgnP9HEuDz23oICW9H7mlh1nMTjza4nftHHgxeHuPzydmC5iRdKlzUB4GwuIkGEg3feATp2BO69V17Gw/SqVfOtYko5TuHDsnACgKioKPTv3x+zZs3CkSNH8M4772DNmjW4+uqrnW4fEaEE6jiJVdbsYKY4hJ1QPSPhJL7fLmrhtG0bMGwY8MMP2uvrvd/scW/bls2fEoxy5GbncVKPrA8bNgxz587F8OHD/Y64B0s4Va+uLOEskpcHjBoF3HknsGePvNxIHJnpuPlznKKj5TY5KZycdpz8FYdITpbFyMMPAzk50MTf8QgFLpf++WzXEeP3NpEff5yNzz//HTwVxwlHUb6X8CfcMquHgoICDaeJw06yGjXEmx0TTsXFUY6NZLM8QuDgwb+893p+LLU6fWKOE7ueROHE3uh2X44JE/4Fnp8I1A9oUmqCiBT4vWHWLHkZF068SBJAjlMkYEs4cQ4fPozp06cjKysLf/zxh274DVH+CMRx0qqyZpX8fBbPv3Hjdp/X1FX1jEL1rAqnQDvwauH000/AV18Br75q7f1GnTnxM/XqxR6dLEduZx4nPrI+f/58xXc/f/58wxF3p4UT/yGqUUPusKs7sWLI1uLF8nM9cWTWMTDjsASjsp7ZHCdOoMIpPh6oXZs9/+AD4PHHtfcbCY4ToF9e34oLK6InsHNzc73nnBNFGPg1W6dO6ZeEgwCAqKgPsWxZB4N3ska0b9/CO3CxfPl33ledCtfjwunBB2/zXu+5uRsVbRdR39v69Ole+oosnDp0aIOsrCx07szs4qlTZwQ0KTVBRApNm/ouI+EUmVgWTmfPnsX777+Pfv36oWHDhpg2bRquv/56bNu2DTl6Q4tEucNf+A/g2wmPidGvsmbFecrIyMCvvy4HAEyZ8oKP8FLnOBmF6omT6QLGxSEA54UT5+hRc+83MwGu2N7evdmjk+XI/c3jpJf0b+e7D7Q4hBruONWoIW9b3YkVR8NF4aQ3/9ScOXNM7dufw+LxeCBJTI06WVnPaqielnA6fVoWTP5C9eLigC++AO6/n/3Pq/WpiQTHCdAfCLCa48Qxmm7BSeHEv9cmTRoiJycHb7xxCT17nkJJSQw+/LArgMt83jNmzBj8+9/PAwAKCk55KwX27HmF1yXkwinQqIBTp/hMvGe8y06ePAIAmDPnY5/1+fH2eH5Beno6fvqJiTmXKwENGjQDIN+f0tLYSZuS0tJW2wgi0mjSRH7Of+9IOEUmloVT7dq18eSTT6Jdu3ZYvXo1tm7diqeffhrNmzcPRvuICMWu42Q0GmsGufPNFUC8T+fbSqgeoBzx1nKcoqPlTmKgccV6wsnsPEVmHCeRq65ij06WIzdynMSCAH/+uU7R8bLz3TtRHEKEH+eaNaHbiRW/4yVLlMdMHXZoVjQBxg4Ld2GPHdsFAHj1VeemezA7j5PW/zw/RZJkoeyvql58PNCtGzBpEvv/8GG5oIRIpDtOdkP19PL63G63rli3g9g+t9uNhx66EcuXV8XQoYAkudCy5Txh7ZEYNep1zJgxA3XqsN/qlSuXeJ2giRMzFJPgBhoVkJ8P5OfzE+S02GoAwIIFi30EGf8869bxAVh24UlSDAoLXd7PCgB16rBHPjE2QZR1xCIqf/wBXHMNMGUK+18UTuJcaE5WXyXMYzkT9JtvvsE111yDqKiAovyIMo4onPTmtNHKcQp08lu5k817swne5XzfZoST2IlMSJA7TVrCCWCf5eLF4DlOfGTJH2Ycp8suA1q3Btq1k0OmzJYjNzO6bmbSUAAYNOga8NHmCRMmYPjw4ZrbM/rugxmqxzuxRUXsjx9TcV8XLgDLlwP9+snL3G63rZwKfuzVDovSiWMxep9+uhD9+0dhzJgxlvejJpDiELGx7LgUFbHzv2pV/6F6/Dk/9woLWaif+OMPRI7jpHc+B1L1LysrC8OHD/e5LwbDcRLb53IBzz0HfP01sG1bO8yc+Sf27duLKVMG4bff2DrLlq0BcAXE4hDZ2dlITn4KQApycv7QdIb59Wtm/rIzXpOpBICoSPmoRKzing1oFbuQL0QeusrPLV7YhYQTUV4Q+wZvvgn8+KP8f+PG8nPxei8qCm9V0oqKZfXTr18/lJSU4IcffsA777yDc6W/NgcPHsR5p6e8JyIW3nlatWq57sikluNkNBprBrmTzRVAgmq53DaxY8zR6mz4c5wA5zrwesLpwgVznSmzTt9ffwGffSYv8xfyZiVUz6g4hNKRk//hHTGr372Twsnj8WDlyi0AlI4ToBSUalfx228D3zcgf79qh0XpuPF7aArGjh1rK/9PTSA5ToBvEQ2tcuTqUD3+yMXSoUO++40Ux0nvfLab48TRqpjHz7niYv17ASCHyc2cOVM3XE5vEOWyy+S5XtaubYemTQcBALZvByZMyMDnny8sXVOZzHThAhtVePvtWZptmjp1qmkXiuc3xcfnAxCHxbkoivUZMJGPh69wKihgX4LacdI6rwiiLCIOOvLf7n79gFdekcOeAeX9iML1woNl4bRnzx60b98eN9xwAx588EEcKx3CzcrKwuN6WcBEuYP/yK1du1qxXAyb0ysOEcj8JbLwkoWTuvPN28Y7KWIHRWsUWexAh0s4Afquk5hrYHYU3OVS/h+MCXC1HCc94QQwgWD1u3fquPPQox9++B0AsGrVN4oOu5Fw2rUrsH2r96F2WJQdSFk4Ac5M7hlIVT1Abi/PfTHrOAGyM6DVwS0rjpOTE/SKn1VvoEQMkxs7dqymUPF4PPj551UAtK/Za65hj9u2yefvxYvAf/7zDnhxCLVw4v+vWrVes10LFy5U/G90bnLhVKdOompuK3ZQu3bt7jNgwo93z56lNduF+0dhIeuqUKgeUV4RRRC/N9x9N/DYY75RAFrvIUKHZeH0yCOPoGvXrjh16hQShV+BYcOGYenSpY42johcfEvhyvARdKNy5IHMX5KVlYXbb/87AOCOO8b6dL5527QcJ71QPY5eJy5cwkmda3D6NOvdGTlOWjhZjpx3NPPyfD+PvP0SAMWK17hAsPLd+ytqYQZlKFwNAMCPP87Dr796vJ1isROrPkZOFWrQc5yULqxSOAGBT+7pL1QvOVkW2jEx+pNCByKcDh703W+kO07BmKBXvCdqCSetAiocLlT4PWHmzFkAgK1bN/msy1OOd+wAdu8WX2kA7Qlwxf+T0KJFC8PPwdE7N7lwqlJFPbcVO6hr127wcaz48a5btyZmzJiBOXPmICZGmRzHvwsjQQ4EXtiCIEKNVt/gyit9l4n3I5rLKTxYFk4rVqzAv//9b8SphuGaNGmCAwcOONYwIrIxEk68gxzMCXCbNmW/nNWr19NtG++QiaMy/kL1QiWcUuR+sfe4qIWTVifqwgV2p7TamQtGOXLAd5Sed4ZjY5Xnhd25Vpw47srOXY3Sx2OK8tBGjpNTwknPcQLYYMCMGTMgz8WT7H0t0Mk9/TlOUVHyd1qliq9bqQ7VM1NVj1Ov9PLkHdwzZ1jRiD//LDuOk5PCKSpKPlZawsmfSF60aJFwT2AN2779Lx+BwIXT3r2AcpP1YUY4bd/Opnno3r07jNA7N0XhBIhzW8mhetnZ2Zg5c6b3PWvWrAMAfPbZhxg7diw2btyIhARlF0XtOB0/7jvq7sR0FwQRatTncdOmQP36vutFRcn3XnKcwoNl4VRSUoLi4mKf5fv370eq2KMiyjVcAPTs2U2xXOwgGzlOgWJmAlyzoXpcOMXH64s7p4VT9epsjqUePeTKd7zi29GjwNNPAytXag1EsINo13Fyohx5XJzcOVZ3NnlnsFKlONvhmCL8uBcXyx12qyg7dzwu7SzS0tI0q5wFSzjpOU6cMWPG4G9/4yWkmbJ2YnJPM3lxXDyow/QA/VA9vap6Ro7Txx8DL74IPP985DtOgeY46aE3fxhgVSTzL7TIR3DVrcv2U1wM/PqrvPzaa8dCFk55yMjIENxOWThxVq1apbt3o3NTLZzk9vGeHjtJeB6fx+PBpk3KdbKzsxEdrewZ8nOrRg12r5Yk5YCTE9NdEEQ4UIsg3i/QgkqShxfLwql///547bXXvP+7XC6cP38ekydPxqBBg5xsGxHB8M7YoEH9dTvIwXScjISA1VA93pHRy28CnBdOsbFs8ttffpGrj/EOwKxZwNSpwJo13TS2kKBoj1kCLUeuDn3RK+EsugiBhGOq2w1ot72wkJVsNejfqULh2AZHjbodbrdbsxOr3o9TE4KaEQqDB7MZi6+++vqABKeImTnXuGDSEk6BhOqpHaf9+9njqVNlx3FyMscJ8K2sJ15bWsVzOBkZGdixY4ewhF+shT6Cy+UCmjUrfVXoXHXvfhN69boWADBx4sN48cUXkZWVhcmTJ0NLOOkxY4bxxLNcOPHzSW6f7DhxsrOzsWjRImGZfMOOilL2DMX59/h9UwzXC3S6C4IIF+pQPa0wPQ4Jp/Bi2QN4+eWXMWDAAFx22WW4dOkSbr/9dmzbtg01atTAxx/7TmpHlE/EUWy98szBdJysCCezoXqhFE4xMXJIFK88xoUTL+UbH18XEyZMEEZQk8A7/uKcD2YIpBx5RkaGYhR3woQJSE3NwvHj+o6TVmdYr2y9mXYD7Nirv6OlS4HMTOCHH5gI1YOXh77mmuq4cAH4178eBqA9CS53GlJT2edzynHiIlMM01TDX6tXrxUCNJq8+AvVA4wdJ7tV9QDfXBSezH/hQuQ7TsEI1QOU55zWtSWWMi8oKMC+ffsAAA0bNkRWVpawJfaFNm/eRPN6at4c2KRKfzpwAIiPZzePtm2beZcPHDgQmZlcXCgv3muvvRaLhZmgMzIy/JbJVztOXBBmZ/sKJxlZCHISE6Nw6pSwhvC2OnWYkykWiAh0uguCCBf8flOpErtvXn+9/roknMKLZcepQYMG+P333/Hkk0/iscceQ+fOnfHiiy9i/fr1qFWrVjDaSEQgdifAdQqjYgd2Q/WMRr6DIZw4NUrTbrhw4h33ixeVFQi//voXb9v5HFVmsVuOXC/0JSaGqQk94aTuDNvNO4iJkUPCtETf0aPKRyPcbjdcLvZF8u/TKMeJT/7qlHAq7f+iQQP9dbhwcnJmBzOOk51QPTvFIY4cYY9i+f1wC6dQ5jgB8vH87bfNumFl3K3Nzc1FZmYmMjMzMXbsWNWWWMPi4qAZiqY1J/2BA/L3KA5CuN1udOjAxUWSYrkomu68805TLqhaOAHsXnbttdco2s4ZOHAgmjVTulIZGRlITVWeHOJ3oVUgItDpLggiXPD7zeuvA1u2+M57J8LvsVQcIjzYmsU2JiYGd9xxB7Kzs/H2229j7NixOHToEPr37+90+4gIxUxnLNw5Tlar6oXaceKoHSdROAFyyFujRp0BsPwodQK/P+yG6umFuLhcrGevHqXXKoAQSN6By2Xcdt5BE0eljeDb4NvUcpz4OrzTd+FC4DO0nzsnO4kNG+qvpxZOTlQHs5LjJHZ0OU6F6kmSUjgZFcsIJXphp8HOcdq1S6PUIORrzqjCHoN9oZs3/6k5GCEKJ/4Z9IQTAPTpw8RFz54DMHnyZMyYMcPnvPvggw9MnYv8XFefT126dOAt8i7jwqZ58zYAgPvuG+MNU1X/hqgdJ8C3JHkg010QRLgwc5/mkOMUXmwJJy3OnTtH5cgrEJHiOJXlUD2OP+HEOXGCPVoN0wP8F4fQE056IS7Vq7MDYiZUL9C8A6Njzztop0/7FzclJb4hiUaOEz/OxcWB/0Bxt6lKFeNQPe4kXrjg69INGTLEloByOlTPSlU93rktKABOnoxMx8lfqF6wcpyqV9dW0AWlJ6D/60MZ2qYejBCFU5cu7HH/fvm+old2vkuXKzFlyhSfyrkcM9etluMEyPeXYcNu8RE2/Hj37t1Ts8BQdLSyIAl3nLTmcnIiv5IgQokVh5uEU3hxTDgRFQs7winUOU68YyBJcl6G0QS44RZOvKoe77iryxWfPMkeq1e3vl8tZ0VEr5OoF/rSqFEVAOaEU6B5B2Ycp6Ii/yF14o+MkeOkDtUDAg/X27uXPRq5TYAsqo4evejjNixcuNBWeWUz7nDr1spHkUBC9eLjZQF66JB2jlOkOE6hDtVr2DBNsxAErzTn//qQq+pxRFEjCqeePdnjsWPyNaO+36mdxUCuW3/CqVq12j7CRut4i78h6u+Bi3K9uZwIoixBwqnsQMKJsIWZUexIcZzEZWUxVI8TiOPER9XVnUOACUujm7ZW6IveKL2WixBo3oEZxwmQO2t6iO83ynHi6yUny8cjUOHEHSezwunsWf3a61bLK5u5Vh97DPj9d+Dee31f0wvVM1OOHJDD9TZvls/tSHSczp2TB1iKiuTr0c5AhRGiWJfn71LCRbNehT2GbzEFUdQ0bix/R127yt8LF696wonfIwK5bv0JJ60O3+nTLD51586t3mUknIiKglY0jB6U4xReSDgRtoiUUD2j4hBih4z/UEdyqN6pU6ydesIpEMeJh2CJQkPdJkD/pq0OfdETYnpV9QLJOzASyaJYsiKc1q/3KLat5TjFxfmKBrtw4dSokfF6XDgVFRmrCSvllc1cq9HRQIcOSjHECSRUD5BDqtavl5cVFsrnTqQ4ToAskPftY58zPl4Wfk6hFutGIXH8uhk8eLDitYyMDFx++RWl/8nFFERRExcHtGFpQ2jb1vdzqO93LVqwx99+k5fZvW79CSd1hy8jIwObNjHBNGHCY15XVTw06nsTL0dupjAMQUQ6WoO6epDjFF5MB0917twZLoOM9ItOTXZClAnMdMbU/YFQFIcoKZFHjdWOkyRpCycuXIyKQgZTOFWrxoogSBJzlYLhOPHO4cWLrA3i/sWbr9l8Dr2EeqPwK72y9f4w6zj5KxAxefLzAJ4AUIju3dMxYcIEJCZmKdoNyMc/Pp65TmfOhN5xunQpVlWKXomV8spmQvWMUIfqaZUjN3KcmjcHliwBli9XLucDAeF2nBIS2GcpLmbnc2oqsGsXe61JE20xGQjqeZz8hcS53W4sWLDAp5z/6dNM5Awffj0mTBigeW19/jmwbRvQrh0TsLt3s+Vt2zJHSuSqq9hnzc1luVC8+qOd69aK4yQXwbijdEkhsrOzMXz4cMTHK4WgCAknojxBoXplB9Nd2aFDhwaxGURZw4xw4qWktTpagaLnQhQX+64DsPbqOSsjRjDhcsMN+vsLpnCKjmZi6MQJFh4UjBwncVT93Dll/o44+ms2n8Oq4xQIZnKc1M/VeDweTJs2A0w4sQ1lZ2fjxhsfA1BHs6peXJyyWEMgWM1xyssDnn+ezeczdepULFy40LuO1fLKZkL1jDBTVU90nNT7adeOPaqjC/lAQLgdJ5eLXR+nTsnn886d7LFpU+f3pxZO8hxHskjW+o7VAoZ3mrp27aQ751fr1nLemvh9ffaZryCsUoUVkfj1VzY594gRVj8Zo6BAPlcWL56Hyy+X55nS6vDJ7ik/cQq9y0XhpL438YGuc+fYsQz3eUQQgUDCqexg+qeUzSxOEAyzpTPj4+UOQiiKQ4jiSByhLCxU3mTEtqSkAP/4h/H+gimcAOZ6qYWTk45TXBw7ZpcuMQdFFE7icTErnPQS6kMtnMzmOLHOGe/dyxu6ePEEgDqajpMYqhcqx0mcn+viRX23wQqBOk5mQvX4tRYX51sqnwsn8doE5O8u3I4TwAYCTp2SHVTuODVrpv8eu2iFh6onvY2Li/PO56SHlfLFAPDww8COHcCbb8ohfGquvpoJpx9/tC+cxGvygQfuAFDsndhXq8MnO27KnK20tDT88IO8nvreVKkSuzfk5zPXSe2gEURZwsr1TDlO4YVynAhbWBFOnFA4TmpXif/YGjlOZgi2cOIu0qlT8s2woEDZ5kAcJ0DOc9Iru6wu92uEleIQgWI0Z5coloxC9VjnjJ+M8q9NnTrsoGjlOPFQPSCwHCdJMi+cEhLk70CcBDeQ8spWO9hqrFTV0wr1bNvW3PbDCb82eGXLUDpOHLfbjY0bN2Ls2LGmJoq2WvXvppvYRMTDh+uvc/XV7PHHH+3PXbZs2YbSZ2cBsJOFFzTh54conOQiFPyDFHkdN6PiEC6X7DrxMvcEUVYhx6nsQMKJsIUd4RQMxyk/X8KcOfIEoaLQiImR91lUZM9Z4fAf/GAJJ1EIiqNIYueKO052hRN3idQFIuyUXdYL1QtGiWm9QiCSZOw4iZPHMuExpvQVtqGMjAy0aMESOYIZqnfihHxceN6IHi6X7yS4gRKsUL2oKPkY79q1BYC2cKpRQ66ApkUkOE4dO7LHlSvZYzAdJ61KjoD1iaIDFcRa9OjB7gN79wLTptkbMPjrrwOlz04rlufm5up2+LKyslCzJqteMWvWDG8RCqPiEIAsnCjPiSjrRLpwcmIy9vICCSfCFuF2nPh2JcmFu+4a4x2dFXOcoqP1hZPVhO9gO07i9v0JJzuheoB+QQe+PyvCKRJC9ZYvX6M4VqLjpJ48NiMjA/fc8xAAoE6dqt4KYVrOpRiq54Rw4m5T7dq+lSa14MJp9eo/HfmhClao3rvvTvce48ceewCAfnERHq6nRSQ4Tn37skceGsYdp2AKJ7XjZHWi6GDMM5WcDAwZwp4/+KB8XDZtAhYvNreNatX4QTutWJ6WlmbY4XO52Mlz+eXtvcuMqjUCVCCCKD9YGQgJtXDS+j2tyJBwImxhtjMWLMfpjz/WCP+x3m92djbWrGG1dKOi2B/fZ2GhsqKeQYFITcIlnPiIryQ5F6rnpOOkF6oXbOGUkZGB3r2vV6zHHSe9kfsNGzYDAGrUqOwNedPqxIqhek7kOJktDMHhwumuux7w+aGyM+oXqOOkV1Xvk08+ENbiJ632BWIknCLBceICYe1aJnT5HE7BCNXTm4za6oSzwXCcAGD2bODZZ9nznBx2Pdx4IzBwoG9lRC2WLl1X+uy0dxkPvTPq8GmVYzYK1QMoVI8oP9hxnEKR42TVCa8IkHAibBFux2n37q3Cf3LPa9u2XYp2iTlOgYzQhls4nT0rd4Cddpz4cTFbihwIb1U9+UZeRbHerl1MWeqN0O/Zc1CxPbGdWhPgio5TIDlO+/ezR7PCyeXiKi0FLC/rRmRnV0OvXv9na9Qv0A62Xqgez19h/AEgFx067NTchiic+LnDiQTHqX59VjChpAR47z22rFo1ebDBSYxynKxMOBsMxwlg38+kSew6kSRWUGJr6e32nXd81xfFvMfjwddfLyt95bR3nWHDhinaaiScxM9jVjiR40SUdaxcz1q5gsHCqhNeEbAlnNauXYsrr7wSvXv3xrfffutdzm+ORPnHqnByuZydD6VVqzQAvLcr/7o2atRM0S6tUD07HchwCyfuNiUm2u9o6jlOgYTqXbggd6Q9Hg927z4MILjFIeQbtrJXe+QI+yB6I/Q1azZUbA/wPwGuE6F6fDTcKM9HCU9uSgbwFoDPAGRg+fJ7AMijD2ZH/YIVqqcUTucAtMIzz6hUeSmicFKHv0WC4wTIrtO777LHYLhNgH6OE2BtwtlgOU4Au1fzKnUrVsiFIr74Avj++7VeoaQO4Zk6dSoAXrLztHd7/Jo1Gim3I5woVI8oL0RqqJ5VJ7wiYKsre//99+PJJ5/ExIkT8cQTT+CBBx5AcXExThvVAybKFWbDf/gPn5NuE8BGZ2WHhPW8MjIy0L59Z0W79EL1rBJs4SQWQNDKcQo0vwnw7zhZOS6bN8uhkufPyzHQW7bsBgDMmzfLfkNVqItDyDfsKor1iouZlaE3ct+4MXufP8fJ6VA9HvbFJ1r2R7VqvIEpAMQfpwTInVKGmVE/p0P1+PZGjrzTZ9358+drbqNtW7adpCSlcBLzEMMNF04HSmsbBCO/CdB3nDhmKygGy3HicOG0bJm8LD8fGDBgrlcoqUN42HxjNUr/O+5dzq9Zow6f1v2ZQvWIikKkFoew6oRXBGwJp8TERFx77bW49tprsWbNGpSUlGDgwIG4GEg8C1GmsOo4OS2cAKBKFdajfu65V7yjs+p2lbVQvZ07D+DcObkXr3ac7OY3Af7LkZsN1cvIyECvXm7wvJbRox8WOlCsV/jllx86FgOtPvbyjZw7TuwgFRTIkyBpjdyLIXgcLcfJ6VA9PhrOO3n+aNCgSumzFACVVK/WUPxnZtTPKcepoICJJi6crrmmj8+6ei5YSgorvPD998pzOBLC9Dj9+rGqcpxOnYKzH70cJ6sEMhBkBrVwio4uTW7DrYbvq1aNn5NMOImdLL0OnyRpC3zxWtW6P1GoHlEeEM//SBNOgDUnvCJgSzhFRUXh8GEWkhMXF4fp06djyJAhWLdunZ93EuUFq8IpGKPKvAPSr9913h9mdbvKWqje7Nkf49w52XLiHXYnHadAikMoE0VZgtOXXwqzVIL3hPMci4HWOvZZWVmYOJHdvJs0YZU+1PM4qUfuRSfJ21oDx8mpUD2rjhMvDvHAAxNQo0Zz1auy6jA76udUjhPAOvv8B37fvt2a6+t97927M2EiTvIbKWF6ADsXfvkF2LIF+Owz4NFHg7cfIHDhFMj9zAxNmrDHQ4fYY8OG/AKrobW6l5MnmUJv376uTydLr8OnN1UEheoRFQGrc0yGcgJcnsMIwPZcguUNW8Jp4cKFqFFDefN8+OGHvWKKKP9EguOkVUpaTzjZDdXjN40dO/4CEDzhdPw4n/skHoA8tPrHH9sBBD6HE6DvOFnJcVJ2inllCDHbnwunS47FQOuJVl72uE0btk+xgIYW/P1mc5zECXADEU5WHScunKpWbYSSEvYPF71ZWe9ZHvULNFRPFDcXL8rba9asseb6/r53UThFkuPEadWKVZETBaOTGOU4WSFUoXqcVq34jLj+1C7rG/z5548+r+gltet1HM2G6h07ZnztE0QkI14PkZTjRGXItbEsnPLy8rB06VK89tpreOmll/DNN98gr7TXoRZTRPklkhwnsUPtZKieeNN48MExPvuyg95xO3u21JZAAkThtHcvW85D9ULtOKkrZu3YsUNsdeljKkaMGCG0H7j77tsdG5lSF4fg8JRKsYOn/mwiWqF6/qrqWclxmjYNePpp3+V2Hafz52WRy/NtqlVLszzqF2ionsulzHPi5cg7d+5gK/Y9Uh2nUOGU4xTM4hCAr3C68kr++y5/ad27d9d4p5zjpHYfzThOVsqR82uqpES+RxJEWUPPcdUjFMKJypDrY+mW+80332Ds2LE4fvy4YnmNGjUwc+ZMXHfddY42johcIsFx0upQOxWq53vTYL3p8+cLIAobq+gdt7p1uSJKhjieUblyXQDOOE5icYhz55hAqFFDP8cpIyPD58aphDlOQ4fehTlzRuDBBx9Er15VkJ8PPPnkePsNVaElkAFZJNWsyQTOxYtMTOmJS61QPbNV9fzlOBUWAg8/zL7fe+6RS48XFckdOquO0/Hj8vnStCmwYYN8HlghUMcJYMc3L0/pOEVHs5DJ4cOHIzc3F2lpaaYEXaQ7TsHG6VC9UDlObdqwx6SkaujTZzAWLlyIVatWabxTFk5q99HpUL2YGHZPPHGCFYgwOzhBEJGE1VC9UAgnViEzFXJkCSM3N7fCh+uZdpxWrVqFG2+8EVdddRVWrlyJkydP4uTJk/jll19w5ZVX4sYbb0ROTk4w20pEEJHkOJkN1bPS0fDN08gv3VeJ78oW0DtuLVs2Kn2mnOSmVq0mAGTXIhBTVyxH3rUr0LIlc1K0QvW0RpvUtGjBlMDQocxtuuIKNwoL2UaCPQEuIDtOVaoAVUuLzV19NXDzzXL5ZBGtUD1/VfXMhurt3St/t+LINx9jcrnMu4V8nwcPyu/lnVjVmJUpnHAmxM6+KJwA81XgOBXdcXK6OESwHKd69ZTbbt2aPebnu0qr52kRA4Cd6A895Os6+xNOLpdykM1fcQiACkQQZR/x/DczbUuwc5w8Hg8WLqwBFlUyRvFaRS5DzjEtnJ599lmMHj0an3/+Obp164YqVaqgSpUq6N69O7744guMGjUKzzzzTDDbSkQQZn+0+QUerhwnMVTPSo6T782B9bqLiwMb3vVXHKJz516K5dzp2LaN2Svnzm23vW/uOO3dC+TmMuGxb5+2oDRT2KFaNfYGHk5WWCiHcYVCOHHHqXJlJp4AYM8eltivJTCMquoVFsqCwE6o3k5h3lcxXFAUvGavAe44ceGUmiqPpNtxnAIN1QPk47B27V8oKGAbtDsvGzlO7FE85+wQbMcpJgZo0IA9r1pVLsRQXBwFcT4xJUw0uVwSXn31KZ9X9YST3r3Zn+MEyO1atSrwvDGCCAdWr+VgO07s939W6X8zvMsHDx5c4d0mwIJwysnJwbhx43Rff/DBB7F69WpHGkVEPmXNcbIaquc7dwHrTRcWuuw3WKN9HHmiYGX56bw8FjK3du0eAEBm5gO2EzS54yTebE+f1g7VMzOqVLs2602fOye3lRPMCXA5ouPkUn0t55TRBQCMHSdx+3aq6onCSSy+wUfBrYQQaQkn7jTacZycCtUDgH/84zEUFjJ1nJX1vK1tVXTHSTznAnGdgu04AbLT2bSp+ruK91l38uTJ+OijJQCAatVcmkKdd/hKSpSiUe/ebEY4cXH31FNAx472xaiYz0kQocSucNq2bRdmzpzp+Hmr/P2XOwxPPeU7GFIRMS2c8vLyUKmSek4RmcqVK+MSDfdUCEpK5FAof6PYoaiqp1Ucgu/PbqgeIM9dMHjwYMjCKQoTJtivLONvAlx1h3/dur9KQ+ZKh1VxxHaCptblKwon8bhoTXonkpGRgebNaynazC9/l0vZ4QkUM44Tn7RU/ZqIUY4TIHditUL1/OU4+XOc7AgnflwrVZJz28LlOBUX8xMzEdxtmD79TVvnITlO8vNA5gcLtuMEyCXJmzZVXjcPPeRbFGTKlCmoW7cDAP2QYrGt4gCO3mcxI5ymTAFGjWLPc3ONC8ToQdXDiHBidRBk4cIvAQA5OeswduxYx89bpau0HwBNeitiWji1bNkSP/7oW16Us3TpUrRs2dKRRhGRjZjIGOnFIeyG6omweH651/6f/7xme3THn+OkFk4//eQBu0x5T4RZGHqhdEajpqmpvuufPq1fjlw96Z16Ajy+Pe6wcOGRkODrAAWCXnEI0XF6/nllDpG65Lr4frEzFhUlO238PNKaAPfCBe28KY6ecLJaihyQhROnUqXAHCcnnIni4vOlz5Ihh2kV25qrSyzzXREdp6goa9Ua9QiF49S5M3u8/HK2H76vjIzJmhNi8vNTb6AgUOGkdX9r3hx47z15PfH3yQxUPYwIN1bnUlyyhOcYKhP/nDpvxWuzZctkmvRWhWnhNHr0aDz++OP49ttvfV5buHAhJkyYgFF82Ico19gRTmUtVI8jdwzFXnu87cldtTo7Ho8HK1cuBQCcOaOOM0kCm/SUd1ZZzyQtLc2nE+Fv1DQ62rdTfuqUflU9QJn4z58DwNy5c3Hq1F4AvqF6TrsI/hynSpWAf/yDddquuIItMxJO6s/JBeCZM0wciaF6vINbUmJcil6s0i7uOxDHiSMKp3BV1atWjSscwS5CMQpsZCdXdMcJkI/B+fPG6xkRCsfpgQeAFSuAf/6T/S/ec7WKgnDhZNVx0hOB4rX6889LdO9vYlEJq8JJ717u1ATeBOEP64Wr+MXj+wYnztt9++TnrVvXIqdJhemf0kceeQSrVq3CkCFD0KpVK7Rp0waSJGHz5s3Ytm0bhg4dikeDNdU6EVGIMeRlaQJcnsxupaMhx/qKHcR425Vl1O2TS35fDeAa5OWpD1QiAG5XHAdQhIyMDMyfP18xSnrnnXfigw8+ULwzOzsbw4cPV9z0KlVSdta4awOYOy7KEuX3AngnLMKppEQWbDx3y+XSn6sK0A7VA5ioOXFCnkSTO0tiqB7Awqq0HBJJUgqnQB0ncZ+AMlTv5EnWRivXkxOhevXqlZYthKjqijF27Fjk5uYiKyvL9LYqeo4TwMTxsWPOOE7BFE6xsUDPnvL/CQns/qE3iOCv+qf4e2HVcVq58ifFa+r7W0wMuzZE4eTxePyWyte7l1P1MCJUWHGP2Xm5qPQ/34vfifN21y75ebAn2S2LmHacoqKi8Nlnn+Hjjz9Gq1atsGXLFmzduhWtW7fGhx9+iC+++AJRdsssET5EcqJqpDlOZifAtdPRkHN9JHDxdP/9j9kegRHbpwwR0bMzksCFU716McjJycGwYcN8QkvUoomjHn3iIoNjFKqnxjekhSmX/fuZUhBD9ZxEKyTz/HlZ4IifSZyrSo1WqB4gu0HHjinPpbg49j3xUW+9Tu6pU8r9OZXjxBGFkyQpxa4Z9BwnK/cY7rxdf/2d4pYBWA8PIcdJOcmxXew46IGiNVgl4s9xcrm0K4KZEU5ikjpHvL+J0QWA+bwlrXxOyucgQokVx8ntduP66weV/qd8g1PnrRh6HqyS52UZy7fcW265Bbfccksw2kKAdWamTp2qmCdjwoQJlkZ0g40onMJZHEKrQ63uJIo/plzXW+1o8Ek+e/d24dIl4J//fMJ2m0XhpBQ1+sLpuuvuwf/+B6SlVYHb7cbcuXNN708dSqUuEHH6tHyz9nfT9g0BYGrh5El21+ffQygcJy5UYmOVnSszwkkdqsdFzdGjyh8Jvl5SEluuJ5zEHxn1vrlwCjTHKTaWPZ49yzqnViZC1hrNVE9u3L17d/Tr1w8DBw7U/OHlwqmwUOzJynOaWZkUkRynwEP1JEm+1wXTcVJjVjgZDRTExiqL9QDmypFrCSdxdJ2f38XF+nlLageeY2ciZ4JwCrPCiTuoPXq48c03QPPml2HSpBmIi4tz9LwVf9OMQtQrKmQRRRB8hEw9uWCkJaqKoT/+igBEYo6TnY6G2+1GcjJ7YyA3ErF9Sktde6MtW3bENdfcBkDufFux4seOHasYadVynIxynER898scp+Ji1gsMVqielrPIxUmlSspzkH8+LeFkFKoHMJHD13G55HPHX0lytXDSCtWz4jhpheoB9gtEqEP1tDqVq1atQmZmpu7IPP9OExJEK0GO2bVyTlb04hCALI7thupZcf2dJFDHCTB2nIzKkfft21vxmnp0XbzX28lbsjqRM0E4hZlQPdFBzch4FABQpUpNjBkzxvHzloSTMZZC9aKjow3/YkJ5By9naHVmRCIpUdVKPC7vcAVjVJT/iItzoeiF6hUWBp4ToFekwApi+5QhItobdbmSffJk/JUKVyMKb94J5yJJLA7h77j47pcJp8JC9iWHMsdJLAwhYpTjZCVULy5OFmT+SpLzHxkuCLRC9aw4Tnoumt2S5GoX1t+9RGughn+2xMTawlK2YavhIdHR8rVLoXr23i+KjlA6TnpzqnH85TgB1kL1xMGcm28eqlnJjyMKJ8pbIsoS/n6DffuH7A1nzgSQJGmAmONkJ1QvklNNnMC00vnyyy91X1u9ejXeeOMNlJSU6K5DGOOvMxNJN3wrwumaa4DBg4GRI51vh1XHiXeEtdptJonYaeEEyCEiq1YdwPjx8nrR0azDe/z4Bfz55wUAtVBb6LNmZWUhLS0NY8eONbVfHkpVty773+1m1bKs5DiJ7c3NzUViYnvcdJNvVb1g5Tjl57MQJZdLdpTUDlqgOU5iRT2Ov9LRvDBEhw5ATo6878JCJkzFfZglJUVur9OOk5l7yaJFixTXAT8GYkd/1qz30Lp1S1sjncnJ7LqtqI5ToKF6ke44+QvVA5QdMr2OY0wMu94lib3Gq3tqId7r+SCP2Nk0Evhm7v8EESz8CSff/iF7w8WLwancEIjjNHLkSEU6QaSlmjiB6VvuDTfc4LNs69atmDhxIv73v//hjjvuwDPPPONo4yoS33//ve5rI0aM8F44kXBTtyKcqlYFFiwITjt4Z87IcVInDAO+Nyd1vofehR4M4QSw77RxYyiEU2zsBRQXJ+PkyUv43/9yAFzv41rE+YutE+Cd5YkTgWbNgLQ0WTiZDdUT2+t2u3HkCPv/3DlW5S7YOU4A62zFxytD9USMQvX85TiJwkncp79QvW3b2GPXrkw4cceJO4XR0co5psyQnCw7S4E6TupzTqtTqSYzMxN5eXne64B/p9u3HwLA1PfIkXfanq+Lf76K7jjZDdULl+MUrFA9vd8UPpn2pUv+P6f6Xq/OWwLYNApqcWT2/k8QwcJfNIzvYBe7eKKinB95OnNG+RtjxXEaMWKEqeq+nLI6YGErx+ngwYO455570L59exQVFWHDhg2YPXs2Gjdu7HT7KgQej0ezKlqPHj0wYsQIzJ07N6JmNA/FxItm4J0us6F6WqM6ViY/DJZwErfNuXTpcOmzJADMajpzRjnqpOccXHvttYr/xZHWevWARx8FGjVir4nCyWoHTJxQ98KF4IfqAfKx1xNORo6TXo4TF6TqUD2O1nkmwgcD//Y39siF0+HSr7B2bbkwiVnEAhGBOk5aVfX45MaDBw/WfZ94HSxa9AUA4K+/2AQfLldJQJMcczFKjpO994sDQaEsZmsknMR7gFOheoB8vVoVToCctzR//nzNCns0+S0RCfirkOkbJs9+zIIhnA4eVP5vtr+j148FtCOqzFa9jEQs3XLPnDmDjIwMtGjRAps2bcLSpUvxv//9D+3atQtW+yoEemF6ffv29amgFgk3dSfmhXECM8JJ/DHVGtWxkkTstHAS44DVnXngdOljIoA6AIBz53Yo1tAro7to0SLDXAAAqFKFPZ46ZS1UTyQxUT4mx46FVjhxcaIXqmc3x8koVE9LOJ07Bxw6xJ537coez55loUV8OQ+PtIKWcArUcVJfr263GwsWLEBOTg6GDh2q+d7c3Fx4PB58993XpUvYlytJxQHdh7hwIMfJ3vtFoRGIgLWKkXDi53tSkm9lSBG7wsmfI64lnABjcUST3xJmefppNiF0MDAzeMkHu+bMmYM5c95TvM9J1IM5Zvs7RteMepC3rA9YmPYMsrOzkZWVhTp16uDjjz/WDN0j7GE1f8lK6d9gEGmOk5i0bySc+Lw/YrutJBE7KZzef/+/mDfvPu/yxx/PACAKHLHnz5zcjh3lHjj/0R8+fLhmGV2jXACAhVDy9vB5gawKJ5eLhf1t3szyfIIlnKKi5BLG/hynQEL1TpyQO4SiuNI6zzg8TK9WLdnFKy5m6/KOZJ06+p9NDy3hxB0+qy6F3jxOHLfbjYkTJ+Krr77yeS0tLa30B5Gf9LwkXnFA96FRo9j1eOWVtt5e5gm0OES47sFGwmnvXvbYqJGxmOPXn9jp07s2xWV2HCfAeHCMikgQZsjPB6ZOZc8nTACaNHF2+1YKNLndbmzapHyfk/B7UlwcG0g0G6qnd81oVfwzuibLQsieacdp4sSJuHTpElq0aIHZs2d7O2zqP8I6es7BwIEDNdcP9009UoSTmRwnf6F6ehXq5s+f77PMSeE0b96HiuUvvZSF6GjJ+3/Llr5l2AYM6ATA1+KeP3++bjlSj8eDKVOmYMqUKYrRnKQk+RjxSlhip8VsVZwWLdjj9u3BKw4B+FbzcjJUj4cVFRfDm7dl1nHi9/+0NOaicFfnzBk5VM8px4kLOL38Ej20rlf192s0CSi73/gKp0DuQw8+CKxda71oRnkh0FC9QKZWCARROD39NBs4SU0FXnpJKZyM0HKceIEZ9fUMBBaqBxgPjtHkt4QZ+O8CEByH12qfSusacgp+T+IRDmb7O1rX0ogRIzBnzhyfdcv6gIVp4TRy5EjcfPPNqFatGipXrqz7R9hDtGF5eFWk3tT9jWCHCidC9QBoCv7s7GzMnDlTscxJ4QQU+bwWHS3Pi9O1azvExclVKhMTWWfLisXNBVZmZqbPHD0ulxyux4sY8ONiJfZYFE7BKg4B+B57J8uRx8bKx2L/fvZoNsdp61b22KoVO6aicHM6VM9fYr4e6lA9ve9X6x4EsB/EG2+8vnRrTDjFxUWH/T5UlnFqHqdwOU4XLwLPPcfKFp8/D8ydC+xj6W9o2NB4G1qdPr2BEEAW1/4KrOgJJ3+/o3rnPUFw+CAY4Ht+OYHVgRD++2SnVLg/uHDi11tBgRyt4w/1taQlmgD/12SkY/q2O2vWrCA2gwC0w6sicUbzSHGcrAonXi1f3W4923js2LHIzc31VlgKtnBKSFDm2CQlRXn/r1ePdczNWtx684KJFW6qVGGFBnbvZq/Vr68vzPSq4nDhtG0bvOXSQyGc/JUjLyhg62rlR/nmk7HO2enTsnAS1+GOk1aonug48facOsWEWyCheuIkuDxEz1+RCj3EgQ5/3y//404lADRs2BAtWjAbITo6FcXFQFJSBa3q4BBOzeMULsfp+HH5fgqw65/fR+w4TkbC6Z13gHXrgC5djLerJ5wA/7+j4m9vWa30RQQPUTgFQ6xYvZ5D6Tjx/VituuuPSOzbmoVmrC0DmD0RQwX/YSosvIi5c78I20lvlOPER9fFGwyvPqW+ORnZw2Kn0t/kj2bg7Rs58g7MmbPGuzwjIwOzZ8d4OxBxcXLuEQBMmmTcVvVyo0RNLrJ4nhOnc2fg+++txR63bMket2/3DSlzEt5h85fjJFb6W74c+O03VuY9Jka78AOnZk3W+TtwwHcdI8GiFk6i4+WE45SYKJ+vTjhOZoS3ujwzozuAfigujvJui7CPvxL3/gh0Mm+78HOQh/cC7NrKywNWr2b/mxVOYgfUSDh16MD+/MHPST1HwMzvKJUmJ7QQhVMwxIrdUD2eu+1k+KDacQLYtWph9hO/iIMTI0aMcG7DISKEhUz1eeutt9CkSRMkJCTA7XZjzZo1/t8E4JNPPoHL5dKtCEUEB36R79q1LaylJMXcE24lq/NYzITq6eU5caaWZoWqO+924G0YP/5hn/AQMTcoLg7o0YM9z8wExozRb6uWxW0kBvlrPDwNYK5I7drWY4+547Rjh9wJDIZw4tvk+9DraEVHy6LjrrvYvFULFih/7PQcJ0A7VE/PcZIkbceJt8+JHCfx8/Hzw6rjJP4o+/t+9ZxKOceJEcoS2OURpxyncIXq8fDeqlVZnhPAisQAzjtOZjFynMxQ1it9EcEj2MLJruMEOB86qOU42e3zaOVKl+Uy5Jyw//zNmzcP48ePx+TJk/Hbb7+hY8eOGDBgAI7yO7MOu3fvxuOPP44rK2pZpjCyceOW0mfyFRuOHxjemZYkWTDx0Xj+Ay/+mBp1NrKysjBjxgzN/SxcuBAej8f2iL+I2Inlc4xw0SN26OPigM8/B9asYUnY6rb6i8nXE4OiyBKFU6dO+u8zij1u2JDdxAsK5HyfYBSHUM9hpJfjJC7jjs9ffylv/EbCiTtOWlX11ILl6FHW4XO5gObN2TIunE6fdqaqnvj57BaHEEP1/H2/+k6l8peTHKfAKOvFIbjjlJoqDxpwyqpwotLkhB6RFqonDuw53R5+T6pcWb7P2xFOWgKpvAxOhD1U75VXXsE999yD0aNHAwCmT5+OhQsX4r333sPEiRM131NcXIw77rgDmZmZWLFiBU6LMU1E0Nm79yCA1lDn6YS6lKTobOTlsc6uurKb+CPN7Wy9m9OYMWPw5ZdfYuHChT6v5ebmIjHR7d2XXYwsebVwqlNHv9NtJuyExxAvWrQIADBw4EDFe7SEk/g+M7HHMTFA06bMeeEjzq1bGzbLFuJcSx6PB4cPtwOQ7JPjBLAbvjiJ3/btyhu/XqgeoB2qp+c47SidVqtRI/m74x2/PXvkHzSnhBM/p8+ezcfcuZ+aDpFVh68afb/6TiUJJycp68UhuHCqVMlXODVoYLyNcAsnvRymsl7piwgekRaqp87dFXNiA4ULp5QU9juYl2ddnOkJpESdcJSyUoacE1bHqaCgAOvWrUPfvn29y6KiotC3b1+s5gHTGjzzzDOoVasWxvD4JQPy8/Nx9uxZxR8RGHXq8F9G5S9UqH9gYmPlkCHeqTVynMzkBTz11FOay9PS0hx3nNSohZMTuN1ubzly9Y1JzHEShRN/n16JczU8zwkAOnYE0tMDaLAOXNh8/PEPSE9Px6lTzEaZPt03rEzd+dq+Xb7xR0drd/r59vn3YybHiZdQFo8jF3Lcfata1Z4Dx3O1RGHIt3PkyFlLYQ5aVTD1vl/9sFUSTk4ihuqZrVglEm7H6eRJ9qh2nGrX9n++a83jFCrhZBQmVNYrfRHBI5JC9TweDz7+eC6io1l1Fq2iRYEgCie7BbGsurRlbXAirMLp+PHjKC4uRm1ejquU2rVr47B4pgr88ssvmDlzJt59911T+3jhhRcU5dIb+quVSvilWTN+ksu/UOH4gXG5fOfYsRuqxzH68SyLwskI0XHq3Nn+dnieEwCMGxeceS64sFm9ehvYbYv1sN599yUfm1/d+dq2zbiiHsAmsBUxU1VPa8JftXCy4zYBwLXXAoMHAw89JC/bunVD6TO5Z2omzMHqaCYPBZ08eTImT56MGTNm4I03XlKsQ8IpMPgIsSTZc7DD7ThxKlVSDpyY+XnVcpyMQm/N4k84mQkTotLkhBaREqonCv/iYjZyF0gEjBZcOG3a5IHLxRrGP7PZ+R31hNDAgQPLxeBE2EP1rHDu3DmMGDEC7777LmrwpAc/TJo0CePHj/f+f/bsWRJPAcJ/mLp06YRHHpkT1lKSiYnsQtcTTlo/0v5uTnqhTHZzTEQiUTglJ8s5OnbgwqlqVeD22wNulibyRKk1AQiTHOGMj82vDt87ckTOjdITTk2bKv834zgZCactpWmAdgpDAOzzLligXLZ//3YAnSAKJ8A4zEGS5LLRVsSOOhT01Cng4Yfl10k4BQYX4wAL1xP/N0O4HSeO2nHyl98EGIfqBTIVJL+nFhf7vubxePD+++9rvm/q1KlYIFxskVbFlggvkhS6UD2j69lX+OcBqAyP50+0bNne8H1WSn7/9lsugDTMnv0mgOcANEJ+vrWKk3wAWlyfCyS3211my5BzwiqcatSogejoaBwRp2UGcOTIEdTRGKrdsWMHdu/ejeuuu867rKS0VxATE4OtW7eiuaoHGB8fj3i93hJhC36RV6tWOeylJNWdWiPHiYfEmOlsaP142q1qJqInnDweD44frw+AhUGGQjhxo7dTp8A6wn//O/DBB8A//mG9A2gWpXDiw9L5AAp8Rre0Rq15/pXece3QgTll/Bwxk+OkJZzEeaQA+8JJizZtmpQ+iwUQDYD1EI3CHMROZCDuhPoWSsIpMKKj2XmTl8cGfuTz2xyR5DjVq8eukYsX7QknSQpuqJ52eX0ZXvynLHbgiOBz7pzyNz+YjpPR9ewb/sZ+kLZt2w9AWzhZLa/v8Xiwe3cRgDQA5wGwD/v55//TdGvT0tLQrl07TRFklEtb1gcnwhqqFxcXhy5dumDp0qXeZSUlJVi6dCm6devms37r1q3x559/YsOGDd6/66+/Hn369MGGDRvISQoRkTIBLuA7lxMXTny5Vo6T3XYHGqpXUiJ3zMU2cPv999/lMvyhEE6DBgFTpgCvvx7YdurWBXJygFGjnGiVNrxjWb16awB8WPqsps0vdr74CPamTexRbwwlJUU5cm6mqp6WcBLDHwFnhVPPnl2F/9jJ6C/MQexEBiJ21MeNypEHTiAFIiLJcYqKksP17AinvDxZ4DstnPTL6yuhynmEHuqskXDlOPkOkLEfoNq1m2iub6eCHbsOeETHefDc1k2btmmuP3bsWMPS4lq5tGbD/SKZsP/8jR8/Hu+++y5mz56NzZs34/7778eFCxe8VfZGjhyJSaWzfyYkJKBdu3aKvypVqiA1NRXt2rVDXCh6m4Rmsnm48JfjJE4UF2hnI1DhJP6g82OnvLnJG96/f6e9nVggIQGYPBno0iXouwoYLpxcrtr473/nAQDq10/VzEHgYikqCujfnz3fuJE9GpnPl18uP7frOA0cqOw82s1x0kLstL711numcjC0zjk7qItqkOMUOIHM5RRJjhMADBnCrpmrrvK/DfUEuNxtcrkCqw6mJZzMCqKylpxOhI5IEU6++dfsB6lx4zaa69spr8+uA1/h1KLFZfoNE/AnzMrDHE5ABAinW265BS+99BKefvppdOrUCRs2bMDixYu9BSP27t2LQ3xCFCIiiETHiXdi1eXIeRsLCwMXTnrOg1m0OrHKm5hcuubECTrnRbhwOnECqFevLQCgVi3t8l28M9eiBdCuHXvOHSejsRWxQIaZHCcupMTwxKpV2dxbPXuyjqCT0QhRUXK7rrvuZlOhDk6F6gFK0UnCKXACmcspkhwnAHj2WVbgwcwgjNpx4sKJu1d20RJOZgRRWUxOJ0KHWjgFI1TPbJ9KLF7SqVMrAPr9ETvl9d1uN5KSeKUkOVSvT59rdaqt+qIlzDweD6ZMmVIu5nACIqQ4xLhx4zBu3DjN15YtW2b43lmzZjnfIMKQSBZORjlOPEk+XKF6WsJJeROThVPDhqoybxUcPou5JAG7d7PnemE9PGK3a1e5cMWePezRyHEShZNeVT1JkqsGajlOAMsdW76clWwWZ193goQE9sNt9hxUh+pZTRQWiY+XxSIJp8AJJFTPTDJ5MNBznLRe00NPOAUSpgdoCye9JPVhw4aV6eR0InREiuPE4flBH37I/tcrR25UoMEYNhry8svP4IMPLsP69awqbVZWFtLS0jB27FjDd6uFmb8cw7I2hxMQIcKJKFtEonDSm8eJ34hOnDiNmJh4AIkRFaqnvLnJwumyy1rCaQLpNIeb2Fjm5pw6JU88q9fRGjoU+Pxz4MorZcHEMSucxFE8fo6VlLAfOO766AkngIkrp0UT39fZs+ZdT9FxmjQpA//5j/lEYTXkODlLIKF6ZpLJg4Ge42SFUAonQD9JvazdA4nwEGnCiWMmAsbKZPYA+73gfakRI4Zh0SL2nLtsY8aMQW5urq4QUgszMzmGZTFMNgK6vgSgHMmOdCJJOPnLcXr33WkA7seBA0fBKpE1R27uJuzd+5tlAeFkqJ4YksJvbs88k4Jvv2XLnE7Xs1pdJxKpWZMJp+3b2f96Ha3YWFbpD2DfWXS0LCCMhJModHgVPkAZinfxojnhFCysind+zkVHlyhEE8DCJIYPH276GiDh5CzlKVTPCuoJcIMtnIDgV/Eqy4NShDGqos9hDdUT0cu9VWPl3Be3pTcBrlqMAdA99/3lGJbVMNmw5zgRwG23sXl0zp0Ld0vMEUnCyShUz+Px4KOPZpeuGQM+TnDvvaNtJSc65TjFxPiKZLfbjcsvb+v930nhZKe6TiTCp27jjpOZOV9SU4G//U3+399x5SXar7xSXhYbKwtdUTSXJeEUFSVpvm6lmpgonKiqXuA4EaoX6nuweuDBjtgJteNkhBMVvspLwjuhDT8/OWXJcbIKH8SJimK/NVrCCVBWy9OqnMfRc5MmT55cpieYpp+/MHPyJDBvHrBrl5zAHulEqnCSJKVwYp1CfpeLAZv/BsIyawIiN/cPAMD58/bunP6OW7AmwLVTXScS4QUi/IXqqenTR37ub0q39evlOak4Lpevsyk+D9bcVVpY/bH0VwHTSpgEOU7OUlEdJz3hFMjkt4B14WRG8PgTVuVlUIrQh1+f/N4bzHmcrFzPZh0nK/DPmpLCfvd4P8TuZ/atBMjIy8srk04Th4RTmFm+XJ7b5+RJ43Ujpf59JAqnixeVF3diIu8U8l/RWGgJJ8CcgMjIyMBttw0FAJw5k29rVDFcwslOdZ1IhAsnPvp1xRXm3nf11fJzf8e1bl3gjjt811Pn0onPy4LjFBsb7fMDZjVMgoSTs5RFx8moOIRZ1MLpzBn72xKxIpzMCB4zwqq8DEoR+nAxUbUqewyG4+Tvetbq+wXTceL3Jj3HyQrDhw/3WVbWBxdIOIUZsWigkXCKpHAAOW8ibE3wIjoB4g0kIYGNdtx9912lS+RQPVlMMfwJCPlHlu8g0daF7+/mKHZK9Dr4dsSz1qhPWYwt5sIJYJ2va64x977u3eXne/fa27eR4xRK4cT3pSec1OeHeM6JpWzthEmQcHIWJ4pDhNpxiolRhmlGUnEIfk6aEU7+BI9ZJ6m8DEoR+oRCOBldz3p9P63BvEBRC6dAHSegfA4ukHAKMz/9JD//7bfdmutEWjhAJDpOeXlyZ9Llkm9AGRnjS9fUDtUzIyDkC5z3VqMBxFi+8AN1nAIRz7zTPHnyZEyePBnDhg2z0PLIQBROV15pvqMlhtL9+qu9fWv9SIUzx0lrlFHr/FCH6hnFo/uDhJOzBBKqF657sMulHOAJxHFST4AbSsfJn+Ax29kzGpSKlAgRIjD49VmtGnsMZajezJkzdft+WoN5gWLVcRLPcb3zvTwOLpBwCiMnTgB//CH//+qrs8pEOECkC6eEBLn4gtxG31C9GTNmmBp1ly9wcZg/wfKF76+EsJFwckI8z58/H5mZmcjMzAy7a2kHUTgNHmztvaNGsccxY+ztO1IcJ71QPb3zY/36PwE4I3RIODlLWXScAPkcjI42P3eTSCQUh/Dnwlvp7Gk5uZEUIUIERihD9cTrOSMjQ3fOpNzc3JA6TlrCSX2O653v5SXiRYSEUxiZMUMtfKqViXCASBROW7fugcfzOwDljzlvY3R0PGThVISMjAyMMdmLli98ubc6btwEyxd+II5ToOI50lxLO4jCadAga++dNg349FPglVfs7TtSHCe9uHa982DXLhab6MS1SlX1nIVXiVSXOzZDOO/B/P5aqZK9KTQiQTgBxqGrVjt7opNbHu61hEwoHSd+Dvub/ygtLS2kjpP6M/trn/p8DzRMPNKgn78wMn36ltJnXM6zK9NKOEA48FepK5T873+fAgDWrv0Lt97K8pm0hFNxcSz46f79999YvnD5hR8byz78v/71lOW2BiKcAhXPeh3rRXyGO0RO8RE9Wrdmx65DB6BVK2vvTUgAbrrJfudM60eKi6hQVtXTc5z0zoPVq1lsotPCiRynwGnUiD3aybuLBMfJTn4TEDnCCTAOXTXT2dO6Z0ZahAhhn5ISuXhLKHOcpk6dqrsu7/uFsziEmXNZqx9rN0w80oiArm/FxOPxYPfu1wHsBRNMt4MLJ71wACszQAeTSHGcPB4PFi36HMDNABIBsF90l+uS97lWx+Lqq7va2p/b7UZSEqsCZWcup0CEExfP4iiPFfGs17HOzMzE2rVrUa1aNcydO9e7PBInyG3cmJULr1Ej9JNFR4rjpCec3G437rzzTnzwwQeK5YsXLwEwxRGhIw5IkHAKnMaN2eOBA6zjZEUERYrjZIdwTIBrF6PJQ/UmFY+0CBHCPnyaEyB0oXoejwcLFy7UXG/GjBneSJlglyMH9ItDmDmXy/P5To5TmGBq/EcADwGYV7q0GgYPHmwqHCDUiCNrkSKc2DGUK91xsSSG1KnbmJISWKcvkElwAy0OEYjdrTefAgAsXLhQIZqAyA0tadcOqFMn9PuNlBwno1HG/v37a7yDnezkOEUetWuz67ykBNi3j02Efv/95jpmZdlxUudNOC2ceESEk6idJaNwvEiLECHsw4WEyyXPMxbsUD09N2fw4MGK9IJwOk5G/Qmg/J/v5DiFCaUa53XIq+Gpp6yHgAUb9cha+/ZrAPwt7MKJHUNROLE7SWqq3JtQtzHQSRYDuVk5MY+T0QioP7KyspCYmIjMzExT6+fm5pbrm58V+Pf+3XfAG2+wv0hynPD/7Z13fBRl/sc/mx5IQuihBEJHWkDKElA5BSkXC8ZTDjUKghVs3P2M3p0E5HdH0fN34nG2KFLuBAugR1ERBRVJ6FIlijSFhCahJQSS+f3xzbNTdmb77sxmv+/XK6+ZnZmdebL77Mzzeb4NRjN81Nk4OYT1iIoC0tOpoPN//wssWkTbz54F5s93/RnrBZOHCn8tTi1a0PLAAZrNt7LFCdC3LHXr1k33WHHPtJKHCOM7QkjUrSvf/4LtqmdkqdGODc20OAHOXlAAIqa/s8XJJNSKnYRTQkILy3U4vZm1nTv3AjDf4mS323HXXaK4Wh0Ii1PDhnUdx2gHFv4KJzMtToFgxIgRHh9bm03t3iIeUitX0kB30SLZhcMqdZz0ZgHvvPNuAGxxsirCXU/pmfOf/wB/+5vr97nL0BlM/LU4tW9P/efcOeDoUVk4+XtvDrRwKioqwpQpU3QtS5UGZgflPbM2xXREKkohoY3NCyRK4eSpxdIKBXCVfTyS+jsLJxMRrlezZ08BAFy6VBfV1epjzA7Y1zcb0xPKbOEEAH/846MAgPr1m+P5518AoJ8cQqB9OHv7+QZTOIlz22zBG5i6M7ELarup3Vu04ujoUXndjOQQRg9LrTvnmDHk2sHCyZoI4bRuHS3btKHl3/9OsZRGWMFVz1cLUXw80LYtra9eTbPZ0dHqrJm+EEjhJFItG1nn4+Li2B0vAhCJIZKSAlMM1gjt2MATt/xQWpyMhFOkwsLJZOx2Ox588A4ANIOtfFhaoRaEK/cfKwgneUBbBy1btgfgOohdCKeioiLcdNNNXn++oXDVi4sLbvIDcVMeMGCA7n5P61tFElpx9MsvtIyODu3g1RPhrpz5E30u0K56nI48MIjMemIw9sc/AlddRc+BV181fl84W5wA+h8BQOQy6dbNt5pQSgIlnNylWgbouVjbUiwzzphhcRKTuQBcWnBCaXEKhlgMZ/jxZwHi4+Uq8qdrwp2sUgtCz0LRtGlLANYSTtoCuAKbTd3OevVkQarNXOPJ5+uPxUncfIzc8NLTgebNgf79vT+3t9jtdqxfvx65ubmq7d7Ut4okjCxOoXTTU17P0/4XyNIBbHEKPMLiJMjMBJ59ltZfesl4UKQd4IQSfy1OgCycvviCln37+tcmIHDCyV2qZaVlKZLckyIR5e8smBYnIZz+/vcZHk/mKjO9Crdxf/HWVS9SYeFkEURxNSGcrFQLQsysZWdnAwBKS08CAJYu/SDkbdGiHEgKk7V25lI5aKyoOO5yNtHd5+uPcHLnXpOYCPz0E7Bmjffn9hUxW6qcNTXbPdSKaC1OIi4j1MLJnaueFjGILC8/6/d3ysIp8GiFU9euwO9/Txn3TpwANm/Wf1+g4oJ8QbS5XTvfz9G5My3FgK+PbxUiVARKOBnFdubn56OgoABdu3ble2OEoEwOEUyLk+izb7wxR7Xd1WSu8pkUKGFz/Dj9w0eO7AMQXLEYzrBwsgha4WTFWhCyhYaeUJ9+utz0B4jy5nHmDC1dCafKyhMuz+fu8w2mxQmgwWmoB6XKWVMruIdaESOBZHWLk3ggb9u22e/vlIVT4BGuegDQsiWQmkoDtPR02mYU5yS2+5uJzheee45isu6+2/dzCIuTwErCySg4v7y8HOPHj3f5O+JJp9pFqF31AOeTG03mKp89gYhzysvLw/ffUzXuP/zhIeTl5bHFyQAWThZBK5ysVgtC/eMVSuSK6dXQlTePX3913gaoLTzp6cYjDU8+X3/8is0M6PYEq7iHWhGjBBBWtzjt2/djzZo8mvT1O2XhFHiEQAIozkcg4ofEwE1LoFJ4+0JiInDddf65fwqLE0ATSd27+9+uQCaH0MYv3XbbbW7vjTzpVPsIhateVZXS1c5ZOBlN5sbGyn1+/fqtfgl2+dkvMhJfwKxZs/Djj3sAsHDSwsLJImiFE+BfwdNAo/7xilHTFaxevdqM5sgtUQTni8/OlcWpW7d0J0GanZ3t8efrzuLkasbRE4uTmVjJPdRqGAmkUGbUA7y3eP7yS2nNmroqqC/fKQunwBMfDzRrRutK8SBiDM6d03+fma56gaBePfn/zswMzD0x0OnIlZZ4d/dGnnSqnYTC4qTsr088MVG1z91krngu3XLLKL8Eu9y/xQONTFglJYcAsKueFhZOFkFPOAHWCT612+245557al7JFicruCWIm4cnwqlePWdBunz5co8/X1cDV3czjla3OFnRPdQqWMXi5K2r3o4de2vW1E97X75TzqoXHEQKcqVwEhYnPeFUVSUP6MywOAUK4a7XqlVpQJ4jQswHugAuYPx72b9/P4qKigyF1bRp0wLfmDBDkoDsbGD4cDiVW7E6oRBOyvP97W9TvZosj40VikZ+EPki2OX+rRZO7dpRIjC2OKnhx59FaNiQllrhZCWGDh1asyZG/vSEMtsiIQa1wlVPK5yUQkXM0PoqSI1c9TyZcbS6xclq7qFWwioxTt646hUVFaGoaEvNK/np7OtEDFucgsPf/gY8/jhw++3yNlfCSbktnIXT7bcDNlsVPvzwjoC4twXa4qTEqP7d1KlT0b9/f3z22We671uxYgXeeustS0wwmsW5c1Q4/NNP1fXvwoFQuOophZMogOvpPToqSjRmFIB/A6Abh7djMrvdjv/5n6chu+pdRF5eHnr3ptkctjipYeFkEYwsTlZCnpUQyqRCs90cxOBVTzgVFRWhokIeafjr2mJkcfLEzc3qFifAWu6hVkJpcVIKCLOEkycWJ+p7orPJT+cbb7zRp2uzcAoOgwYBL7+s7mOuhJNw04uPV38n4Ubv3kWQpBgAXzu2+ePeFkzhBMj3Rr1yDQsXLsTAgQN13+cuoURtR5m44MAB89rhC6F21fM2bjA5WQzh/wzgLgC/A+DbmGzq1JmO9TVrlmPGjBl+J4fYt4+yg9Y2WDhZhHAQTvKsmyycrGCRMBJOwnXu+HF5mitYwskTNzdxw7WqxUlgFfdQK6EUSD166G8PZTs8EU7U90Rnq9Rs9x4WTqHDlXAyM6NeIAl0TGWwhRMALFmyBG+99ZbuviFDhrh9fyTGPSmF08GDpjXDJ0JpcYqO9r7wfYMGWh/ynj6PyS5ckNcHDaLCav78zyUl9Kz04GcRdrBwsghCOJ06ZW473DFz5kw0b94WAFBQMMcSFgkhhsRnl5CgdZ2Tp4h++mm7X9cycpXyxM1N3HxOnToW0a4b4UjLliRaunVTp5A2KzlEebm66KFRUpJOnUTgDP0G/JnoYOEUOjyxOIVrYghBoGMqgy2c9NyxlYwYMULXnU+L2a7toYYtTq4R4wJfPFG0z5+ePcf4PCYT35OyJIq451dWel9kd+dOel9t7O4BqCfPBIL69WkprCZWRpJo9Na7d1eTW0I0b05L8cNOSNA+nOQnaWlpMYCePl/L1Yz/zJkzkZOTg+LiYnTs2NFpgCpuuIsXL8DixeSy8fTTT2PmzJnaUzEWIzWVHgBJSYBybGSWxQmgh1J8PIkh5YBODN5o23MAgFatmuO99wr9siKycAodngincLc4ickmZd/1R9gHWzi5Ejyi3Xa73fEMqKysxPjx452ONdu1PdTUFotTsIVTfDwciUb0xg96aJ8/+/enoKoK2LAB6N3bu+fThg3fAchEfPwVCGmg9I4RzxtXVFUBX39NddkOUUI+VFTQe63uaeMNLJwsgpg9NCp4aCWEaNAmYTCLli3VrxMTgZYtlQ8n+UmamZnh17XcxZiIh6cehw+XAEiD0gI2a9Ys5OTksFtcGCD6mZjkAMyLcQKoD27frp+URIae9ocP/wign1/X5qx6oSMSXPUA95NN3hBs4WQkeAoKCjBu3DhUVADPPw/cfLMdubn0fxQXFwdMGIYrtcXiJAb+ly/TJK23bnVGiPihy5fPoX///o7tnkyqai1O584BOTnAxx8D06YBf/mLZ22gybfVALbi7NkS5OW9gpkzZ6ru+Z4Ip0WLgHvuASZOpMlGwdmzQKNGnrUlHODHn0Vg4eQ7LVqoXyckaF3nSKjExFRi4ED/Bo/ifz5w4JjXrnYnT4ovV+0wHGmuG+GOmcIpNlZ+YJeXe9J35OQQ/vYztjiFjkhw1RMEKqZSCKeqKtfH+YqRO7ZIFrFyJTB9OqDM/8DJdmqfxQkIrDgXFqeLF8+otnsSD6f3/Pn4Y1pOmSIZhgQoXbtlF1Q5Fbm4ttJK5EmCiJ07ablxo/q7Fves2gILJ4sg1PnFi4E3BQcSSZJ/QFYWToD80OrYkWKyGjXy31b8wQfzAQC7d+/3OktS3bpixO1/TR3GPEQ8IhB64WSzydfcsGEb9u/f7+Yd4mlf6Xc/Y+EUOkQBXDFwU1JbXPUCTSiSQ7gSQseO0VK4KAkiPdmOUjgdORLc7yfQiIQJWuEUyDGaLEiclYm7yS7l80drAauq+ko3m6O23qRca0xdw6m4uBjR0fK93pMEEUeO0PL779W/g3AwCHgDu+pZBOVDsKzMumbNK1fkInZWF04APbSaN6f4FH9naIuKirBs2SIA90JkFvTG1a5+/SY1a/JdNxJdN8IdMy1OAPXvixeBnJy7AHzvtD8vLw+SJNXMItJkQVZWX7/7GQun0BEprnqBJBTCCTB2xz55kpZHj9Izkt1ZCaVwqqoCfv4ZyMgwrTleoeeqB5CICFRiIFmQOAsnd5NdyjZccw3FFyn2OtbEOEWsK1mxYkXNmqjhdEF17fh4+g49sTgJ4XT2LLB1q7y9tgkn/mlbhJgYoG5Nv7VyJ1PG9lhFOGljnIwK4PornGj2R3wACZrt7hE3yKeemhDRrhvhjlI4hTqrHgBER4snrbqjDxw40NGnxMz49dcPAwAMHz7Y7+uycAodkeSqFyhCJZyMEFldr1wBjh9X7zPKehkJaDPQhkucU2Wl/MyuWzf4FqcmTVJV2z2ZVFVO3P3ud9q9DVSviouLDccq2dnZUFqclNcWgtET4fTzz/K6Mr15bXPVY4uThahXjzpbuAgnqxRfFFn1BFrhJB6o/g40aAZGfACJmu3uETfb9u0zkJub4V9jGNMw01UPAGy2CpAlSd3R169fr3ptt9vRpg3w5ZeBKbrMwil0COF0/ryz9YJd9fQxQzgdPAjMnUvB8MLiBAC//AKkpdG6XtbLSMqkqrQ4AeET56Qc+NetS65wMTHUvwJZy0kIkjZtWuDjjwt9TpQyahTF2R09eg47dyZDK5xcjVOee+45dO/eGDNmANde2wczZshF0pUpyV1RXa0WTkqsPKb1BbY4WYhwSBAhhFN8fOCyyvhLYqJ6MGsknPwdaNjtdtx33yhxFQDeudqJG09tSssZiZjtqpeUJOa7nM1d2hlFIdYDIZyU52A3pOAihBOgHsAB7KpnhLjPS5LsTh5sZs6kTHpvvKEWTmIAqVf7KdKK4GqFU7hYnISbXlyc/MwORkpy5bjA23i4khJ5vWlT4JNPgM8/FzeP+hBDfGW6fKN6k2lpFAveokV91X4hnLSWQy3Hjxt/LmxxYoKGSBBx5oyZrXCN1TLqCVq2BE6fpvVgueoBQF7eE5g3D6hbtxHWrPGuLk4gB7GMeZgtnJo3r4OffgLowahGO6sYyD5ns9FD9NIltjgFm8REEqfV1eSupxRS7KqnT4xiNHPlSmgmqH74gZaHDjlbnABjN+7i4uKIiW0VwslmI1EbbsJJJGoB6D5aXh4cVz1fPHiOHnXepnw+zZnzH/TunQEAWLBgASorK9GtWzcUFBQgLi5OZdkS35PW/bxBA+DwYfc1Ro2sTYC1jQG+wMLJQoSbxclKtGgB7NhB69rBbKBc9QBZlElSvNcPPrY41Q5SUuRBrRnCqXFjWvbrdxM2bvzQsV3P+hnoPsfCKTTYbCSWysqc45zYVU8fM4STyBx29Ki+xcnIPcqbDJd6RVG9LZRqJmJA3q4d8OOPsti0OnrCSfSpzZt3oLDwu4B8/v7cox98EFizBhg2TN4WG0v3jnPngBtvHIWCgjwnqydALqO5ubmO18KyrRVODRvSUtm/9RCJIeLinN362OLEBA1PhVMob5raa1nV4qTMrBesGCdAHiiXl3tfBI8tTrWDqCiyDp8+bY5walKTnPG3vx2D2bOvcnkvCHSfExMmLJyCj5FwYlc9fbTCKdhUV9NMPEDCSSSHAGSLk3CN8rUIrl58FICwipkSwqlXLxJO+/YFtoBssBC/O5G0C5Dvo3fddS+A7wD4//n7Y3G64w6gY0egUyf19oYNqf1ffbVLVzQBzhmBxfek/H/FuQB1/9ZDCKdrrgG++ILWhYiysjHAF1g4WQhPhFMoA031rnXTTXQtKwsn7Q2oc2dadu/u/3VkixMNSr2ZJWKLU+2hZ09K/dquXeivLSxOJ04Yp0YWsHAKX7SZ9bZuJbHOrnr6KPtkKIRTSYl8T9+/Xx3Lo3RbmjlzJnJyclQTHJ5MfhrFR2nxpiSGGYjPJTMT+OADCkU4fpxicqyMsLCI+y0ASNIlAPEQZR4A+vw7duzoKITsLaIP+SKcbDZ6Fmlp0ICScOzdW+ry/cKVtLi4GAcPDgXQ1MniJErjeGpx6tED2LWLvuMuXYDt22ufxYlDfC2EiHEyEk6hDDQ1uta2bXsBWE84iZTksbHOg7q//IVuIjVlDPxC+X+7C5bUpqBli1PtYcUKctNp1iz01xYPcm3KYz1YOIUvwkXo3DmybowYQS45IiCcLU5qQi2clAU+tQNDYXESKIP+tQVIjYqoe1rmwttjQ40QTg0aAG3a0Pr3zuXnLIf4ncXGnnI8x0k4AXJhcWL8+PGG36M7hMUpkBOqIllWUlJrl8e99tprjr64dOmnAIxd9dxZnMRkQXo6MHgwWYCHDqVttc3ixMLJQogZRKPkEK4CTQON0TkPHqS7idWEk7A4iXYpRYvNBrR2ff/wmPh4eRDqzjKofTiyxan2kJBgjmgCZFe9EyfcH8vCKXxRWpwOHiShrMwWF+4Wp0DXNrLZ5H4ZauGk5eefyStBy1tvveVy8lP5mXgTB+XNsaFGmXRAeH/s22deezxFCKc1axY6nuMVFcJv1vmG6uskthBOhw//ELDfghBODRq0d8qip+Tbb79VvCLFVFqqzt4hLE6euuq1bAksWACUlgJ9+9I2Fk5M0HDnqheIQFNPMTpno0Zk2rGacGrfnpYNG+qLlkBhs7k3XRtZ686dIxMVW5wYf1C66rkjGMkhAE5HHgqUtZxE4hu9/eFIsO7RoazlpFePSLjuXrjgbIXKy8vD+PHjdc81d+5cx2chltOmTVMF7xvhTcyUGSiFk4jFCQeL086dwqQv5/wuKxM3Xf0bqi+T2CtXfg4AWLv204D9FoRwOn0ajmLo8+fPR0FBAfLz8w3eRcFNZWXqVH3eJodIT6cJjAYNZKs4u+oxQcOdcHKVgz/QGF2rVasOAKwnnDp0oFmOZ5/dHXR3RjFwNbqRGN08y8vpac4WJ8Yf2FUvMlBanHbuVO9TWr7DjWC6nHsrnPbuBf7zHxKmehYiV+hZnFq1ktNBK+Oc9P5nJa+//joWLFig2rZixQosWLAA99xzj+Fgt6CgADNmzPCu4SFGz+IUDsLp8GGRc1wZJyS26f/4vJ3ELioqwoYNW2pekekpEL8FpXACZFfRcePGoZ1hYC5ZnDIymqi2emJxqqqSU6Onp8vbwyFTtC+wcLIQ7mKcAPXsQWFhYVBvmjk5OcjPz0d+fr7jWlbNqgcAHToUYevWV3T3BdKd0Z3FyfjmSYopXAc8jDUQrnqnTrkv9Blo4ZSZSdYmMQBigodSOGktTsK9JxwJpsu5t8LpppuAu++mfj1xonfX0hNOjRrJbuPKOCd//reFCxca7ovTzMIF2v0xEISrq15FRWrNmqLKrAvh5MskNvUL8R1Warb7jlY4KTEen5BwyszsoNrqicWpuJh+cwkJQFqavF0Ip9pmceKsehbCXYyTwF0mLX8pKirCtGnTsGLFCse28vJyS6cj12YA1BJId0Z3wkkvBW1ubi4+/phGuWxxYvxBPMiqq+nBKPqjHoEWTv/6F/C//+v6mkxgcCWcwplgupx7K5xEOnEAWL/eu2sJV71OnWQh0LAhCaddu9QWp2DFIHXs2NGRoW/16tUqq5VV0pSLJEp16gAZGbR+4ADVhLTaOELJuXMiL7dSOAlxIz/E8/PzMWLECJ/GZNQvRNzUJc1233ElnPTGJ9nZ2dizpysOHHBODqG0OBmlkV+zhpYDB6rLAihd9cIhBb2nsMXJQljBrCl8z5WiCZDNx1YUTu7cIALtzihuJK5iTJSWwdzcXCxYsABlZfQEeeyxhy01I8iEF7GxsjuQO3e9QAunqCgWTaFCCKfSUrlo6DXXmNeeQBFMl3NvhNPly+rj3E1YKpEk2eI0YIC8vVEjecZd+dvU+5+HDx/u8fU++OADp215eXlYsmSJIy5K6+oXrIy73qK0ODVpQp41kmS9QrhKa11Vlfz9TZp0t8PrpnVrUfeEbqh5eXmYMmWKz33Xbreje/c+Na8qHef097cgJtf0hBPg7Lm0fPlySBIpJqOsehUV6pT7SoRwGjxYvV2MaauqjN8bjrDFyUKYLZzcCZDi4mJUVNAP2krCycisPXLkSDzzzDMBt86JgePu3aWYMuVVANCdcRKv77333potdLNdt241+vd/3TIzgkz40aQJ8Ouv7hNECOHEVs7wQwinoiIaaDZuDLz/PnDLLcCYMaY2zW/0ahsFAiGcqqrcH6stJ+GNcDp1Sh4I2u3A3Lm03qiRnJBFO6mh/J+11iF37N6922lbhw4dDJNNCIqLi01PHKEUTjYbuesVFgJ79gSmtmIg0HqsTJgwFVVVkwFU46WXngFwBeXl5ejadSYOHQLGj5+A8eMf9/uzLSoqQnIyBa2OHp2DJ564NSDfl7A4uYpL0nouKb8nJUlJciHbU6ecC+RWVQFr19L6DTeo99WpQ/GwVVU0rtW+N1xhi5OFEMLp0iW1D/srrwBt2wI//SRvC4Yvszu/2o4dO1rS4mRk1l62bBmWLFkS8OsJ4bR06VeYOnUqpk6dqsqGo/xu1J+p2pfZKjOCTPjhaWY9MYjjuLrwQwin/ftp2aMHWTM2bgQefdS8dgUKZW2jQOGNxUk7A372rPuYQYFw00tLo2ezoFEjOQZRzxpst9vRsWNHl6IpOzvbozZs2rTJ7TGVlZWmxjxJkvOAXBRs3bbNlCY5oTdhPGeOsPCdBECdadasWdi/n7Ja9O07wO9+K7x7vv12MwDgxImfA/ZbcOWqZ8SFC7TUCiebzXWc07ZtNOmQkgL07u38XuGuV5sSRLBwshApKbIPqLKTvfce+QSvW0evg5XK1ZVfrTAfC+HkS5XrYKHnBiEIhjgpKxM+BmqfpVmzZjmllP3ss88UR4jR62XHFisXLmSsi6eZ9bjocvgiCuAKevQwpx3hhDfCSVicxHskyfMgdjGJ2bYt0Ly5vF0pnEpLnd8HGN/zH3roIRQWFuK5557zqA19RZEcA+x2O8aPHx+UshyecvmybP1LTKTl1VfTcuvWkDdHF/3vQ2Q4KFFt3beP0lu+//4yv66pFms0ofr55ysDXsfp1189mwxQClw9q5CrzHrCTe83v1HHNwlqY0pyFk4WIipKnmVUug2I9bKy4KZy1RMg2dnZqux9VrQ4AeQGYZSyNdDi5Ny5gzVrzsEe2pnEhQsX1tTiiIb8c5Oz51i5cCFjXTwtgsvCKXxR1mlKTATuv9+8toQLvlicUlPlQf2vv3p2HWEFbNfOWDgZTWoY3fPHjh3rcJ/SPoe1loi8vDyMGzfOZXFT7ZjADA8HpVVPWDKUwsnbFPDBQP4+mgMQVaX1hZN4dn/++Tq/Pkv1mETMQl8K2Fjlhx82AqDP1xNLz6VL8nehtTgBri1OX31FS62bnsDsEJRgwMLJYuh1MqVw8iSVqz9ufHpBg8qbtlWFE0BxRnoEWpx06yZuqp5Fyd94441Yu1ZZoZtGs1YvXMhYF09d9Vg4hS+iPAUAFBQA3bqZ1pSwwReLU5068mftaZyTEE5t29J7GzQgb5EWLdwLJ0+SY2ifw+JPW4bE1YShHqH2cBDCKTpavgd160bf06lTctFUM7Hb7ZgwYSqAfQC+BYX+GwknOR25P5+lekwiu/AHYqySl5eH666zAzhf89p9HLVS4IpJBCWuLE7ffUdLIwMoW5yYoONOOLlL5RoINz5XvudWFk6hKhB8/fUiolUtnIyqvHfs2BE9e/ZzvH7rrdeCXoOLqd2wq17tp29f4L77KMb1rrvMbk144IvFKTFRzlLprXBq144E00cfAR9+SDFPTZvSvhMnjN2kPKnHqH0OGz2XjSYM9Qi1h4M2MQRAbv5du9K6Vdz1Ro6cDCAJQBcAd8IT4eTPZ6keq8TXtOG3AUk2IXskUYDTm29+6HYSXXxPsbH6zwoji9OpU3K9MqOJHbY4MUFHWwT3yhXgPE0c4OxZ1+IgmG58AisLJyA0BYLldMyxyMub7khVOn/+fMPv5rIc1oQxY+5mSxPjF5646kkSZ9ULZ2JigHfe8b4wayTji3DyxeIkYpzataPlNdcAt91G6+L5UFXl2vUvUMkxjMYE2m2BTsThCUaZ2qwW56QsZty48YsAmtW8KtF8luSqN2DA9X5/lmKs0qYNVQUeN05/4tUb1FYwoXIaurWOGSWGEBhZnHZSyBfatJEtS1pqo8WJ05FbDG0RXKVKF+tGqVxXrVqle85ApiS1unACgl8gOCGBArfPnwfGj38G7dvL7pE5OTm6343IbhYdTbFsDOMPnliclINHtjgxkYAvrnqJid4Jp0uXZBczIZyUxMWRBevXXylBhJitDybaMQFAz/3hw4fjk08+AUDxt82aNQtpCQxXwmnuXGsKpxMnmgEYDQCYOvVRTJ7cHgCQk5ODyZNT8NlnwKBBQwJyXbvd7ohlDETCLbUVTAinxm6tY0bfk8DI4iQKc7tKXMMWJyboiBu4mKlS3siVHU87W5WXl4epU6fqntPVj8bbeCiRJt3KwikUiBmYkyed3SOXLFniNLvHLlNMIHEXRwFAZeXkfsdEAtHRtAymxengQbLm1q0r/w61ePL7DDRiTKAsiitEkyDUCSJ8sTj98AMwaRJw9Ghw26bk8GFaaot7X3NNe8e63W5HZuZVAOSJUH8QY6+yMlLwgRBOausjuSNcf/0otxPJrjLqAcYWJ0+Ek3jvnj0umxBWsHCyGC1b0lLMgBgJJyWuCte6ivHxJR4qHCxOoUDcDNav3+fSPVLcHDdu3A6AXaaYwNCsxpPk5EnjhzgLJybS8NdVz5OsesrEECJuR4uIcwqlcALcF7EHQpsgQpmAQ1BUVIRduxYBAI4dc3bheukl4P/+j9xUQ4UYb73wAjBlCq1HRQHaOWdxH1XeW31BOfY6dOgYgMCNDYQL4NChpE7tdvd1wXy1OInEEK6EU04OLZcuDa0YDiYsnCxGmza0PHCAlkrhpOcjWlRUhLmibLmG/Px8wxgfX+OhWDgRQjjt26eTnxP0cFLeHO+8k6K7eQDLBIKGDeW+VKKNX66BhRMTaYTCVU+ZGMIIMyxOgGeiKJQJIrQDcvFMfOih0RCJF374Qf2en3+mZTBcu8rLqWCrNg26EE7t2wP5+ZTwYMcOeSJbEAjh5Dz2IlNTcfFO30+qwW6349pryTrmLvMq4F44pdXkylAKn6oqYNcuWnclnHr1ohjAK1eA115z35ZwgIWTxXAlnLQ3EnETev311w3PZySEPElrrgcLJ0IIp/j4dN39lZWVmpsjTSfZbAGw8TMRj80m148xmsUTD3eOq2MihVAkh7CycHInirKz3VsfAonyM3YWDKSYVq78UfUeUThYCNtA8sQT5Ca4erW8rapKjllr3ZqWzZvLmf+UCKuQP656zmMsOukvvxzw/aQ6KMMJ3OFOOGVk0LK0VP5efvyRxoOJia5/CwDw+OO0fO01OdwjnOHHqcUQwkn4URsJJ09M8lOnTnW44GljmdylNTeChRMhbkp16rTSzWgU52R3p6kqSWLhxAQG4a5nJJzEw52tTUykEArhpM2op4cQTkIEhAq9DHu5ubkOwbRixQqfy5T4gvIznjZtmmYvCaft28+rtgZTOO3bR8stW+Rtx45Rf4mJURcz1iMQFifnMRZZnDp0aOX7SXXwtNYf4D6rXv36lBALkOPBNlKNXXTtKscWGjFyJHlJnDghu/eFMyycLEarVjSbfPEizVYpb+QXL8o/WG/8lGfNmuUUy+RrzSMWToS4KZ08qZ8C3fnmSEIqMZETWTKBQTzkjx3T388JSZhIw1dXPU/rOEkSsHs3rbdta3ycMsbJqJZTsNA+jyZMmIAVK1aojglVkgghnC5ePOHUBqC4Zp/sDydJsnBSFmUNFGLy+eBBeZtw02vZ0r0AEPdSfyxOzmMvEk79+vX0/aQ6KMco7hBWVPEeLTabbI0Tn9fixbQcPtz9+WNj5YmG2hDnxMLJYsTHU/VxgNz1tDdyEefkj5+yuGn6UvNICKdAZIAJZ4TFSczm6BUrVN8c6Y6bnBzhipMJGJ666rFwYiKFYFucvviCLE516gCu5hiFxWnZMjr3K6+4b08gUT6PfHXLDwTiM66o0Mu6QRan06flVHbnzsmuXMGwOInvVymchAWllQcGH+FI4m9yCDH2mjdvPoRwCvSYSjtGccWnn9LyhhuMjxHuegcPUnY98R5Pi3MLDwmjib5wgoWTBVHGORkJJ7vdjnvuucfna4ibprdF+NjiRDRoQEtXWZiUwvTll18FwINYJnCwcGIYNb4IJ2VyCHdZ9V56iZb33y+/Rw9lmvJz54APP3TfnmDhq1t+IBCfcXS0nomGxiDFxXKyBqVrYzCEk7A4Kes2iXVhUXFVosXIVc/bsi4Ajb1+/3u56G0ghJOyHcJ69Ouvrn8PpaWUMAMAhg41Pk5pcfrgAzpnr17AVVd51jYWTkxQUcY5aYWTMs5pwYIFyM1VV5vWqxiuh683TRZOhBBOp08bH3PqFNCrFwnTdu3o7sLpyJlA4S7GSTzcuc8xkYIQTlVV7o9VpsoWIuj0aUoe0KsXDQx37QLmzCHXrL17gZUryW3piSdcn1tb30mbOS6UeOqW78vg3x1COH3++UdO+yZNGgmAxjiiPpBSOAXaVU+S5InnQ4dksaYUTu5KtOglh/ClrItAeR5/79Padsya9YwjXb62/pISkSijVy/jumSA2uL0n//QuqfWJoCFU8CZM2cOMjIykJCQALvdjo0i6kyHN998E9deey3q16+P+vXrY8iQIS6PD0dcWZy0mfWEm53S3W7mzJkoKCgwPL8nsUxaJIkeJGLmItKFk6hrYCScSkuB9HTg5pvpNc/+M4HGXYwTJ4dgIg1/XfUuXqTZ9+3bSSg98AAwcSIJpYcfpmNGjqS01a4Qg0TB0aPA+fP6x4YCd275/gz+XXHwoEgrqFZBBQUF+PvfpyG9Jint7NmrUFRUFFSL0/nzcrxZRYUs0oRwqqr6yW2JFlEgVnyX3pR10ROmygxz/lic9Nrx4oszkZJCAw9XcU7C5W7YMNfXEBanrVuBr7+m9VGjPG8jC6cAsnjxYkyaNAn5+fnYunUrMjMzMWzYMBw3yOO5du1ajB49Gl9++SU2bNiA9PR0DB06FL/88kuIWx48XAmnkydp1ktZ00nP3W7cuHFOs0wDBgxAfn4+OnTo4NXMUnU1kJUFZGbK2yJdOCktTtqaEABl7ykvlzPIiEEsz/4zgYJd9RhGja+uevXqOe//5htg82Zaf+014KuvgORkYOZM9+dOTgaWLwdWrJAn2X780fV7fMEbK5GRW76vNR094cSJmnRtGuEkss7abPShTJu2qKa0ylLHMYEWTtpJ50OH6H/fuZP8M69c+Un3fcpYMG0snKfxY0bCVIwLoqLcJ6ZwhVE76tShz90ozkmSZIuTp8Lp++/pfV27wiF8PaE2CSdIJtOvXz9pwoQJjtdVVVVS8+bNpenTp3v0/itXrkjJycnSvHnzPDq+rKxMAiCVlZX51N5QsG6dJAGS1K6dJHXrRusxMbS89lpaPv64Z+cqLCyU5s+fL+Xm5koAnP6efvppt+c4doyuqfy7fNnPfzLMOX9e/izOn3fev2IF7UtOptcLF9LrIUNC206m9nLqlNwHKyqc93/+Oe3r1i30bWMYM3jwQerz06a5P3bAADp2yRJ6Xbeu+hnXq5fzc2/xYu/b1L+/7+91xdNPP+31s1xLYWGhNHLkSN2xwfz58/1u48CBp2s+u3GqcxcWFkqFhYUS8K+a/e9LQKwETHF81m3b+n15Fbt2ab/fGTXtOSEBknTTTc/ofg6FhYWOc2zeTO9t0YJe0//g+j2ujjlwgM6XmOjf/2Z0jczMMgmQpPfe03+f8hlSXu76GiUl6s/vySe9a6P47Jo18+59ocIbbWCqxamyshJbtmzBkCFDHNuioqIwZMgQbNiwwaNzXLx4EZcvX0YDYQLQcOnSJZw9e1b1Z3WExenwYdk3VVSw/vZbWn7zjWfnstvt6NixIxYsWKC7f9asWXjrrbcAGM9elZSo3xMTI8/sRSp16sjWIz13vXPnaHnhAt1m2OLEBJr69WX3Dr1ZPLY4MZGGr656gJySXCAC5rOzgSlTgLfeAu680/s2iXDiQMY5+WMlKioqwpQpUzBw4ED0798fy5Yt0z0uEMkjLlwQH6rsESRCBchKIlKU/w7AOgCtHccF2+K0bdtpULZbSj+3fPmbujHjSgud6CMiiYgn8WOurFLCVc/fcYFRO9q0SQFg7Kon6jfFxrr3ImrSRH2Mq0QSegiLU2mpZzGIVsbU4e/JkydRVVWFpqLoQQ1NmzbF999/79E58vLy0Lx5c5X4UjJ9+nRMnTrV77aGkubNyX2gvFweELVqRUF5osPt2kUDI08GRe7Sjo4fPx5vvPGGKlbs6aefxswanwTtoCzS3fQAChBu0IBE5enTziZrIZyqq0k08SCWCTQ2Gz2MDh6k36gI3hVwcggm0vDVVQ8gN6yff3Y+bsAA4E9/8r1NHTrQMpDCydVg3FX8cl5enpPgMjrO2zhoPcTnuWDBTEjSXejYsaPjvCTMVgAYCeAdAFkA5HiAQCeHcE413xpAWs16JYBTuPHGGzFhwgQUFxer2ipQxsJVVtK9debMmcjJyTF8j6ushmJCNRAZ9fTa8eCDtM/IVU8IJxG75QpRy2nfPvq/r7vOu/Y1aULnqK6m9qSluX+PVTE9xskfZsyYgUWLFmHp0qVIMBjNP/vssygrK3P8HTlyJMSt9J7oaKBfP/U2bY2BykryNfUET2aOtAk2lLNXLJz0cZVZTxkILG6yAA9imcDiKs6Jk0MwkYYvBXCFxUkMiq+5hmKUBP7qB/H4DWTZJF9SjOtZqfTIz8/3qKajOy5dki0dI0b0cIqvkq0kHwEQHjF1HPuDbXECMgCILB7kViMEh1GJFmUsnFKIuXqPnjUoOzsbAAJmcTJqh7siuFqrqztEnNOAAZ6JLSUxMXLWvnCPczJVODVq1AjR0dEoVaZSAVBaWoo0N3L0xRdfxIwZM/DZZ5+hR48ehsfFx8cjJSVF9RcOXHut+nXr1s7HCFcCd+j9cD1BzGppXfVYOBGuhJOwOAE0q8MWJyYYuBJO3OeYSMNfixNAM+m9e9O6zQb07etfm4JhcfI0xbgSTwvepqenByQtubgnxcfLz0olRUVF6NatGwoKCvDYY92d9ldWAvPmLQxYevTt2w/UrIlwjQzIwumYR1a26GhADCHd1fxSIrIaCsG0YsUK9O/fH//4x78ABL74rcBdEVxvLE4ApSwHgFtv9a09tSVBhKnCKS4uDr1798aaNWsc26qrq7FmzRpkZWUZvm/WrFmYNm0aPvnkE/Tp0ycUTQ05SuFUt66cmUfJ9u2en0+ZjvRPHvodiNkrtjjp46lwYosTEyyEcFIWdBSwcGIiDX8sTmPHkqfH2LGycLrqKnmg7CtCOJ086d1g2x3uUoxr8cTzxG63Y/z48Y7sb9qYH28QiY5btICjnpBAmWVu/PjxANY5HQMAY8Y8HJD06OSi+FrNqx01y9YQwum66zp4bGUTcU7Orn/uWbFiher1v//9PoDgCSd3FidvhdOf/0yZIh9/3Lf2sHAKEJMmTcKbb76JefPmYe/evXjkkUdw4cIFjB07FgBw77334tlnn3UcP3PmTDz33HN4++23kZGRgZKSEpSUlOC8mUUSgkD//pSiEqCZML10qd4IJ0A24/71r391mq3q2rWr6rVy9kXbyfVucJGIN8KJB7FMMBAlAjZtct7HfY6JNERKZ3fCSZKc3ZRycoCiIqrRdPvt9PwdPdr/NiUlyQPGN95QFz31F1cuYnrH6nmeiDIlBQUFTtadhQsX4t577zU8p6t06CK+SSS2Ur5H6zL4yitT0L79BThD5kB/0qPL1xODqB0ArgCoi5tvpvj3Ll30k4vpoU0Q4Wkb5s6dq7OHZlKDNaHqzuIkfgOeCqfkZOC3v5XHpt7CwilAjBo1Ci+++CImT56Mnj17Yvv27fjkk08cCSMOHz6MY4pP+dVXX0VlZSV+97vfoVmzZo6/F1980ax/ISikpAA9e9J6aqp61kvcI7dt068hBABLllAF9D179PcrZ6tyc3Oxe/dux7577rlHNfuiddWrRSWz/EIIJ72q3Eodf+ECW5yY4DBgAC03bnQekLFwYiINTy1OFRXyunDVU5KVRYPKP/85MO0aOJCWzzwDtGsH/POfgY/h8QTx3M/Pz0d+fj4KCwuxfv16TJkyxVFbSYuRMHJXNFdpcVJi5DLYtu1hAEBs7BUA4guSvxxPXQ21yO8TwukkgP0AgF27KOhGW7DYFdpaTnooBaX4nF5//XWdI8nUZLbFydMYJ3+pLcLJEkmlJ06ciIkTJ+ruW7t2rer1wYMHg98gi3DttVSlWWtxuu022n7mDLnoaLNpAZQ6dds24L33KJWqHmKWSjujtHDhQkycONHQ4hTobDfhClucGLPp1In64enTZIFWJpXhrHpMpOGpcFKKFj3hBAR2MDt/PomnWbPIEvPYY8Bf/wr88Y/AQw+RVSpU2O12XQuVK1c+bbY+o3ToOTk5juOMhJPRdYYMicKnnwJNmlzBL79cBJAAZbIIX9Ojy+8Tg6gyAHsBdMKBmrAnb4STO4uTp5kLAeDWW0fho49CY3GSJGdvIW9d9fyltggn0y1OjDE330zLLl3Uwql7d9katWiR/ntFzIORxUmgP4vzCsaObY3ycvqxaS1ODOFNcgi2ODHBwGaTrU6ixpuAs+oxkYanwklM/sXGhub3kZgIPPkk8NNPwL/+RVlyS0pIOGVkAH/7m17Wt9Bit9txzz336O7TihZX6dAFRsLJKLHFk092wlNPARMmHEZ8vHClSXTs9zU9uny91JotZejaNVp1jDepsY2Ek6iP5Yloeuihh1BYWIg77rgbQPAtTpWVai8YgVnCKQySW7uEhZOFGTyYUo7PmaMWTp06ycF5L76oHqQDJHY8FU76szgPYe/eNHz2GZ1bPGSEiZohvElHzhYnJlgYCSfuc0yk4a1wMrI2BYuEBOCRRyjD3ltvkdveqVPkEpiRAeTn6z9PQsWCBQvcFoEFPEuHLmKctMIJ0E9sERMDxMbm4U9/6oRLl0iVZGUN9ijxhTtmzpyJJk1qsnTgDHbvfl+1319XPeGO52nN0LFjx8Jutwc8HbmWOnXkPq4X5+RtjJO/9OxJk32bN3sfo28lWDhZnE6daODTtCk9FOrVo9Tkv/891Yc4dYo6Y716wKpV9J5ff5UH7sXF8gBKD+fZn2RQNW3gk09kk2pyMvDvf9O6IldHRONLVj0exDKBRsRPrF+vjnlk4cREGt666oUqtkNLXBxw//00MbpwIWXvO3MGeP55er4/8wxw/Lg5bRNixlW2Pk/Sof/0E6mCsrLd0EOb2ELt/kcj+g0btvv538Bx7uPHa1SKw1VPxh9XPU/rYwmUn1MgC+Aa4SrOKdQxTm3bAqNG0frzz4fmmsGAhVOYkJoKfPop8Nln9HCIiQGee472/fQTcPasLGyUqYkvXwb273d9buXsz5IlXzm2r1olC6dmzSibyvHj5JvN+FbHiV31mEDTpw/dD44eBQ4flrezcGIiDatbnLTExAB33w3s2gW8/z5lyTx/Hpg5kyxQTz1lTjImT7L1uUqH/vTTz6CkhAJqHnhghNt04s5Z50QQWiKKi4tdZu/zBHIhVMY4fe/YZ7PRxLSnaIWTN0krtMWFhcUpFMJJz+IUalc9gMatNhuwdCnw3Xehu24gYeEURtxwgzr4e/RousHedx+9FgVxtTVd3LnrAfKNsmXLno5thw4BIjeH8AFu3JjTkQvYVY+xAnXqAKIG+ObN8nYW60yk4a1wMsvipCUqCvjd7+gZ/vHHVHS3vBz4xz9olv7RR/VrtYUKI+GiJ7CKiorwwgtvg1JtVwM45jKduH7WOSGc6mD16tUus/d5ArkQKoXTeQA0y9S4sdxvPEHrqudN0ooRI0aoXoci9tlVSnIzhFOXLsAtt9D6ypWhu24gYeEUxkRHA08/DUyfTq+//54eCL4IJ4E2tfbbb9PSG1N2pCCEU3m5OkuTJHEBXCa0XH01LbdulbexeygTaQgh5K44qdmuekbYbJQUqqiIPEyuuYZ+x6++SvWlxo0DfvwxtG0ySjtuJKbIAiOKNx0H1UwCpk2b5nRuYzc3UrYDBgzGggULVHt8qenUt68dgKjpQlk4MjIo5Xm9ehe8smZpLU5G9bG06MWKhdLipOeqF+oYJ0GnTrQ0yx3VX1g41QKaNSNTc3U1sGOHLJzEIH3XLqCwUD+rihbtj0tkP2Hh5ExKilxwUZlhp6ICqKqSXytd9XgQywSD3r1pqRRO3OeYSEPUcd+xw7jGIWAdVz0jbDZg6FDg66/J62PwYLKivf02DTrvuce7CVFfMUo7LkSUnhWILDBCOMl+hitWrDAQWc60aUMuLmfP6gdoe1vTicY+NNwtKHgJhYWFuPVWshT98MNXXlmz9LLqKd0WCwoKHO6L7mLFgp0cAvDM4hTqCQRX7oPhAAunWoKYcd62TY5zGDSIlu+9RwX9RCY+VwiL0w03AG3ayNu9SdcZKdhs8k1U6a6nFahscWKCjfj9b9kiDxhZODGRRpcu5Hb166/qeD8tVnPVc8WgQcDnn1PWzOxsmiD997+Bbt2AO+4IbnYyI4Hiygpkt9vRpctNNXsOuDyfcXa+VgCAXbt+MtjvuXtcUVER3nrrQwAiKcddsNvtaN5ctEX2b/bEmmVUAFe4LY4bN87hvuguVswqySFCbXFqQnWHWTgx5tKrFy23bZMtTsKdVgykvv7a/XmEcOrcmW7I999P1qwbbwxoc2sNenFO2vTwbHFigk337mT9PHFCDibnPsdEGvHxstXJlaCwqqueK7KygOXLaXLkttvouf7BB/Tsv+UWYOPGwF+zUozsPUApirp1qwligTozlVbwGGXna9WqxkwCZ5Ngdna2xzWdhJvhpElTAAAxMecdMdrNmhUBaA0g3/D/0ENMlp45QyLWH6xicQq1cBJijl31GFPRE06DBpHVKKXGtXf/fvfuemJWomFDet9bb1GhPjGjzagRwkkZG6YVTmxxYoJNYqI8YNyyhZacHIKJRERxeFfCyequeq64+mpgyRJg504qS2KzAf/9L2C3A8OGeTZB6gl5eXkYP368x8crRdGFC8JFRbYYGRWx1cvOJ38vzl/QcyKdsBvUboaUGOLixWMOixK19zAAtU+nO2uWsDhpY5l9wWyLk1mWV3bVYyyBEE6bN8udsW1burn+/DPFKEkSsFu/pIIDIQAaNgxeW2sTrVvT8ocf5G16woln/5lgo00QwX2OiUS8EU7hZHHS0q0b8O67wN69wJgxZHH+7DPguutk9z5XcV6u8Kc2EUAlUgDg5ZefdBnjI9C6tInvpXfv61xexxWrRGFLAMqMesKi5EktKj0SEugPUMc5+UIokkNY2eJ04oTvfdRMWDjVEtq2dRY7qan0g0hOltMVu8ubL4ST+LExrunenZY7d8rbtFa9CxfY4sQEH2WcE8BZ9ZjIROl9YYRw1QtHi5OWTp2AuXNp8u7BB+n3/tVX5F4/YACwYoX3g1N/ahNVVwMHakKbbrrpKrf1oPQQ30ufPte6TbCgR15eHqZOnarYIgZHv6osSq5qUblCL0GEL4RiXGDFGCfRpspK/612ZsDCqZZgswHz5smvMzPV+4Vw2rHD9XmUrnqMe/SEE1ucGDPo0oWWYraX+xwTiYhn36FDxgPb2mBx0tKmDfD66/T7f/xxsooUFgI33URZN5cs8Twmx5/aRMeOUWbZ6GggPd2b/0BGCKfycs+K8SrRt5ZRpqsePeo5ncfb8wPGCSK8JZTpyM+ckZ8JArOEU5068m8vHN31WDjVIrKzKX34o48C2okT8TBxJ5zY4uQdQjjt3SvflIRwEjcGjnFiQoH4zYpEJSycmEgkNVXOCGtkdRJxwLUxW2zLlsDLL5PV549/pEHxtm3A7bfTBOq776rLZejhT20iMXHTurX39x5RG+r48YMAZIHrDfrWMuoQo0b18/6EOuhl0/WFUCSHqF+fCiwD6lhsSTJ3AiGc45xYONUyWrYE5swBhg9Xb1danFyZ7dni5B2tW9ODqbJSLkwoXPWaNqUlZ9VjQoEyUYkkcZ9jIpdrrqHlBx/o7xcTiGLiqzaSlga88AJw8CDw5z9Tsqfdu4G77gKuugp45x1nC4QS4caWn5+vu7+goEDXtW1/TSK9tm29a6+y0O6LLz4PQF1YHjAuuqtE31pGjVGWWPEHcR5/MxmGIjlEVJQ8nlOKlIoKeSwYaosTEN6Z9Vg4RQidOtEAqqzMuL7FxYv0YwJYOHlKVBQF6QKyu56wOInZTLY4MaFA/GavXCHxzln1mEjlvvto+e67zoPvc+dkq0htFk6CRo2A//1fsrI9/zxNsPzwAzB2LNCxI7n3CcuHFrvdjilTpugmURg3bpzjtVLQiM+2XTvP2+jsXkdfWmlpmeqaRkV3tW3Wtjc5mWaOvRVzRtxSk219yRL/khuI8Vaw79F6CSKEmx5gjnAK51pOLJwihLg4mmUC1PE4SoQZNzaWEkownqGNcxLCiS1OTChJTJRnLk+d4uQQTORy/fXkDXDmDLBsmXqfyCzbrJk86x0JpKYCzz1HFqiZM2ngevAg8PDDJHJmzzZ2jXOVREEraD78kNJ6eiNSnN3rqCG//ko3Mb24JVfFanNycpCfn4/8/Hx89VURzp9PBRA4i9OIEXSv/fFH95mKXREqDx+9BBHiu46Pp3i0UMOuekxYICzYP/2kv1/5IxZF4hj3aIWTcNUTMyrl5fKMHg9imWBhs8kP4NOnWawzkUtUlGx1evtt9b5IcNNzRXIy8PTTFAP18stAixZUNPuJJ0hYvPCC2hoh0EuioCdo9uyhh503Fidn9zqyOEVFUfCNUZY/ve1CyE2dOhVTp07F/PlrIUkUxxMooZycDAwdSutLlvh+ntJSWgY71s6VxcmsBCksnJiwQNQcEoGxWjgxhG+4c9UDyEUSYLcpJrgo45xYODGRTG4uLb/8Uu2KJu7TIu43UqlTh7Lv7d8PvPYajQ+OHydRdc89np1DX9CQYvLG4uTsXif8K8mHzCjLn3a7npArKFgDgERhICeEc3Jo6atwunJFnqwW3inBQs/iZFZGPQELJyYscCecODGEbwjh9NNPZF0SwklYnAB5ENuiRWjbxkQWbHFiGKJdO6BePcogpyxQHukWJy3x8cBDD9Fn9Pe/07Zvv/Xsvc6CJhUAPfjOnNnkVTuU7oDvvPMvAHJ8ml7cUm5uLoqLi1Xueq4y6gUqvklw883k4vbdd3JCDG8QxV+jooI/We3K4sTCyXtYOEUQnlqcWDh5R+PGcl2HH36QXfVSUuQK4wBZoJKSQt48JoLQszixlZOJRGw2Oa53zx5aShJbnIyIjQUeeIDWjx/3rLirs6Cp+cBxGDfc0M8wgYOr8+Xm5qJvX1K1ysQeSmGVm5uLBQsWOCWK0LdMkXAKVHyToGFDYNAgWl+61PP3rVxJxws3vcaNgx9j5CrGyWzhxFn1GEvDrnrBwWajrIUAsG+fbHFKTlb7D3foEPq2MZGFmPTg5BAMIxeFFsLpl19IEERHy6KKkUlOBpo3p/V9+zx7T47wWQMgC6fvAbhO4OAKUQBXm6zCbrejY8eOWLBggWq7uI6eZapTJyrQG2jhBHjvrnfpEtXTuuMOqv0IBN9ND9C37nCMk++wcIoghHA6cUI/+JNd9XxHTHQVFwNnz9J6UpJ6NoeFExNshMWJXfUYxlk4CU+udu2CWzsnnFFOAnqC2j2uc81yr8F+zxCD+fJy53TfRudbtWoVFixYgJycHFUGwKQkMi0GQziNHEnLDRuAY8fcH//LL5SCvKoK+OYb2haKIsxiMtyqMU7+pHQ3AxZOEURqKrmPAfq1nNji5DviYfPdd7I/fdu26tmc9u1D3y4mslBanFg4MZGOVjiJ515GhinNCQu8FU5q9zhhcdprsN8zhMUJcK4x9dlnn+m+Z+rUqQ7XvSVLliA3Nxf9+tkdWYSDIZxatABEkkFt2ns9fv5ZXhdxZGZbnMwSTiIGvKJCfyLfyrBwijBcuetxjJPviIfNf/9LA9a0NLpRs8WJCSVscWIYGSGciovp9yCee61amdcmq+OtcFK7x6ld9fLy8lTpyz1FKZyU7npFRUVYuHCh2/cL1709e8g1MyEheM/f226j5YoV7o89ckReF0lKQiGclBYnYd0xO8apbl05Bjzc3PVizG4AE1pat6bgWD3hxK56viMm1UQl8IEDKfaJLU5MKNGzOHFyCCZSSU+nAdqFC5T5TFicWDgZ07nG285T4QRQ4obf/vZ2XH99W0gS8MorE9C370yfRBNAkz0xMZSy+5//JPf3I0eAQYMOenyO4uJiHD9O1//Nb9RiLJAMGULLr7+m9sa4GFUrLU7V1bQMpave5cv0WdarZ36Mk80GbNpEnlDNmpnTBl9h4RRheGJxYlc97+nQgW4EYjZnwABasnBiQgmnI2cYmagoEgJbtlAwPgsn9wiL048/UiyOpxnfUlP7QZKA+vWBCRPu8LtmUlYWiZH8fHlbWlp/j9/fsWNHzJtH68OH+9cWV/TsSUKkrAzYvh3o08f4WKVwEoTC4pSYKE8gnDgBFBbKY0CzLE6AXMol3GBXvQjDlXBii5PvJCaqH8YDB9JS3JSaNeNU5EzwUaYj56x6DKOOcxLCSTwHGWdataLEGZcuGWfg1eN78s5D586BKTT7+efAm28C11wju9mdO9faKWueEYsWLcfXX9P6iBH+t8eI6Gjguuto/csvXR+rdNUThMLiBMhxTu+/T0Jy7lx6baZwCldYOEUYesLpyhUaZIn6Q2xx8g3hrpeQAPTqRevC4sTWJiYUKC1OwvoZLBcVhgkHunal5fbtbHHyhOhoWah4464n0msHKs17XBwwfjxZnaZPp227dsn1nB566CGX7//HP7ajspKSNAU7vvg3v6Hl2rW0vHgRWLDAOZ26WRYnQB7XrVql3s7CyXtYOEUYWuE0bhzNeIisQ1FRZHZmvEe4OPTtK8eVCOHEiSGYUCAsTkI0derEv2cmsunbl5arV1MMqs1G2dAYY7xNEAHIhYWFhS+QCPG7ezfFBtntdowdO9bg6CgAUwFQrafhwwNjAXPF9dfTUsQ5/fnPwL33Ai++qD7OTOEkLE6bNqm3h1sqcCvAwinCECk5RT2BJUvIrUdkhGnQgMQT4z05ORQYOmaMvO3qq2kpbqwME0zi49UziIMHm9cWhrECffrQwLmsjF6npXENJ3f4Ipy2b6dlz56Bbg15bMTFkQXn4EHaplfslsgCMBlAKjIyypGXF/j2aOnRg5IcnDtH8UMffEDbCwvlYy5dAkpLaV0UGY6ODl1ohLA4iQRWnTrR9Xls4j08RI4wGjemGWhJAjZuBM6coe3bttGS3fR85/rrKSD//vvlbQ8/TMGY99xjXruYyEJYnQAWTgyTkqJ2H+P4JvcI4STiltxx9iwc9ZIyMwPfnpgY+TvctUveLtz25s+fj9zc3Jqt9AW3aHEQ+/cnhsQtMzoauPlmWn/qKdmyJFKOA8DRo7RMSAD69aP1xo09T77hL8LiJJg3j743HxMfRjQsnCIMm012G1PWHRDCiRNDBB4Wo0woEb9hm032vWeYSEYMVAGOb/IEby1OQiC0bBm8553IwLZ7t3q73W5Hbm4u5s+fj8LCQowaNQkAcP31GSH1nnn4YVpu3ixv++UXOVuxEFMtW8oxz6Fy0wOchVOHDualIg93WDhFICKJgVI4idkiHuQzTHgjLE5XX622PiqzAZ8AABbKSURBVDFMpMLCyTuEcDp2jKwS7hBuesGwNglEnJPS4qTFbrcjLa03gNDHsWVlAd27O28XsV8io17LlvIYLJRtVI7tGjbkZ4M/sHCKQMSPVjtzA7DFiWHCHTGzyG56DEOwcPKO1FSgSRNaLy52f/x339EyGPFNAmFxciWcALLyAHIcUaiw2WSrU3Q0cO21tC6Ek9LidOedwGOPAZMnh659SosTJ6vyDy6AG4EI4aQHCyeGCW8mTKAA4EcfNbslDGMNevSQaxNxjJNndOoEHD9O7nquiroCwU0MIRDC6fvvKXNdjMHoVcQShVo4AZRJb8UKoHdviiP/+mvZjVFYnNLTKc589uzQtk1pcXI1BmTcwxanCMTVj4Zd9RgmvLn2WmDZMh4gMowgNhYYNYrckzgY3jM6d6aluzinK1dkq0owXfVataKMv5WVlHDJCGFxMiPlfFISCafnnyexDsjCSZSASU8PfbsAtjgFEhZOEYirHw1bnBiGYZjaxrx5lA46lAH54YynCSL27iVLXt26QLt2wWtPdLQ8sSvSemuRJHMtTkqEcNq1C6iqAn78kV6LxBChRjkpzsLJP1g4RSApKfLDw2ajuhYCtjgxDMMwtREj9y7GGU+F08qVtLzmmuDXgBRjlZIS/f2nTlFJEABo1iy4bXFH+/aUevziRRJN+/fL280gNVVOfc6uev7BwilCET+c9HT17ANbnBiGYRgmshHCqbgYqK4G9uwB/u//yHqi5KOPaHnrrcFvk5jwNbI4CTe9xo2pYK6ZREfLcVmrVpGLYWyseclJoqIo7nX4cP3sf4znsHCKUIRw6thR7XPLwolhGIZhIps2bSiJQXk5sGgRMGIEMGkSsHixfExpKVBYSOu33BL8NrmzOFnFTU8g3PWWLKFl27ahK3irx+zZJOLY8uofLJwilL59adm7t1o4sasewzAMw0Q2MTHA44/T+tixwOHDtL5unXzMf/9LcUV9+oQmGYORxen0aWDqVGDrVnptRmIIPYRl55tvaGmWmx4TWFh3Rij3308uev37A3Pnytvr1zevTQzDMAzDWIOnniIrRVmZvO3bb+V1YUkJhZseYGxxmjEDeOEFitkGrGdxkiRasnCqHbDFKUKJjQVuuAGoU0f2ua1fn024DMMwDMPQmOCpp2hdZMzbvRs4cwbYto3cvmw24He/C017jCxOX39NSyFQrCKctLFELJxqByycGGRmqgMZGYZhGIZhnn0W+Ne/gNWraeAvScCGDbQdAO66S675FGz0LE6XLskuegKruOo1bqzO7sfCqXbA9gUGrVpR5hxlgTSGYRiGYSKbuDjgkUdofeBASq39178C69eTh8rzz4euLUI4KS1OW7dSxjolVrE4AeSud+wYrbNwqh2wxYkBQNlekpPNbgXDMAzDMFZk4EBarl9Py0cfpbFDqBCuesp6TRs20PK3v6VMgFFRQJcuoWuTO4S7XkwMkJFhalOYAMHCiWEYhmEYhnHJNdfI62PGUEKGUNKwoZzO+/hxWopkFddeSxn/vv02tGLOHSJBROvWHENeW+CvkWEYhmEYhnHJVVcBL74IJCUBDz4oZ7ELFVFRQJMm5PpWUkIuecLilJVFpVWU5VWsQHY2WepGjTK7JUygYOHEMAzDMAzDuOUPfzD3+mlpJJxKS4GffqKit9HRVEvKijRoINdxYmoHLJwYhmEYhmEYyyPinEpKgC1baH3QIKBuXfPaxEQWHOPEMAzDMAzDWB5lSvL582n9vvvMaw8TebBwYhiGYRiGYSyPsDgtW0ap0evWBXJyTG0SE2GwcGIYhmEYhmEsjyhuu2kTLW+/nZJVMEyoYOHEMAzDMAzDWJ5Ro4CRIym1d1QUZfdjmFDCySEYhmEYhmEYy9OkCbB0KXDyJHDmDNC+vdktYiINFk4MwzAMwzBM2NCoEf0xTKhhVz2GYRiGYRiGYRg3sHBiGIZhGIZhGIZxAwsnhmEYhmEYhmEYN7BwYhiGYRiGYRiGcQMLJ4ZhGIZhGIZhGDdYQjjNmTMHGRkZSEhIgN1ux8aNG10e//7776Nz585ISEhA9+7dsXLlyhC1lGEYhmEYhmGYSMR04bR48WJMmjQJ+fn52Lp1KzIzMzFs2DAcP35c9/hvv/0Wo0ePxrhx47Bt2zaMHDkSI0eOxK5du0LccoZhGIZhGIZhIgWbJEmSmQ2w2+3o27cv/vnPfwIAqqurkZ6ejsceewzPPPOM0/GjRo3ChQsXsHz5cse2/v37o2fPnnjttdecjr906RIuXbrkeH327Fmkp6ejrKwMKSkpQfiPGIZhGIZhGIYJB86ePYt69ep5pA1MtThVVlZiy5YtGDJkiGNbVFQUhgwZgg0bNui+Z8OGDarjAWDYsGGGx0+fPh316tVz/KWnpwfuH2AYhmEYhmEYJiIwVTidPHkSVVVVaNq0qWp706ZNUVJSovuekpISr45/9tlnUVZW5vg7cuRIYBrPMAzDMAzDMEzEEGN2A4JNfHw84uPjzW4GwzAMwzAMwzBhjKkWp0aNGiE6OhqlpaWq7aWlpUhLS9N9T1pamlfHMwzDMAzDMAzD+IupwikuLg69e/fGmjVrHNuqq6uxZs0aZGVl6b4nKytLdTwArF692vB4hmEYhmEYhmEYfzHdVW/SpEm477770KdPH/Tr1w//+Mc/cOHCBYwdOxYAcO+996JFixaYPn06AOCJJ57AoEGD8Pe//x3Z2dlYtGgRNm/ejDfeeMPMf4NhGIZhGIZhmFqM6cJp1KhROHHiBCZPnoySkhL07NkTn3zyiSMBxOHDhxEVJRvGBgwYgP/85z/4y1/+gj/96U/o0KEDli1bhm7dunl0PZF9/ezZs4H/ZxiGYRiGYRiGCRuEJvCkQpPpdZxCzc8//8wpyRmGYRiGYRiGcXDkyBG0bNnS5TERJ5yqq6tx9OhRJCcnw2azmd0cR0HeI0eOcEFexlS4LzJWgfsiYyW4PzJWgfticJAkCefOnUPz5s1VXm56mO6qF2qioqLcqkkzSElJ4R8BYwm4LzJWgfsiYyW4PzJWgfti4KlXr55Hx5maVY9hGIZhGIZhGCYcYOHEMAzDMAzDMAzjBhZOJhMfH4/8/HzEx8eb3RQmwuG+yFgF7ouMleD+yFgF7ovmE3HJIRiGYRiGYRiGYbyFLU4MwzAMwzAMwzBuYOHEMAzDMAzDMAzjBhZODMMwDMMwDMMwbmDhxDAMwzAMwzAM4wYWTiYyZ84cZGRkICEhAXa7HRs3bjS7SUwt46uvvsLNN9+M5s2bw2azYdmyZar9kiRh8uTJaNasGRITEzFkyBD88MMPqmNOnz6Nu+++GykpKUhNTcW4ceNw/vz5EP4XTG1g+vTp6Nu3L5KTk9GkSROMHDkS+/btUx1TUVGBCRMmoGHDhkhKSsLtt9+O0tJS1TGHDx9GdnY26tSpgyZNmuB//ud/cOXKlVD+K0wt4NVXX0WPHj0chUSzsrKwatUqx37ui4xZzJgxAzabDU8++aRjG/dH68DCySQWL16MSZMmIT8/H1u3bkVmZiaGDRuG48ePm900phZx4cIFZGZmYs6cObr7Z82ahdmzZ+O1115DUVER6tati2HDhqGiosJxzN13343du3dj9erVWL58Ob766is8+OCDofoXmFrCunXrMGHCBBQWFmL16tW4fPkyhg4digsXLjiOeeqpp/Df//4X77//PtatW4ejR48iJyfHsb+qqgrZ2dmorKzEt99+i3nz5uGdd97B5MmTzfiXmDCmZcuWmDFjBrZs2YLNmzfjhhtuwK233ordu3cD4L7ImMOmTZvw+uuvo0ePHqrt3B8thMSYQr9+/aQJEyY4XldVVUnNmzeXpk+fbmKrmNoMAGnp0qWO19XV1VJaWpr0wgsvOLadOXNGio+Pl959911JkiRpz549EgBp06ZNjmNWrVol2Ww26ZdffglZ25nax/HjxyUA0rp16yRJor4XGxsrvf/++45j9u7dKwGQNmzYIEmSJK1cuVKKioqSSkpKHMe8+uqrUkpKinTp0qXQ/gNMraN+/fpSQUEB90XGFM6dOyd16NBBWr16tTRo0CDpiSeekCSJ741Wgy1OJlBZWYktW7ZgyJAhjm1RUVEYMmQINmzYYGLLmEjiwIEDKCkpUfXDevXqwW63O/rhhg0bkJqaij59+jiOGTJkCKKiolBUVBTyNjO1h7KyMgBAgwYNAABbtmzB5cuXVf2xc+fOaNWqlao/du/eHU2bNnUcM2zYMJw9e9ZhKWAYb6mqqsKiRYtw4cIFZGVlcV9kTGHChAnIzs5W9TuA741WI8bsBkQiJ0+eRFVVlaqDA0DTpk3x/fffm9QqJtIoKSkBAN1+KPaVlJSgSZMmqv0xMTFo0KCB4xiG8Zbq6mo8+eSTGDhwILp16waA+lpcXBxSU1NVx2r7o15/FfsYxht27tyJrKwsVFRUICkpCUuXLkWXLl2wfft27otMSFm0aBG2bt2KTZs2Oe3je6O1YOHEMAzDhJQJEyZg165d+Oabb8xuChPBdOrUCdu3b0dZWRk++OAD3HfffVi3bp3ZzWIijCNHjuCJJ57A6tWrkZCQYHZzGDewq54JNGrUCNHR0U4ZUUpLS5GWlmZSq5hIQ/Q1V/0wLS3NKWHJlStXcPr0ae6rjE9MnDgRy5cvx5dffomWLVs6tqelpaGyshJnzpxRHa/tj3r9VexjGG+Ii4tD+/bt0bt3b0yfPh2ZmZl4+eWXuS8yIWXLli04fvw4rr76asTExCAmJgbr1q3D7NmzERMTg6ZNm3J/tBAsnEwgLi4OvXv3xpo1axzbqqursWbNGmRlZZnYMiaSaNOmDdLS0lT98OzZsygqKnL0w6ysLJw5cwZbtmxxHPPFF1+guroadrs95G1mwhdJkjBx4kQsXboUX3zxBdq0aaPa37t3b8TGxqr64759+3D48GFVf9y5c6dKzK9evRopKSno0qVLaP4RptZSXV2NS5cucV9kQsrgwYOxc+dObN++3fHXp08f3H333Y517o8WwuzsFJHKokWLpPj4eOmdd96R9uzZIz344INSamqqKiMKw/jLuXPnpG3btknbtm2TAEgvvfSStG3bNunQoUOSJEnSjBkzpNTUVOmjjz6SduzYId16661SmzZtpPLycsc5hg8fLvXq1UsqKiqSvvnmG6lDhw7S6NGjzfqXmDDlkUcekerVqyetXbtWOnbsmOPv4sWLjmMefvhhqVWrVtIXX3whbd68WcrKypKysrIc+69cuSJ169ZNGjp0qLR9+3bpk08+kRo3biw9++yzZvxLTBjzzDPPSOvWrZMOHDgg7dixQ3rmmWckm80mffbZZ5IkcV9kzEWZVU+SuD9aCRZOJvLKK69IrVq1kuLi4qR+/fpJhYWFZjeJqWV8+eWXEgCnv/vuu0+SJEpJ/txzz0lNmzaV4uPjpcGDB0v79u1TnePUqVPS6NGjpaSkJCklJUUaO3asdO7cORP+Gyac0euHAKS5c+c6jikvL5ceffRRqX79+lKdOnWk2267TTp27JjqPAcPHpRGjBghJSYmSo0aNZL+8Ic/SJcvXw7xf8OEO/fff7/UunVrKS4uTmrcuLE0ePBgh2iSJO6LjLlohRP3R+tgkyRJMsfWxTAMwzAMwzAMEx5wjBPDMAzDMAzDMIwbWDgxDMMwDMMwDMO4gYUTwzAMwzAMwzCMG1g4MQzDMAzDMAzDuIGFE8MwDMMwDMMwjBtYODEMwzAMwzAMw7iBhRPDMAzDMAzDMIwbWDgxDMMwDMMwDMO4gYUTwzAMYznGjBmDkSNHmt0MhmEYhnHAwolhGIYJKTabzeXflClT8PLLL+Odd94xpX1vvvkmMjMzkZSUhNTUVPTq1QvTp0937GdRxzAME5nEmN0AhmEYJrI4duyYY33x4sWYPHky9u3b59iWlJSEpKQkM5qGt99+G08++SRmz56NQYMG4dKlS9ixYwd27dplSnsYhmEY68AWJ4ZhGCakpKWlOf7q1asHm82m2paUlORk1fnNb36Dxx57DE8++STq16+Ppk2b4s0338SFCxcwduxYJCcno3379li1apXqWrt27cKIESOQlJSEpk2bIjc3FydPnjRs28cff4w777wT48aNQ/v27dG1a1eMHj0af/3rXwEAU6ZMwbx58/DRRx85LGRr164FABw5cgR33nknUlNT0aBBA9x66604ePCg49zif5o6dSoaN26MlJQUPPzww6isrAzYZ8swDMMEDxZODMMwTFgwb948NGrUCBs3bsRjjz2GRx55BHfccQcGDBiArVu3YujQocjNzcXFixcBAGfOnMENN9yAXr16YfPmzfjkk09QWlqKO++80/AaaWlpKCwsxKFDh3T3//GPf8Sdd96J4cOH49ixYzh27BgGDBiAy5cvY9iwYUhOTsbXX3+N9evXIykpCcOHD1cJozVr1mDv3r1Yu3Yt3n33XSxZsgRTp04N7AfFMAzDBAUWTgzDMExYkJmZib/85S/o0KEDnn32WSQkJKBRo0Z44IEH0KFDB0yePBmnTp3Cjh07AAD//Oc/0atXL/ztb39D586d0atXL7z99tv48ssvUVxcrHuN/Px8pKamIiMjA506dcKYMWPw3nvvobq6GgC5ESYmJiI+Pt5hIYuLi8PixYtRXV2NgoICdO/eHVdddRXmzp2Lw4cPOyxSABAXF4e3334bXbt2RXZ2Np5//nnMnj3bcX6GYRjGurBwYhiGYcKCHj16ONajo6PRsGFDdO/e3bGtadOmAIDjx48DAL777jt8+eWXjpippKQkdO7cGQCwf/9+3Ws0a9YMGzZswM6dO/HEE0/gypUruO+++zB8+HCX4ua7777Djz/+iOTkZMe1GjRogIqKCtW1MjMzUadOHcfrrKwsnD9/HkeOHPHhE2EYhmFCCSeHYBiGYcKC2NhY1WubzabaZrPZAMAhcM6fP4+bb74ZM2fOdDpXs2bNXF6rW7du6NatGx599FE8/PDDuPbaa7Fu3Tpcf/31usefP38evXv3xr///W+nfY0bN3b9jzEMwzBhAQsnhmEYplZy9dVX48MPP0RGRgZiYnx/3HXp0gUAcOHCBQDkbldVVeV0rcWLF6NJkyZISUkxPNd3332H8vJyJCYmAgAKCwuRlJSE9PR0n9vHMAzDhAZ21WMYhmFqJRMmTMDp06cxevRobNq0Cfv378enn36KsWPHOgkfwSOPPIJp06Zh/fr1OHToEAoLC3HvvfeicePGyMrKAgBkZGRgx44d2LdvH06ePInLly/j7rvvRqNGjXDrrbfi66+/xoEDB7B27Vo8/vjj+Pnnnx3nr6ysxLhx47Bnzx6sXLkS+fn5mDhxIqKi+HHMMAxjdfhOzTAMw9RKmjdvjvXr16OqqgpDhw5F9+7d8eSTTyI1NdVQqAwZMgSFhYW444470LFjR9x+++1ISEjAmjVr0LBhQwDAAw88gE6dOqFPnz5o3Lgx1q9fjzp16uCrr75Cq1atkJOTg6uuugrjxo1DRUWFygI1ePBgdOjQAddddx1GjRqFW265BVOmTAnFx8EwDMP4iU2SJMnsRjAMwzBMbWfMmDE4c+YMli1bZnZTGIZhGB9gixPDMAzDMAzDMIwbWDgxDMMwDMMwDMO4gV31GIZhGIZhGIZh3MAWJ4ZhGIZhGIZhGDewcGIYhmEYhmEYhnEDCyeGYRiGYRiGYRg3sHBiGIZhGIZhGIZxAwsnhmEYhmEYhmEYN7BwYhiGYRiGYRiGcQMLJ4ZhGIZhGIZhGDewcGIYhmEYhmEYhnHD/wNWZgX9dmEffAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "# Iterate through the test set and collect predictions & ground truth\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_test, y_true = batch  # Get input and ground truth\n",
    "        x_test = x_test.to(\"cpu\")  # Ensure data is on CPU if needed\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = best_model(x_test)\n",
    "\n",
    "        # Store results\n",
    "        y_preds.append(y_pred.cpu())\n",
    "        y_trues.append(y_true.cpu())\n",
    "\n",
    "# Convert lists to tensors\n",
    "y_preds = torch.cat(y_preds, dim=0).numpy()\n",
    "y_trues = torch.cat(y_trues, dim=0).numpy()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_trues.flatten(), label=\"Ground Truth (NO)\", linestyle=\"-\", color=\"blue\")\n",
    "plt.scatter(range(len(y_preds.flatten())), y_preds.flatten(), label=\"Predictions\", color=\"black\", s=10)\n",
    "\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"NO Level\")\n",
    "plt.title(\"Predictions vs. Ground Truth\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
