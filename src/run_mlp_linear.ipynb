{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLP with extended physics loss function (Option 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loading models, go to the ``src/results/models``:\n",
    "- Baseline NO2 2017 with MLP and MSE loss: ``best_mlp_no2_baseline.pth``\n",
    "- Exp 1: NO2 2017 with MLP and option 1 simple physics loss: ``best_mlp_no2_adjusted_dist.pth`` (naming because I updated the distance between T and B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachel/forecasting_smog_PEML/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "\n",
    "from modelling.MLP import BasicMLP\n",
    "from modelling import *\n",
    "\n",
    "\n",
    "import optuna\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rachel/forecasting_smog_PEML/src')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/rachel/forecasting_smog_PEML\n",
      "MODEL_PATH:  /home/rachel/forecasting_smog_PEML/src/results/models\n",
      "MINMAX_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/pollutants_minmax.csv\n"
     ]
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "\n",
    "BASE_DIR = Path.cwd().parents[0] # set it to the root directory of the project, not src\n",
    "MODEL_PATH = BASE_DIR /\"src\" / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR  / \"data\" / \"data_combined\" / \"pollutants_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 24 * 3               # number of hours to use for input (number of days * 24 hours)\n",
    "N_HOURS_Y = 24                    # number of hours to predict (1 day * 24 hours)\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "\n",
    "# Change this according to the data you want to use\n",
    "YEARS = [2017]\n",
    "TRAIN_YEARS = [2017]\n",
    "VAL_YEARS = [2017]\n",
    "TEST_YEARS = [2017]\n",
    "\n",
    "LOSS_FUNC = \"Physics_Linear_MSE\" # choose from \"MSE\" and \"Physics_Linear_MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2017_combined_u.csv\n",
      "Imported train_2017_combined_y.csv\n",
      "Imported val_2017_combined_u.csv\n",
      "Imported val_2017_combined_y.csv\n",
      "Imported test_2017_combined_u.csv\n",
      "Imported test_2017_combined_y.csv\n",
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', YEARS)\n",
    "train_output_frames = get_dataframes('train', 'y', YEARS)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', YEARS)\n",
    "val_output_frames = get_dataframes('val', 'y', YEARS)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', YEARS)\n",
    "test_output_frames = get_dataframes('test', 'y', YEARS) \n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    len(TRAIN_YEARS),                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    len(VAL_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    len(TEST_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                           DD   FF        FH        FX       NO2         P  \\\n",
       " DateTime                                                                     \n",
       " 2017-08-01 00:00:00  0.166667  0.1  0.111111  0.000000  0.242115  0.562982   \n",
       " 2017-08-01 01:00:00  0.000000  0.0  0.111111  0.052632  0.223158  0.570694   \n",
       " 2017-08-01 02:00:00  0.000000  0.0  0.000000  0.000000  0.165911  0.560411   \n",
       " 2017-08-01 03:00:00  0.277778  0.1  0.000000  0.000000  0.142363  0.555270   \n",
       " 2017-08-01 04:00:00  0.805556  0.2  0.111111  0.105263  0.156297  0.555270   \n",
       " ...                       ...  ...       ...       ...       ...       ...   \n",
       " 2017-11-16 19:00:00  0.750000  0.2  0.333333  0.210526  0.523871  0.789203   \n",
       " 2017-11-16 20:00:00  0.972222  0.3  0.333333  0.421053  0.512314  0.814910   \n",
       " 2017-11-16 21:00:00  0.888889  0.1  0.222222  0.263158  0.232880  0.827763   \n",
       " 2017-11-16 22:00:00  0.944444  0.2  0.111111  0.105263  0.108123  0.832905   \n",
       " 2017-11-16 23:00:00  0.861111  0.1  0.222222  0.105263  0.205120  0.845758   \n",
       " \n",
       "                       SQ         T        TD  \n",
       " DateTime                                      \n",
       " 2017-08-01 00:00:00  0.0  0.536667  0.726852  \n",
       " 2017-08-01 01:00:00  0.0  0.546667  0.740741  \n",
       " 2017-08-01 02:00:00  0.0  0.506667  0.689815  \n",
       " 2017-08-01 03:00:00  0.0  0.463333  0.634259  \n",
       " 2017-08-01 04:00:00  0.0  0.493333  0.662037  \n",
       " ...                  ...       ...       ...  \n",
       " 2017-11-16 19:00:00  0.0  0.390000  0.513889  \n",
       " 2017-11-16 20:00:00  0.0  0.353333  0.462963  \n",
       " 2017-11-16 21:00:00  0.0  0.330000  0.435185  \n",
       " 2017-11-16 22:00:00  0.0  0.306667  0.407407  \n",
       " 2017-11-16 23:00:00  0.0  0.250000  0.319444  \n",
       " \n",
       " [2592 rows x 9 columns]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                          NO2\n",
       " DateTime                     \n",
       " 2017-08-01 00:00:00  0.223698\n",
       " 2017-08-01 01:00:00  0.145496\n",
       " 2017-08-01 02:00:00  0.275978\n",
       " 2017-08-01 03:00:00  0.423742\n",
       " 2017-08-01 04:00:00  0.478721\n",
       " ...                       ...\n",
       " 2017-11-16 19:00:00  0.606502\n",
       " 2017-11-16 20:00:00  0.456470\n",
       " 2017-11-16 21:00:00  0.483258\n",
       " 2017-11-16 22:00:00  0.468784\n",
       " 2017-11-16 23:00:00  0.473428\n",
       " \n",
       " [2592 rows x 1 columns]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.1000, 0.1111, 0.0000, 0.2421, 0.5630, 0.0000, 0.5367, 0.7269],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.2232, 0.5707, 0.0000, 0.5467, 0.7407],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1659, 0.5604, 0.0000, 0.5067, 0.6898],\n",
       "        [0.2778, 0.1000, 0.0000, 0.0000, 0.1424, 0.5553, 0.0000, 0.4633, 0.6343],\n",
       "        [0.8056, 0.2000, 0.1111, 0.1053, 0.1563, 0.5553, 0.0000, 0.4933, 0.6620],\n",
       "        [0.0000, 0.0000, 0.1111, 0.1053, 0.3135, 0.5681, 0.3000, 0.6200, 0.7593],\n",
       "        [0.7222, 0.1000, 0.1111, 0.0526, 0.5326, 0.5913, 0.0000, 0.6433, 0.7269],\n",
       "        [0.7500, 0.1000, 0.1111, 0.1053, 0.5367, 0.5938, 0.0000, 0.6500, 0.7037],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1053, 0.5172, 0.5964, 0.0000, 0.6733, 0.6574],\n",
       "        [0.7500, 0.2000, 0.2222, 0.2105, 0.4459, 0.5990, 0.3000, 0.7133, 0.6157],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3129, 0.6041, 0.0000, 0.7167, 0.6019],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3478, 0.6067, 0.0000, 0.7133, 0.5926],\n",
       "        [0.6528, 0.1000, 0.2222, 0.1053, 0.3649, 0.6041, 0.2000, 0.7800, 0.6435],\n",
       "        [0.6944, 0.2000, 0.2222, 0.1053, 0.3019, 0.6015, 0.2000, 0.7867, 0.4861],\n",
       "        [0.7222, 0.2000, 0.2222, 0.2105, 0.2268, 0.5990, 0.4000, 0.7733, 0.5046],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.2246, 0.6015, 0.1000, 0.7633, 0.5972],\n",
       "        [0.6944, 0.2000, 0.2222, 0.2105, 0.2855, 0.6041, 0.7000, 0.7700, 0.5880],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.2469, 0.6015, 0.2000, 0.7267, 0.6435],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1579, 0.2171, 0.6144, 0.2000, 0.6967, 0.6944],\n",
       "        [0.6944, 0.1000, 0.1111, 0.1053, 0.2834, 0.6298, 0.0000, 0.5933, 0.7130],\n",
       "        [0.4167, 0.1000, 0.1111, 0.0526, 0.3918, 0.6298, 0.0000, 0.5267, 0.6944],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0000, 0.4752, 0.6375, 0.0000, 0.5200, 0.6991],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0526, 0.5745, 0.6298, 0.0000, 0.5067, 0.6852],\n",
       "        [0.5000, 0.1000, 0.1111, 0.1053, 0.5891, 0.6247, 0.0000, 0.5033, 0.6713],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.5491, 0.6298, 0.0000, 0.5000, 0.6806],\n",
       "        [0.4444, 0.1000, 0.1111, 0.0526, 0.5092, 0.6221, 0.0000, 0.5300, 0.6944],\n",
       "        [0.4167, 0.2000, 0.1111, 0.0526, 0.3212, 0.6144, 0.0000, 0.5267, 0.6898],\n",
       "        [0.3889, 0.1000, 0.2222, 0.1053, 0.2835, 0.6118, 0.0000, 0.5467, 0.6898],\n",
       "        [0.5000, 0.1000, 0.1111, 0.0526, 0.4099, 0.6144, 0.0000, 0.5533, 0.7083],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.4797, 0.6272, 0.0000, 0.6067, 0.7083],\n",
       "        [0.6389, 0.2000, 0.2222, 0.2105, 0.5086, 0.6272, 0.0000, 0.6333, 0.6991],\n",
       "        [0.6667, 0.4000, 0.3333, 0.2632, 0.4155, 0.6375, 0.0000, 0.6567, 0.6574],\n",
       "        [0.6667, 0.4000, 0.4444, 0.3158, 0.3375, 0.6478, 0.1000, 0.6800, 0.6574],\n",
       "        [0.6389, 0.3000, 0.3333, 0.3684, 0.2610, 0.6478, 0.1000, 0.7067, 0.6713],\n",
       "        [0.5556, 0.4000, 0.4444, 0.2632, 0.2420, 0.6555, 0.3000, 0.7300, 0.6296],\n",
       "        [0.5833, 0.4000, 0.4444, 0.3158, 0.2146, 0.6427, 0.2000, 0.7433, 0.5741],\n",
       "        [0.5556, 0.3000, 0.3333, 0.2632, 0.1782, 0.6221, 0.1000, 0.7600, 0.5370],\n",
       "        [0.5000, 0.4000, 0.4444, 0.2632, 0.1985, 0.6195, 0.0000, 0.7533, 0.5556],\n",
       "        [0.5000, 0.5000, 0.4444, 0.3684, 0.2416, 0.5964, 0.0000, 0.7600, 0.6204],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4737, 0.2883, 0.5938, 0.0000, 0.7267, 0.5833],\n",
       "        [0.5833, 0.4000, 0.7778, 0.5789, 0.2718, 0.6093, 0.0000, 0.5933, 0.6806],\n",
       "        [0.5556, 0.2000, 0.3333, 0.3158, 0.1936, 0.5964, 0.0000, 0.5600, 0.7083],\n",
       "        [0.4861, 0.1000, 0.1111, 0.1579, 0.2238, 0.5835, 0.0000, 0.5533, 0.7361],\n",
       "        [0.4167, 0.2000, 0.2222, 0.1579, 0.2430, 0.5656, 0.0000, 0.5533, 0.7269],\n",
       "        [0.3889, 0.2000, 0.2222, 0.1579, 0.3154, 0.5681, 0.0000, 0.5567, 0.7315],\n",
       "        [0.4444, 0.3000, 0.2222, 0.1579, 0.2860, 0.5553, 0.0000, 0.5800, 0.7639],\n",
       "        [0.5000, 0.3000, 0.3333, 0.2632, 0.2077, 0.5373, 0.0000, 0.5967, 0.7778],\n",
       "        [0.5000, 0.2000, 0.2222, 0.2105, 0.1640, 0.5167, 0.0000, 0.6000, 0.7778],\n",
       "        [0.4444, 0.3000, 0.3333, 0.2632, 0.1525, 0.4961, 0.0000, 0.5967, 0.7824],\n",
       "        [0.4722, 0.4000, 0.3333, 0.3158, 0.1328, 0.4730, 0.0000, 0.6000, 0.7824],\n",
       "        [0.5000, 0.4000, 0.4444, 0.3158, 0.1252, 0.4422, 0.0000, 0.6100, 0.7639],\n",
       "        [0.4722, 0.2000, 0.3333, 0.2632, 0.1161, 0.4293, 0.0000, 0.6000, 0.7639],\n",
       "        [0.4722, 0.2000, 0.2222, 0.1579, 0.1766, 0.4165, 0.0000, 0.5967, 0.7731],\n",
       "        [0.5556, 0.4000, 0.3333, 0.3158, 0.2840, 0.4139, 0.0000, 0.6533, 0.8056],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4211, 0.3435, 0.4010, 0.5000, 0.6967, 0.8009],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.3057, 0.3985, 0.0000, 0.6867, 0.8194],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.2615, 0.4036, 0.0000, 0.6633, 0.8519],\n",
       "        [0.6389, 0.5000, 0.6667, 0.5789, 0.2453, 0.4190, 0.0000, 0.6133, 0.7454],\n",
       "        [0.6389, 0.7000, 0.7778, 0.6316, 0.1434, 0.4216, 0.6000, 0.7067, 0.6806],\n",
       "        [0.6667, 0.9000, 0.8889, 0.7368, 0.1046, 0.4267, 0.7000, 0.7467, 0.6435],\n",
       "        [0.6667, 0.8000, 0.7778, 0.7368, 0.0607, 0.4319, 0.8000, 0.7667, 0.6019],\n",
       "        [0.6667, 0.9000, 1.0000, 0.8947, 0.0700, 0.4319, 1.0000, 0.7867, 0.6065],\n",
       "        [0.6944, 0.8000, 0.8889, 0.7895, 0.0594, 0.4422, 0.4000, 0.7400, 0.6389],\n",
       "        [0.6667, 0.8000, 0.7778, 0.6842, 0.0823, 0.4370, 0.5000, 0.7633, 0.6389],\n",
       "        [0.6667, 0.7000, 0.8889, 0.7368, 0.1016, 0.4473, 1.0000, 0.7467, 0.6944],\n",
       "        [0.6389, 0.9000, 0.8889, 0.8421, 0.1005, 0.4550, 0.5000, 0.7067, 0.5278],\n",
       "        [0.6667, 0.7000, 1.0000, 0.8947, 0.0701, 0.4627, 0.3000, 0.6900, 0.5602],\n",
       "        [0.6389, 1.0000, 0.8889, 0.9474, 0.0572, 0.4653, 0.0000, 0.6800, 0.5648],\n",
       "        [0.6667, 0.9000, 0.8889, 0.8421, 0.0533, 0.4627, 0.0000, 0.6567, 0.5741],\n",
       "        [0.6667, 0.7000, 0.7778, 0.7368, 0.0475, 0.4627, 0.0000, 0.6300, 0.6019],\n",
       "        [0.6389, 0.4000, 0.5556, 0.5789, 0.0376, 0.4576, 0.0000, 0.6100, 0.6204],\n",
       "        [0.6111, 0.5000, 0.4444, 0.4211, 0.0373, 0.4499, 0.0000, 0.5933, 0.6296]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1965],\n",
       "        [0.1501],\n",
       "        [0.1518],\n",
       "        [0.2622],\n",
       "        [0.5524],\n",
       "        [0.4840],\n",
       "        [0.3544],\n",
       "        [0.2754],\n",
       "        [0.1948],\n",
       "        [0.1734],\n",
       "        [0.1505],\n",
       "        [0.1352],\n",
       "        [0.0778],\n",
       "        [0.1184],\n",
       "        [0.1293],\n",
       "        [0.1238],\n",
       "        [0.1043],\n",
       "        [0.0997],\n",
       "        [0.0812],\n",
       "        [0.0823],\n",
       "        [0.1155],\n",
       "        [0.0837],\n",
       "        [0.0570],\n",
       "        [0.1006]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2 index:  4\n",
      "DD index (wind direction):  0\n",
      "FH index (Hourly wind speed):  2\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataset.u[0] is a pandas Index object with column names\n",
    "column_names = list(train_dataset.u[0])  # Convert Index to list\n",
    "\n",
    "# Now, find the indices of the columns 'NO2', 'DD', 'FH'\n",
    "no2_idx = column_names.index('NO2')\n",
    "dd_idx = column_names.index('DD')\n",
    "fh_idx = column_names.index('FH')\n",
    "\n",
    "print(\"NO2 index: \", no2_idx)\n",
    "print(\"DD index (wind direction): \", dd_idx)\n",
    "print(\"FH index (Hourly wind speed): \", fh_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.242115\n",
       "2017-08-01 01:00:00    0.223158\n",
       "2017-08-01 02:00:00    0.165911\n",
       "2017-08-01 03:00:00    0.142363\n",
       "2017-08-01 04:00:00    0.156297\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.523871\n",
       "2017-11-16 20:00:00    0.512314\n",
       "2017-11-16 21:00:00    0.232880\n",
       "2017-11-16 22:00:00    0.108123\n",
       "2017-11-16 23:00:00    0.205120\n",
       "Name: NO2, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,no2_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.166667\n",
       "2017-08-01 01:00:00    0.000000\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.277778\n",
       "2017-08-01 04:00:00    0.805556\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.750000\n",
       "2017-11-16 20:00:00    0.972222\n",
       "2017-11-16 21:00:00    0.888889\n",
       "2017-11-16 22:00:00    0.944444\n",
       "2017-11-16 23:00:00    0.861111\n",
       "Name: DD, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,dd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.111111\n",
       "2017-08-01 01:00:00    0.111111\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.000000\n",
       "2017-08-01 04:00:00    0.111111\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.333333\n",
       "2017-11-16 20:00:00    0.333333\n",
       "2017-11-16 21:00:00    0.222222\n",
       "2017-11-16 22:00:00    0.111111\n",
       "2017-11-16 23:00:00    0.222222\n",
       "Name: FH, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,fh_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start hyperparameter searching with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:11:13,248] A new study created in RDB with name: mlp_hyperparameter_optimization_phy_adjusted_dist2\n",
      "/tmp/ipykernel_13522/924588510.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_13522/924588510.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.088270 - Val Loss: 0.117394\n",
      "Epoch 2/50 - Train Loss: 0.041907 - Val Loss: 0.066720\n",
      "Epoch 3/50 - Train Loss: 0.036499 - Val Loss: 0.047006\n",
      "Epoch 4/50 - Train Loss: 0.035674 - Val Loss: 0.048454\n",
      "Epoch 5/50 - Train Loss: 0.031430 - Val Loss: 0.046411\n",
      "Epoch 6/50 - Train Loss: 0.031175 - Val Loss: 0.045541\n",
      "Epoch 7/50 - Train Loss: 0.028487 - Val Loss: 0.045210\n",
      "Epoch 8/50 - Train Loss: 0.031712 - Val Loss: 0.045236\n",
      "Epoch 9/50 - Train Loss: 0.034118 - Val Loss: 0.044757\n",
      "Epoch 10/50 - Train Loss: 0.030641 - Val Loss: 0.048745\n",
      "Epoch 11/50 - Train Loss: 0.032833 - Val Loss: 0.048219\n",
      "Epoch 12/50 - Train Loss: 0.031366 - Val Loss: 0.045378\n",
      "Epoch 13/50 - Train Loss: 0.029554 - Val Loss: 0.046936\n",
      "Epoch 14/50 - Train Loss: 0.034172 - Val Loss: 0.045695\n",
      "Epoch 15/50 - Train Loss: 0.033362 - Val Loss: 0.050931\n",
      "Epoch 16/50 - Train Loss: 0.031065 - Val Loss: 0.047407\n",
      "Epoch 17/50 - Train Loss: 0.030316 - Val Loss: 0.045119\n",
      "Epoch 18/50 - Train Loss: 0.030858 - Val Loss: 0.046313\n",
      "Epoch 19/50 - Train Loss: 0.029840 - Val Loss: 0.045581\n",
      "Epoch 20/50 - Train Loss: 0.030438 - Val Loss: 0.045417\n",
      "Epoch 21/50 - Train Loss: 0.030594 - Val Loss: 0.045185\n",
      "Epoch 22/50 - Train Loss: 0.031096 - Val Loss: 0.045848\n",
      "Epoch 23/50 - Train Loss: 0.032098 - Val Loss: 0.045188\n",
      "Epoch 24/50 - Train Loss: 0.032876 - Val Loss: 0.044676\n",
      "Epoch 25/50 - Train Loss: 0.030617 - Val Loss: 0.044976\n",
      "Epoch 26/50 - Train Loss: 0.031662 - Val Loss: 0.044950\n",
      "Epoch 27/50 - Train Loss: 0.028628 - Val Loss: 0.053411\n",
      "Epoch 28/50 - Train Loss: 0.032984 - Val Loss: 0.060500\n",
      "Epoch 29/50 - Train Loss: 0.029341 - Val Loss: 0.049902\n",
      "Epoch 30/50 - Train Loss: 0.029755 - Val Loss: 0.051894\n",
      "Epoch 31/50 - Train Loss: 0.027422 - Val Loss: 0.045324\n",
      "Epoch 32/50 - Train Loss: 0.029342 - Val Loss: 0.046612\n",
      "Epoch 33/50 - Train Loss: 0.031953 - Val Loss: 0.053309\n",
      "Epoch 34/50 - Train Loss: 0.032977 - Val Loss: 0.047413\n",
      "Epoch 35/50 - Train Loss: 0.029764 - Val Loss: 0.045625\n",
      "Epoch 36/50 - Train Loss: 0.029625 - Val Loss: 0.046897\n",
      "Epoch 37/50 - Train Loss: 0.029061 - Val Loss: 0.046200\n",
      "Epoch 38/50 - Train Loss: 0.029697 - Val Loss: 0.047000\n",
      "Epoch 39/50 - Train Loss: 0.031127 - Val Loss: 0.044628\n",
      "Epoch 40/50 - Train Loss: 0.028972 - Val Loss: 0.044959\n",
      "Epoch 41/50 - Train Loss: 0.030998 - Val Loss: 0.044602\n",
      "Epoch 42/50 - Train Loss: 0.030665 - Val Loss: 0.046200\n",
      "Epoch 43/50 - Train Loss: 0.033510 - Val Loss: 0.045784\n",
      "Epoch 44/50 - Train Loss: 0.031697 - Val Loss: 0.045999\n",
      "Epoch 45/50 - Train Loss: 0.033174 - Val Loss: 0.044895\n",
      "Epoch 46/50 - Train Loss: 0.031176 - Val Loss: 0.044766\n",
      "Epoch 47/50 - Train Loss: 0.028964 - Val Loss: 0.044456\n",
      "Epoch 48/50 - Train Loss: 0.030255 - Val Loss: 0.046447\n",
      "Epoch 49/50 - Train Loss: 0.029657 - Val Loss: 0.047180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:11:32,121] Trial 0 finished with value: 0.043870266526937485 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 112, 'lr': 0.0037790304978257013, 'weight_decay': 1.0323985192821294e-08, 'batch_size': 8}. Best is trial 0 with value: 0.043870266526937485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029064 - Val Loss: 0.043870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13522/924588510.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_13522/924588510.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.206932 - Val Loss: 0.380834\n",
      "Epoch 2/50 - Train Loss: 0.210825 - Val Loss: 0.377415\n",
      "Epoch 3/50 - Train Loss: 0.201555 - Val Loss: 0.374010\n",
      "Epoch 4/50 - Train Loss: 0.180794 - Val Loss: 0.370679\n",
      "Epoch 5/50 - Train Loss: 0.206907 - Val Loss: 0.367415\n",
      "Epoch 6/50 - Train Loss: 0.204302 - Val Loss: 0.364029\n",
      "Epoch 7/50 - Train Loss: 0.189370 - Val Loss: 0.360562\n",
      "Epoch 8/50 - Train Loss: 0.180058 - Val Loss: 0.357139\n",
      "Epoch 9/50 - Train Loss: 0.190917 - Val Loss: 0.353663\n",
      "Epoch 10/50 - Train Loss: 0.200474 - Val Loss: 0.350087\n",
      "Epoch 11/50 - Train Loss: 0.182898 - Val Loss: 0.346473\n",
      "Epoch 12/50 - Train Loss: 0.196771 - Val Loss: 0.342913\n",
      "Epoch 13/50 - Train Loss: 0.178659 - Val Loss: 0.339326\n",
      "Epoch 14/50 - Train Loss: 0.192184 - Val Loss: 0.335740\n",
      "Epoch 15/50 - Train Loss: 0.162692 - Val Loss: 0.332159\n",
      "Epoch 16/50 - Train Loss: 0.173607 - Val Loss: 0.328596\n",
      "Epoch 17/50 - Train Loss: 0.188242 - Val Loss: 0.324996\n",
      "Epoch 18/50 - Train Loss: 0.156791 - Val Loss: 0.321305\n",
      "Epoch 19/50 - Train Loss: 0.151686 - Val Loss: 0.317660\n",
      "Epoch 20/50 - Train Loss: 0.151743 - Val Loss: 0.314039\n",
      "Epoch 21/50 - Train Loss: 0.153919 - Val Loss: 0.310338\n",
      "Epoch 22/50 - Train Loss: 0.144013 - Val Loss: 0.306460\n",
      "Epoch 23/50 - Train Loss: 0.156090 - Val Loss: 0.302557\n",
      "Epoch 24/50 - Train Loss: 0.137988 - Val Loss: 0.298521\n",
      "Epoch 25/50 - Train Loss: 0.160805 - Val Loss: 0.294479\n",
      "Epoch 26/50 - Train Loss: 0.135699 - Val Loss: 0.290409\n",
      "Epoch 27/50 - Train Loss: 0.136671 - Val Loss: 0.286221\n",
      "Epoch 28/50 - Train Loss: 0.146199 - Val Loss: 0.281904\n",
      "Epoch 29/50 - Train Loss: 0.132576 - Val Loss: 0.277412\n",
      "Epoch 30/50 - Train Loss: 0.122481 - Val Loss: 0.273054\n",
      "Epoch 31/50 - Train Loss: 0.138839 - Val Loss: 0.268496\n",
      "Epoch 32/50 - Train Loss: 0.125835 - Val Loss: 0.263735\n",
      "Epoch 33/50 - Train Loss: 0.122421 - Val Loss: 0.258798\n",
      "Epoch 34/50 - Train Loss: 0.119736 - Val Loss: 0.253806\n",
      "Epoch 35/50 - Train Loss: 0.121349 - Val Loss: 0.248566\n",
      "Epoch 36/50 - Train Loss: 0.106899 - Val Loss: 0.243259\n",
      "Epoch 37/50 - Train Loss: 0.107150 - Val Loss: 0.238045\n",
      "Epoch 38/50 - Train Loss: 0.105661 - Val Loss: 0.232744\n",
      "Epoch 39/50 - Train Loss: 0.109147 - Val Loss: 0.227285\n",
      "Epoch 40/50 - Train Loss: 0.098673 - Val Loss: 0.221607\n",
      "Epoch 41/50 - Train Loss: 0.100250 - Val Loss: 0.215830\n",
      "Epoch 42/50 - Train Loss: 0.112110 - Val Loss: 0.209835\n",
      "Epoch 43/50 - Train Loss: 0.115378 - Val Loss: 0.203673\n",
      "Epoch 44/50 - Train Loss: 0.084642 - Val Loss: 0.197225\n",
      "Epoch 45/50 - Train Loss: 0.081439 - Val Loss: 0.190713\n",
      "Epoch 46/50 - Train Loss: 0.077385 - Val Loss: 0.184046\n",
      "Epoch 47/50 - Train Loss: 0.072369 - Val Loss: 0.177298\n",
      "Epoch 48/50 - Train Loss: 0.086049 - Val Loss: 0.170462\n",
      "Epoch 49/50 - Train Loss: 0.075061 - Val Loss: 0.163571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:11:50,503] Trial 1 finished with value: 0.15690720081329346 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 77, 'lr': 4.502544878809345e-05, 'weight_decay': 5.721312571085596e-08, 'batch_size': 32}. Best is trial 0 with value: 0.043870266526937485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.061992 - Val Loss: 0.156907\n",
      "Epoch 1/50 - Train Loss: 0.053789 - Val Loss: 0.080968\n",
      "Epoch 2/50 - Train Loss: 0.043279 - Val Loss: 0.094535\n",
      "Epoch 3/50 - Train Loss: 0.042910 - Val Loss: 0.089117\n",
      "Epoch 4/50 - Train Loss: 0.039341 - Val Loss: 0.061498\n",
      "Epoch 5/50 - Train Loss: 0.036203 - Val Loss: 0.049618\n",
      "Epoch 6/50 - Train Loss: 0.036772 - Val Loss: 0.048614\n",
      "Epoch 7/50 - Train Loss: 0.033099 - Val Loss: 0.053415\n",
      "Epoch 8/50 - Train Loss: 0.032203 - Val Loss: 0.051839\n",
      "Epoch 9/50 - Train Loss: 0.028556 - Val Loss: 0.050751\n",
      "Epoch 10/50 - Train Loss: 0.033061 - Val Loss: 0.045332\n",
      "Epoch 11/50 - Train Loss: 0.034173 - Val Loss: 0.047156\n",
      "Epoch 12/50 - Train Loss: 0.034833 - Val Loss: 0.046138\n",
      "Epoch 13/50 - Train Loss: 0.033008 - Val Loss: 0.046804\n",
      "Epoch 14/50 - Train Loss: 0.030548 - Val Loss: 0.049126\n",
      "Epoch 15/50 - Train Loss: 0.034059 - Val Loss: 0.048375\n",
      "Epoch 16/50 - Train Loss: 0.031816 - Val Loss: 0.046925\n",
      "Epoch 17/50 - Train Loss: 0.030805 - Val Loss: 0.044871\n",
      "Epoch 18/50 - Train Loss: 0.028450 - Val Loss: 0.046002\n",
      "Epoch 19/50 - Train Loss: 0.032955 - Val Loss: 0.052620\n",
      "Epoch 20/50 - Train Loss: 0.033658 - Val Loss: 0.046119\n",
      "Epoch 21/50 - Train Loss: 0.034548 - Val Loss: 0.050395\n",
      "Epoch 22/50 - Train Loss: 0.032768 - Val Loss: 0.045218\n",
      "Epoch 23/50 - Train Loss: 0.032610 - Val Loss: 0.045182\n",
      "Epoch 24/50 - Train Loss: 0.031577 - Val Loss: 0.044851\n",
      "Epoch 25/50 - Train Loss: 0.033607 - Val Loss: 0.046244\n",
      "Epoch 26/50 - Train Loss: 0.031531 - Val Loss: 0.046252\n",
      "Epoch 27/50 - Train Loss: 0.031814 - Val Loss: 0.048959\n",
      "Epoch 28/50 - Train Loss: 0.030987 - Val Loss: 0.047833\n",
      "Epoch 29/50 - Train Loss: 0.031400 - Val Loss: 0.048854\n",
      "Epoch 30/50 - Train Loss: 0.031830 - Val Loss: 0.045078\n",
      "Epoch 31/50 - Train Loss: 0.028948 - Val Loss: 0.045622\n",
      "Epoch 32/50 - Train Loss: 0.030062 - Val Loss: 0.047373\n",
      "Epoch 33/50 - Train Loss: 0.031573 - Val Loss: 0.050503\n",
      "Epoch 34/50 - Train Loss: 0.034960 - Val Loss: 0.048584\n",
      "Epoch 35/50 - Train Loss: 0.032436 - Val Loss: 0.044700\n",
      "Epoch 36/50 - Train Loss: 0.032685 - Val Loss: 0.045522\n",
      "Epoch 37/50 - Train Loss: 0.031705 - Val Loss: 0.046472\n",
      "Epoch 38/50 - Train Loss: 0.030792 - Val Loss: 0.046438\n",
      "Epoch 39/50 - Train Loss: 0.030297 - Val Loss: 0.046769\n",
      "Epoch 40/50 - Train Loss: 0.029895 - Val Loss: 0.046042\n",
      "Epoch 41/50 - Train Loss: 0.030058 - Val Loss: 0.046452\n",
      "Epoch 42/50 - Train Loss: 0.028343 - Val Loss: 0.045836\n",
      "Epoch 43/50 - Train Loss: 0.030951 - Val Loss: 0.049096\n",
      "Epoch 44/50 - Train Loss: 0.031614 - Val Loss: 0.045074\n",
      "Epoch 45/50 - Train Loss: 0.030038 - Val Loss: 0.049907\n",
      "Epoch 46/50 - Train Loss: 0.031537 - Val Loss: 0.045878\n",
      "Epoch 47/50 - Train Loss: 0.032169 - Val Loss: 0.049287\n",
      "Epoch 48/50 - Train Loss: 0.030518 - Val Loss: 0.046102\n",
      "Epoch 49/50 - Train Loss: 0.032061 - Val Loss: 0.045144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:12:29,058] Trial 2 finished with value: 0.044699677576621376 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 109, 'lr': 0.005940512724879179, 'weight_decay': 1.944288324432578e-05, 'batch_size': 8}. Best is trial 0 with value: 0.043870266526937485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029691 - Val Loss: 0.045163\n",
      "Epoch 1/50 - Train Loss: 0.094418 - Val Loss: 0.210334\n",
      "Epoch 2/50 - Train Loss: 0.087318 - Val Loss: 0.208167\n",
      "Epoch 3/50 - Train Loss: 0.095158 - Val Loss: 0.205995\n",
      "Epoch 4/50 - Train Loss: 0.090159 - Val Loss: 0.203823\n",
      "Epoch 5/50 - Train Loss: 0.097684 - Val Loss: 0.201605\n",
      "Epoch 6/50 - Train Loss: 0.088036 - Val Loss: 0.199475\n",
      "Epoch 7/50 - Train Loss: 0.088385 - Val Loss: 0.197321\n",
      "Epoch 8/50 - Train Loss: 0.085620 - Val Loss: 0.195154\n",
      "Epoch 9/50 - Train Loss: 0.087657 - Val Loss: 0.192987\n",
      "Epoch 10/50 - Train Loss: 0.083700 - Val Loss: 0.190780\n",
      "Epoch 11/50 - Train Loss: 0.080215 - Val Loss: 0.188582\n",
      "Epoch 12/50 - Train Loss: 0.083769 - Val Loss: 0.186454\n",
      "Epoch 13/50 - Train Loss: 0.082021 - Val Loss: 0.184275\n",
      "Epoch 14/50 - Train Loss: 0.078376 - Val Loss: 0.182136\n",
      "Epoch 15/50 - Train Loss: 0.075754 - Val Loss: 0.180022\n",
      "Epoch 16/50 - Train Loss: 0.074936 - Val Loss: 0.177958\n",
      "Epoch 17/50 - Train Loss: 0.077565 - Val Loss: 0.175956\n",
      "Epoch 18/50 - Train Loss: 0.075487 - Val Loss: 0.173886\n",
      "Epoch 19/50 - Train Loss: 0.074944 - Val Loss: 0.171875\n",
      "Epoch 20/50 - Train Loss: 0.079054 - Val Loss: 0.169791\n",
      "Epoch 21/50 - Train Loss: 0.074213 - Val Loss: 0.167726\n",
      "Epoch 22/50 - Train Loss: 0.067616 - Val Loss: 0.165763\n",
      "Epoch 23/50 - Train Loss: 0.066727 - Val Loss: 0.163851\n",
      "Epoch 24/50 - Train Loss: 0.065041 - Val Loss: 0.161977\n",
      "Epoch 25/50 - Train Loss: 0.070802 - Val Loss: 0.160131\n",
      "Epoch 26/50 - Train Loss: 0.067084 - Val Loss: 0.158279\n",
      "Epoch 27/50 - Train Loss: 0.064520 - Val Loss: 0.156445\n",
      "Epoch 28/50 - Train Loss: 0.059536 - Val Loss: 0.154618\n",
      "Epoch 29/50 - Train Loss: 0.067603 - Val Loss: 0.152793\n",
      "Epoch 30/50 - Train Loss: 0.057776 - Val Loss: 0.151008\n",
      "Epoch 31/50 - Train Loss: 0.060804 - Val Loss: 0.149287\n",
      "Epoch 32/50 - Train Loss: 0.064331 - Val Loss: 0.147475\n",
      "Epoch 33/50 - Train Loss: 0.062327 - Val Loss: 0.145716\n",
      "Epoch 34/50 - Train Loss: 0.058309 - Val Loss: 0.144043\n",
      "Epoch 35/50 - Train Loss: 0.060338 - Val Loss: 0.142361\n",
      "Epoch 36/50 - Train Loss: 0.059734 - Val Loss: 0.140818\n",
      "Epoch 37/50 - Train Loss: 0.059366 - Val Loss: 0.139192\n",
      "Epoch 38/50 - Train Loss: 0.057831 - Val Loss: 0.137610\n",
      "Epoch 39/50 - Train Loss: 0.058973 - Val Loss: 0.136025\n",
      "Epoch 40/50 - Train Loss: 0.055875 - Val Loss: 0.134506\n",
      "Epoch 41/50 - Train Loss: 0.053453 - Val Loss: 0.132973\n",
      "Epoch 42/50 - Train Loss: 0.052419 - Val Loss: 0.131506\n",
      "Epoch 43/50 - Train Loss: 0.056636 - Val Loss: 0.130076\n",
      "Epoch 44/50 - Train Loss: 0.055474 - Val Loss: 0.128640\n",
      "Epoch 45/50 - Train Loss: 0.053047 - Val Loss: 0.127233\n",
      "Epoch 46/50 - Train Loss: 0.051294 - Val Loss: 0.125870\n",
      "Epoch 47/50 - Train Loss: 0.055262 - Val Loss: 0.124534\n",
      "Epoch 48/50 - Train Loss: 0.049910 - Val Loss: 0.123246\n",
      "Epoch 49/50 - Train Loss: 0.053727 - Val Loss: 0.121963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:12:49,738] Trial 3 finished with value: 0.12064294517040253 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 36, 'lr': 3.3494636025567576e-05, 'weight_decay': 8.265481121184787e-08, 'batch_size': 16}. Best is trial 0 with value: 0.043870266526937485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.047829 - Val Loss: 0.120643\n",
      "Epoch 1/50 - Train Loss: 0.093504 - Val Loss: 0.090677\n",
      "Epoch 2/50 - Train Loss: 0.050195 - Val Loss: 0.097574\n",
      "Epoch 3/50 - Train Loss: 0.046334 - Val Loss: 0.081653\n",
      "Epoch 4/50 - Train Loss: 0.042819 - Val Loss: 0.080032\n",
      "Epoch 5/50 - Train Loss: 0.038525 - Val Loss: 0.064267\n",
      "Epoch 6/50 - Train Loss: 0.033320 - Val Loss: 0.061475\n",
      "Epoch 7/50 - Train Loss: 0.032294 - Val Loss: 0.058544\n",
      "Epoch 8/50 - Train Loss: 0.032984 - Val Loss: 0.051270\n",
      "Epoch 9/50 - Train Loss: 0.033864 - Val Loss: 0.050808\n",
      "Epoch 10/50 - Train Loss: 0.032537 - Val Loss: 0.049732\n",
      "Epoch 11/50 - Train Loss: 0.029785 - Val Loss: 0.057358\n",
      "Epoch 12/50 - Train Loss: 0.034392 - Val Loss: 0.048590\n",
      "Epoch 13/50 - Train Loss: 0.031899 - Val Loss: 0.050228\n",
      "Epoch 14/50 - Train Loss: 0.030768 - Val Loss: 0.056687\n",
      "Epoch 15/50 - Train Loss: 0.030754 - Val Loss: 0.051295\n",
      "Epoch 16/50 - Train Loss: 0.033675 - Val Loss: 0.049975\n",
      "Epoch 17/50 - Train Loss: 0.032242 - Val Loss: 0.047237\n",
      "Epoch 18/50 - Train Loss: 0.030116 - Val Loss: 0.047860\n",
      "Epoch 19/50 - Train Loss: 0.031371 - Val Loss: 0.047417\n",
      "Epoch 20/50 - Train Loss: 0.034023 - Val Loss: 0.051633\n",
      "Epoch 21/50 - Train Loss: 0.034090 - Val Loss: 0.056091\n",
      "Epoch 22/50 - Train Loss: 0.030638 - Val Loss: 0.053878\n",
      "Epoch 23/50 - Train Loss: 0.030044 - Val Loss: 0.048006\n",
      "Epoch 24/50 - Train Loss: 0.031324 - Val Loss: 0.051743\n",
      "Epoch 25/50 - Train Loss: 0.033311 - Val Loss: 0.049595\n",
      "Epoch 26/50 - Train Loss: 0.032778 - Val Loss: 0.050183\n",
      "Epoch 27/50 - Train Loss: 0.030254 - Val Loss: 0.047413\n",
      "Epoch 28/50 - Train Loss: 0.030779 - Val Loss: 0.046750\n",
      "Epoch 29/50 - Train Loss: 0.031882 - Val Loss: 0.049032\n",
      "Epoch 30/50 - Train Loss: 0.032375 - Val Loss: 0.048928\n",
      "Epoch 31/50 - Train Loss: 0.031063 - Val Loss: 0.047290\n",
      "Epoch 32/50 - Train Loss: 0.030815 - Val Loss: 0.047034\n",
      "Epoch 33/50 - Train Loss: 0.030598 - Val Loss: 0.049649\n",
      "Epoch 34/50 - Train Loss: 0.031849 - Val Loss: 0.049194\n",
      "Epoch 35/50 - Train Loss: 0.031451 - Val Loss: 0.049614\n",
      "Epoch 36/50 - Train Loss: 0.029811 - Val Loss: 0.049734\n",
      "Epoch 37/50 - Train Loss: 0.031847 - Val Loss: 0.046768\n",
      "Epoch 38/50 - Train Loss: 0.030190 - Val Loss: 0.047161\n",
      "Epoch 39/50 - Train Loss: 0.030170 - Val Loss: 0.051102\n",
      "Epoch 40/50 - Train Loss: 0.030669 - Val Loss: 0.049103\n",
      "Epoch 41/50 - Train Loss: 0.030821 - Val Loss: 0.048247\n",
      "Epoch 42/50 - Train Loss: 0.028095 - Val Loss: 0.047032\n",
      "Epoch 43/50 - Train Loss: 0.032541 - Val Loss: 0.047791\n",
      "Epoch 44/50 - Train Loss: 0.033213 - Val Loss: 0.048312\n",
      "Epoch 45/50 - Train Loss: 0.029668 - Val Loss: 0.048159\n",
      "Epoch 46/50 - Train Loss: 0.031555 - Val Loss: 0.047696\n",
      "Epoch 47/50 - Train Loss: 0.029273 - Val Loss: 0.048912\n",
      "Epoch 48/50 - Train Loss: 0.031641 - Val Loss: 0.046750\n",
      "Epoch 49/50 - Train Loss: 0.029709 - Val Loss: 0.045849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:13:26,819] Trial 4 finished with value: 0.0458485372364521 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 153, 'lr': 0.0005147746385641371, 'weight_decay': 0.000660676701229777, 'batch_size': 8}. Best is trial 0 with value: 0.043870266526937485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032134 - Val Loss: 0.047663\n",
      "Epoch 1/50 - Train Loss: 0.093860 - Val Loss: 0.216390\n",
      "Epoch 2/50 - Train Loss: 0.095644 - Val Loss: 0.214386\n",
      "Epoch 3/50 - Train Loss: 0.092631 - Val Loss: 0.212381\n",
      "Epoch 4/50 - Train Loss: 0.096446 - Val Loss: 0.210422\n",
      "Epoch 5/50 - Train Loss: 0.088141 - Val Loss: 0.208458\n",
      "Epoch 6/50 - Train Loss: 0.088462 - Val Loss: 0.206570\n",
      "Epoch 7/50 - Train Loss: 0.086166 - Val Loss: 0.204641\n",
      "Epoch 8/50 - Train Loss: 0.090091 - Val Loss: 0.202703\n",
      "Epoch 9/50 - Train Loss: 0.083484 - Val Loss: 0.200797\n",
      "Epoch 10/50 - Train Loss: 0.087531 - Val Loss: 0.198862\n",
      "Epoch 11/50 - Train Loss: 0.085338 - Val Loss: 0.196876\n",
      "Epoch 12/50 - Train Loss: 0.077625 - Val Loss: 0.194965\n",
      "Epoch 13/50 - Train Loss: 0.083502 - Val Loss: 0.192955\n",
      "Epoch 14/50 - Train Loss: 0.082544 - Val Loss: 0.190863\n",
      "Epoch 15/50 - Train Loss: 0.079374 - Val Loss: 0.188800\n",
      "Epoch 16/50 - Train Loss: 0.076800 - Val Loss: 0.186678\n",
      "Epoch 17/50 - Train Loss: 0.078472 - Val Loss: 0.184545\n",
      "Epoch 18/50 - Train Loss: 0.070974 - Val Loss: 0.182413\n",
      "Epoch 19/50 - Train Loss: 0.069482 - Val Loss: 0.180347\n",
      "Epoch 20/50 - Train Loss: 0.074144 - Val Loss: 0.178280\n",
      "Epoch 21/50 - Train Loss: 0.075549 - Val Loss: 0.176197\n",
      "Epoch 22/50 - Train Loss: 0.079154 - Val Loss: 0.173978\n",
      "Epoch 23/50 - Train Loss: 0.070692 - Val Loss: 0.171736\n",
      "Epoch 24/50 - Train Loss: 0.073321 - Val Loss: 0.169596\n",
      "Epoch 25/50 - Train Loss: 0.066688 - Val Loss: 0.167427\n",
      "Epoch 26/50 - Train Loss: 0.065853 - Val Loss: 0.165324\n",
      "Epoch 27/50 - Train Loss: 0.071409 - Val Loss: 0.163228\n",
      "Epoch 28/50 - Train Loss: 0.068496 - Val Loss: 0.161139\n",
      "Epoch 29/50 - Train Loss: 0.063386 - Val Loss: 0.159150\n",
      "Epoch 30/50 - Train Loss: 0.061690 - Val Loss: 0.157176\n",
      "Epoch 31/50 - Train Loss: 0.066695 - Val Loss: 0.155201\n",
      "Epoch 32/50 - Train Loss: 0.061809 - Val Loss: 0.153126\n",
      "Epoch 33/50 - Train Loss: 0.064169 - Val Loss: 0.151204\n",
      "Epoch 34/50 - Train Loss: 0.057111 - Val Loss: 0.149357\n",
      "Epoch 35/50 - Train Loss: 0.059554 - Val Loss: 0.147552\n",
      "Epoch 36/50 - Train Loss: 0.057051 - Val Loss: 0.145769\n",
      "Epoch 37/50 - Train Loss: 0.059544 - Val Loss: 0.144037\n",
      "Epoch 38/50 - Train Loss: 0.061883 - Val Loss: 0.142167\n",
      "Epoch 39/50 - Train Loss: 0.058144 - Val Loss: 0.140354\n",
      "Epoch 40/50 - Train Loss: 0.056699 - Val Loss: 0.138705\n",
      "Epoch 41/50 - Train Loss: 0.057491 - Val Loss: 0.137001\n",
      "Epoch 42/50 - Train Loss: 0.057692 - Val Loss: 0.135257\n",
      "Epoch 43/50 - Train Loss: 0.058497 - Val Loss: 0.133598\n",
      "Epoch 44/50 - Train Loss: 0.053320 - Val Loss: 0.132072\n",
      "Epoch 45/50 - Train Loss: 0.057000 - Val Loss: 0.130603\n",
      "Epoch 46/50 - Train Loss: 0.055335 - Val Loss: 0.129130\n",
      "Epoch 47/50 - Train Loss: 0.055748 - Val Loss: 0.127634\n",
      "Epoch 48/50 - Train Loss: 0.054683 - Val Loss: 0.126199\n",
      "Epoch 49/50 - Train Loss: 0.048939 - Val Loss: 0.124812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:13:46,058] Trial 5 finished with value: 0.1234702467918396 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 56, 'lr': 2.1993864666808667e-05, 'weight_decay': 0.0006746200759243931, 'batch_size': 16}. Best is trial 0 with value: 0.043870266526937485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.048776 - Val Loss: 0.123470\n",
      "Epoch 1/50 - Train Loss: 0.182454 - Val Loss: 0.185094\n",
      "Epoch 2/50 - Train Loss: 0.056789 - Val Loss: 0.144236\n",
      "Epoch 3/50 - Train Loss: 0.051158 - Val Loss: 0.051810\n",
      "Epoch 4/50 - Train Loss: 0.035117 - Val Loss: 0.066402\n",
      "Epoch 5/50 - Train Loss: 0.034537 - Val Loss: 0.051142\n",
      "Epoch 6/50 - Train Loss: 0.033031 - Val Loss: 0.048305\n",
      "Epoch 7/50 - Train Loss: 0.034433 - Val Loss: 0.051790\n",
      "Epoch 8/50 - Train Loss: 0.038570 - Val Loss: 0.055488\n",
      "Epoch 9/50 - Train Loss: 0.034543 - Val Loss: 0.045961\n",
      "Epoch 10/50 - Train Loss: 0.031212 - Val Loss: 0.051607\n",
      "Epoch 11/50 - Train Loss: 0.033745 - Val Loss: 0.048497\n",
      "Epoch 12/50 - Train Loss: 0.031578 - Val Loss: 0.049038\n",
      "Epoch 13/50 - Train Loss: 0.034798 - Val Loss: 0.052400\n",
      "Epoch 14/50 - Train Loss: 0.031412 - Val Loss: 0.047024\n",
      "Epoch 15/50 - Train Loss: 0.033732 - Val Loss: 0.056282\n",
      "Epoch 16/50 - Train Loss: 0.035280 - Val Loss: 0.050913\n",
      "Epoch 17/50 - Train Loss: 0.033527 - Val Loss: 0.044563\n",
      "Epoch 18/50 - Train Loss: 0.032503 - Val Loss: 0.044983\n",
      "Epoch 19/50 - Train Loss: 0.025689 - Val Loss: 0.043914\n",
      "Epoch 20/50 - Train Loss: 0.033068 - Val Loss: 0.044291\n",
      "Epoch 21/50 - Train Loss: 0.035852 - Val Loss: 0.044504\n",
      "Epoch 22/50 - Train Loss: 0.034637 - Val Loss: 0.047795\n",
      "Epoch 23/50 - Train Loss: 0.030902 - Val Loss: 0.055215\n",
      "Epoch 24/50 - Train Loss: 0.032110 - Val Loss: 0.047316\n",
      "Epoch 25/50 - Train Loss: 0.030362 - Val Loss: 0.045948\n",
      "Epoch 26/50 - Train Loss: 0.031421 - Val Loss: 0.046006\n",
      "Epoch 27/50 - Train Loss: 0.026529 - Val Loss: 0.043944\n",
      "Epoch 28/50 - Train Loss: 0.034532 - Val Loss: 0.044765\n",
      "Epoch 29/50 - Train Loss: 0.028995 - Val Loss: 0.044634\n",
      "Epoch 30/50 - Train Loss: 0.033480 - Val Loss: 0.047647\n",
      "Epoch 31/50 - Train Loss: 0.031650 - Val Loss: 0.062469\n",
      "Epoch 32/50 - Train Loss: 0.029700 - Val Loss: 0.045867\n",
      "Epoch 33/50 - Train Loss: 0.029639 - Val Loss: 0.045276\n",
      "Epoch 34/50 - Train Loss: 0.032061 - Val Loss: 0.043570\n",
      "Epoch 35/50 - Train Loss: 0.030934 - Val Loss: 0.054913\n",
      "Epoch 36/50 - Train Loss: 0.033646 - Val Loss: 0.047659\n",
      "Epoch 37/50 - Train Loss: 0.029560 - Val Loss: 0.043912\n",
      "Epoch 38/50 - Train Loss: 0.026973 - Val Loss: 0.044358\n",
      "Epoch 39/50 - Train Loss: 0.031296 - Val Loss: 0.044144\n",
      "Epoch 40/50 - Train Loss: 0.031547 - Val Loss: 0.061436\n",
      "Epoch 41/50 - Train Loss: 0.031592 - Val Loss: 0.045888\n",
      "Epoch 42/50 - Train Loss: 0.031684 - Val Loss: 0.044115\n",
      "Epoch 43/50 - Train Loss: 0.032447 - Val Loss: 0.044954\n",
      "Epoch 44/50 - Train Loss: 0.030945 - Val Loss: 0.043956\n",
      "Epoch 45/50 - Train Loss: 0.034589 - Val Loss: 0.045211\n",
      "Epoch 46/50 - Train Loss: 0.029116 - Val Loss: 0.047655\n",
      "Epoch 47/50 - Train Loss: 0.031793 - Val Loss: 0.044205\n",
      "Epoch 48/50 - Train Loss: 0.035225 - Val Loss: 0.044048\n",
      "Epoch 49/50 - Train Loss: 0.030087 - Val Loss: 0.045080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:14:12,127] Trial 6 finished with value: 0.04356965236365795 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 164, 'lr': 0.009243975318693429, 'weight_decay': 8.139882081584634e-07, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031498 - Val Loss: 0.050242\n",
      "Epoch 1/50 - Train Loss: 0.173639 - Val Loss: 0.332815\n",
      "Epoch 2/50 - Train Loss: 0.162100 - Val Loss: 0.310834\n",
      "Epoch 3/50 - Train Loss: 0.139640 - Val Loss: 0.290278\n",
      "Epoch 4/50 - Train Loss: 0.129920 - Val Loss: 0.270738\n",
      "Epoch 5/50 - Train Loss: 0.118904 - Val Loss: 0.251695\n",
      "Epoch 6/50 - Train Loss: 0.106166 - Val Loss: 0.233250\n",
      "Epoch 7/50 - Train Loss: 0.110933 - Val Loss: 0.215359\n",
      "Epoch 8/50 - Train Loss: 0.083890 - Val Loss: 0.198303\n",
      "Epoch 9/50 - Train Loss: 0.080573 - Val Loss: 0.182251\n",
      "Epoch 10/50 - Train Loss: 0.067215 - Val Loss: 0.167440\n",
      "Epoch 11/50 - Train Loss: 0.062870 - Val Loss: 0.154139\n",
      "Epoch 12/50 - Train Loss: 0.064009 - Val Loss: 0.141733\n",
      "Epoch 13/50 - Train Loss: 0.053578 - Val Loss: 0.130685\n",
      "Epoch 14/50 - Train Loss: 0.045353 - Val Loss: 0.121555\n",
      "Epoch 15/50 - Train Loss: 0.047603 - Val Loss: 0.113876\n",
      "Epoch 16/50 - Train Loss: 0.050475 - Val Loss: 0.107891\n",
      "Epoch 17/50 - Train Loss: 0.046893 - Val Loss: 0.102766\n",
      "Epoch 18/50 - Train Loss: 0.043340 - Val Loss: 0.098914\n",
      "Epoch 19/50 - Train Loss: 0.047189 - Val Loss: 0.096057\n",
      "Epoch 20/50 - Train Loss: 0.048318 - Val Loss: 0.093484\n",
      "Epoch 21/50 - Train Loss: 0.044850 - Val Loss: 0.091495\n",
      "Epoch 22/50 - Train Loss: 0.045152 - Val Loss: 0.090114\n",
      "Epoch 23/50 - Train Loss: 0.045042 - Val Loss: 0.089273\n",
      "Epoch 24/50 - Train Loss: 0.035600 - Val Loss: 0.088428\n",
      "Epoch 25/50 - Train Loss: 0.041936 - Val Loss: 0.088001\n",
      "Epoch 26/50 - Train Loss: 0.044392 - Val Loss: 0.087312\n",
      "Epoch 27/50 - Train Loss: 0.037096 - Val Loss: 0.086649\n",
      "Epoch 28/50 - Train Loss: 0.046591 - Val Loss: 0.086159\n",
      "Epoch 29/50 - Train Loss: 0.044037 - Val Loss: 0.085497\n",
      "Epoch 30/50 - Train Loss: 0.045309 - Val Loss: 0.084179\n",
      "Epoch 31/50 - Train Loss: 0.042751 - Val Loss: 0.083467\n",
      "Epoch 32/50 - Train Loss: 0.040191 - Val Loss: 0.082800\n",
      "Epoch 33/50 - Train Loss: 0.039410 - Val Loss: 0.081728\n",
      "Epoch 34/50 - Train Loss: 0.037281 - Val Loss: 0.080545\n",
      "Epoch 35/50 - Train Loss: 0.044668 - Val Loss: 0.079127\n",
      "Epoch 36/50 - Train Loss: 0.040849 - Val Loss: 0.077272\n",
      "Epoch 37/50 - Train Loss: 0.030080 - Val Loss: 0.075975\n",
      "Epoch 38/50 - Train Loss: 0.031764 - Val Loss: 0.074953\n",
      "Epoch 39/50 - Train Loss: 0.039579 - Val Loss: 0.074713\n",
      "Epoch 40/50 - Train Loss: 0.039680 - Val Loss: 0.074152\n",
      "Epoch 41/50 - Train Loss: 0.034036 - Val Loss: 0.073246\n",
      "Epoch 42/50 - Train Loss: 0.034881 - Val Loss: 0.072648\n",
      "Epoch 43/50 - Train Loss: 0.039771 - Val Loss: 0.072105\n",
      "Epoch 44/50 - Train Loss: 0.033229 - Val Loss: 0.071141\n",
      "Epoch 45/50 - Train Loss: 0.034416 - Val Loss: 0.069979\n",
      "Epoch 46/50 - Train Loss: 0.031705 - Val Loss: 0.068537\n",
      "Epoch 47/50 - Train Loss: 0.033256 - Val Loss: 0.067828\n",
      "Epoch 48/50 - Train Loss: 0.035794 - Val Loss: 0.066726\n",
      "Epoch 49/50 - Train Loss: 0.037248 - Val Loss: 0.065314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:14:36,872] Trial 7 finished with value: 0.06447701156139374 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 242, 'lr': 4.3885662735085897e-05, 'weight_decay': 6.878860850948933e-08, 'batch_size': 32}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032703 - Val Loss: 0.064477\n",
      "Epoch 1/50 - Train Loss: 0.181932 - Val Loss: 0.326311\n",
      "Epoch 2/50 - Train Loss: 0.157503 - Val Loss: 0.281331\n",
      "Epoch 3/50 - Train Loss: 0.141934 - Val Loss: 0.239369\n",
      "Epoch 4/50 - Train Loss: 0.110532 - Val Loss: 0.198745\n",
      "Epoch 5/50 - Train Loss: 0.087862 - Val Loss: 0.159438\n",
      "Epoch 6/50 - Train Loss: 0.064442 - Val Loss: 0.124779\n",
      "Epoch 7/50 - Train Loss: 0.050928 - Val Loss: 0.097409\n",
      "Epoch 8/50 - Train Loss: 0.044116 - Val Loss: 0.081196\n",
      "Epoch 9/50 - Train Loss: 0.046452 - Val Loss: 0.075132\n",
      "Epoch 10/50 - Train Loss: 0.052715 - Val Loss: 0.076121\n",
      "Epoch 11/50 - Train Loss: 0.044843 - Val Loss: 0.082157\n",
      "Epoch 12/50 - Train Loss: 0.048247 - Val Loss: 0.087571\n",
      "Epoch 13/50 - Train Loss: 0.043484 - Val Loss: 0.091402\n",
      "Epoch 14/50 - Train Loss: 0.041212 - Val Loss: 0.091286\n",
      "Epoch 15/50 - Train Loss: 0.038838 - Val Loss: 0.089669\n",
      "Epoch 16/50 - Train Loss: 0.037712 - Val Loss: 0.087144\n",
      "Epoch 17/50 - Train Loss: 0.035265 - Val Loss: 0.083192\n",
      "Epoch 18/50 - Train Loss: 0.038979 - Val Loss: 0.079539\n",
      "Epoch 19/50 - Train Loss: 0.036544 - Val Loss: 0.078873\n",
      "Epoch 20/50 - Train Loss: 0.041674 - Val Loss: 0.079285\n",
      "Epoch 21/50 - Train Loss: 0.040865 - Val Loss: 0.079886\n",
      "Epoch 22/50 - Train Loss: 0.040113 - Val Loss: 0.077111\n",
      "Epoch 23/50 - Train Loss: 0.044004 - Val Loss: 0.071664\n",
      "Epoch 24/50 - Train Loss: 0.034166 - Val Loss: 0.068336\n",
      "Epoch 25/50 - Train Loss: 0.038100 - Val Loss: 0.068568\n",
      "Epoch 26/50 - Train Loss: 0.036619 - Val Loss: 0.069144\n",
      "Epoch 27/50 - Train Loss: 0.033535 - Val Loss: 0.064061\n",
      "Epoch 28/50 - Train Loss: 0.038545 - Val Loss: 0.061427\n",
      "Epoch 29/50 - Train Loss: 0.035265 - Val Loss: 0.061279\n",
      "Epoch 30/50 - Train Loss: 0.032272 - Val Loss: 0.059221\n",
      "Epoch 31/50 - Train Loss: 0.031907 - Val Loss: 0.054340\n",
      "Epoch 32/50 - Train Loss: 0.032872 - Val Loss: 0.053041\n",
      "Epoch 33/50 - Train Loss: 0.027722 - Val Loss: 0.054252\n",
      "Epoch 34/50 - Train Loss: 0.034622 - Val Loss: 0.051841\n",
      "Epoch 35/50 - Train Loss: 0.033056 - Val Loss: 0.050362\n",
      "Epoch 36/50 - Train Loss: 0.035251 - Val Loss: 0.049778\n",
      "Epoch 37/50 - Train Loss: 0.032849 - Val Loss: 0.049521\n",
      "Epoch 38/50 - Train Loss: 0.033701 - Val Loss: 0.050151\n",
      "Epoch 39/50 - Train Loss: 0.033182 - Val Loss: 0.050646\n",
      "Epoch 40/50 - Train Loss: 0.034247 - Val Loss: 0.049212\n",
      "Epoch 41/50 - Train Loss: 0.035497 - Val Loss: 0.048873\n",
      "Epoch 42/50 - Train Loss: 0.028861 - Val Loss: 0.049592\n",
      "Epoch 43/50 - Train Loss: 0.033476 - Val Loss: 0.048685\n",
      "Epoch 44/50 - Train Loss: 0.028452 - Val Loss: 0.048175\n",
      "Epoch 45/50 - Train Loss: 0.032134 - Val Loss: 0.048918\n",
      "Epoch 46/50 - Train Loss: 0.033987 - Val Loss: 0.049162\n",
      "Epoch 47/50 - Train Loss: 0.036476 - Val Loss: 0.047707\n",
      "Epoch 48/50 - Train Loss: 0.030129 - Val Loss: 0.047686\n",
      "Epoch 49/50 - Train Loss: 0.032646 - Val Loss: 0.050410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:14:57,978] Trial 8 finished with value: 0.047686319798231125 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 85, 'lr': 0.0003823418692933857, 'weight_decay': 0.00015108618779837544, 'batch_size': 32}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029436 - Val Loss: 0.050162\n",
      "Epoch 1/50 - Train Loss: 0.112424 - Val Loss: 0.075290\n",
      "Epoch 2/50 - Train Loss: 0.048067 - Val Loss: 0.088924\n",
      "Epoch 3/50 - Train Loss: 0.041368 - Val Loss: 0.073239\n",
      "Epoch 4/50 - Train Loss: 0.038141 - Val Loss: 0.072904\n",
      "Epoch 5/50 - Train Loss: 0.038022 - Val Loss: 0.058200\n",
      "Epoch 6/50 - Train Loss: 0.032456 - Val Loss: 0.049856\n",
      "Epoch 7/50 - Train Loss: 0.032909 - Val Loss: 0.048147\n",
      "Epoch 8/50 - Train Loss: 0.035325 - Val Loss: 0.048650\n",
      "Epoch 9/50 - Train Loss: 0.034887 - Val Loss: 0.047663\n",
      "Epoch 10/50 - Train Loss: 0.034156 - Val Loss: 0.047042\n",
      "Epoch 11/50 - Train Loss: 0.033543 - Val Loss: 0.046078\n",
      "Epoch 12/50 - Train Loss: 0.028823 - Val Loss: 0.045855\n",
      "Epoch 13/50 - Train Loss: 0.030537 - Val Loss: 0.045977\n",
      "Epoch 14/50 - Train Loss: 0.032807 - Val Loss: 0.053487\n",
      "Epoch 15/50 - Train Loss: 0.029262 - Val Loss: 0.049473\n",
      "Epoch 16/50 - Train Loss: 0.030080 - Val Loss: 0.053133\n",
      "Epoch 17/50 - Train Loss: 0.032138 - Val Loss: 0.048253\n",
      "Epoch 18/50 - Train Loss: 0.033187 - Val Loss: 0.047278\n",
      "Epoch 19/50 - Train Loss: 0.033275 - Val Loss: 0.046160\n",
      "Epoch 20/50 - Train Loss: 0.031893 - Val Loss: 0.049219\n",
      "Epoch 21/50 - Train Loss: 0.029699 - Val Loss: 0.044980\n",
      "Epoch 22/50 - Train Loss: 0.030747 - Val Loss: 0.047193\n",
      "Epoch 23/50 - Train Loss: 0.029516 - Val Loss: 0.045438\n",
      "Epoch 24/50 - Train Loss: 0.030126 - Val Loss: 0.052010\n",
      "Epoch 25/50 - Train Loss: 0.031645 - Val Loss: 0.048464\n",
      "Epoch 26/50 - Train Loss: 0.033963 - Val Loss: 0.046005\n",
      "Epoch 27/50 - Train Loss: 0.030816 - Val Loss: 0.045954\n",
      "Epoch 28/50 - Train Loss: 0.032999 - Val Loss: 0.045538\n",
      "Epoch 29/50 - Train Loss: 0.030084 - Val Loss: 0.045728\n",
      "Epoch 30/50 - Train Loss: 0.026899 - Val Loss: 0.044930\n",
      "Epoch 31/50 - Train Loss: 0.030435 - Val Loss: 0.047277\n",
      "Epoch 32/50 - Train Loss: 0.032073 - Val Loss: 0.046814\n",
      "Epoch 33/50 - Train Loss: 0.029900 - Val Loss: 0.045140\n",
      "Epoch 34/50 - Train Loss: 0.031007 - Val Loss: 0.049533\n",
      "Epoch 35/50 - Train Loss: 0.030383 - Val Loss: 0.045743\n",
      "Epoch 36/50 - Train Loss: 0.029457 - Val Loss: 0.044591\n",
      "Epoch 37/50 - Train Loss: 0.030000 - Val Loss: 0.045678\n",
      "Epoch 38/50 - Train Loss: 0.031100 - Val Loss: 0.045875\n",
      "Epoch 39/50 - Train Loss: 0.031458 - Val Loss: 0.045805\n",
      "Epoch 40/50 - Train Loss: 0.029309 - Val Loss: 0.051937\n",
      "Epoch 41/50 - Train Loss: 0.032957 - Val Loss: 0.052283\n",
      "Epoch 42/50 - Train Loss: 0.028180 - Val Loss: 0.045504\n",
      "Epoch 43/50 - Train Loss: 0.029176 - Val Loss: 0.044842\n",
      "Epoch 44/50 - Train Loss: 0.031207 - Val Loss: 0.047121\n",
      "Epoch 45/50 - Train Loss: 0.028498 - Val Loss: 0.047637\n",
      "Epoch 46/50 - Train Loss: 0.028334 - Val Loss: 0.045570\n",
      "Epoch 47/50 - Train Loss: 0.032371 - Val Loss: 0.045848\n",
      "Epoch 48/50 - Train Loss: 0.029319 - Val Loss: 0.047123\n",
      "Epoch 49/50 - Train Loss: 0.029199 - Val Loss: 0.046165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:15:55,194] Trial 9 finished with value: 0.044591274112463 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 131, 'lr': 0.0008134097219153354, 'weight_decay': 1.66044333733546e-06, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030620 - Val Loss: 0.048389\n",
      "Epoch 1/50 - Train Loss: 0.093658 - Val Loss: 0.074661\n",
      "Epoch 2/50 - Train Loss: 0.068313 - Val Loss: 0.067282\n",
      "Epoch 3/50 - Train Loss: 0.057629 - Val Loss: 0.078750\n",
      "Epoch 4/50 - Train Loss: 0.039074 - Val Loss: 0.095406\n",
      "Epoch 5/50 - Train Loss: 0.048774 - Val Loss: 0.092919\n",
      "Epoch 6/50 - Train Loss: 0.039409 - Val Loss: 0.075756\n",
      "Epoch 7/50 - Train Loss: 0.042907 - Val Loss: 0.060544\n",
      "Epoch 8/50 - Train Loss: 0.041980 - Val Loss: 0.054214\n",
      "Epoch 9/50 - Train Loss: 0.037226 - Val Loss: 0.052725\n",
      "Epoch 10/50 - Train Loss: 0.033392 - Val Loss: 0.054548\n",
      "Epoch 11/50 - Train Loss: 0.032361 - Val Loss: 0.057560\n",
      "Epoch 12/50 - Train Loss: 0.039194 - Val Loss: 0.057884\n",
      "Epoch 13/50 - Train Loss: 0.041122 - Val Loss: 0.054590\n",
      "Epoch 14/50 - Train Loss: 0.039422 - Val Loss: 0.050422\n",
      "Epoch 15/50 - Train Loss: 0.039280 - Val Loss: 0.048703\n",
      "Epoch 16/50 - Train Loss: 0.030681 - Val Loss: 0.048229\n",
      "Epoch 17/50 - Train Loss: 0.038625 - Val Loss: 0.048431\n",
      "Epoch 18/50 - Train Loss: 0.029640 - Val Loss: 0.048648\n",
      "Epoch 19/50 - Train Loss: 0.037172 - Val Loss: 0.048515\n",
      "Epoch 20/50 - Train Loss: 0.037686 - Val Loss: 0.047951\n",
      "Epoch 21/50 - Train Loss: 0.036874 - Val Loss: 0.047628\n",
      "Epoch 22/50 - Train Loss: 0.036712 - Val Loss: 0.047616\n",
      "Epoch 23/50 - Train Loss: 0.037551 - Val Loss: 0.048184\n",
      "Epoch 24/50 - Train Loss: 0.029827 - Val Loss: 0.048131\n",
      "Epoch 25/50 - Train Loss: 0.038505 - Val Loss: 0.048039\n",
      "Epoch 26/50 - Train Loss: 0.036881 - Val Loss: 0.047999\n",
      "Epoch 27/50 - Train Loss: 0.029399 - Val Loss: 0.047679\n",
      "Epoch 28/50 - Train Loss: 0.035795 - Val Loss: 0.047298\n",
      "Epoch 29/50 - Train Loss: 0.035772 - Val Loss: 0.047148\n",
      "Epoch 30/50 - Train Loss: 0.028696 - Val Loss: 0.046577\n",
      "Epoch 31/50 - Train Loss: 0.029236 - Val Loss: 0.046000\n",
      "Epoch 32/50 - Train Loss: 0.037518 - Val Loss: 0.046090\n",
      "Epoch 33/50 - Train Loss: 0.038246 - Val Loss: 0.046984\n",
      "Epoch 34/50 - Train Loss: 0.029699 - Val Loss: 0.046441\n",
      "Epoch 35/50 - Train Loss: 0.028483 - Val Loss: 0.045426\n",
      "Epoch 36/50 - Train Loss: 0.028504 - Val Loss: 0.045474\n",
      "Epoch 37/50 - Train Loss: 0.036261 - Val Loss: 0.046875\n",
      "Epoch 38/50 - Train Loss: 0.028933 - Val Loss: 0.047792\n",
      "Epoch 39/50 - Train Loss: 0.037319 - Val Loss: 0.046649\n",
      "Epoch 40/50 - Train Loss: 0.030216 - Val Loss: 0.045559\n",
      "Epoch 41/50 - Train Loss: 0.029644 - Val Loss: 0.045851\n",
      "Epoch 42/50 - Train Loss: 0.028758 - Val Loss: 0.048028\n",
      "Epoch 43/50 - Train Loss: 0.029028 - Val Loss: 0.047787\n",
      "Epoch 44/50 - Train Loss: 0.035608 - Val Loss: 0.045966\n",
      "Epoch 45/50 - Train Loss: 0.028353 - Val Loss: 0.045410\n",
      "Epoch 46/50 - Train Loss: 0.035059 - Val Loss: 0.046439\n",
      "Epoch 47/50 - Train Loss: 0.028845 - Val Loss: 0.046818\n",
      "Epoch 48/50 - Train Loss: 0.028818 - Val Loss: 0.045817\n",
      "Epoch 49/50 - Train Loss: 0.028143 - Val Loss: 0.045432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:16:07,266] Trial 10 finished with value: 0.04540974646806717 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 189, 'lr': 0.0015393969240453788, 'weight_decay': 1.8915930870876385e-06, 'batch_size': 64}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.037296 - Val Loss: 0.046087\n",
      "Epoch 1/50 - Train Loss: 0.190263 - Val Loss: 0.297904\n",
      "Epoch 2/50 - Train Loss: 0.088903 - Val Loss: 0.078998\n",
      "Epoch 3/50 - Train Loss: 0.052749 - Val Loss: 0.060933\n",
      "Epoch 4/50 - Train Loss: 0.034997 - Val Loss: 0.098967\n",
      "Epoch 5/50 - Train Loss: 0.040951 - Val Loss: 0.060478\n",
      "Epoch 6/50 - Train Loss: 0.036600 - Val Loss: 0.053243\n",
      "Epoch 7/50 - Train Loss: 0.034196 - Val Loss: 0.058274\n",
      "Epoch 8/50 - Train Loss: 0.037321 - Val Loss: 0.049915\n",
      "Epoch 9/50 - Train Loss: 0.035425 - Val Loss: 0.049019\n",
      "Epoch 10/50 - Train Loss: 0.033882 - Val Loss: 0.047909\n",
      "Epoch 11/50 - Train Loss: 0.032432 - Val Loss: 0.046615\n",
      "Epoch 12/50 - Train Loss: 0.033755 - Val Loss: 0.046715\n",
      "Epoch 13/50 - Train Loss: 0.033919 - Val Loss: 0.046164\n",
      "Epoch 14/50 - Train Loss: 0.030642 - Val Loss: 0.052946\n",
      "Epoch 15/50 - Train Loss: 0.030479 - Val Loss: 0.045211\n",
      "Epoch 16/50 - Train Loss: 0.030889 - Val Loss: 0.054724\n",
      "Epoch 17/50 - Train Loss: 0.035808 - Val Loss: 0.045976\n",
      "Epoch 18/50 - Train Loss: 0.029726 - Val Loss: 0.048407\n",
      "Epoch 19/50 - Train Loss: 0.029215 - Val Loss: 0.049411\n",
      "Epoch 20/50 - Train Loss: 0.029009 - Val Loss: 0.045647\n",
      "Epoch 21/50 - Train Loss: 0.032275 - Val Loss: 0.051288\n",
      "Epoch 22/50 - Train Loss: 0.030436 - Val Loss: 0.047731\n",
      "Epoch 23/50 - Train Loss: 0.031244 - Val Loss: 0.046524\n",
      "Epoch 24/50 - Train Loss: 0.033322 - Val Loss: 0.049106\n",
      "Epoch 25/50 - Train Loss: 0.033312 - Val Loss: 0.045466\n",
      "Epoch 26/50 - Train Loss: 0.030256 - Val Loss: 0.049732\n",
      "Epoch 27/50 - Train Loss: 0.030630 - Val Loss: 0.048407\n",
      "Epoch 28/50 - Train Loss: 0.033967 - Val Loss: 0.044635\n",
      "Epoch 29/50 - Train Loss: 0.030278 - Val Loss: 0.066790\n",
      "Epoch 30/50 - Train Loss: 0.033871 - Val Loss: 0.045153\n",
      "Epoch 31/50 - Train Loss: 0.032674 - Val Loss: 0.059345\n",
      "Epoch 32/50 - Train Loss: 0.031027 - Val Loss: 0.044621\n",
      "Epoch 33/50 - Train Loss: 0.030045 - Val Loss: 0.049414\n",
      "Epoch 34/50 - Train Loss: 0.031905 - Val Loss: 0.044793\n",
      "Epoch 35/50 - Train Loss: 0.029423 - Val Loss: 0.049005\n",
      "Epoch 36/50 - Train Loss: 0.031241 - Val Loss: 0.046066\n",
      "Epoch 37/50 - Train Loss: 0.032742 - Val Loss: 0.046347\n",
      "Epoch 38/50 - Train Loss: 0.031437 - Val Loss: 0.044676\n",
      "Epoch 39/50 - Train Loss: 0.030277 - Val Loss: 0.046782\n",
      "Epoch 40/50 - Train Loss: 0.029534 - Val Loss: 0.047680\n",
      "Epoch 41/50 - Train Loss: 0.032090 - Val Loss: 0.044679\n",
      "Epoch 42/50 - Train Loss: 0.029669 - Val Loss: 0.049642\n",
      "Epoch 43/50 - Train Loss: 0.029704 - Val Loss: 0.044439\n",
      "Epoch 44/50 - Train Loss: 0.027973 - Val Loss: 0.047066\n",
      "Epoch 45/50 - Train Loss: 0.030116 - Val Loss: 0.046506\n",
      "Epoch 46/50 - Train Loss: 0.035303 - Val Loss: 0.047643\n",
      "Epoch 47/50 - Train Loss: 0.030474 - Val Loss: 0.045438\n",
      "Epoch 48/50 - Train Loss: 0.031120 - Val Loss: 0.045785\n",
      "Epoch 49/50 - Train Loss: 0.030079 - Val Loss: 0.046716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:16:28,946] Trial 11 finished with value: 0.04443853907287121 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 177, 'lr': 0.007428290693454997, 'weight_decay': 1.850208298028261e-08, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027562 - Val Loss: 0.045029\n",
      "Epoch 1/50 - Train Loss: 0.143938 - Val Loss: 0.103224\n",
      "Epoch 2/50 - Train Loss: 0.093622 - Val Loss: 0.122816\n",
      "Epoch 3/50 - Train Loss: 0.059121 - Val Loss: 0.157176\n",
      "Epoch 4/50 - Train Loss: 0.060436 - Val Loss: 0.119252\n",
      "Epoch 5/50 - Train Loss: 0.045849 - Val Loss: 0.078488\n",
      "Epoch 6/50 - Train Loss: 0.045393 - Val Loss: 0.066891\n",
      "Epoch 7/50 - Train Loss: 0.056951 - Val Loss: 0.072577\n",
      "Epoch 8/50 - Train Loss: 0.038377 - Val Loss: 0.096297\n",
      "Epoch 9/50 - Train Loss: 0.040347 - Val Loss: 0.097803\n",
      "Epoch 10/50 - Train Loss: 0.047015 - Val Loss: 0.076730\n",
      "Epoch 11/50 - Train Loss: 0.034991 - Val Loss: 0.054829\n",
      "Epoch 12/50 - Train Loss: 0.035794 - Val Loss: 0.049423\n",
      "Epoch 13/50 - Train Loss: 0.035658 - Val Loss: 0.055696\n",
      "Epoch 14/50 - Train Loss: 0.039158 - Val Loss: 0.066968\n",
      "Epoch 15/50 - Train Loss: 0.034637 - Val Loss: 0.063251\n",
      "Epoch 16/50 - Train Loss: 0.039616 - Val Loss: 0.050471\n",
      "Epoch 17/50 - Train Loss: 0.039727 - Val Loss: 0.048043\n",
      "Epoch 18/50 - Train Loss: 0.039580 - Val Loss: 0.051867\n",
      "Epoch 19/50 - Train Loss: 0.038704 - Val Loss: 0.054522\n",
      "Epoch 20/50 - Train Loss: 0.038504 - Val Loss: 0.048914\n",
      "Epoch 21/50 - Train Loss: 0.029710 - Val Loss: 0.046709\n",
      "Epoch 22/50 - Train Loss: 0.039287 - Val Loss: 0.047860\n",
      "Epoch 23/50 - Train Loss: 0.037214 - Val Loss: 0.050626\n",
      "Epoch 24/50 - Train Loss: 0.036988 - Val Loss: 0.049762\n",
      "Epoch 25/50 - Train Loss: 0.038206 - Val Loss: 0.047320\n",
      "Epoch 26/50 - Train Loss: 0.036564 - Val Loss: 0.046571\n",
      "Epoch 27/50 - Train Loss: 0.029414 - Val Loss: 0.046419\n",
      "Epoch 28/50 - Train Loss: 0.030619 - Val Loss: 0.047193\n",
      "Epoch 29/50 - Train Loss: 0.030544 - Val Loss: 0.047617\n",
      "Epoch 30/50 - Train Loss: 0.037959 - Val Loss: 0.046711\n",
      "Epoch 31/50 - Train Loss: 0.037552 - Val Loss: 0.046995\n",
      "Epoch 32/50 - Train Loss: 0.030394 - Val Loss: 0.047744\n",
      "Epoch 33/50 - Train Loss: 0.030158 - Val Loss: 0.046699\n",
      "Epoch 34/50 - Train Loss: 0.030416 - Val Loss: 0.046087\n",
      "Epoch 35/50 - Train Loss: 0.037456 - Val Loss: 0.048107\n",
      "Epoch 36/50 - Train Loss: 0.030307 - Val Loss: 0.048677\n",
      "Epoch 37/50 - Train Loss: 0.029573 - Val Loss: 0.046739\n",
      "Epoch 38/50 - Train Loss: 0.038204 - Val Loss: 0.045886\n",
      "Epoch 39/50 - Train Loss: 0.036530 - Val Loss: 0.048050\n",
      "Epoch 40/50 - Train Loss: 0.029535 - Val Loss: 0.047490\n",
      "Epoch 41/50 - Train Loss: 0.028779 - Val Loss: 0.045439\n",
      "Epoch 42/50 - Train Loss: 0.037269 - Val Loss: 0.046360\n",
      "Epoch 43/50 - Train Loss: 0.028372 - Val Loss: 0.048030\n",
      "Epoch 44/50 - Train Loss: 0.029288 - Val Loss: 0.046236\n",
      "Epoch 45/50 - Train Loss: 0.029223 - Val Loss: 0.044884\n",
      "Epoch 46/50 - Train Loss: 0.036673 - Val Loss: 0.046506\n",
      "Epoch 47/50 - Train Loss: 0.028930 - Val Loss: 0.047422\n",
      "Epoch 48/50 - Train Loss: 0.036674 - Val Loss: 0.045737\n",
      "Epoch 49/50 - Train Loss: 0.036955 - Val Loss: 0.046815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:16:44,328] Trial 12 finished with value: 0.04488436132669449 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 213, 'lr': 0.0028290304075664465, 'weight_decay': 4.544621686566988e-07, 'batch_size': 64}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028496 - Val Loss: 0.046817\n",
      "Epoch 1/50 - Train Loss: 0.121369 - Val Loss: 0.129331\n",
      "Epoch 2/50 - Train Loss: 0.050508 - Val Loss: 0.109603\n",
      "Epoch 3/50 - Train Loss: 0.043345 - Val Loss: 0.054270\n",
      "Epoch 4/50 - Train Loss: 0.037323 - Val Loss: 0.067268\n",
      "Epoch 5/50 - Train Loss: 0.038503 - Val Loss: 0.049636\n",
      "Epoch 6/50 - Train Loss: 0.037286 - Val Loss: 0.047485\n",
      "Epoch 7/50 - Train Loss: 0.035899 - Val Loss: 0.050122\n",
      "Epoch 8/50 - Train Loss: 0.035017 - Val Loss: 0.047086\n",
      "Epoch 9/50 - Train Loss: 0.038897 - Val Loss: 0.047715\n",
      "Epoch 10/50 - Train Loss: 0.029087 - Val Loss: 0.051386\n",
      "Epoch 11/50 - Train Loss: 0.028497 - Val Loss: 0.050496\n",
      "Epoch 12/50 - Train Loss: 0.031506 - Val Loss: 0.047406\n",
      "Epoch 13/50 - Train Loss: 0.031889 - Val Loss: 0.048747\n",
      "Epoch 14/50 - Train Loss: 0.036905 - Val Loss: 0.046422\n",
      "Epoch 15/50 - Train Loss: 0.035418 - Val Loss: 0.050535\n",
      "Epoch 16/50 - Train Loss: 0.032826 - Val Loss: 0.052125\n",
      "Epoch 17/50 - Train Loss: 0.032532 - Val Loss: 0.045972\n",
      "Epoch 18/50 - Train Loss: 0.031889 - Val Loss: 0.044788\n",
      "Epoch 19/50 - Train Loss: 0.030975 - Val Loss: 0.055766\n",
      "Epoch 20/50 - Train Loss: 0.031849 - Val Loss: 0.049967\n",
      "Epoch 21/50 - Train Loss: 0.033079 - Val Loss: 0.046373\n",
      "Epoch 22/50 - Train Loss: 0.031180 - Val Loss: 0.045567\n",
      "Epoch 23/50 - Train Loss: 0.030947 - Val Loss: 0.045381\n",
      "Epoch 24/50 - Train Loss: 0.029809 - Val Loss: 0.047000\n",
      "Epoch 25/50 - Train Loss: 0.029884 - Val Loss: 0.045492\n",
      "Epoch 26/50 - Train Loss: 0.034094 - Val Loss: 0.045855\n",
      "Epoch 27/50 - Train Loss: 0.031796 - Val Loss: 0.052203\n",
      "Epoch 28/50 - Train Loss: 0.029371 - Val Loss: 0.056835\n",
      "Epoch 29/50 - Train Loss: 0.031955 - Val Loss: 0.047148\n",
      "Epoch 30/50 - Train Loss: 0.031757 - Val Loss: 0.050976\n",
      "Epoch 31/50 - Train Loss: 0.037260 - Val Loss: 0.045693\n",
      "Epoch 32/50 - Train Loss: 0.031624 - Val Loss: 0.044617\n",
      "Epoch 33/50 - Train Loss: 0.033504 - Val Loss: 0.050591\n",
      "Epoch 34/50 - Train Loss: 0.030187 - Val Loss: 0.050358\n",
      "Epoch 35/50 - Train Loss: 0.031544 - Val Loss: 0.046107\n",
      "Epoch 36/50 - Train Loss: 0.034212 - Val Loss: 0.045164\n",
      "Epoch 37/50 - Train Loss: 0.037955 - Val Loss: 0.052601\n",
      "Epoch 38/50 - Train Loss: 0.031782 - Val Loss: 0.048497\n",
      "Epoch 39/50 - Train Loss: 0.031391 - Val Loss: 0.046086\n",
      "Epoch 40/50 - Train Loss: 0.032505 - Val Loss: 0.059351\n",
      "Epoch 41/50 - Train Loss: 0.031715 - Val Loss: 0.044717\n",
      "Epoch 42/50 - Train Loss: 0.032696 - Val Loss: 0.044527\n",
      "Epoch 43/50 - Train Loss: 0.029563 - Val Loss: 0.053944\n",
      "Epoch 44/50 - Train Loss: 0.034678 - Val Loss: 0.050192\n",
      "Epoch 45/50 - Train Loss: 0.034675 - Val Loss: 0.045983\n",
      "Epoch 46/50 - Train Loss: 0.036462 - Val Loss: 0.056088\n",
      "Epoch 47/50 - Train Loss: 0.029832 - Val Loss: 0.044485\n",
      "Epoch 48/50 - Train Loss: 0.029137 - Val Loss: 0.048878\n",
      "Epoch 49/50 - Train Loss: 0.031926 - Val Loss: 0.047267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:17:09,732] Trial 13 finished with value: 0.04448533616960049 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 135, 'lr': 0.009658490756086916, 'weight_decay': 1.5611483771367834e-05, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031779 - Val Loss: 0.047133\n",
      "Epoch 1/50 - Train Loss: 0.077390 - Val Loss: 0.129588\n",
      "Epoch 2/50 - Train Loss: 0.050529 - Val Loss: 0.089885\n",
      "Epoch 3/50 - Train Loss: 0.047540 - Val Loss: 0.085878\n",
      "Epoch 4/50 - Train Loss: 0.045193 - Val Loss: 0.083718\n",
      "Epoch 5/50 - Train Loss: 0.039593 - Val Loss: 0.077676\n",
      "Epoch 6/50 - Train Loss: 0.037680 - Val Loss: 0.070723\n",
      "Epoch 7/50 - Train Loss: 0.037758 - Val Loss: 0.069117\n",
      "Epoch 8/50 - Train Loss: 0.038482 - Val Loss: 0.064798\n",
      "Epoch 9/50 - Train Loss: 0.035279 - Val Loss: 0.061573\n",
      "Epoch 10/50 - Train Loss: 0.034577 - Val Loss: 0.057985\n",
      "Epoch 11/50 - Train Loss: 0.030442 - Val Loss: 0.055870\n",
      "Epoch 12/50 - Train Loss: 0.031951 - Val Loss: 0.055656\n",
      "Epoch 13/50 - Train Loss: 0.034509 - Val Loss: 0.052935\n",
      "Epoch 14/50 - Train Loss: 0.032806 - Val Loss: 0.053471\n",
      "Epoch 15/50 - Train Loss: 0.032821 - Val Loss: 0.051727\n",
      "Epoch 16/50 - Train Loss: 0.032028 - Val Loss: 0.050459\n",
      "Epoch 17/50 - Train Loss: 0.032850 - Val Loss: 0.049734\n",
      "Epoch 18/50 - Train Loss: 0.031522 - Val Loss: 0.049837\n",
      "Epoch 19/50 - Train Loss: 0.029700 - Val Loss: 0.048459\n",
      "Epoch 20/50 - Train Loss: 0.032940 - Val Loss: 0.051025\n",
      "Epoch 21/50 - Train Loss: 0.031300 - Val Loss: 0.048135\n",
      "Epoch 22/50 - Train Loss: 0.031410 - Val Loss: 0.048582\n",
      "Epoch 23/50 - Train Loss: 0.029085 - Val Loss: 0.047626\n",
      "Epoch 24/50 - Train Loss: 0.031526 - Val Loss: 0.047499\n",
      "Epoch 25/50 - Train Loss: 0.029852 - Val Loss: 0.048089\n",
      "Epoch 26/50 - Train Loss: 0.031428 - Val Loss: 0.047716\n",
      "Epoch 27/50 - Train Loss: 0.030278 - Val Loss: 0.046986\n",
      "Epoch 28/50 - Train Loss: 0.032856 - Val Loss: 0.047502\n",
      "Epoch 29/50 - Train Loss: 0.030796 - Val Loss: 0.046788\n",
      "Epoch 30/50 - Train Loss: 0.030697 - Val Loss: 0.047185\n",
      "Epoch 31/50 - Train Loss: 0.031477 - Val Loss: 0.045832\n",
      "Epoch 32/50 - Train Loss: 0.029378 - Val Loss: 0.047178\n",
      "Epoch 33/50 - Train Loss: 0.027790 - Val Loss: 0.045981\n",
      "Epoch 34/50 - Train Loss: 0.031479 - Val Loss: 0.046712\n",
      "Epoch 35/50 - Train Loss: 0.029915 - Val Loss: 0.046559\n",
      "Epoch 36/50 - Train Loss: 0.031119 - Val Loss: 0.047381\n",
      "Epoch 37/50 - Train Loss: 0.030973 - Val Loss: 0.045728\n",
      "Epoch 38/50 - Train Loss: 0.030665 - Val Loss: 0.046865\n",
      "Epoch 39/50 - Train Loss: 0.031902 - Val Loss: 0.045821\n",
      "Epoch 40/50 - Train Loss: 0.028374 - Val Loss: 0.046900\n",
      "Epoch 41/50 - Train Loss: 0.029749 - Val Loss: 0.045988\n",
      "Epoch 42/50 - Train Loss: 0.029937 - Val Loss: 0.046571\n",
      "Epoch 43/50 - Train Loss: 0.030507 - Val Loss: 0.046106\n",
      "Epoch 44/50 - Train Loss: 0.031317 - Val Loss: 0.046413\n",
      "Epoch 45/50 - Train Loss: 0.030387 - Val Loss: 0.046048\n",
      "Epoch 46/50 - Train Loss: 0.031447 - Val Loss: 0.046120\n",
      "Epoch 47/50 - Train Loss: 0.031342 - Val Loss: 0.046291\n",
      "Epoch 48/50 - Train Loss: 0.031363 - Val Loss: 0.045955\n",
      "Epoch 49/50 - Train Loss: 0.029333 - Val Loss: 0.046270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:17:44,335] Trial 14 finished with value: 0.04486109813054403 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 161, 'lr': 0.00011751093192967859, 'weight_decay': 5.227029959622337e-07, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029935 - Val Loss: 0.044861\n",
      "Epoch 1/50 - Train Loss: 0.058969 - Val Loss: 0.078710\n",
      "Epoch 2/50 - Train Loss: 0.041141 - Val Loss: 0.058027\n",
      "Epoch 3/50 - Train Loss: 0.037964 - Val Loss: 0.071848\n",
      "Epoch 4/50 - Train Loss: 0.034058 - Val Loss: 0.047470\n",
      "Epoch 5/50 - Train Loss: 0.036015 - Val Loss: 0.063659\n",
      "Epoch 6/50 - Train Loss: 0.033056 - Val Loss: 0.048202\n",
      "Epoch 7/50 - Train Loss: 0.036371 - Val Loss: 0.051823\n",
      "Epoch 8/50 - Train Loss: 0.034441 - Val Loss: 0.049363\n",
      "Epoch 9/50 - Train Loss: 0.033551 - Val Loss: 0.045602\n",
      "Epoch 10/50 - Train Loss: 0.029370 - Val Loss: 0.065416\n",
      "Epoch 11/50 - Train Loss: 0.033263 - Val Loss: 0.044879\n",
      "Epoch 12/50 - Train Loss: 0.032323 - Val Loss: 0.053908\n",
      "Epoch 13/50 - Train Loss: 0.030189 - Val Loss: 0.051398\n",
      "Epoch 14/50 - Train Loss: 0.033412 - Val Loss: 0.046569\n",
      "Epoch 15/50 - Train Loss: 0.031359 - Val Loss: 0.052855\n",
      "Epoch 16/50 - Train Loss: 0.033601 - Val Loss: 0.047193\n",
      "Epoch 17/50 - Train Loss: 0.030695 - Val Loss: 0.044775\n",
      "Epoch 18/50 - Train Loss: 0.029397 - Val Loss: 0.050620\n",
      "Epoch 19/50 - Train Loss: 0.033274 - Val Loss: 0.046018\n",
      "Epoch 20/50 - Train Loss: 0.033736 - Val Loss: 0.044798\n",
      "Epoch 21/50 - Train Loss: 0.032830 - Val Loss: 0.056570\n",
      "Epoch 22/50 - Train Loss: 0.031896 - Val Loss: 0.045077\n",
      "Epoch 23/50 - Train Loss: 0.031558 - Val Loss: 0.045453\n",
      "Epoch 24/50 - Train Loss: 0.032489 - Val Loss: 0.050089\n",
      "Epoch 25/50 - Train Loss: 0.032569 - Val Loss: 0.048360\n",
      "Epoch 26/50 - Train Loss: 0.030788 - Val Loss: 0.044797\n",
      "Epoch 27/50 - Train Loss: 0.034492 - Val Loss: 0.056710\n",
      "Epoch 28/50 - Train Loss: 0.029026 - Val Loss: 0.045497\n",
      "Epoch 29/50 - Train Loss: 0.031399 - Val Loss: 0.046901\n",
      "Epoch 30/50 - Train Loss: 0.031941 - Val Loss: 0.047645\n",
      "Epoch 31/50 - Train Loss: 0.031892 - Val Loss: 0.044962\n",
      "Epoch 32/50 - Train Loss: 0.029939 - Val Loss: 0.047430\n",
      "Epoch 33/50 - Train Loss: 0.033095 - Val Loss: 0.044744\n",
      "Epoch 34/50 - Train Loss: 0.031749 - Val Loss: 0.047410\n",
      "Epoch 35/50 - Train Loss: 0.029691 - Val Loss: 0.047598\n",
      "Epoch 36/50 - Train Loss: 0.028439 - Val Loss: 0.044917\n",
      "Epoch 37/50 - Train Loss: 0.026328 - Val Loss: 0.046727\n",
      "Epoch 38/50 - Train Loss: 0.027935 - Val Loss: 0.044625\n",
      "Epoch 39/50 - Train Loss: 0.030804 - Val Loss: 0.044854\n",
      "Epoch 40/50 - Train Loss: 0.031640 - Val Loss: 0.046672\n",
      "Epoch 41/50 - Train Loss: 0.029672 - Val Loss: 0.044575\n",
      "Epoch 42/50 - Train Loss: 0.029872 - Val Loss: 0.044617\n",
      "Epoch 43/50 - Train Loss: 0.031980 - Val Loss: 0.049609\n",
      "Epoch 44/50 - Train Loss: 0.031723 - Val Loss: 0.046425\n",
      "Epoch 45/50 - Train Loss: 0.031544 - Val Loss: 0.044863\n",
      "Epoch 46/50 - Train Loss: 0.028934 - Val Loss: 0.045577\n",
      "Epoch 47/50 - Train Loss: 0.032462 - Val Loss: 0.048294\n",
      "Epoch 48/50 - Train Loss: 0.030615 - Val Loss: 0.046626\n",
      "Epoch 49/50 - Train Loss: 0.030289 - Val Loss: 0.045446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:18:08,621] Trial 15 finished with value: 0.04457511007785797 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 104, 'lr': 0.0029619713684506752, 'weight_decay': 1.0074842787671318e-08, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028593 - Val Loss: 0.047117\n",
      "Epoch 1/50 - Train Loss: 0.087013 - Val Loss: 0.077775\n",
      "Epoch 2/50 - Train Loss: 0.041220 - Val Loss: 0.065821\n",
      "Epoch 3/50 - Train Loss: 0.034937 - Val Loss: 0.051713\n",
      "Epoch 4/50 - Train Loss: 0.034589 - Val Loss: 0.049751\n",
      "Epoch 5/50 - Train Loss: 0.031560 - Val Loss: 0.045607\n",
      "Epoch 6/50 - Train Loss: 0.034499 - Val Loss: 0.045480\n",
      "Epoch 7/50 - Train Loss: 0.032764 - Val Loss: 0.044515\n",
      "Epoch 8/50 - Train Loss: 0.032153 - Val Loss: 0.044421\n",
      "Epoch 9/50 - Train Loss: 0.033832 - Val Loss: 0.046000\n",
      "Epoch 10/50 - Train Loss: 0.029139 - Val Loss: 0.044562\n",
      "Epoch 11/50 - Train Loss: 0.031179 - Val Loss: 0.044592\n",
      "Epoch 12/50 - Train Loss: 0.034071 - Val Loss: 0.046186\n",
      "Epoch 13/50 - Train Loss: 0.032005 - Val Loss: 0.045036\n",
      "Epoch 14/50 - Train Loss: 0.033563 - Val Loss: 0.046608\n",
      "Epoch 15/50 - Train Loss: 0.031859 - Val Loss: 0.047271\n",
      "Epoch 16/50 - Train Loss: 0.031206 - Val Loss: 0.045195\n",
      "Epoch 17/50 - Train Loss: 0.027556 - Val Loss: 0.044421\n",
      "Epoch 18/50 - Train Loss: 0.028823 - Val Loss: 0.044783\n",
      "Epoch 19/50 - Train Loss: 0.030280 - Val Loss: 0.044711\n",
      "Epoch 20/50 - Train Loss: 0.030031 - Val Loss: 0.047132\n",
      "Epoch 21/50 - Train Loss: 0.032231 - Val Loss: 0.046610\n",
      "Epoch 22/50 - Train Loss: 0.029388 - Val Loss: 0.046180\n",
      "Epoch 23/50 - Train Loss: 0.029996 - Val Loss: 0.045196\n",
      "Epoch 24/50 - Train Loss: 0.030810 - Val Loss: 0.044779\n",
      "Epoch 25/50 - Train Loss: 0.030495 - Val Loss: 0.046903\n",
      "Epoch 26/50 - Train Loss: 0.030816 - Val Loss: 0.045097\n",
      "Epoch 27/50 - Train Loss: 0.030622 - Val Loss: 0.044970\n",
      "Epoch 28/50 - Train Loss: 0.033783 - Val Loss: 0.048568\n",
      "Epoch 29/50 - Train Loss: 0.031238 - Val Loss: 0.046074\n",
      "Epoch 30/50 - Train Loss: 0.029389 - Val Loss: 0.052822\n",
      "Epoch 31/50 - Train Loss: 0.031995 - Val Loss: 0.048199\n",
      "Epoch 32/50 - Train Loss: 0.028958 - Val Loss: 0.044825\n",
      "Epoch 33/50 - Train Loss: 0.030401 - Val Loss: 0.052174\n",
      "Epoch 34/50 - Train Loss: 0.032820 - Val Loss: 0.048300\n",
      "Epoch 35/50 - Train Loss: 0.029962 - Val Loss: 0.045528\n",
      "Epoch 36/50 - Train Loss: 0.029445 - Val Loss: 0.045548\n",
      "Epoch 37/50 - Train Loss: 0.030001 - Val Loss: 0.049973\n",
      "Epoch 38/50 - Train Loss: 0.034007 - Val Loss: 0.047656\n",
      "Epoch 39/50 - Train Loss: 0.029109 - Val Loss: 0.045276\n",
      "Epoch 40/50 - Train Loss: 0.027177 - Val Loss: 0.045802\n",
      "Epoch 41/50 - Train Loss: 0.028428 - Val Loss: 0.047423\n",
      "Epoch 42/50 - Train Loss: 0.030308 - Val Loss: 0.045731\n",
      "Epoch 43/50 - Train Loss: 0.029732 - Val Loss: 0.045923\n",
      "Epoch 44/50 - Train Loss: 0.029844 - Val Loss: 0.045038\n",
      "Epoch 45/50 - Train Loss: 0.028581 - Val Loss: 0.045542\n",
      "Epoch 46/50 - Train Loss: 0.028168 - Val Loss: 0.045650\n",
      "Epoch 47/50 - Train Loss: 0.028739 - Val Loss: 0.045614\n",
      "Epoch 48/50 - Train Loss: 0.029985 - Val Loss: 0.045531\n",
      "Epoch 49/50 - Train Loss: 0.030279 - Val Loss: 0.046751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:18:41,679] Trial 16 finished with value: 0.04442053288221359 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 202, 'lr': 0.0027213828413364042, 'weight_decay': 1.1028645937933765e-05, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026889 - Val Loss: 0.045486\n",
      "Epoch 1/50 - Train Loss: 0.138685 - Val Loss: 0.228458\n",
      "Epoch 2/50 - Train Loss: 0.104152 - Val Loss: 0.161084\n",
      "Epoch 3/50 - Train Loss: 0.061903 - Val Loss: 0.112274\n",
      "Epoch 4/50 - Train Loss: 0.055729 - Val Loss: 0.084349\n",
      "Epoch 5/50 - Train Loss: 0.057947 - Val Loss: 0.074019\n",
      "Epoch 6/50 - Train Loss: 0.058252 - Val Loss: 0.071515\n",
      "Epoch 7/50 - Train Loss: 0.055233 - Val Loss: 0.072654\n",
      "Epoch 8/50 - Train Loss: 0.050099 - Val Loss: 0.076780\n",
      "Epoch 9/50 - Train Loss: 0.047345 - Val Loss: 0.081903\n",
      "Epoch 10/50 - Train Loss: 0.038817 - Val Loss: 0.084867\n",
      "Epoch 11/50 - Train Loss: 0.046891 - Val Loss: 0.082797\n",
      "Epoch 12/50 - Train Loss: 0.038118 - Val Loss: 0.075612\n",
      "Epoch 13/50 - Train Loss: 0.043370 - Val Loss: 0.067417\n",
      "Epoch 14/50 - Train Loss: 0.041186 - Val Loss: 0.060645\n",
      "Epoch 15/50 - Train Loss: 0.033918 - Val Loss: 0.056338\n",
      "Epoch 16/50 - Train Loss: 0.034027 - Val Loss: 0.054729\n",
      "Epoch 17/50 - Train Loss: 0.032306 - Val Loss: 0.054418\n",
      "Epoch 18/50 - Train Loss: 0.040949 - Val Loss: 0.056052\n",
      "Epoch 19/50 - Train Loss: 0.041621 - Val Loss: 0.057616\n",
      "Epoch 20/50 - Train Loss: 0.040402 - Val Loss: 0.056604\n",
      "Epoch 21/50 - Train Loss: 0.038778 - Val Loss: 0.053831\n",
      "Epoch 22/50 - Train Loss: 0.030893 - Val Loss: 0.050889\n",
      "Epoch 23/50 - Train Loss: 0.031391 - Val Loss: 0.049693\n",
      "Epoch 24/50 - Train Loss: 0.031805 - Val Loss: 0.049634\n",
      "Epoch 25/50 - Train Loss: 0.030782 - Val Loss: 0.049760\n",
      "Epoch 26/50 - Train Loss: 0.030922 - Val Loss: 0.049976\n",
      "Epoch 27/50 - Train Loss: 0.030340 - Val Loss: 0.049905\n",
      "Epoch 28/50 - Train Loss: 0.038585 - Val Loss: 0.049978\n",
      "Epoch 29/50 - Train Loss: 0.030229 - Val Loss: 0.049166\n",
      "Epoch 30/50 - Train Loss: 0.030771 - Val Loss: 0.048396\n",
      "Epoch 31/50 - Train Loss: 0.037513 - Val Loss: 0.048148\n",
      "Epoch 32/50 - Train Loss: 0.030651 - Val Loss: 0.048142\n",
      "Epoch 33/50 - Train Loss: 0.036802 - Val Loss: 0.048527\n",
      "Epoch 34/50 - Train Loss: 0.030601 - Val Loss: 0.048274\n",
      "Epoch 35/50 - Train Loss: 0.030229 - Val Loss: 0.047615\n",
      "Epoch 36/50 - Train Loss: 0.031335 - Val Loss: 0.046598\n",
      "Epoch 37/50 - Train Loss: 0.029652 - Val Loss: 0.046136\n",
      "Epoch 38/50 - Train Loss: 0.038098 - Val Loss: 0.046558\n",
      "Epoch 39/50 - Train Loss: 0.037518 - Val Loss: 0.047662\n",
      "Epoch 40/50 - Train Loss: 0.028863 - Val Loss: 0.047786\n",
      "Epoch 41/50 - Train Loss: 0.029779 - Val Loss: 0.046727\n",
      "Epoch 42/50 - Train Loss: 0.037087 - Val Loss: 0.045843\n",
      "Epoch 43/50 - Train Loss: 0.037171 - Val Loss: 0.045997\n",
      "Epoch 44/50 - Train Loss: 0.029419 - Val Loss: 0.046498\n",
      "Epoch 45/50 - Train Loss: 0.037006 - Val Loss: 0.046892\n",
      "Epoch 46/50 - Train Loss: 0.036456 - Val Loss: 0.046424\n",
      "Epoch 47/50 - Train Loss: 0.036787 - Val Loss: 0.046042\n",
      "Epoch 48/50 - Train Loss: 0.029178 - Val Loss: 0.046398\n",
      "Epoch 49/50 - Train Loss: 0.036042 - Val Loss: 0.046243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:19:05,060] Trial 17 finished with value: 0.04584318399429321 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 119, 'lr': 0.000918809903872673, 'weight_decay': 2.813792218331737e-07, 'batch_size': 64}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029347 - Val Loss: 0.045924\n",
      "Epoch 1/50 - Train Loss: 0.148974 - Val Loss: 0.206018\n",
      "Epoch 2/50 - Train Loss: 0.056146 - Val Loss: 0.079850\n",
      "Epoch 3/50 - Train Loss: 0.046469 - Val Loss: 0.076981\n",
      "Epoch 4/50 - Train Loss: 0.042591 - Val Loss: 0.091189\n",
      "Epoch 5/50 - Train Loss: 0.042645 - Val Loss: 0.072457\n",
      "Epoch 6/50 - Train Loss: 0.040316 - Val Loss: 0.073725\n",
      "Epoch 7/50 - Train Loss: 0.036531 - Val Loss: 0.068319\n",
      "Epoch 8/50 - Train Loss: 0.037143 - Val Loss: 0.064232\n",
      "Epoch 9/50 - Train Loss: 0.037112 - Val Loss: 0.065229\n",
      "Epoch 10/50 - Train Loss: 0.035499 - Val Loss: 0.057530\n",
      "Epoch 11/50 - Train Loss: 0.033074 - Val Loss: 0.057665\n",
      "Epoch 12/50 - Train Loss: 0.031544 - Val Loss: 0.054656\n",
      "Epoch 13/50 - Train Loss: 0.032008 - Val Loss: 0.050377\n",
      "Epoch 14/50 - Train Loss: 0.032263 - Val Loss: 0.049621\n",
      "Epoch 15/50 - Train Loss: 0.030801 - Val Loss: 0.051365\n",
      "Epoch 16/50 - Train Loss: 0.032428 - Val Loss: 0.049813\n",
      "Epoch 17/50 - Train Loss: 0.030192 - Val Loss: 0.051934\n",
      "Epoch 18/50 - Train Loss: 0.031025 - Val Loss: 0.049424\n",
      "Epoch 19/50 - Train Loss: 0.031700 - Val Loss: 0.049446\n",
      "Epoch 20/50 - Train Loss: 0.031315 - Val Loss: 0.046604\n",
      "Epoch 21/50 - Train Loss: 0.031448 - Val Loss: 0.050308\n",
      "Epoch 22/50 - Train Loss: 0.031498 - Val Loss: 0.048855\n",
      "Epoch 23/50 - Train Loss: 0.031659 - Val Loss: 0.047628\n",
      "Epoch 24/50 - Train Loss: 0.031132 - Val Loss: 0.046486\n",
      "Epoch 25/50 - Train Loss: 0.030951 - Val Loss: 0.046383\n",
      "Epoch 26/50 - Train Loss: 0.030628 - Val Loss: 0.049094\n",
      "Epoch 27/50 - Train Loss: 0.033295 - Val Loss: 0.047040\n",
      "Epoch 28/50 - Train Loss: 0.031956 - Val Loss: 0.046382\n",
      "Epoch 29/50 - Train Loss: 0.030861 - Val Loss: 0.051927\n",
      "Epoch 30/50 - Train Loss: 0.031620 - Val Loss: 0.047515\n",
      "Epoch 31/50 - Train Loss: 0.029711 - Val Loss: 0.045808\n",
      "Epoch 32/50 - Train Loss: 0.029396 - Val Loss: 0.049389\n",
      "Epoch 33/50 - Train Loss: 0.030651 - Val Loss: 0.045630\n",
      "Epoch 34/50 - Train Loss: 0.030850 - Val Loss: 0.046576\n",
      "Epoch 35/50 - Train Loss: 0.029313 - Val Loss: 0.048973\n",
      "Epoch 36/50 - Train Loss: 0.032704 - Val Loss: 0.048035\n",
      "Epoch 37/50 - Train Loss: 0.031575 - Val Loss: 0.045603\n",
      "Epoch 38/50 - Train Loss: 0.031852 - Val Loss: 0.047265\n",
      "Epoch 39/50 - Train Loss: 0.028538 - Val Loss: 0.045667\n",
      "Epoch 40/50 - Train Loss: 0.029895 - Val Loss: 0.046788\n",
      "Epoch 41/50 - Train Loss: 0.028100 - Val Loss: 0.045827\n",
      "Epoch 42/50 - Train Loss: 0.028015 - Val Loss: 0.045756\n",
      "Epoch 43/50 - Train Loss: 0.031478 - Val Loss: 0.045241\n",
      "Epoch 44/50 - Train Loss: 0.031276 - Val Loss: 0.045472\n",
      "Epoch 45/50 - Train Loss: 0.029470 - Val Loss: 0.046388\n",
      "Epoch 46/50 - Train Loss: 0.029921 - Val Loss: 0.047928\n",
      "Epoch 47/50 - Train Loss: 0.029196 - Val Loss: 0.045148\n",
      "Epoch 48/50 - Train Loss: 0.029197 - Val Loss: 0.046193\n",
      "Epoch 49/50 - Train Loss: 0.030567 - Val Loss: 0.051066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:20:24,605] Trial 18 finished with value: 0.04514782006541888 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 242, 'lr': 0.00014647745526987776, 'weight_decay': 6.726322646213726e-05, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031015 - Val Loss: 0.045837\n",
      "Epoch 1/50 - Train Loss: 0.112271 - Val Loss: 0.118195\n",
      "Epoch 2/50 - Train Loss: 0.052883 - Val Loss: 0.054182\n",
      "Epoch 3/50 - Train Loss: 0.042277 - Val Loss: 0.055010\n",
      "Epoch 4/50 - Train Loss: 0.042791 - Val Loss: 0.061188\n",
      "Epoch 5/50 - Train Loss: 0.035401 - Val Loss: 0.049017\n",
      "Epoch 6/50 - Train Loss: 0.031841 - Val Loss: 0.050478\n",
      "Epoch 7/50 - Train Loss: 0.032552 - Val Loss: 0.050286\n",
      "Epoch 8/50 - Train Loss: 0.029176 - Val Loss: 0.046400\n",
      "Epoch 9/50 - Train Loss: 0.033314 - Val Loss: 0.047443\n",
      "Epoch 10/50 - Train Loss: 0.035145 - Val Loss: 0.047411\n",
      "Epoch 11/50 - Train Loss: 0.031033 - Val Loss: 0.045476\n",
      "Epoch 12/50 - Train Loss: 0.032292 - Val Loss: 0.046597\n",
      "Epoch 13/50 - Train Loss: 0.035363 - Val Loss: 0.050980\n",
      "Epoch 14/50 - Train Loss: 0.029817 - Val Loss: 0.044755\n",
      "Epoch 15/50 - Train Loss: 0.033483 - Val Loss: 0.051238\n",
      "Epoch 16/50 - Train Loss: 0.034070 - Val Loss: 0.050943\n",
      "Epoch 17/50 - Train Loss: 0.031178 - Val Loss: 0.046571\n",
      "Epoch 18/50 - Train Loss: 0.030495 - Val Loss: 0.045382\n",
      "Epoch 19/50 - Train Loss: 0.031790 - Val Loss: 0.047195\n",
      "Epoch 20/50 - Train Loss: 0.031009 - Val Loss: 0.047866\n",
      "Epoch 21/50 - Train Loss: 0.029259 - Val Loss: 0.044782\n",
      "Epoch 22/50 - Train Loss: 0.029292 - Val Loss: 0.048929\n",
      "Epoch 23/50 - Train Loss: 0.030154 - Val Loss: 0.050201\n",
      "Epoch 24/50 - Train Loss: 0.027666 - Val Loss: 0.044606\n",
      "Epoch 25/50 - Train Loss: 0.033877 - Val Loss: 0.045432\n",
      "Epoch 26/50 - Train Loss: 0.031567 - Val Loss: 0.045686\n",
      "Epoch 27/50 - Train Loss: 0.032830 - Val Loss: 0.053838\n",
      "Epoch 28/50 - Train Loss: 0.032292 - Val Loss: 0.046432\n",
      "Epoch 29/50 - Train Loss: 0.032729 - Val Loss: 0.044739\n",
      "Epoch 30/50 - Train Loss: 0.031707 - Val Loss: 0.044806\n",
      "Epoch 31/50 - Train Loss: 0.032470 - Val Loss: 0.049649\n",
      "Epoch 32/50 - Train Loss: 0.028615 - Val Loss: 0.054656\n",
      "Epoch 33/50 - Train Loss: 0.029922 - Val Loss: 0.046872\n",
      "Epoch 34/50 - Train Loss: 0.031700 - Val Loss: 0.045642\n",
      "Epoch 35/50 - Train Loss: 0.028583 - Val Loss: 0.044964\n",
      "Epoch 36/50 - Train Loss: 0.027142 - Val Loss: 0.044835\n",
      "Epoch 37/50 - Train Loss: 0.027827 - Val Loss: 0.046473\n",
      "Epoch 38/50 - Train Loss: 0.030402 - Val Loss: 0.046916\n",
      "Epoch 39/50 - Train Loss: 0.027496 - Val Loss: 0.044864\n",
      "Epoch 40/50 - Train Loss: 0.027051 - Val Loss: 0.044495\n",
      "Epoch 41/50 - Train Loss: 0.029092 - Val Loss: 0.044862\n",
      "Epoch 42/50 - Train Loss: 0.028815 - Val Loss: 0.047176\n",
      "Epoch 43/50 - Train Loss: 0.029799 - Val Loss: 0.044835\n",
      "Epoch 44/50 - Train Loss: 0.029021 - Val Loss: 0.044867\n",
      "Epoch 45/50 - Train Loss: 0.030882 - Val Loss: 0.045261\n",
      "Epoch 46/50 - Train Loss: 0.033479 - Val Loss: 0.045339\n",
      "Epoch 47/50 - Train Loss: 0.029390 - Val Loss: 0.047742\n",
      "Epoch 48/50 - Train Loss: 0.028410 - Val Loss: 0.047297\n",
      "Epoch 49/50 - Train Loss: 0.027138 - Val Loss: 0.045060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:20:44,800] Trial 19 finished with value: 0.04449520632624626 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 167, 'lr': 0.005351485001111017, 'weight_decay': 1.9126141481458142e-07, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028970 - Val Loss: 0.047762\n",
      "Epoch 1/50 - Train Loss: 0.070229 - Val Loss: 0.062331\n",
      "Epoch 2/50 - Train Loss: 0.045141 - Val Loss: 0.088438\n",
      "Epoch 3/50 - Train Loss: 0.041113 - Val Loss: 0.071458\n",
      "Epoch 4/50 - Train Loss: 0.040285 - Val Loss: 0.058584\n",
      "Epoch 5/50 - Train Loss: 0.040140 - Val Loss: 0.054126\n",
      "Epoch 6/50 - Train Loss: 0.032446 - Val Loss: 0.048826\n",
      "Epoch 7/50 - Train Loss: 0.032609 - Val Loss: 0.047489\n",
      "Epoch 8/50 - Train Loss: 0.034380 - Val Loss: 0.049995\n",
      "Epoch 9/50 - Train Loss: 0.034792 - Val Loss: 0.047786\n",
      "Epoch 10/50 - Train Loss: 0.031686 - Val Loss: 0.046319\n",
      "Epoch 11/50 - Train Loss: 0.034549 - Val Loss: 0.048215\n",
      "Epoch 12/50 - Train Loss: 0.030786 - Val Loss: 0.046154\n",
      "Epoch 13/50 - Train Loss: 0.028995 - Val Loss: 0.049086\n",
      "Epoch 14/50 - Train Loss: 0.028292 - Val Loss: 0.044946\n",
      "Epoch 15/50 - Train Loss: 0.032822 - Val Loss: 0.045861\n",
      "Epoch 16/50 - Train Loss: 0.032860 - Val Loss: 0.048858\n",
      "Epoch 17/50 - Train Loss: 0.033047 - Val Loss: 0.045476\n",
      "Epoch 18/50 - Train Loss: 0.032702 - Val Loss: 0.047781\n",
      "Epoch 19/50 - Train Loss: 0.027953 - Val Loss: 0.045342\n",
      "Epoch 20/50 - Train Loss: 0.032847 - Val Loss: 0.045486\n",
      "Epoch 21/50 - Train Loss: 0.030973 - Val Loss: 0.045879\n",
      "Epoch 22/50 - Train Loss: 0.027806 - Val Loss: 0.045658\n",
      "Epoch 23/50 - Train Loss: 0.034405 - Val Loss: 0.044710\n",
      "Epoch 24/50 - Train Loss: 0.032982 - Val Loss: 0.050574\n",
      "Epoch 25/50 - Train Loss: 0.034002 - Val Loss: 0.045300\n",
      "Epoch 26/50 - Train Loss: 0.030650 - Val Loss: 0.045595\n",
      "Epoch 27/50 - Train Loss: 0.033540 - Val Loss: 0.045575\n",
      "Epoch 28/50 - Train Loss: 0.029513 - Val Loss: 0.044466\n",
      "Epoch 29/50 - Train Loss: 0.030949 - Val Loss: 0.046723\n",
      "Epoch 30/50 - Train Loss: 0.029031 - Val Loss: 0.044709\n",
      "Epoch 31/50 - Train Loss: 0.031092 - Val Loss: 0.045267\n",
      "Epoch 32/50 - Train Loss: 0.031915 - Val Loss: 0.045578\n",
      "Epoch 33/50 - Train Loss: 0.033773 - Val Loss: 0.044657\n",
      "Epoch 34/50 - Train Loss: 0.032437 - Val Loss: 0.045313\n",
      "Epoch 35/50 - Train Loss: 0.030111 - Val Loss: 0.044629\n",
      "Epoch 36/50 - Train Loss: 0.032006 - Val Loss: 0.044255\n",
      "Epoch 37/50 - Train Loss: 0.028552 - Val Loss: 0.044855\n",
      "Epoch 38/50 - Train Loss: 0.031149 - Val Loss: 0.044796\n",
      "Epoch 39/50 - Train Loss: 0.030128 - Val Loss: 0.044393\n",
      "Epoch 40/50 - Train Loss: 0.029653 - Val Loss: 0.045997\n",
      "Epoch 41/50 - Train Loss: 0.031814 - Val Loss: 0.044848\n",
      "Epoch 42/50 - Train Loss: 0.030802 - Val Loss: 0.045295\n",
      "Epoch 43/50 - Train Loss: 0.030276 - Val Loss: 0.047441\n",
      "Epoch 44/50 - Train Loss: 0.031250 - Val Loss: 0.044489\n",
      "Epoch 45/50 - Train Loss: 0.026950 - Val Loss: 0.044813\n",
      "Epoch 46/50 - Train Loss: 0.027876 - Val Loss: 0.044781\n",
      "Epoch 47/50 - Train Loss: 0.030455 - Val Loss: 0.044502\n",
      "Epoch 48/50 - Train Loss: 0.026911 - Val Loss: 0.044541\n",
      "Epoch 49/50 - Train Loss: 0.031066 - Val Loss: 0.044789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:21:08,808] Trial 20 finished with value: 0.04425464943051338 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 92, 'lr': 0.001632263501123696, 'weight_decay': 3.513278763119692e-06, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031319 - Val Loss: 0.044389\n",
      "Epoch 1/50 - Train Loss: 0.061260 - Val Loss: 0.064674\n",
      "Epoch 2/50 - Train Loss: 0.049889 - Val Loss: 0.077021\n",
      "Epoch 3/50 - Train Loss: 0.041962 - Val Loss: 0.076615\n",
      "Epoch 4/50 - Train Loss: 0.040696 - Val Loss: 0.062839\n",
      "Epoch 5/50 - Train Loss: 0.038336 - Val Loss: 0.066178\n",
      "Epoch 6/50 - Train Loss: 0.032867 - Val Loss: 0.051013\n",
      "Epoch 7/50 - Train Loss: 0.035451 - Val Loss: 0.054616\n",
      "Epoch 8/50 - Train Loss: 0.027824 - Val Loss: 0.047734\n",
      "Epoch 9/50 - Train Loss: 0.031383 - Val Loss: 0.047638\n",
      "Epoch 10/50 - Train Loss: 0.033971 - Val Loss: 0.047988\n",
      "Epoch 11/50 - Train Loss: 0.034773 - Val Loss: 0.046547\n",
      "Epoch 12/50 - Train Loss: 0.031580 - Val Loss: 0.050047\n",
      "Epoch 13/50 - Train Loss: 0.032495 - Val Loss: 0.046638\n",
      "Epoch 14/50 - Train Loss: 0.032824 - Val Loss: 0.051360\n",
      "Epoch 15/50 - Train Loss: 0.031942 - Val Loss: 0.046624\n",
      "Epoch 16/50 - Train Loss: 0.033822 - Val Loss: 0.048368\n",
      "Epoch 17/50 - Train Loss: 0.032277 - Val Loss: 0.048049\n",
      "Epoch 18/50 - Train Loss: 0.030865 - Val Loss: 0.048025\n",
      "Epoch 19/50 - Train Loss: 0.035496 - Val Loss: 0.048514\n",
      "Epoch 20/50 - Train Loss: 0.030374 - Val Loss: 0.047102\n",
      "Epoch 21/50 - Train Loss: 0.031278 - Val Loss: 0.046578\n",
      "Epoch 22/50 - Train Loss: 0.033499 - Val Loss: 0.046840\n",
      "Epoch 23/50 - Train Loss: 0.032785 - Val Loss: 0.048293\n",
      "Epoch 24/50 - Train Loss: 0.033862 - Val Loss: 0.050803\n",
      "Epoch 25/50 - Train Loss: 0.030068 - Val Loss: 0.046960\n",
      "Epoch 26/50 - Train Loss: 0.031219 - Val Loss: 0.047755\n",
      "Epoch 27/50 - Train Loss: 0.028022 - Val Loss: 0.045317\n",
      "Epoch 28/50 - Train Loss: 0.032039 - Val Loss: 0.050542\n",
      "Epoch 29/50 - Train Loss: 0.033285 - Val Loss: 0.048463\n",
      "Epoch 30/50 - Train Loss: 0.033110 - Val Loss: 0.045373\n",
      "Epoch 31/50 - Train Loss: 0.033374 - Val Loss: 0.054468\n",
      "Epoch 32/50 - Train Loss: 0.031541 - Val Loss: 0.046684\n",
      "Epoch 33/50 - Train Loss: 0.028743 - Val Loss: 0.049225\n",
      "Epoch 34/50 - Train Loss: 0.029738 - Val Loss: 0.046786\n",
      "Epoch 35/50 - Train Loss: 0.032011 - Val Loss: 0.047541\n",
      "Epoch 36/50 - Train Loss: 0.028535 - Val Loss: 0.046550\n",
      "Epoch 37/50 - Train Loss: 0.032166 - Val Loss: 0.047163\n",
      "Epoch 38/50 - Train Loss: 0.030024 - Val Loss: 0.047926\n",
      "Epoch 39/50 - Train Loss: 0.032708 - Val Loss: 0.044868\n",
      "Epoch 40/50 - Train Loss: 0.032303 - Val Loss: 0.048092\n",
      "Epoch 41/50 - Train Loss: 0.031187 - Val Loss: 0.047535\n",
      "Epoch 42/50 - Train Loss: 0.031293 - Val Loss: 0.048688\n",
      "Epoch 43/50 - Train Loss: 0.027004 - Val Loss: 0.044986\n",
      "Epoch 44/50 - Train Loss: 0.029750 - Val Loss: 0.046038\n",
      "Epoch 45/50 - Train Loss: 0.027506 - Val Loss: 0.047321\n",
      "Epoch 46/50 - Train Loss: 0.034169 - Val Loss: 0.044902\n",
      "Epoch 47/50 - Train Loss: 0.032138 - Val Loss: 0.054827\n",
      "Epoch 48/50 - Train Loss: 0.031023 - Val Loss: 0.045284\n",
      "Epoch 49/50 - Train Loss: 0.028653 - Val Loss: 0.051429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:21:24,795] Trial 21 finished with value: 0.04486767202615738 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 88, 'lr': 0.001288006204497336, 'weight_decay': 5.00069253639266e-06, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028780 - Val Loss: 0.044888\n",
      "Epoch 1/50 - Train Loss: 0.114499 - Val Loss: 0.072113\n",
      "Epoch 2/50 - Train Loss: 0.053510 - Val Loss: 0.087134\n",
      "Epoch 3/50 - Train Loss: 0.043707 - Val Loss: 0.081529\n",
      "Epoch 4/50 - Train Loss: 0.038025 - Val Loss: 0.057273\n",
      "Epoch 5/50 - Train Loss: 0.038410 - Val Loss: 0.065426\n",
      "Epoch 6/50 - Train Loss: 0.035384 - Val Loss: 0.053663\n",
      "Epoch 7/50 - Train Loss: 0.030282 - Val Loss: 0.052130\n",
      "Epoch 8/50 - Train Loss: 0.030171 - Val Loss: 0.064452\n",
      "Epoch 9/50 - Train Loss: 0.031176 - Val Loss: 0.047748\n",
      "Epoch 10/50 - Train Loss: 0.034626 - Val Loss: 0.047074\n",
      "Epoch 11/50 - Train Loss: 0.033378 - Val Loss: 0.058383\n",
      "Epoch 12/50 - Train Loss: 0.038479 - Val Loss: 0.046173\n",
      "Epoch 13/50 - Train Loss: 0.033692 - Val Loss: 0.054718\n",
      "Epoch 14/50 - Train Loss: 0.034654 - Val Loss: 0.045420\n",
      "Epoch 15/50 - Train Loss: 0.037327 - Val Loss: 0.048750\n",
      "Epoch 16/50 - Train Loss: 0.032297 - Val Loss: 0.050745\n",
      "Epoch 17/50 - Train Loss: 0.030309 - Val Loss: 0.045088\n",
      "Epoch 18/50 - Train Loss: 0.034637 - Val Loss: 0.051509\n",
      "Epoch 19/50 - Train Loss: 0.031906 - Val Loss: 0.050494\n",
      "Epoch 20/50 - Train Loss: 0.032543 - Val Loss: 0.046018\n",
      "Epoch 21/50 - Train Loss: 0.033841 - Val Loss: 0.051088\n",
      "Epoch 22/50 - Train Loss: 0.032283 - Val Loss: 0.045706\n",
      "Epoch 23/50 - Train Loss: 0.035576 - Val Loss: 0.048534\n",
      "Epoch 24/50 - Train Loss: 0.030762 - Val Loss: 0.049117\n",
      "Epoch 25/50 - Train Loss: 0.032104 - Val Loss: 0.044206\n",
      "Epoch 26/50 - Train Loss: 0.030718 - Val Loss: 0.055812\n",
      "Epoch 27/50 - Train Loss: 0.030929 - Val Loss: 0.044983\n",
      "Epoch 28/50 - Train Loss: 0.031250 - Val Loss: 0.047813\n",
      "Epoch 29/50 - Train Loss: 0.030579 - Val Loss: 0.049677\n",
      "Epoch 30/50 - Train Loss: 0.032317 - Val Loss: 0.045824\n",
      "Epoch 31/50 - Train Loss: 0.028351 - Val Loss: 0.045162\n",
      "Epoch 32/50 - Train Loss: 0.031687 - Val Loss: 0.046901\n",
      "Epoch 33/50 - Train Loss: 0.032462 - Val Loss: 0.046591\n",
      "Epoch 34/50 - Train Loss: 0.027713 - Val Loss: 0.045345\n",
      "Epoch 35/50 - Train Loss: 0.034093 - Val Loss: 0.049634\n",
      "Epoch 36/50 - Train Loss: 0.030805 - Val Loss: 0.045375\n",
      "Epoch 37/50 - Train Loss: 0.027426 - Val Loss: 0.044132\n",
      "Epoch 38/50 - Train Loss: 0.032488 - Val Loss: 0.045440\n",
      "Epoch 39/50 - Train Loss: 0.029277 - Val Loss: 0.050083\n",
      "Epoch 40/50 - Train Loss: 0.035822 - Val Loss: 0.043985\n",
      "Epoch 41/50 - Train Loss: 0.030175 - Val Loss: 0.046268\n",
      "Epoch 42/50 - Train Loss: 0.028671 - Val Loss: 0.054150\n",
      "Epoch 43/50 - Train Loss: 0.036292 - Val Loss: 0.044062\n",
      "Epoch 44/50 - Train Loss: 0.033253 - Val Loss: 0.047244\n",
      "Epoch 45/50 - Train Loss: 0.029681 - Val Loss: 0.046024\n",
      "Epoch 46/50 - Train Loss: 0.031034 - Val Loss: 0.050077\n",
      "Epoch 47/50 - Train Loss: 0.032645 - Val Loss: 0.044003\n",
      "Epoch 48/50 - Train Loss: 0.025715 - Val Loss: 0.047145\n",
      "Epoch 49/50 - Train Loss: 0.027363 - Val Loss: 0.045286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:21:39,144] Trial 22 finished with value: 0.0439853984862566 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 60, 'lr': 0.0033744770357130283, 'weight_decay': 7.876790365126145e-07, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030920 - Val Loss: 0.046473\n",
      "Epoch 1/50 - Train Loss: 0.114884 - Val Loss: 0.065788\n",
      "Epoch 2/50 - Train Loss: 0.058925 - Val Loss: 0.121320\n",
      "Epoch 3/50 - Train Loss: 0.047631 - Val Loss: 0.082451\n",
      "Epoch 4/50 - Train Loss: 0.044980 - Val Loss: 0.066263\n",
      "Epoch 5/50 - Train Loss: 0.046844 - Val Loss: 0.087894\n",
      "Epoch 6/50 - Train Loss: 0.038787 - Val Loss: 0.055573\n",
      "Epoch 7/50 - Train Loss: 0.042588 - Val Loss: 0.069304\n",
      "Epoch 8/50 - Train Loss: 0.033851 - Val Loss: 0.049708\n",
      "Epoch 9/50 - Train Loss: 0.035356 - Val Loss: 0.052636\n",
      "Epoch 10/50 - Train Loss: 0.035706 - Val Loss: 0.054880\n",
      "Epoch 11/50 - Train Loss: 0.032730 - Val Loss: 0.046540\n",
      "Epoch 12/50 - Train Loss: 0.033484 - Val Loss: 0.050776\n",
      "Epoch 13/50 - Train Loss: 0.035702 - Val Loss: 0.049231\n",
      "Epoch 14/50 - Train Loss: 0.031322 - Val Loss: 0.045680\n",
      "Epoch 15/50 - Train Loss: 0.036407 - Val Loss: 0.050282\n",
      "Epoch 16/50 - Train Loss: 0.032666 - Val Loss: 0.052053\n",
      "Epoch 17/50 - Train Loss: 0.031991 - Val Loss: 0.047431\n",
      "Epoch 18/50 - Train Loss: 0.029262 - Val Loss: 0.045835\n",
      "Epoch 19/50 - Train Loss: 0.035532 - Val Loss: 0.051645\n",
      "Epoch 20/50 - Train Loss: 0.033206 - Val Loss: 0.045625\n",
      "Epoch 21/50 - Train Loss: 0.035655 - Val Loss: 0.052794\n",
      "Epoch 22/50 - Train Loss: 0.031978 - Val Loss: 0.049914\n",
      "Epoch 23/50 - Train Loss: 0.033144 - Val Loss: 0.046598\n",
      "Epoch 24/50 - Train Loss: 0.028819 - Val Loss: 0.053612\n",
      "Epoch 25/50 - Train Loss: 0.034434 - Val Loss: 0.047253\n",
      "Epoch 26/50 - Train Loss: 0.035837 - Val Loss: 0.046708\n",
      "Epoch 27/50 - Train Loss: 0.029989 - Val Loss: 0.045915\n",
      "Epoch 28/50 - Train Loss: 0.032960 - Val Loss: 0.052726\n",
      "Epoch 29/50 - Train Loss: 0.028894 - Val Loss: 0.045229\n",
      "Epoch 30/50 - Train Loss: 0.035191 - Val Loss: 0.046102\n",
      "Epoch 31/50 - Train Loss: 0.033908 - Val Loss: 0.049514\n",
      "Epoch 32/50 - Train Loss: 0.030528 - Val Loss: 0.045618\n",
      "Epoch 33/50 - Train Loss: 0.027275 - Val Loss: 0.046604\n",
      "Epoch 34/50 - Train Loss: 0.030158 - Val Loss: 0.051039\n",
      "Epoch 35/50 - Train Loss: 0.031147 - Val Loss: 0.049863\n",
      "Epoch 36/50 - Train Loss: 0.029931 - Val Loss: 0.047423\n",
      "Epoch 37/50 - Train Loss: 0.032554 - Val Loss: 0.048439\n",
      "Epoch 38/50 - Train Loss: 0.031152 - Val Loss: 0.045321\n",
      "Epoch 39/50 - Train Loss: 0.030000 - Val Loss: 0.045933\n",
      "Epoch 40/50 - Train Loss: 0.031400 - Val Loss: 0.048241\n",
      "Epoch 41/50 - Train Loss: 0.032771 - Val Loss: 0.056741\n",
      "Epoch 42/50 - Train Loss: 0.035148 - Val Loss: 0.045857\n",
      "Epoch 43/50 - Train Loss: 0.031719 - Val Loss: 0.047645\n",
      "Epoch 44/50 - Train Loss: 0.029603 - Val Loss: 0.046236\n",
      "Epoch 45/50 - Train Loss: 0.032126 - Val Loss: 0.045095\n",
      "Epoch 46/50 - Train Loss: 0.030501 - Val Loss: 0.049321\n",
      "Epoch 47/50 - Train Loss: 0.030743 - Val Loss: 0.051839\n",
      "Epoch 48/50 - Train Loss: 0.028340 - Val Loss: 0.044859\n",
      "Epoch 49/50 - Train Loss: 0.029979 - Val Loss: 0.045169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:21:56,074] Trial 23 finished with value: 0.044859057292342186 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 64, 'lr': 0.003917257753487933, 'weight_decay': 1.0439975970825029e-06, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028822 - Val Loss: 0.047022\n",
      "Epoch 1/50 - Train Loss: 0.101433 - Val Loss: 0.103609\n",
      "Epoch 2/50 - Train Loss: 0.048289 - Val Loss: 0.066683\n",
      "Epoch 3/50 - Train Loss: 0.046402 - Val Loss: 0.089634\n",
      "Epoch 4/50 - Train Loss: 0.038236 - Val Loss: 0.071142\n",
      "Epoch 5/50 - Train Loss: 0.040700 - Val Loss: 0.072272\n",
      "Epoch 6/50 - Train Loss: 0.039516 - Val Loss: 0.073716\n",
      "Epoch 7/50 - Train Loss: 0.038886 - Val Loss: 0.058072\n",
      "Epoch 8/50 - Train Loss: 0.038980 - Val Loss: 0.054039\n",
      "Epoch 9/50 - Train Loss: 0.035417 - Val Loss: 0.067423\n",
      "Epoch 10/50 - Train Loss: 0.036785 - Val Loss: 0.057871\n",
      "Epoch 11/50 - Train Loss: 0.037534 - Val Loss: 0.048003\n",
      "Epoch 12/50 - Train Loss: 0.040179 - Val Loss: 0.058556\n",
      "Epoch 13/50 - Train Loss: 0.033574 - Val Loss: 0.060289\n",
      "Epoch 14/50 - Train Loss: 0.034551 - Val Loss: 0.046788\n",
      "Epoch 15/50 - Train Loss: 0.029826 - Val Loss: 0.050145\n",
      "Epoch 16/50 - Train Loss: 0.030019 - Val Loss: 0.047566\n",
      "Epoch 17/50 - Train Loss: 0.031583 - Val Loss: 0.046112\n",
      "Epoch 18/50 - Train Loss: 0.032500 - Val Loss: 0.046184\n",
      "Epoch 19/50 - Train Loss: 0.039536 - Val Loss: 0.057560\n",
      "Epoch 20/50 - Train Loss: 0.035640 - Val Loss: 0.076720\n",
      "Epoch 21/50 - Train Loss: 0.040874 - Val Loss: 0.045669\n",
      "Epoch 22/50 - Train Loss: 0.036842 - Val Loss: 0.046283\n",
      "Epoch 23/50 - Train Loss: 0.031740 - Val Loss: 0.060177\n",
      "Epoch 24/50 - Train Loss: 0.032307 - Val Loss: 0.045615\n",
      "Epoch 25/50 - Train Loss: 0.034771 - Val Loss: 0.056160\n",
      "Epoch 26/50 - Train Loss: 0.031291 - Val Loss: 0.054346\n",
      "Epoch 27/50 - Train Loss: 0.030943 - Val Loss: 0.047040\n",
      "Epoch 28/50 - Train Loss: 0.030321 - Val Loss: 0.051860\n",
      "Epoch 29/50 - Train Loss: 0.032915 - Val Loss: 0.050447\n",
      "Epoch 30/50 - Train Loss: 0.032148 - Val Loss: 0.046289\n",
      "Epoch 31/50 - Train Loss: 0.034350 - Val Loss: 0.045111\n",
      "Epoch 32/50 - Train Loss: 0.033783 - Val Loss: 0.046378\n",
      "Epoch 33/50 - Train Loss: 0.034278 - Val Loss: 0.049448\n",
      "Epoch 34/50 - Train Loss: 0.029900 - Val Loss: 0.051327\n",
      "Epoch 35/50 - Train Loss: 0.031476 - Val Loss: 0.050857\n",
      "Epoch 36/50 - Train Loss: 0.027901 - Val Loss: 0.053404\n",
      "Epoch 37/50 - Train Loss: 0.032867 - Val Loss: 0.049756\n",
      "Epoch 38/50 - Train Loss: 0.033060 - Val Loss: 0.045329\n",
      "Epoch 39/50 - Train Loss: 0.034149 - Val Loss: 0.059021\n",
      "Epoch 40/50 - Train Loss: 0.033531 - Val Loss: 0.057937\n",
      "Epoch 41/50 - Train Loss: 0.029253 - Val Loss: 0.045318\n",
      "Epoch 42/50 - Train Loss: 0.032648 - Val Loss: 0.049434\n",
      "Epoch 43/50 - Train Loss: 0.029850 - Val Loss: 0.048920\n",
      "Epoch 44/50 - Train Loss: 0.030251 - Val Loss: 0.045627\n",
      "Epoch 45/50 - Train Loss: 0.030754 - Val Loss: 0.047978\n",
      "Epoch 46/50 - Train Loss: 0.029569 - Val Loss: 0.054997\n",
      "Epoch 47/50 - Train Loss: 0.027339 - Val Loss: 0.050961\n",
      "Epoch 48/50 - Train Loss: 0.033135 - Val Loss: 0.046416\n",
      "Epoch 49/50 - Train Loss: 0.033654 - Val Loss: 0.045024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:22:17,970] Trial 24 finished with value: 0.045023899525403976 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 40, 'lr': 0.009854118837445396, 'weight_decay': 3.025035339053847e-08, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033929 - Val Loss: 0.054461\n",
      "Epoch 1/50 - Train Loss: 0.063706 - Val Loss: 0.058537\n",
      "Epoch 2/50 - Train Loss: 0.035136 - Val Loss: 0.050900\n",
      "Epoch 3/50 - Train Loss: 0.033985 - Val Loss: 0.050325\n",
      "Epoch 4/50 - Train Loss: 0.033222 - Val Loss: 0.047600\n",
      "Epoch 5/50 - Train Loss: 0.032064 - Val Loss: 0.046234\n",
      "Epoch 6/50 - Train Loss: 0.031258 - Val Loss: 0.045961\n",
      "Epoch 7/50 - Train Loss: 0.033061 - Val Loss: 0.045486\n",
      "Epoch 8/50 - Train Loss: 0.032340 - Val Loss: 0.048685\n",
      "Epoch 9/50 - Train Loss: 0.032078 - Val Loss: 0.045241\n",
      "Epoch 10/50 - Train Loss: 0.033430 - Val Loss: 0.056274\n",
      "Epoch 11/50 - Train Loss: 0.033247 - Val Loss: 0.059209\n",
      "Epoch 12/50 - Train Loss: 0.031386 - Val Loss: 0.055399\n",
      "Epoch 13/50 - Train Loss: 0.033348 - Val Loss: 0.048226\n",
      "Epoch 14/50 - Train Loss: 0.028861 - Val Loss: 0.048849\n",
      "Epoch 15/50 - Train Loss: 0.031757 - Val Loss: 0.046558\n",
      "Epoch 16/50 - Train Loss: 0.030628 - Val Loss: 0.047289\n",
      "Epoch 17/50 - Train Loss: 0.030301 - Val Loss: 0.044512\n",
      "Epoch 18/50 - Train Loss: 0.031930 - Val Loss: 0.045028\n",
      "Epoch 19/50 - Train Loss: 0.029122 - Val Loss: 0.044822\n",
      "Epoch 20/50 - Train Loss: 0.029262 - Val Loss: 0.045830\n",
      "Epoch 21/50 - Train Loss: 0.029843 - Val Loss: 0.044208\n",
      "Epoch 22/50 - Train Loss: 0.030572 - Val Loss: 0.044544\n",
      "Epoch 23/50 - Train Loss: 0.029481 - Val Loss: 0.044028\n",
      "Epoch 24/50 - Train Loss: 0.029538 - Val Loss: 0.044585\n",
      "Epoch 25/50 - Train Loss: 0.027603 - Val Loss: 0.047225\n",
      "Epoch 26/50 - Train Loss: 0.028584 - Val Loss: 0.046185\n",
      "Epoch 27/50 - Train Loss: 0.033392 - Val Loss: 0.045959\n",
      "Epoch 28/50 - Train Loss: 0.027749 - Val Loss: 0.045094\n",
      "Epoch 29/50 - Train Loss: 0.033455 - Val Loss: 0.045386\n",
      "Epoch 30/50 - Train Loss: 0.029373 - Val Loss: 0.044764\n",
      "Epoch 31/50 - Train Loss: 0.029688 - Val Loss: 0.045722\n",
      "Epoch 32/50 - Train Loss: 0.027191 - Val Loss: 0.045438\n",
      "Epoch 33/50 - Train Loss: 0.030301 - Val Loss: 0.044873\n",
      "Epoch 34/50 - Train Loss: 0.030500 - Val Loss: 0.045991\n",
      "Epoch 35/50 - Train Loss: 0.029421 - Val Loss: 0.044908\n",
      "Epoch 36/50 - Train Loss: 0.029336 - Val Loss: 0.045979\n",
      "Epoch 37/50 - Train Loss: 0.029387 - Val Loss: 0.046294\n",
      "Epoch 38/50 - Train Loss: 0.029012 - Val Loss: 0.048095\n",
      "Epoch 39/50 - Train Loss: 0.031214 - Val Loss: 0.045878\n",
      "Epoch 40/50 - Train Loss: 0.028769 - Val Loss: 0.045792\n",
      "Epoch 41/50 - Train Loss: 0.028951 - Val Loss: 0.045826\n",
      "Epoch 42/50 - Train Loss: 0.033950 - Val Loss: 0.054400\n",
      "Epoch 43/50 - Train Loss: 0.032754 - Val Loss: 0.051812\n",
      "Epoch 44/50 - Train Loss: 0.032095 - Val Loss: 0.047391\n",
      "Epoch 45/50 - Train Loss: 0.029860 - Val Loss: 0.045125\n",
      "Epoch 46/50 - Train Loss: 0.029641 - Val Loss: 0.047141\n",
      "Epoch 47/50 - Train Loss: 0.029707 - Val Loss: 0.044994\n",
      "Epoch 48/50 - Train Loss: 0.030274 - Val Loss: 0.044629\n",
      "Epoch 49/50 - Train Loss: 0.027679 - Val Loss: 0.044707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:22:41,935] Trial 25 finished with value: 0.04402841627597809 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 147, 'lr': 0.004163580181366946, 'weight_decay': 1.6412721922263226e-07, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029523 - Val Loss: 0.045915\n",
      "Epoch 1/50 - Train Loss: 0.233887 - Val Loss: 0.320868\n",
      "Epoch 2/50 - Train Loss: 0.140641 - Val Loss: 0.234412\n",
      "Epoch 3/50 - Train Loss: 0.106851 - Val Loss: 0.156406\n",
      "Epoch 4/50 - Train Loss: 0.064084 - Val Loss: 0.099035\n",
      "Epoch 5/50 - Train Loss: 0.053572 - Val Loss: 0.076228\n",
      "Epoch 6/50 - Train Loss: 0.064295 - Val Loss: 0.073851\n",
      "Epoch 7/50 - Train Loss: 0.062376 - Val Loss: 0.077586\n",
      "Epoch 8/50 - Train Loss: 0.044440 - Val Loss: 0.088295\n",
      "Epoch 9/50 - Train Loss: 0.050424 - Val Loss: 0.100615\n",
      "Epoch 10/50 - Train Loss: 0.042044 - Val Loss: 0.107692\n",
      "Epoch 11/50 - Train Loss: 0.052241 - Val Loss: 0.107488\n",
      "Epoch 12/50 - Train Loss: 0.051613 - Val Loss: 0.101071\n",
      "Epoch 13/50 - Train Loss: 0.049162 - Val Loss: 0.091987\n",
      "Epoch 14/50 - Train Loss: 0.048597 - Val Loss: 0.083357\n",
      "Epoch 15/50 - Train Loss: 0.038385 - Val Loss: 0.076523\n",
      "Epoch 16/50 - Train Loss: 0.039684 - Val Loss: 0.072341\n",
      "Epoch 17/50 - Train Loss: 0.039944 - Val Loss: 0.070988\n",
      "Epoch 18/50 - Train Loss: 0.038312 - Val Loss: 0.072218\n",
      "Epoch 19/50 - Train Loss: 0.045504 - Val Loss: 0.075606\n",
      "Epoch 20/50 - Train Loss: 0.036616 - Val Loss: 0.078719\n",
      "Epoch 21/50 - Train Loss: 0.045068 - Val Loss: 0.079028\n",
      "Epoch 22/50 - Train Loss: 0.035923 - Val Loss: 0.074170\n",
      "Epoch 23/50 - Train Loss: 0.036213 - Val Loss: 0.068046\n",
      "Epoch 24/50 - Train Loss: 0.041903 - Val Loss: 0.063127\n",
      "Epoch 25/50 - Train Loss: 0.043013 - Val Loss: 0.061388\n",
      "Epoch 26/50 - Train Loss: 0.033466 - Val Loss: 0.061487\n",
      "Epoch 27/50 - Train Loss: 0.032963 - Val Loss: 0.062097\n",
      "Epoch 28/50 - Train Loss: 0.039933 - Val Loss: 0.060835\n",
      "Epoch 29/50 - Train Loss: 0.039996 - Val Loss: 0.057986\n",
      "Epoch 30/50 - Train Loss: 0.033446 - Val Loss: 0.054263\n",
      "Epoch 31/50 - Train Loss: 0.031966 - Val Loss: 0.052281\n",
      "Epoch 32/50 - Train Loss: 0.038888 - Val Loss: 0.053613\n",
      "Epoch 33/50 - Train Loss: 0.039084 - Val Loss: 0.055285\n",
      "Epoch 34/50 - Train Loss: 0.040017 - Val Loss: 0.054351\n",
      "Epoch 35/50 - Train Loss: 0.038547 - Val Loss: 0.050372\n",
      "Epoch 36/50 - Train Loss: 0.038017 - Val Loss: 0.049732\n",
      "Epoch 37/50 - Train Loss: 0.037963 - Val Loss: 0.050954\n",
      "Epoch 38/50 - Train Loss: 0.039312 - Val Loss: 0.051930\n",
      "Epoch 39/50 - Train Loss: 0.031518 - Val Loss: 0.049324\n",
      "Epoch 40/50 - Train Loss: 0.030727 - Val Loss: 0.047420\n",
      "Epoch 41/50 - Train Loss: 0.029422 - Val Loss: 0.047080\n",
      "Epoch 42/50 - Train Loss: 0.030159 - Val Loss: 0.047936\n",
      "Epoch 43/50 - Train Loss: 0.030241 - Val Loss: 0.049280\n",
      "Epoch 44/50 - Train Loss: 0.038254 - Val Loss: 0.047277\n",
      "Epoch 45/50 - Train Loss: 0.029751 - Val Loss: 0.046513\n",
      "Epoch 46/50 - Train Loss: 0.037639 - Val Loss: 0.046525\n",
      "Epoch 47/50 - Train Loss: 0.030466 - Val Loss: 0.047513\n",
      "Epoch 48/50 - Train Loss: 0.030581 - Val Loss: 0.047670\n",
      "Epoch 49/50 - Train Loss: 0.038647 - Val Loss: 0.046800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:22:55,130] Trial 26 finished with value: 0.046512577682733536 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 57, 'lr': 0.002069659124296238, 'weight_decay': 9.841322156802296e-07, 'batch_size': 64}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.038037 - Val Loss: 0.046776\n",
      "Epoch 1/50 - Train Loss: 0.231976 - Val Loss: 0.470686\n",
      "Epoch 2/50 - Train Loss: 0.261687 - Val Loss: 0.441160\n",
      "Epoch 3/50 - Train Loss: 0.229679 - Val Loss: 0.411779\n",
      "Epoch 4/50 - Train Loss: 0.216804 - Val Loss: 0.382867\n",
      "Epoch 5/50 - Train Loss: 0.205426 - Val Loss: 0.353575\n",
      "Epoch 6/50 - Train Loss: 0.180926 - Val Loss: 0.323133\n",
      "Epoch 7/50 - Train Loss: 0.147035 - Val Loss: 0.291177\n",
      "Epoch 8/50 - Train Loss: 0.127673 - Val Loss: 0.257734\n",
      "Epoch 9/50 - Train Loss: 0.110966 - Val Loss: 0.224350\n",
      "Epoch 10/50 - Train Loss: 0.107202 - Val Loss: 0.190532\n",
      "Epoch 11/50 - Train Loss: 0.069352 - Val Loss: 0.156658\n",
      "Epoch 12/50 - Train Loss: 0.065733 - Val Loss: 0.127216\n",
      "Epoch 13/50 - Train Loss: 0.049862 - Val Loss: 0.103234\n",
      "Epoch 14/50 - Train Loss: 0.048829 - Val Loss: 0.088353\n",
      "Epoch 15/50 - Train Loss: 0.044516 - Val Loss: 0.081674\n",
      "Epoch 16/50 - Train Loss: 0.050048 - Val Loss: 0.080433\n",
      "Epoch 17/50 - Train Loss: 0.042799 - Val Loss: 0.083484\n",
      "Epoch 18/50 - Train Loss: 0.043212 - Val Loss: 0.088388\n",
      "Epoch 19/50 - Train Loss: 0.043604 - Val Loss: 0.093029\n",
      "Epoch 20/50 - Train Loss: 0.045201 - Val Loss: 0.096359\n",
      "Epoch 21/50 - Train Loss: 0.038812 - Val Loss: 0.095882\n",
      "Epoch 22/50 - Train Loss: 0.046403 - Val Loss: 0.092780\n",
      "Epoch 23/50 - Train Loss: 0.043170 - Val Loss: 0.088573\n",
      "Epoch 24/50 - Train Loss: 0.045841 - Val Loss: 0.084497\n",
      "Epoch 25/50 - Train Loss: 0.040440 - Val Loss: 0.080013\n",
      "Epoch 26/50 - Train Loss: 0.045002 - Val Loss: 0.077092\n",
      "Epoch 27/50 - Train Loss: 0.045022 - Val Loss: 0.077358\n",
      "Epoch 28/50 - Train Loss: 0.044637 - Val Loss: 0.079304\n",
      "Epoch 29/50 - Train Loss: 0.043089 - Val Loss: 0.081256\n",
      "Epoch 30/50 - Train Loss: 0.040672 - Val Loss: 0.081519\n",
      "Epoch 31/50 - Train Loss: 0.042324 - Val Loss: 0.081017\n",
      "Epoch 32/50 - Train Loss: 0.039562 - Val Loss: 0.078713\n",
      "Epoch 33/50 - Train Loss: 0.038513 - Val Loss: 0.075667\n",
      "Epoch 34/50 - Train Loss: 0.034422 - Val Loss: 0.072668\n",
      "Epoch 35/50 - Train Loss: 0.039116 - Val Loss: 0.071679\n",
      "Epoch 36/50 - Train Loss: 0.039207 - Val Loss: 0.073261\n",
      "Epoch 37/50 - Train Loss: 0.039380 - Val Loss: 0.071988\n",
      "Epoch 38/50 - Train Loss: 0.036775 - Val Loss: 0.069264\n",
      "Epoch 39/50 - Train Loss: 0.032794 - Val Loss: 0.067795\n",
      "Epoch 40/50 - Train Loss: 0.037363 - Val Loss: 0.068143\n",
      "Epoch 41/50 - Train Loss: 0.028719 - Val Loss: 0.068644\n",
      "Epoch 42/50 - Train Loss: 0.038005 - Val Loss: 0.067663\n",
      "Epoch 43/50 - Train Loss: 0.038589 - Val Loss: 0.067952\n",
      "Epoch 44/50 - Train Loss: 0.037860 - Val Loss: 0.064288\n",
      "Epoch 45/50 - Train Loss: 0.036912 - Val Loss: 0.059869\n",
      "Epoch 46/50 - Train Loss: 0.033413 - Val Loss: 0.057726\n",
      "Epoch 47/50 - Train Loss: 0.031211 - Val Loss: 0.059731\n",
      "Epoch 48/50 - Train Loss: 0.033198 - Val Loss: 0.062831\n",
      "Epoch 49/50 - Train Loss: 0.037942 - Val Loss: 0.060839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:23:13,164] Trial 27 finished with value: 0.05772577226161957 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 121, 'lr': 0.0001623075088673916, 'weight_decay': 5.0995010820786584e-05, 'batch_size': 32}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030392 - Val Loss: 0.057894\n",
      "Epoch 1/50 - Train Loss: 0.127367 - Val Loss: 0.256664\n",
      "Epoch 2/50 - Train Loss: 0.124750 - Val Loss: 0.247946\n",
      "Epoch 3/50 - Train Loss: 0.113226 - Val Loss: 0.239633\n",
      "Epoch 4/50 - Train Loss: 0.103579 - Val Loss: 0.231415\n",
      "Epoch 5/50 - Train Loss: 0.097670 - Val Loss: 0.223463\n",
      "Epoch 6/50 - Train Loss: 0.101580 - Val Loss: 0.215793\n",
      "Epoch 7/50 - Train Loss: 0.095071 - Val Loss: 0.208132\n",
      "Epoch 8/50 - Train Loss: 0.088984 - Val Loss: 0.200677\n",
      "Epoch 9/50 - Train Loss: 0.085571 - Val Loss: 0.193143\n",
      "Epoch 10/50 - Train Loss: 0.084988 - Val Loss: 0.185696\n",
      "Epoch 11/50 - Train Loss: 0.073556 - Val Loss: 0.178569\n",
      "Epoch 12/50 - Train Loss: 0.072795 - Val Loss: 0.171755\n",
      "Epoch 13/50 - Train Loss: 0.073867 - Val Loss: 0.165081\n",
      "Epoch 14/50 - Train Loss: 0.065366 - Val Loss: 0.158577\n",
      "Epoch 15/50 - Train Loss: 0.063738 - Val Loss: 0.152440\n",
      "Epoch 16/50 - Train Loss: 0.067106 - Val Loss: 0.146721\n",
      "Epoch 17/50 - Train Loss: 0.058821 - Val Loss: 0.140879\n",
      "Epoch 18/50 - Train Loss: 0.057418 - Val Loss: 0.135247\n",
      "Epoch 19/50 - Train Loss: 0.055602 - Val Loss: 0.129897\n",
      "Epoch 20/50 - Train Loss: 0.055382 - Val Loss: 0.124895\n",
      "Epoch 21/50 - Train Loss: 0.050738 - Val Loss: 0.120129\n",
      "Epoch 22/50 - Train Loss: 0.050664 - Val Loss: 0.115891\n",
      "Epoch 23/50 - Train Loss: 0.047347 - Val Loss: 0.111893\n",
      "Epoch 24/50 - Train Loss: 0.047566 - Val Loss: 0.108002\n",
      "Epoch 25/50 - Train Loss: 0.046838 - Val Loss: 0.104535\n",
      "Epoch 26/50 - Train Loss: 0.048130 - Val Loss: 0.101307\n",
      "Epoch 27/50 - Train Loss: 0.042164 - Val Loss: 0.098110\n",
      "Epoch 28/50 - Train Loss: 0.047451 - Val Loss: 0.095273\n",
      "Epoch 29/50 - Train Loss: 0.047465 - Val Loss: 0.092680\n",
      "Epoch 30/50 - Train Loss: 0.043753 - Val Loss: 0.090266\n",
      "Epoch 31/50 - Train Loss: 0.045429 - Val Loss: 0.088119\n",
      "Epoch 32/50 - Train Loss: 0.041947 - Val Loss: 0.086105\n",
      "Epoch 33/50 - Train Loss: 0.043422 - Val Loss: 0.084283\n",
      "Epoch 34/50 - Train Loss: 0.044529 - Val Loss: 0.082840\n",
      "Epoch 35/50 - Train Loss: 0.042797 - Val Loss: 0.081315\n",
      "Epoch 36/50 - Train Loss: 0.037729 - Val Loss: 0.080015\n",
      "Epoch 37/50 - Train Loss: 0.039502 - Val Loss: 0.078739\n",
      "Epoch 38/50 - Train Loss: 0.039894 - Val Loss: 0.077538\n",
      "Epoch 39/50 - Train Loss: 0.038957 - Val Loss: 0.076421\n",
      "Epoch 40/50 - Train Loss: 0.043604 - Val Loss: 0.075558\n",
      "Epoch 41/50 - Train Loss: 0.041932 - Val Loss: 0.074879\n",
      "Epoch 42/50 - Train Loss: 0.036887 - Val Loss: 0.073968\n",
      "Epoch 43/50 - Train Loss: 0.039308 - Val Loss: 0.073088\n",
      "Epoch 44/50 - Train Loss: 0.041591 - Val Loss: 0.072669\n",
      "Epoch 45/50 - Train Loss: 0.038461 - Val Loss: 0.071958\n",
      "Epoch 46/50 - Train Loss: 0.038517 - Val Loss: 0.070915\n",
      "Epoch 47/50 - Train Loss: 0.040070 - Val Loss: 0.070187\n",
      "Epoch 48/50 - Train Loss: 0.038652 - Val Loss: 0.069778\n",
      "Epoch 49/50 - Train Loss: 0.035926 - Val Loss: 0.069131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:23:35,595] Trial 28 finished with value: 0.0687384158372879 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 219, 'lr': 1.1590878359226669e-05, 'weight_decay': 2.8149917128006654e-08, 'batch_size': 16}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.034177 - Val Loss: 0.068738\n",
      "Epoch 1/50 - Train Loss: 0.153079 - Val Loss: 0.284487\n",
      "Epoch 2/50 - Train Loss: 0.120650 - Val Loss: 0.227075\n",
      "Epoch 3/50 - Train Loss: 0.099723 - Val Loss: 0.167866\n",
      "Epoch 4/50 - Train Loss: 0.052453 - Val Loss: 0.114562\n",
      "Epoch 5/50 - Train Loss: 0.046870 - Val Loss: 0.080949\n",
      "Epoch 6/50 - Train Loss: 0.048630 - Val Loss: 0.072055\n",
      "Epoch 7/50 - Train Loss: 0.049198 - Val Loss: 0.074697\n",
      "Epoch 8/50 - Train Loss: 0.042832 - Val Loss: 0.085909\n",
      "Epoch 9/50 - Train Loss: 0.034383 - Val Loss: 0.095973\n",
      "Epoch 10/50 - Train Loss: 0.045055 - Val Loss: 0.094675\n",
      "Epoch 11/50 - Train Loss: 0.042083 - Val Loss: 0.084529\n",
      "Epoch 12/50 - Train Loss: 0.046214 - Val Loss: 0.072549\n",
      "Epoch 13/50 - Train Loss: 0.036408 - Val Loss: 0.065497\n",
      "Epoch 14/50 - Train Loss: 0.038709 - Val Loss: 0.064111\n",
      "Epoch 15/50 - Train Loss: 0.034895 - Val Loss: 0.065773\n",
      "Epoch 16/50 - Train Loss: 0.035912 - Val Loss: 0.069062\n",
      "Epoch 17/50 - Train Loss: 0.033363 - Val Loss: 0.065247\n",
      "Epoch 18/50 - Train Loss: 0.039102 - Val Loss: 0.057413\n",
      "Epoch 19/50 - Train Loss: 0.036050 - Val Loss: 0.054206\n",
      "Epoch 20/50 - Train Loss: 0.035431 - Val Loss: 0.056017\n",
      "Epoch 21/50 - Train Loss: 0.031958 - Val Loss: 0.058406\n",
      "Epoch 22/50 - Train Loss: 0.033134 - Val Loss: 0.054104\n",
      "Epoch 23/50 - Train Loss: 0.040346 - Val Loss: 0.051840\n",
      "Epoch 24/50 - Train Loss: 0.030096 - Val Loss: 0.054349\n",
      "Epoch 25/50 - Train Loss: 0.035980 - Val Loss: 0.054508\n",
      "Epoch 26/50 - Train Loss: 0.033205 - Val Loss: 0.052247\n",
      "Epoch 27/50 - Train Loss: 0.030728 - Val Loss: 0.050979\n",
      "Epoch 28/50 - Train Loss: 0.030819 - Val Loss: 0.051916\n",
      "Epoch 29/50 - Train Loss: 0.032147 - Val Loss: 0.052165\n",
      "Epoch 30/50 - Train Loss: 0.029193 - Val Loss: 0.050005\n",
      "Epoch 31/50 - Train Loss: 0.032497 - Val Loss: 0.050330\n",
      "Epoch 32/50 - Train Loss: 0.028504 - Val Loss: 0.050138\n",
      "Epoch 33/50 - Train Loss: 0.033188 - Val Loss: 0.049591\n",
      "Epoch 34/50 - Train Loss: 0.028139 - Val Loss: 0.048138\n",
      "Epoch 35/50 - Train Loss: 0.035630 - Val Loss: 0.049940\n",
      "Epoch 36/50 - Train Loss: 0.026545 - Val Loss: 0.050022\n",
      "Epoch 37/50 - Train Loss: 0.037751 - Val Loss: 0.047647\n",
      "Epoch 38/50 - Train Loss: 0.032002 - Val Loss: 0.048360\n",
      "Epoch 39/50 - Train Loss: 0.033535 - Val Loss: 0.052378\n",
      "Epoch 40/50 - Train Loss: 0.033755 - Val Loss: 0.049732\n",
      "Epoch 41/50 - Train Loss: 0.032123 - Val Loss: 0.049615\n",
      "Epoch 42/50 - Train Loss: 0.032481 - Val Loss: 0.052504\n",
      "Epoch 43/50 - Train Loss: 0.037303 - Val Loss: 0.051708\n",
      "Epoch 44/50 - Train Loss: 0.029248 - Val Loss: 0.050029\n",
      "Epoch 45/50 - Train Loss: 0.032034 - Val Loss: 0.049153\n",
      "Epoch 46/50 - Train Loss: 0.030416 - Val Loss: 0.050749\n",
      "Epoch 47/50 - Train Loss: 0.033516 - Val Loss: 0.050024\n",
      "Epoch 48/50 - Train Loss: 0.038992 - Val Loss: 0.050086\n",
      "Epoch 49/50 - Train Loss: 0.028890 - Val Loss: 0.049764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:23:49,458] Trial 29 finished with value: 0.04764662683010101 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 73, 'lr': 0.0007661477870323897, 'weight_decay': 7.207674639040002e-08, 'batch_size': 32}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.034523 - Val Loss: 0.050757\n",
      "Epoch 1/50 - Train Loss: 0.063858 - Val Loss: 0.077399\n",
      "Epoch 2/50 - Train Loss: 0.037244 - Val Loss: 0.084950\n",
      "Epoch 3/50 - Train Loss: 0.033904 - Val Loss: 0.058352\n",
      "Epoch 4/50 - Train Loss: 0.033412 - Val Loss: 0.048725\n",
      "Epoch 5/50 - Train Loss: 0.033048 - Val Loss: 0.046328\n",
      "Epoch 6/50 - Train Loss: 0.032412 - Val Loss: 0.046234\n",
      "Epoch 7/50 - Train Loss: 0.031155 - Val Loss: 0.047004\n",
      "Epoch 8/50 - Train Loss: 0.031099 - Val Loss: 0.045769\n",
      "Epoch 9/50 - Train Loss: 0.031575 - Val Loss: 0.046657\n",
      "Epoch 10/50 - Train Loss: 0.031583 - Val Loss: 0.047364\n",
      "Epoch 11/50 - Train Loss: 0.030607 - Val Loss: 0.048933\n",
      "Epoch 12/50 - Train Loss: 0.031679 - Val Loss: 0.048122\n",
      "Epoch 13/50 - Train Loss: 0.028728 - Val Loss: 0.045829\n",
      "Epoch 14/50 - Train Loss: 0.029104 - Val Loss: 0.048040\n",
      "Epoch 15/50 - Train Loss: 0.028930 - Val Loss: 0.046382\n",
      "Epoch 16/50 - Train Loss: 0.029298 - Val Loss: 0.046247\n",
      "Epoch 17/50 - Train Loss: 0.028556 - Val Loss: 0.045271\n",
      "Epoch 18/50 - Train Loss: 0.030368 - Val Loss: 0.048344\n",
      "Epoch 19/50 - Train Loss: 0.032150 - Val Loss: 0.051406\n",
      "Epoch 20/50 - Train Loss: 0.028374 - Val Loss: 0.047182\n",
      "Epoch 21/50 - Train Loss: 0.029685 - Val Loss: 0.052224\n",
      "Epoch 22/50 - Train Loss: 0.030107 - Val Loss: 0.047024\n",
      "Epoch 23/50 - Train Loss: 0.031113 - Val Loss: 0.058795\n",
      "Epoch 24/50 - Train Loss: 0.033112 - Val Loss: 0.044655\n",
      "Epoch 25/50 - Train Loss: 0.028968 - Val Loss: 0.044918\n",
      "Epoch 26/50 - Train Loss: 0.028156 - Val Loss: 0.050785\n",
      "Epoch 27/50 - Train Loss: 0.032342 - Val Loss: 0.046493\n",
      "Epoch 28/50 - Train Loss: 0.028847 - Val Loss: 0.049450\n",
      "Epoch 29/50 - Train Loss: 0.032960 - Val Loss: 0.045641\n",
      "Epoch 30/50 - Train Loss: 0.029211 - Val Loss: 0.048906\n",
      "Epoch 31/50 - Train Loss: 0.031999 - Val Loss: 0.044591\n",
      "Epoch 32/50 - Train Loss: 0.030923 - Val Loss: 0.044302\n",
      "Epoch 33/50 - Train Loss: 0.032142 - Val Loss: 0.045456\n",
      "Epoch 34/50 - Train Loss: 0.030652 - Val Loss: 0.046015\n",
      "Epoch 35/50 - Train Loss: 0.030320 - Val Loss: 0.057454\n",
      "Epoch 36/50 - Train Loss: 0.029931 - Val Loss: 0.050462\n",
      "Epoch 37/50 - Train Loss: 0.029503 - Val Loss: 0.045909\n",
      "Epoch 38/50 - Train Loss: 0.028144 - Val Loss: 0.045205\n",
      "Epoch 39/50 - Train Loss: 0.029668 - Val Loss: 0.045543\n",
      "Epoch 40/50 - Train Loss: 0.028501 - Val Loss: 0.050763\n",
      "Epoch 41/50 - Train Loss: 0.031843 - Val Loss: 0.047106\n",
      "Epoch 42/50 - Train Loss: 0.030534 - Val Loss: 0.045739\n",
      "Epoch 43/50 - Train Loss: 0.029226 - Val Loss: 0.046531\n",
      "Epoch 44/50 - Train Loss: 0.030469 - Val Loss: 0.046614\n",
      "Epoch 45/50 - Train Loss: 0.031880 - Val Loss: 0.045653\n",
      "Epoch 46/50 - Train Loss: 0.031889 - Val Loss: 0.043994\n",
      "Epoch 47/50 - Train Loss: 0.030058 - Val Loss: 0.044005\n",
      "Epoch 48/50 - Train Loss: 0.030310 - Val Loss: 0.048496\n",
      "Epoch 49/50 - Train Loss: 0.033719 - Val Loss: 0.045577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:24:10,944] Trial 30 finished with value: 0.04379580790797869 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 103, 'lr': 0.00513310185949492, 'weight_decay': 4.77969303362812e-07, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028019 - Val Loss: 0.043796\n",
      "Epoch 1/50 - Train Loss: 0.068817 - Val Loss: 0.064099\n",
      "Epoch 2/50 - Train Loss: 0.037900 - Val Loss: 0.054443\n",
      "Epoch 3/50 - Train Loss: 0.032178 - Val Loss: 0.051457\n",
      "Epoch 4/50 - Train Loss: 0.035398 - Val Loss: 0.054625\n",
      "Epoch 5/50 - Train Loss: 0.030879 - Val Loss: 0.050933\n",
      "Epoch 6/50 - Train Loss: 0.032205 - Val Loss: 0.049131\n",
      "Epoch 7/50 - Train Loss: 0.033313 - Val Loss: 0.056409\n",
      "Epoch 8/50 - Train Loss: 0.030344 - Val Loss: 0.054363\n",
      "Epoch 9/50 - Train Loss: 0.034069 - Val Loss: 0.054053\n",
      "Epoch 10/50 - Train Loss: 0.031608 - Val Loss: 0.050293\n",
      "Epoch 11/50 - Train Loss: 0.030674 - Val Loss: 0.052463\n",
      "Epoch 12/50 - Train Loss: 0.031955 - Val Loss: 0.048209\n",
      "Epoch 13/50 - Train Loss: 0.027864 - Val Loss: 0.045951\n",
      "Epoch 14/50 - Train Loss: 0.030255 - Val Loss: 0.056674\n",
      "Epoch 15/50 - Train Loss: 0.031182 - Val Loss: 0.051040\n",
      "Epoch 16/50 - Train Loss: 0.031173 - Val Loss: 0.053851\n",
      "Epoch 17/50 - Train Loss: 0.029746 - Val Loss: 0.050243\n",
      "Epoch 18/50 - Train Loss: 0.032330 - Val Loss: 0.045993\n",
      "Epoch 19/50 - Train Loss: 0.030858 - Val Loss: 0.046084\n",
      "Epoch 20/50 - Train Loss: 0.027067 - Val Loss: 0.049620\n",
      "Epoch 21/50 - Train Loss: 0.033130 - Val Loss: 0.047839\n",
      "Epoch 22/50 - Train Loss: 0.032043 - Val Loss: 0.045391\n",
      "Epoch 23/50 - Train Loss: 0.031847 - Val Loss: 0.045952\n",
      "Epoch 24/50 - Train Loss: 0.033544 - Val Loss: 0.046259\n",
      "Epoch 25/50 - Train Loss: 0.029773 - Val Loss: 0.045192\n",
      "Epoch 26/50 - Train Loss: 0.030299 - Val Loss: 0.047462\n",
      "Epoch 27/50 - Train Loss: 0.030627 - Val Loss: 0.045431\n",
      "Epoch 28/50 - Train Loss: 0.032168 - Val Loss: 0.045071\n",
      "Epoch 29/50 - Train Loss: 0.031449 - Val Loss: 0.046212\n",
      "Epoch 30/50 - Train Loss: 0.031689 - Val Loss: 0.046722\n",
      "Epoch 31/50 - Train Loss: 0.030324 - Val Loss: 0.045907\n",
      "Epoch 32/50 - Train Loss: 0.029512 - Val Loss: 0.046970\n",
      "Epoch 33/50 - Train Loss: 0.028942 - Val Loss: 0.045182\n",
      "Epoch 34/50 - Train Loss: 0.030853 - Val Loss: 0.046246\n",
      "Epoch 35/50 - Train Loss: 0.032556 - Val Loss: 0.044610\n",
      "Epoch 36/50 - Train Loss: 0.031544 - Val Loss: 0.045484\n",
      "Epoch 37/50 - Train Loss: 0.029336 - Val Loss: 0.045142\n",
      "Epoch 38/50 - Train Loss: 0.030062 - Val Loss: 0.045669\n",
      "Epoch 39/50 - Train Loss: 0.031258 - Val Loss: 0.047010\n",
      "Epoch 40/50 - Train Loss: 0.030110 - Val Loss: 0.045859\n",
      "Epoch 41/50 - Train Loss: 0.031396 - Val Loss: 0.045180\n",
      "Epoch 42/50 - Train Loss: 0.029153 - Val Loss: 0.046513\n",
      "Epoch 43/50 - Train Loss: 0.029115 - Val Loss: 0.044598\n",
      "Epoch 44/50 - Train Loss: 0.029535 - Val Loss: 0.045315\n",
      "Epoch 45/50 - Train Loss: 0.028846 - Val Loss: 0.044780\n",
      "Epoch 46/50 - Train Loss: 0.029662 - Val Loss: 0.045187\n",
      "Epoch 47/50 - Train Loss: 0.028343 - Val Loss: 0.045040\n",
      "Epoch 48/50 - Train Loss: 0.030026 - Val Loss: 0.045173\n",
      "Epoch 49/50 - Train Loss: 0.030026 - Val Loss: 0.045399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:24:31,295] Trial 31 finished with value: 0.04459803054730097 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 98, 'lr': 0.005141113233368203, 'weight_decay': 6.507658054435728e-06, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030649 - Val Loss: 0.044992\n",
      "Epoch 1/50 - Train Loss: 0.062963 - Val Loss: 0.059893\n",
      "Epoch 2/50 - Train Loss: 0.039365 - Val Loss: 0.060125\n",
      "Epoch 3/50 - Train Loss: 0.033746 - Val Loss: 0.047952\n",
      "Epoch 4/50 - Train Loss: 0.031550 - Val Loss: 0.047724\n",
      "Epoch 5/50 - Train Loss: 0.031476 - Val Loss: 0.044656\n",
      "Epoch 6/50 - Train Loss: 0.033807 - Val Loss: 0.053654\n",
      "Epoch 7/50 - Train Loss: 0.035652 - Val Loss: 0.052757\n",
      "Epoch 8/50 - Train Loss: 0.032155 - Val Loss: 0.047374\n",
      "Epoch 9/50 - Train Loss: 0.030216 - Val Loss: 0.047900\n",
      "Epoch 10/50 - Train Loss: 0.029863 - Val Loss: 0.044823\n",
      "Epoch 11/50 - Train Loss: 0.029023 - Val Loss: 0.050709\n",
      "Epoch 12/50 - Train Loss: 0.034392 - Val Loss: 0.053278\n",
      "Epoch 13/50 - Train Loss: 0.030192 - Val Loss: 0.045851\n",
      "Epoch 14/50 - Train Loss: 0.031773 - Val Loss: 0.046668\n",
      "Epoch 15/50 - Train Loss: 0.028650 - Val Loss: 0.046397\n",
      "Epoch 16/50 - Train Loss: 0.028752 - Val Loss: 0.047576\n",
      "Epoch 17/50 - Train Loss: 0.032036 - Val Loss: 0.054022\n",
      "Epoch 18/50 - Train Loss: 0.033558 - Val Loss: 0.047623\n",
      "Epoch 19/50 - Train Loss: 0.030312 - Val Loss: 0.044828\n",
      "Epoch 20/50 - Train Loss: 0.033214 - Val Loss: 0.044709\n",
      "Epoch 21/50 - Train Loss: 0.032288 - Val Loss: 0.052333\n",
      "Epoch 22/50 - Train Loss: 0.033429 - Val Loss: 0.045545\n",
      "Epoch 23/50 - Train Loss: 0.030644 - Val Loss: 0.043990\n",
      "Epoch 24/50 - Train Loss: 0.031626 - Val Loss: 0.053402\n",
      "Epoch 25/50 - Train Loss: 0.030473 - Val Loss: 0.045845\n",
      "Epoch 26/50 - Train Loss: 0.031850 - Val Loss: 0.046637\n",
      "Epoch 27/50 - Train Loss: 0.029781 - Val Loss: 0.044172\n",
      "Epoch 28/50 - Train Loss: 0.029943 - Val Loss: 0.045172\n",
      "Epoch 29/50 - Train Loss: 0.028157 - Val Loss: 0.044643\n",
      "Epoch 30/50 - Train Loss: 0.029779 - Val Loss: 0.048325\n",
      "Epoch 31/50 - Train Loss: 0.030059 - Val Loss: 0.043598\n",
      "Epoch 32/50 - Train Loss: 0.031129 - Val Loss: 0.044184\n",
      "Epoch 33/50 - Train Loss: 0.028799 - Val Loss: 0.044870\n",
      "Epoch 34/50 - Train Loss: 0.029579 - Val Loss: 0.044681\n",
      "Epoch 35/50 - Train Loss: 0.029525 - Val Loss: 0.045789\n",
      "Epoch 36/50 - Train Loss: 0.032070 - Val Loss: 0.045103\n",
      "Epoch 37/50 - Train Loss: 0.031701 - Val Loss: 0.049188\n",
      "Epoch 38/50 - Train Loss: 0.028770 - Val Loss: 0.044651\n",
      "Epoch 39/50 - Train Loss: 0.029808 - Val Loss: 0.048610\n",
      "Epoch 40/50 - Train Loss: 0.030867 - Val Loss: 0.049536\n",
      "Epoch 41/50 - Train Loss: 0.030293 - Val Loss: 0.045243\n",
      "Epoch 42/50 - Train Loss: 0.029510 - Val Loss: 0.046307\n",
      "Epoch 43/50 - Train Loss: 0.031946 - Val Loss: 0.048143\n",
      "Epoch 44/50 - Train Loss: 0.032675 - Val Loss: 0.046231\n",
      "Epoch 45/50 - Train Loss: 0.030617 - Val Loss: 0.044810\n",
      "Epoch 46/50 - Train Loss: 0.033353 - Val Loss: 0.046025\n",
      "Epoch 47/50 - Train Loss: 0.030448 - Val Loss: 0.050822\n",
      "Epoch 48/50 - Train Loss: 0.027510 - Val Loss: 0.043919\n",
      "Epoch 49/50 - Train Loss: 0.028766 - Val Loss: 0.045939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:24:49,149] Trial 32 finished with value: 0.04359789316852888 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 115, 'lr': 0.006209399434965003, 'weight_decay': 5.737863773821671e-07, 'batch_size': 8}. Best is trial 6 with value: 0.04356965236365795.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031195 - Val Loss: 0.048328\n",
      "Epoch 1/50 - Train Loss: 0.060585 - Val Loss: 0.056787\n",
      "Epoch 2/50 - Train Loss: 0.040869 - Val Loss: 0.057716\n",
      "Epoch 3/50 - Train Loss: 0.035985 - Val Loss: 0.049838\n",
      "Epoch 4/50 - Train Loss: 0.036446 - Val Loss: 0.049560\n",
      "Epoch 5/50 - Train Loss: 0.032543 - Val Loss: 0.064476\n",
      "Epoch 6/50 - Train Loss: 0.032475 - Val Loss: 0.054165\n",
      "Epoch 7/50 - Train Loss: 0.033259 - Val Loss: 0.048984\n",
      "Epoch 8/50 - Train Loss: 0.033762 - Val Loss: 0.046591\n",
      "Epoch 9/50 - Train Loss: 0.031581 - Val Loss: 0.044848\n",
      "Epoch 10/50 - Train Loss: 0.030186 - Val Loss: 0.056272\n",
      "Epoch 11/50 - Train Loss: 0.028453 - Val Loss: 0.055962\n",
      "Epoch 12/50 - Train Loss: 0.034940 - Val Loss: 0.045822\n",
      "Epoch 13/50 - Train Loss: 0.035229 - Val Loss: 0.045076\n",
      "Epoch 14/50 - Train Loss: 0.031644 - Val Loss: 0.045417\n",
      "Epoch 15/50 - Train Loss: 0.031149 - Val Loss: 0.048074\n",
      "Epoch 16/50 - Train Loss: 0.032029 - Val Loss: 0.044238\n",
      "Epoch 17/50 - Train Loss: 0.030158 - Val Loss: 0.047959\n",
      "Epoch 18/50 - Train Loss: 0.031293 - Val Loss: 0.048209\n",
      "Epoch 19/50 - Train Loss: 0.027944 - Val Loss: 0.044362\n",
      "Epoch 20/50 - Train Loss: 0.031686 - Val Loss: 0.044987\n",
      "Epoch 21/50 - Train Loss: 0.028353 - Val Loss: 0.044506\n",
      "Epoch 22/50 - Train Loss: 0.029471 - Val Loss: 0.044533\n",
      "Epoch 23/50 - Train Loss: 0.028983 - Val Loss: 0.044691\n",
      "Epoch 24/50 - Train Loss: 0.028128 - Val Loss: 0.046069\n",
      "Epoch 25/50 - Train Loss: 0.029357 - Val Loss: 0.048015\n",
      "Epoch 26/50 - Train Loss: 0.031752 - Val Loss: 0.045163\n",
      "Epoch 27/50 - Train Loss: 0.030224 - Val Loss: 0.043902\n",
      "Epoch 28/50 - Train Loss: 0.030261 - Val Loss: 0.044721\n",
      "Epoch 29/50 - Train Loss: 0.031044 - Val Loss: 0.044243\n",
      "Epoch 30/50 - Train Loss: 0.031645 - Val Loss: 0.045993\n",
      "Epoch 31/50 - Train Loss: 0.030643 - Val Loss: 0.044072\n",
      "Epoch 32/50 - Train Loss: 0.030892 - Val Loss: 0.046034\n",
      "Epoch 33/50 - Train Loss: 0.029773 - Val Loss: 0.043532\n",
      "Epoch 34/50 - Train Loss: 0.029084 - Val Loss: 0.047389\n",
      "Epoch 35/50 - Train Loss: 0.033152 - Val Loss: 0.045092\n",
      "Epoch 36/50 - Train Loss: 0.030136 - Val Loss: 0.056784\n",
      "Epoch 37/50 - Train Loss: 0.030234 - Val Loss: 0.044305\n",
      "Epoch 38/50 - Train Loss: 0.029297 - Val Loss: 0.045292\n",
      "Epoch 39/50 - Train Loss: 0.032303 - Val Loss: 0.046008\n",
      "Epoch 40/50 - Train Loss: 0.030816 - Val Loss: 0.044586\n",
      "Epoch 41/50 - Train Loss: 0.031154 - Val Loss: 0.044388\n",
      "Epoch 42/50 - Train Loss: 0.029849 - Val Loss: 0.046330\n",
      "Epoch 43/50 - Train Loss: 0.028482 - Val Loss: 0.044500\n",
      "Epoch 44/50 - Train Loss: 0.030585 - Val Loss: 0.046785\n",
      "Epoch 45/50 - Train Loss: 0.028920 - Val Loss: 0.046945\n",
      "Epoch 46/50 - Train Loss: 0.031131 - Val Loss: 0.044762\n",
      "Epoch 47/50 - Train Loss: 0.029681 - Val Loss: 0.044587\n",
      "Epoch 48/50 - Train Loss: 0.027638 - Val Loss: 0.044338\n",
      "Epoch 49/50 - Train Loss: 0.029049 - Val Loss: 0.044400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:25:15,675] Trial 33 finished with value: 0.04353224113583565 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 136, 'lr': 0.0064124748690973414, 'weight_decay': 1.5420318800752357e-07, 'batch_size': 8}. Best is trial 33 with value: 0.04353224113583565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030581 - Val Loss: 0.049066\n",
      "Epoch 1/50 - Train Loss: 0.080325 - Val Loss: 0.065053\n",
      "Epoch 2/50 - Train Loss: 0.038774 - Val Loss: 0.051258\n",
      "Epoch 3/50 - Train Loss: 0.038147 - Val Loss: 0.054159\n",
      "Epoch 4/50 - Train Loss: 0.033267 - Val Loss: 0.056146\n",
      "Epoch 5/50 - Train Loss: 0.031850 - Val Loss: 0.057031\n",
      "Epoch 6/50 - Train Loss: 0.032846 - Val Loss: 0.052616\n",
      "Epoch 7/50 - Train Loss: 0.029613 - Val Loss: 0.054525\n",
      "Epoch 8/50 - Train Loss: 0.032264 - Val Loss: 0.054092\n",
      "Epoch 9/50 - Train Loss: 0.033107 - Val Loss: 0.046787\n",
      "Epoch 10/50 - Train Loss: 0.030599 - Val Loss: 0.048263\n",
      "Epoch 11/50 - Train Loss: 0.031413 - Val Loss: 0.046338\n",
      "Epoch 12/50 - Train Loss: 0.030725 - Val Loss: 0.046130\n",
      "Epoch 13/50 - Train Loss: 0.030797 - Val Loss: 0.053789\n",
      "Epoch 14/50 - Train Loss: 0.032898 - Val Loss: 0.047583\n",
      "Epoch 15/50 - Train Loss: 0.034397 - Val Loss: 0.047479\n",
      "Epoch 16/50 - Train Loss: 0.033094 - Val Loss: 0.051677\n",
      "Epoch 17/50 - Train Loss: 0.032785 - Val Loss: 0.045584\n",
      "Epoch 18/50 - Train Loss: 0.031379 - Val Loss: 0.048252\n",
      "Epoch 19/50 - Train Loss: 0.032220 - Val Loss: 0.050219\n",
      "Epoch 20/50 - Train Loss: 0.030522 - Val Loss: 0.056875\n",
      "Epoch 21/50 - Train Loss: 0.032047 - Val Loss: 0.045565\n",
      "Epoch 22/50 - Train Loss: 0.035162 - Val Loss: 0.045530\n",
      "Epoch 23/50 - Train Loss: 0.033234 - Val Loss: 0.046236\n",
      "Epoch 24/50 - Train Loss: 0.030613 - Val Loss: 0.046354\n",
      "Epoch 25/50 - Train Loss: 0.032171 - Val Loss: 0.052573\n",
      "Epoch 26/50 - Train Loss: 0.030548 - Val Loss: 0.055394\n",
      "Epoch 27/50 - Train Loss: 0.032452 - Val Loss: 0.046063\n",
      "Epoch 28/50 - Train Loss: 0.030651 - Val Loss: 0.052684\n",
      "Epoch 29/50 - Train Loss: 0.032475 - Val Loss: 0.051077\n",
      "Epoch 30/50 - Train Loss: 0.032540 - Val Loss: 0.050383\n",
      "Epoch 31/50 - Train Loss: 0.033487 - Val Loss: 0.049031\n",
      "Epoch 32/50 - Train Loss: 0.028818 - Val Loss: 0.052482\n",
      "Epoch 33/50 - Train Loss: 0.031717 - Val Loss: 0.051770\n",
      "Epoch 34/50 - Train Loss: 0.032076 - Val Loss: 0.044966\n",
      "Epoch 35/50 - Train Loss: 0.032179 - Val Loss: 0.045207\n",
      "Epoch 36/50 - Train Loss: 0.028267 - Val Loss: 0.044453\n",
      "Epoch 37/50 - Train Loss: 0.031743 - Val Loss: 0.044498\n",
      "Epoch 38/50 - Train Loss: 0.031254 - Val Loss: 0.044938\n",
      "Epoch 39/50 - Train Loss: 0.031875 - Val Loss: 0.047564\n",
      "Epoch 40/50 - Train Loss: 0.030141 - Val Loss: 0.046858\n",
      "Epoch 41/50 - Train Loss: 0.029696 - Val Loss: 0.047006\n",
      "Epoch 42/50 - Train Loss: 0.032170 - Val Loss: 0.045139\n",
      "Epoch 43/50 - Train Loss: 0.030931 - Val Loss: 0.045218\n",
      "Epoch 44/50 - Train Loss: 0.030254 - Val Loss: 0.045207\n",
      "Epoch 45/50 - Train Loss: 0.030726 - Val Loss: 0.044751\n",
      "Epoch 46/50 - Train Loss: 0.030964 - Val Loss: 0.049570\n",
      "Epoch 47/50 - Train Loss: 0.031884 - Val Loss: 0.044921\n",
      "Epoch 48/50 - Train Loss: 0.028878 - Val Loss: 0.044872\n",
      "Epoch 49/50 - Train Loss: 0.029037 - Val Loss: 0.045595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:25:39,909] Trial 34 finished with value: 0.04445314531524976 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 143, 'lr': 0.00700475694111276, 'weight_decay': 1.367150553446488e-07, 'batch_size': 8}. Best is trial 33 with value: 0.04353224113583565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030153 - Val Loss: 0.045230\n",
      "Epoch 1/50 - Train Loss: 0.103776 - Val Loss: 0.063210\n",
      "Epoch 2/50 - Train Loss: 0.048741 - Val Loss: 0.059940\n",
      "Epoch 3/50 - Train Loss: 0.039749 - Val Loss: 0.075891\n",
      "Epoch 4/50 - Train Loss: 0.036813 - Val Loss: 0.055086\n",
      "Epoch 5/50 - Train Loss: 0.032729 - Val Loss: 0.049787\n",
      "Epoch 6/50 - Train Loss: 0.033745 - Val Loss: 0.048341\n",
      "Epoch 7/50 - Train Loss: 0.032800 - Val Loss: 0.047146\n",
      "Epoch 8/50 - Train Loss: 0.030508 - Val Loss: 0.053439\n",
      "Epoch 9/50 - Train Loss: 0.033929 - Val Loss: 0.047938\n",
      "Epoch 10/50 - Train Loss: 0.033028 - Val Loss: 0.045358\n",
      "Epoch 11/50 - Train Loss: 0.034500 - Val Loss: 0.045498\n",
      "Epoch 12/50 - Train Loss: 0.032509 - Val Loss: 0.047823\n",
      "Epoch 13/50 - Train Loss: 0.029998 - Val Loss: 0.044883\n",
      "Epoch 14/50 - Train Loss: 0.029425 - Val Loss: 0.046824\n",
      "Epoch 15/50 - Train Loss: 0.031840 - Val Loss: 0.049166\n",
      "Epoch 16/50 - Train Loss: 0.033824 - Val Loss: 0.049253\n",
      "Epoch 17/50 - Train Loss: 0.031577 - Val Loss: 0.044882\n",
      "Epoch 18/50 - Train Loss: 0.032162 - Val Loss: 0.046900\n",
      "Epoch 19/50 - Train Loss: 0.030524 - Val Loss: 0.045561\n",
      "Epoch 20/50 - Train Loss: 0.033441 - Val Loss: 0.045308\n",
      "Epoch 21/50 - Train Loss: 0.031045 - Val Loss: 0.045052\n",
      "Epoch 22/50 - Train Loss: 0.031139 - Val Loss: 0.046166\n",
      "Epoch 23/50 - Train Loss: 0.031227 - Val Loss: 0.050831\n",
      "Epoch 24/50 - Train Loss: 0.031171 - Val Loss: 0.047351\n",
      "Epoch 25/50 - Train Loss: 0.032839 - Val Loss: 0.045446\n",
      "Epoch 26/50 - Train Loss: 0.031066 - Val Loss: 0.048015\n",
      "Epoch 27/50 - Train Loss: 0.028947 - Val Loss: 0.044874\n",
      "Epoch 28/50 - Train Loss: 0.032550 - Val Loss: 0.045457\n",
      "Epoch 29/50 - Train Loss: 0.032208 - Val Loss: 0.046755\n",
      "Epoch 30/50 - Train Loss: 0.031754 - Val Loss: 0.046445\n",
      "Epoch 31/50 - Train Loss: 0.029044 - Val Loss: 0.045417\n",
      "Epoch 32/50 - Train Loss: 0.026551 - Val Loss: 0.046157\n",
      "Epoch 33/50 - Train Loss: 0.028473 - Val Loss: 0.046148\n",
      "Epoch 34/50 - Train Loss: 0.030605 - Val Loss: 0.046709\n",
      "Epoch 35/50 - Train Loss: 0.029759 - Val Loss: 0.046061\n",
      "Epoch 36/50 - Train Loss: 0.027818 - Val Loss: 0.045219\n",
      "Epoch 37/50 - Train Loss: 0.030647 - Val Loss: 0.047162\n",
      "Epoch 38/50 - Train Loss: 0.031019 - Val Loss: 0.047028\n",
      "Epoch 39/50 - Train Loss: 0.029557 - Val Loss: 0.047218\n",
      "Epoch 40/50 - Train Loss: 0.028689 - Val Loss: 0.045160\n",
      "Epoch 41/50 - Train Loss: 0.031994 - Val Loss: 0.047662\n",
      "Epoch 42/50 - Train Loss: 0.032895 - Val Loss: 0.046551\n",
      "Epoch 43/50 - Train Loss: 0.031794 - Val Loss: 0.050072\n",
      "Epoch 44/50 - Train Loss: 0.030492 - Val Loss: 0.045154\n",
      "Epoch 45/50 - Train Loss: 0.030524 - Val Loss: 0.061627\n",
      "Epoch 46/50 - Train Loss: 0.034002 - Val Loss: 0.045689\n",
      "Epoch 47/50 - Train Loss: 0.031485 - Val Loss: 0.051659\n",
      "Epoch 48/50 - Train Loss: 0.030848 - Val Loss: 0.050604\n",
      "Epoch 49/50 - Train Loss: 0.029965 - Val Loss: 0.051467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:25:58,848] Trial 35 finished with value: 0.044873762875795364 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 115, 'lr': 0.009979122079600194, 'weight_decay': 3.9861880441085906e-07, 'batch_size': 8}. Best is trial 33 with value: 0.04353224113583565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031902 - Val Loss: 0.051323\n",
      "Epoch 1/50 - Train Loss: 0.059031 - Val Loss: 0.070947\n",
      "Epoch 2/50 - Train Loss: 0.036521 - Val Loss: 0.054315\n",
      "Epoch 3/50 - Train Loss: 0.034128 - Val Loss: 0.049930\n",
      "Epoch 4/50 - Train Loss: 0.032619 - Val Loss: 0.046527\n",
      "Epoch 5/50 - Train Loss: 0.030373 - Val Loss: 0.046649\n",
      "Epoch 6/50 - Train Loss: 0.031701 - Val Loss: 0.050935\n",
      "Epoch 7/50 - Train Loss: 0.031669 - Val Loss: 0.045893\n",
      "Epoch 8/50 - Train Loss: 0.033544 - Val Loss: 0.050998\n",
      "Epoch 9/50 - Train Loss: 0.032152 - Val Loss: 0.049163\n",
      "Epoch 10/50 - Train Loss: 0.031978 - Val Loss: 0.045654\n",
      "Epoch 11/50 - Train Loss: 0.030399 - Val Loss: 0.046374\n",
      "Epoch 12/50 - Train Loss: 0.028146 - Val Loss: 0.049260\n",
      "Epoch 13/50 - Train Loss: 0.030904 - Val Loss: 0.047693\n",
      "Epoch 14/50 - Train Loss: 0.032459 - Val Loss: 0.048651\n",
      "Epoch 15/50 - Train Loss: 0.031285 - Val Loss: 0.046392\n",
      "Epoch 16/50 - Train Loss: 0.030093 - Val Loss: 0.044048\n",
      "Epoch 17/50 - Train Loss: 0.030195 - Val Loss: 0.045414\n",
      "Epoch 18/50 - Train Loss: 0.029583 - Val Loss: 0.054656\n",
      "Epoch 19/50 - Train Loss: 0.031168 - Val Loss: 0.044922\n",
      "Epoch 20/50 - Train Loss: 0.028352 - Val Loss: 0.047438\n",
      "Epoch 21/50 - Train Loss: 0.032159 - Val Loss: 0.045400\n",
      "Epoch 22/50 - Train Loss: 0.031528 - Val Loss: 0.057352\n",
      "Epoch 23/50 - Train Loss: 0.032542 - Val Loss: 0.056372\n",
      "Epoch 24/50 - Train Loss: 0.031140 - Val Loss: 0.048923\n",
      "Epoch 25/50 - Train Loss: 0.030537 - Val Loss: 0.045878\n",
      "Epoch 26/50 - Train Loss: 0.029824 - Val Loss: 0.044602\n",
      "Epoch 27/50 - Train Loss: 0.030966 - Val Loss: 0.049336\n",
      "Epoch 28/50 - Train Loss: 0.028514 - Val Loss: 0.045307\n",
      "Epoch 29/50 - Train Loss: 0.032837 - Val Loss: 0.069130\n",
      "Epoch 30/50 - Train Loss: 0.033125 - Val Loss: 0.045163\n",
      "Epoch 31/50 - Train Loss: 0.028077 - Val Loss: 0.045552\n",
      "Epoch 32/50 - Train Loss: 0.030515 - Val Loss: 0.046678\n",
      "Epoch 33/50 - Train Loss: 0.032014 - Val Loss: 0.048356\n",
      "Epoch 34/50 - Train Loss: 0.030056 - Val Loss: 0.047974\n",
      "Epoch 35/50 - Train Loss: 0.028175 - Val Loss: 0.046579\n",
      "Epoch 36/50 - Train Loss: 0.030942 - Val Loss: 0.046089\n",
      "Epoch 37/50 - Train Loss: 0.031649 - Val Loss: 0.044417\n",
      "Epoch 38/50 - Train Loss: 0.030034 - Val Loss: 0.044737\n",
      "Epoch 39/50 - Train Loss: 0.029160 - Val Loss: 0.046217\n",
      "Epoch 40/50 - Train Loss: 0.030430 - Val Loss: 0.047096\n",
      "Epoch 41/50 - Train Loss: 0.030723 - Val Loss: 0.046247\n",
      "Epoch 42/50 - Train Loss: 0.027951 - Val Loss: 0.045520\n",
      "Epoch 43/50 - Train Loss: 0.032979 - Val Loss: 0.048292\n",
      "Epoch 44/50 - Train Loss: 0.030312 - Val Loss: 0.045903\n",
      "Epoch 45/50 - Train Loss: 0.028224 - Val Loss: 0.044431\n",
      "Epoch 46/50 - Train Loss: 0.029794 - Val Loss: 0.050904\n",
      "Epoch 47/50 - Train Loss: 0.031164 - Val Loss: 0.044479\n",
      "Epoch 48/50 - Train Loss: 0.027953 - Val Loss: 0.048059\n",
      "Epoch 49/50 - Train Loss: 0.030437 - Val Loss: 0.046637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:26:31,241] Trial 36 finished with value: 0.04404826958974203 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 128, 'lr': 0.006711580855576349, 'weight_decay': 1.620942804788241e-06, 'batch_size': 8}. Best is trial 33 with value: 0.04353224113583565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031034 - Val Loss: 0.048800\n",
      "Epoch 1/50 - Train Loss: 0.077051 - Val Loss: 0.134103\n",
      "Epoch 2/50 - Train Loss: 0.042556 - Val Loss: 0.053090\n",
      "Epoch 3/50 - Train Loss: 0.035607 - Val Loss: 0.069372\n",
      "Epoch 4/50 - Train Loss: 0.036615 - Val Loss: 0.050601\n",
      "Epoch 5/50 - Train Loss: 0.033984 - Val Loss: 0.050572\n",
      "Epoch 6/50 - Train Loss: 0.030000 - Val Loss: 0.050880\n",
      "Epoch 7/50 - Train Loss: 0.033338 - Val Loss: 0.052667\n",
      "Epoch 8/50 - Train Loss: 0.030721 - Val Loss: 0.048170\n",
      "Epoch 9/50 - Train Loss: 0.030567 - Val Loss: 0.046068\n",
      "Epoch 10/50 - Train Loss: 0.033036 - Val Loss: 0.053038\n",
      "Epoch 11/50 - Train Loss: 0.031061 - Val Loss: 0.052380\n",
      "Epoch 12/50 - Train Loss: 0.031672 - Val Loss: 0.049776\n",
      "Epoch 13/50 - Train Loss: 0.034040 - Val Loss: 0.055087\n",
      "Epoch 14/50 - Train Loss: 0.033044 - Val Loss: 0.049518\n",
      "Epoch 15/50 - Train Loss: 0.032566 - Val Loss: 0.050825\n",
      "Epoch 16/50 - Train Loss: 0.032113 - Val Loss: 0.046677\n",
      "Epoch 17/50 - Train Loss: 0.033719 - Val Loss: 0.045701\n",
      "Epoch 18/50 - Train Loss: 0.033932 - Val Loss: 0.045975\n",
      "Epoch 19/50 - Train Loss: 0.028527 - Val Loss: 0.047123\n",
      "Epoch 20/50 - Train Loss: 0.026882 - Val Loss: 0.045485\n",
      "Epoch 21/50 - Train Loss: 0.031975 - Val Loss: 0.046084\n",
      "Epoch 22/50 - Train Loss: 0.031151 - Val Loss: 0.045653\n",
      "Epoch 23/50 - Train Loss: 0.031180 - Val Loss: 0.047524\n",
      "Epoch 24/50 - Train Loss: 0.031783 - Val Loss: 0.045988\n",
      "Epoch 25/50 - Train Loss: 0.030921 - Val Loss: 0.045638\n",
      "Epoch 26/50 - Train Loss: 0.029881 - Val Loss: 0.047633\n",
      "Epoch 27/50 - Train Loss: 0.031622 - Val Loss: 0.048485\n",
      "Epoch 28/50 - Train Loss: 0.031200 - Val Loss: 0.046329\n",
      "Epoch 29/50 - Train Loss: 0.031787 - Val Loss: 0.046930\n",
      "Epoch 30/50 - Train Loss: 0.031541 - Val Loss: 0.047882\n",
      "Epoch 31/50 - Train Loss: 0.030839 - Val Loss: 0.048698\n",
      "Epoch 32/50 - Train Loss: 0.032173 - Val Loss: 0.045867\n",
      "Epoch 33/50 - Train Loss: 0.031423 - Val Loss: 0.045751\n",
      "Epoch 34/50 - Train Loss: 0.029903 - Val Loss: 0.045978\n",
      "Epoch 35/50 - Train Loss: 0.031179 - Val Loss: 0.050770\n",
      "Epoch 36/50 - Train Loss: 0.030438 - Val Loss: 0.048167\n",
      "Epoch 37/50 - Train Loss: 0.029720 - Val Loss: 0.045838\n",
      "Epoch 38/50 - Train Loss: 0.028129 - Val Loss: 0.048896\n",
      "Epoch 39/50 - Train Loss: 0.030189 - Val Loss: 0.047805\n",
      "Epoch 40/50 - Train Loss: 0.028867 - Val Loss: 0.048404\n",
      "Epoch 41/50 - Train Loss: 0.031436 - Val Loss: 0.046388\n",
      "Epoch 42/50 - Train Loss: 0.030304 - Val Loss: 0.047927\n",
      "Epoch 43/50 - Train Loss: 0.031563 - Val Loss: 0.046698\n",
      "Epoch 44/50 - Train Loss: 0.029500 - Val Loss: 0.046087\n",
      "Epoch 45/50 - Train Loss: 0.029703 - Val Loss: 0.046789\n",
      "Epoch 46/50 - Train Loss: 0.030525 - Val Loss: 0.048464\n",
      "Epoch 47/50 - Train Loss: 0.026225 - Val Loss: 0.048569\n",
      "Epoch 48/50 - Train Loss: 0.029900 - Val Loss: 0.047021\n",
      "Epoch 49/50 - Train Loss: 0.030038 - Val Loss: 0.046758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:26:55,888] Trial 37 finished with value: 0.04548546796043714 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 182, 'lr': 0.0020383235027970157, 'weight_decay': 1.0488444792094672e-07, 'batch_size': 8}. Best is trial 33 with value: 0.04353224113583565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029821 - Val Loss: 0.047313\n",
      "Epoch 1/50 - Train Loss: 0.071806 - Val Loss: 0.060590\n",
      "Epoch 2/50 - Train Loss: 0.040588 - Val Loss: 0.049118\n",
      "Epoch 3/50 - Train Loss: 0.036234 - Val Loss: 0.051368\n",
      "Epoch 4/50 - Train Loss: 0.033872 - Val Loss: 0.051288\n",
      "Epoch 5/50 - Train Loss: 0.032564 - Val Loss: 0.047618\n",
      "Epoch 6/50 - Train Loss: 0.036630 - Val Loss: 0.053275\n",
      "Epoch 7/50 - Train Loss: 0.033982 - Val Loss: 0.046353\n",
      "Epoch 8/50 - Train Loss: 0.032724 - Val Loss: 0.048394\n",
      "Epoch 9/50 - Train Loss: 0.032842 - Val Loss: 0.045981\n",
      "Epoch 10/50 - Train Loss: 0.032455 - Val Loss: 0.044854\n",
      "Epoch 11/50 - Train Loss: 0.028122 - Val Loss: 0.044993\n",
      "Epoch 12/50 - Train Loss: 0.030744 - Val Loss: 0.045916\n",
      "Epoch 13/50 - Train Loss: 0.032117 - Val Loss: 0.045872\n",
      "Epoch 14/50 - Train Loss: 0.031455 - Val Loss: 0.044637\n",
      "Epoch 15/50 - Train Loss: 0.032288 - Val Loss: 0.044743\n",
      "Epoch 16/50 - Train Loss: 0.029415 - Val Loss: 0.045323\n",
      "Epoch 17/50 - Train Loss: 0.031412 - Val Loss: 0.047122\n",
      "Epoch 18/50 - Train Loss: 0.031960 - Val Loss: 0.044283\n",
      "Epoch 19/50 - Train Loss: 0.031949 - Val Loss: 0.049091\n",
      "Epoch 20/50 - Train Loss: 0.029461 - Val Loss: 0.045441\n",
      "Epoch 21/50 - Train Loss: 0.028828 - Val Loss: 0.044611\n",
      "Epoch 22/50 - Train Loss: 0.030988 - Val Loss: 0.050490\n",
      "Epoch 23/50 - Train Loss: 0.031017 - Val Loss: 0.050534\n",
      "Epoch 24/50 - Train Loss: 0.032168 - Val Loss: 0.049031\n",
      "Epoch 25/50 - Train Loss: 0.029798 - Val Loss: 0.044644\n",
      "Epoch 26/50 - Train Loss: 0.028710 - Val Loss: 0.044508\n",
      "Epoch 27/50 - Train Loss: 0.030469 - Val Loss: 0.045196\n",
      "Epoch 28/50 - Train Loss: 0.029756 - Val Loss: 0.044757\n",
      "Epoch 29/50 - Train Loss: 0.033049 - Val Loss: 0.045167\n",
      "Epoch 30/50 - Train Loss: 0.030661 - Val Loss: 0.044956\n",
      "Epoch 31/50 - Train Loss: 0.029830 - Val Loss: 0.052671\n",
      "Epoch 32/50 - Train Loss: 0.030824 - Val Loss: 0.045151\n",
      "Epoch 33/50 - Train Loss: 0.031083 - Val Loss: 0.045602\n",
      "Epoch 34/50 - Train Loss: 0.029605 - Val Loss: 0.046052\n",
      "Epoch 35/50 - Train Loss: 0.030988 - Val Loss: 0.044626\n",
      "Epoch 36/50 - Train Loss: 0.029873 - Val Loss: 0.046124\n",
      "Epoch 37/50 - Train Loss: 0.028353 - Val Loss: 0.046965\n",
      "Epoch 38/50 - Train Loss: 0.030153 - Val Loss: 0.045301\n",
      "Epoch 39/50 - Train Loss: 0.031738 - Val Loss: 0.046051\n",
      "Epoch 40/50 - Train Loss: 0.030707 - Val Loss: 0.046641\n",
      "Epoch 41/50 - Train Loss: 0.030865 - Val Loss: 0.046219\n",
      "Epoch 42/50 - Train Loss: 0.029415 - Val Loss: 0.045415\n",
      "Epoch 43/50 - Train Loss: 0.030934 - Val Loss: 0.046036\n",
      "Epoch 44/50 - Train Loss: 0.031425 - Val Loss: 0.045103\n",
      "Epoch 45/50 - Train Loss: 0.028945 - Val Loss: 0.047138\n",
      "Epoch 46/50 - Train Loss: 0.029263 - Val Loss: 0.048852\n",
      "Epoch 47/50 - Train Loss: 0.031306 - Val Loss: 0.050249\n",
      "Epoch 48/50 - Train Loss: 0.029602 - Val Loss: 0.045338\n",
      "Epoch 49/50 - Train Loss: 0.030637 - Val Loss: 0.046602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:27:43,096] Trial 38 finished with value: 0.044283051043748856 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 157, 'lr': 0.005306086982550095, 'weight_decay': 2.1659731859496984e-07, 'batch_size': 8}. Best is trial 33 with value: 0.04353224113583565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027653 - Val Loss: 0.047967\n",
      "Epoch 1/50 - Train Loss: 0.099472 - Val Loss: 0.163788\n",
      "Epoch 2/50 - Train Loss: 0.050299 - Val Loss: 0.099694\n",
      "Epoch 3/50 - Train Loss: 0.044363 - Val Loss: 0.081303\n",
      "Epoch 4/50 - Train Loss: 0.041151 - Val Loss: 0.086365\n",
      "Epoch 5/50 - Train Loss: 0.039053 - Val Loss: 0.083723\n",
      "Epoch 6/50 - Train Loss: 0.038209 - Val Loss: 0.077661\n",
      "Epoch 7/50 - Train Loss: 0.036215 - Val Loss: 0.073075\n",
      "Epoch 8/50 - Train Loss: 0.035204 - Val Loss: 0.068248\n",
      "Epoch 9/50 - Train Loss: 0.034908 - Val Loss: 0.066004\n",
      "Epoch 10/50 - Train Loss: 0.035058 - Val Loss: 0.061284\n",
      "Epoch 11/50 - Train Loss: 0.032949 - Val Loss: 0.061981\n",
      "Epoch 12/50 - Train Loss: 0.033902 - Val Loss: 0.058690\n",
      "Epoch 13/50 - Train Loss: 0.033886 - Val Loss: 0.059128\n",
      "Epoch 14/50 - Train Loss: 0.034718 - Val Loss: 0.055316\n",
      "Epoch 15/50 - Train Loss: 0.034880 - Val Loss: 0.056723\n",
      "Epoch 16/50 - Train Loss: 0.032382 - Val Loss: 0.054110\n",
      "Epoch 17/50 - Train Loss: 0.031774 - Val Loss: 0.054811\n",
      "Epoch 18/50 - Train Loss: 0.032679 - Val Loss: 0.052876\n",
      "Epoch 19/50 - Train Loss: 0.033553 - Val Loss: 0.053065\n",
      "Epoch 20/50 - Train Loss: 0.034274 - Val Loss: 0.052824\n",
      "Epoch 21/50 - Train Loss: 0.031169 - Val Loss: 0.051956\n",
      "Epoch 22/50 - Train Loss: 0.031492 - Val Loss: 0.052355\n",
      "Epoch 23/50 - Train Loss: 0.031397 - Val Loss: 0.051273\n",
      "Epoch 24/50 - Train Loss: 0.030481 - Val Loss: 0.050838\n",
      "Epoch 25/50 - Train Loss: 0.031390 - Val Loss: 0.050648\n",
      "Epoch 26/50 - Train Loss: 0.030234 - Val Loss: 0.049723\n",
      "Epoch 27/50 - Train Loss: 0.029448 - Val Loss: 0.049289\n",
      "Epoch 28/50 - Train Loss: 0.032246 - Val Loss: 0.050366\n",
      "Epoch 29/50 - Train Loss: 0.030626 - Val Loss: 0.049321\n",
      "Epoch 30/50 - Train Loss: 0.030584 - Val Loss: 0.050093\n",
      "Epoch 31/50 - Train Loss: 0.031561 - Val Loss: 0.049499\n",
      "Epoch 32/50 - Train Loss: 0.030547 - Val Loss: 0.050081\n",
      "Epoch 33/50 - Train Loss: 0.031665 - Val Loss: 0.049219\n",
      "Epoch 34/50 - Train Loss: 0.030837 - Val Loss: 0.049248\n",
      "Epoch 35/50 - Train Loss: 0.030787 - Val Loss: 0.049423\n",
      "Epoch 36/50 - Train Loss: 0.030671 - Val Loss: 0.049012\n",
      "Epoch 37/50 - Train Loss: 0.030689 - Val Loss: 0.048872\n",
      "Epoch 38/50 - Train Loss: 0.032416 - Val Loss: 0.049357\n",
      "Epoch 39/50 - Train Loss: 0.031112 - Val Loss: 0.048777\n",
      "Epoch 40/50 - Train Loss: 0.032194 - Val Loss: 0.049238\n",
      "Epoch 41/50 - Train Loss: 0.030753 - Val Loss: 0.049212\n",
      "Epoch 42/50 - Train Loss: 0.030163 - Val Loss: 0.048306\n",
      "Epoch 43/50 - Train Loss: 0.030598 - Val Loss: 0.048189\n",
      "Epoch 44/50 - Train Loss: 0.029649 - Val Loss: 0.048825\n",
      "Epoch 45/50 - Train Loss: 0.031018 - Val Loss: 0.049367\n",
      "Epoch 46/50 - Train Loss: 0.031064 - Val Loss: 0.047802\n",
      "Epoch 47/50 - Train Loss: 0.028659 - Val Loss: 0.048898\n",
      "Epoch 48/50 - Train Loss: 0.027730 - Val Loss: 0.047788\n",
      "Epoch 49/50 - Train Loss: 0.032378 - Val Loss: 0.047784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:28:08,357] Trial 39 finished with value: 0.04778374979893366 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 110, 'lr': 0.00024481741194550693, 'weight_decay': 4.016221298455575e-08, 'batch_size': 8}. Best is trial 33 with value: 0.04353224113583565.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030033 - Val Loss: 0.048050\n",
      "Epoch 1/50 - Train Loss: 0.073256 - Val Loss: 0.057887\n",
      "Epoch 2/50 - Train Loss: 0.038759 - Val Loss: 0.050663\n",
      "Epoch 3/50 - Train Loss: 0.036867 - Val Loss: 0.047947\n",
      "Epoch 4/50 - Train Loss: 0.035405 - Val Loss: 0.046191\n",
      "Epoch 5/50 - Train Loss: 0.030490 - Val Loss: 0.045645\n",
      "Epoch 6/50 - Train Loss: 0.031511 - Val Loss: 0.047408\n",
      "Epoch 7/50 - Train Loss: 0.034181 - Val Loss: 0.049548\n",
      "Epoch 8/50 - Train Loss: 0.027907 - Val Loss: 0.049967\n",
      "Epoch 9/50 - Train Loss: 0.033659 - Val Loss: 0.044627\n",
      "Epoch 10/50 - Train Loss: 0.032199 - Val Loss: 0.044788\n",
      "Epoch 11/50 - Train Loss: 0.031105 - Val Loss: 0.046013\n",
      "Epoch 12/50 - Train Loss: 0.031748 - Val Loss: 0.045617\n",
      "Epoch 13/50 - Train Loss: 0.031829 - Val Loss: 0.048003\n",
      "Epoch 14/50 - Train Loss: 0.033061 - Val Loss: 0.046845\n",
      "Epoch 15/50 - Train Loss: 0.035657 - Val Loss: 0.044297\n",
      "Epoch 16/50 - Train Loss: 0.029700 - Val Loss: 0.046060\n",
      "Epoch 17/50 - Train Loss: 0.033485 - Val Loss: 0.051922\n",
      "Epoch 18/50 - Train Loss: 0.031455 - Val Loss: 0.045767\n",
      "Epoch 19/50 - Train Loss: 0.030712 - Val Loss: 0.043814\n",
      "Epoch 20/50 - Train Loss: 0.030912 - Val Loss: 0.044212\n",
      "Epoch 21/50 - Train Loss: 0.030051 - Val Loss: 0.044301\n",
      "Epoch 22/50 - Train Loss: 0.031728 - Val Loss: 0.045282\n",
      "Epoch 23/50 - Train Loss: 0.030902 - Val Loss: 0.044402\n",
      "Epoch 24/50 - Train Loss: 0.030264 - Val Loss: 0.043557\n",
      "Epoch 25/50 - Train Loss: 0.032313 - Val Loss: 0.056687\n",
      "Epoch 26/50 - Train Loss: 0.033126 - Val Loss: 0.044874\n",
      "Epoch 27/50 - Train Loss: 0.031363 - Val Loss: 0.044002\n",
      "Epoch 28/50 - Train Loss: 0.032502 - Val Loss: 0.044084\n",
      "Epoch 29/50 - Train Loss: 0.030939 - Val Loss: 0.045051\n",
      "Epoch 30/50 - Train Loss: 0.029846 - Val Loss: 0.054744\n",
      "Epoch 31/50 - Train Loss: 0.029655 - Val Loss: 0.049762\n",
      "Epoch 32/50 - Train Loss: 0.030890 - Val Loss: 0.043560\n",
      "Epoch 33/50 - Train Loss: 0.029547 - Val Loss: 0.045183\n",
      "Epoch 34/50 - Train Loss: 0.033412 - Val Loss: 0.044407\n",
      "Epoch 35/50 - Train Loss: 0.026947 - Val Loss: 0.043484\n",
      "Epoch 36/50 - Train Loss: 0.031384 - Val Loss: 0.043426\n",
      "Epoch 37/50 - Train Loss: 0.029390 - Val Loss: 0.043101\n",
      "Epoch 38/50 - Train Loss: 0.030815 - Val Loss: 0.047374\n",
      "Epoch 39/50 - Train Loss: 0.029032 - Val Loss: 0.043638\n",
      "Epoch 40/50 - Train Loss: 0.028483 - Val Loss: 0.048284\n",
      "Epoch 41/50 - Train Loss: 0.031761 - Val Loss: 0.043502\n",
      "Epoch 42/50 - Train Loss: 0.030852 - Val Loss: 0.043704\n",
      "Epoch 43/50 - Train Loss: 0.027762 - Val Loss: 0.043496\n",
      "Epoch 44/50 - Train Loss: 0.032765 - Val Loss: 0.045388\n",
      "Epoch 45/50 - Train Loss: 0.029655 - Val Loss: 0.046272\n",
      "Epoch 46/50 - Train Loss: 0.029610 - Val Loss: 0.044704\n",
      "Epoch 47/50 - Train Loss: 0.030458 - Val Loss: 0.048874\n",
      "Epoch 48/50 - Train Loss: 0.028961 - Val Loss: 0.044799\n",
      "Epoch 49/50 - Train Loss: 0.027748 - Val Loss: 0.047345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:28:46,730] Trial 40 finished with value: 0.043100775529940925 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 139, 'lr': 0.00434625111972206, 'weight_decay': 6.21222641225517e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030060 - Val Loss: 0.044804\n",
      "Epoch 1/50 - Train Loss: 0.058164 - Val Loss: 0.055019\n",
      "Epoch 2/50 - Train Loss: 0.033857 - Val Loss: 0.047886\n",
      "Epoch 3/50 - Train Loss: 0.032783 - Val Loss: 0.051566\n",
      "Epoch 4/50 - Train Loss: 0.029251 - Val Loss: 0.049218\n",
      "Epoch 5/50 - Train Loss: 0.031718 - Val Loss: 0.046752\n",
      "Epoch 6/50 - Train Loss: 0.035334 - Val Loss: 0.055753\n",
      "Epoch 7/50 - Train Loss: 0.033146 - Val Loss: 0.059658\n",
      "Epoch 8/50 - Train Loss: 0.030093 - Val Loss: 0.056154\n",
      "Epoch 9/50 - Train Loss: 0.032583 - Val Loss: 0.048068\n",
      "Epoch 10/50 - Train Loss: 0.032284 - Val Loss: 0.045046\n",
      "Epoch 11/50 - Train Loss: 0.034076 - Val Loss: 0.047109\n",
      "Epoch 12/50 - Train Loss: 0.031888 - Val Loss: 0.045827\n",
      "Epoch 13/50 - Train Loss: 0.030890 - Val Loss: 0.046617\n",
      "Epoch 14/50 - Train Loss: 0.031190 - Val Loss: 0.044931\n",
      "Epoch 15/50 - Train Loss: 0.032744 - Val Loss: 0.049460\n",
      "Epoch 16/50 - Train Loss: 0.030620 - Val Loss: 0.048227\n",
      "Epoch 17/50 - Train Loss: 0.031943 - Val Loss: 0.044508\n",
      "Epoch 18/50 - Train Loss: 0.030274 - Val Loss: 0.057373\n",
      "Epoch 19/50 - Train Loss: 0.030536 - Val Loss: 0.045402\n",
      "Epoch 20/50 - Train Loss: 0.031618 - Val Loss: 0.045445\n",
      "Epoch 21/50 - Train Loss: 0.030584 - Val Loss: 0.049678\n",
      "Epoch 22/50 - Train Loss: 0.033041 - Val Loss: 0.044824\n",
      "Epoch 23/50 - Train Loss: 0.032886 - Val Loss: 0.045334\n",
      "Epoch 24/50 - Train Loss: 0.031636 - Val Loss: 0.047255\n",
      "Epoch 25/50 - Train Loss: 0.029668 - Val Loss: 0.048103\n",
      "Epoch 26/50 - Train Loss: 0.033818 - Val Loss: 0.047082\n",
      "Epoch 27/50 - Train Loss: 0.029310 - Val Loss: 0.050069\n",
      "Epoch 28/50 - Train Loss: 0.032211 - Val Loss: 0.045799\n",
      "Epoch 29/50 - Train Loss: 0.027932 - Val Loss: 0.046627\n",
      "Epoch 30/50 - Train Loss: 0.025999 - Val Loss: 0.045262\n",
      "Epoch 31/50 - Train Loss: 0.030362 - Val Loss: 0.050273\n",
      "Epoch 32/50 - Train Loss: 0.027298 - Val Loss: 0.044668\n",
      "Epoch 33/50 - Train Loss: 0.026491 - Val Loss: 0.044583\n",
      "Epoch 34/50 - Train Loss: 0.029223 - Val Loss: 0.044763\n",
      "Epoch 35/50 - Train Loss: 0.030926 - Val Loss: 0.045576\n",
      "Epoch 36/50 - Train Loss: 0.030740 - Val Loss: 0.047811\n",
      "Epoch 37/50 - Train Loss: 0.029377 - Val Loss: 0.047296\n",
      "Epoch 38/50 - Train Loss: 0.030050 - Val Loss: 0.047453\n",
      "Epoch 39/50 - Train Loss: 0.028736 - Val Loss: 0.046598\n",
      "Epoch 40/50 - Train Loss: 0.028515 - Val Loss: 0.048002\n",
      "Epoch 41/50 - Train Loss: 0.033245 - Val Loss: 0.045250\n",
      "Epoch 42/50 - Train Loss: 0.030680 - Val Loss: 0.045022\n",
      "Epoch 43/50 - Train Loss: 0.028501 - Val Loss: 0.045944\n",
      "Epoch 44/50 - Train Loss: 0.031139 - Val Loss: 0.045388\n",
      "Epoch 45/50 - Train Loss: 0.028179 - Val Loss: 0.047628\n",
      "Epoch 46/50 - Train Loss: 0.027812 - Val Loss: 0.044656\n",
      "Epoch 47/50 - Train Loss: 0.032056 - Val Loss: 0.055390\n",
      "Epoch 48/50 - Train Loss: 0.029434 - Val Loss: 0.044082\n",
      "Epoch 49/50 - Train Loss: 0.029419 - Val Loss: 0.044825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:29:17,890] Trial 41 finished with value: 0.04408192137877146 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 138, 'lr': 0.004335650886789298, 'weight_decay': 5.230743003789298e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031354 - Val Loss: 0.045272\n",
      "Epoch 1/50 - Train Loss: 0.130264 - Val Loss: 0.060214\n",
      "Epoch 2/50 - Train Loss: 0.040340 - Val Loss: 0.066914\n",
      "Epoch 3/50 - Train Loss: 0.035109 - Val Loss: 0.048580\n",
      "Epoch 4/50 - Train Loss: 0.035841 - Val Loss: 0.048924\n",
      "Epoch 5/50 - Train Loss: 0.034307 - Val Loss: 0.046770\n",
      "Epoch 6/50 - Train Loss: 0.032075 - Val Loss: 0.047829\n",
      "Epoch 7/50 - Train Loss: 0.034463 - Val Loss: 0.047764\n",
      "Epoch 8/50 - Train Loss: 0.031429 - Val Loss: 0.052643\n",
      "Epoch 9/50 - Train Loss: 0.032670 - Val Loss: 0.046236\n",
      "Epoch 10/50 - Train Loss: 0.031336 - Val Loss: 0.051683\n",
      "Epoch 11/50 - Train Loss: 0.031970 - Val Loss: 0.046879\n",
      "Epoch 12/50 - Train Loss: 0.032649 - Val Loss: 0.049454\n",
      "Epoch 13/50 - Train Loss: 0.029711 - Val Loss: 0.051104\n",
      "Epoch 14/50 - Train Loss: 0.032946 - Val Loss: 0.046735\n",
      "Epoch 15/50 - Train Loss: 0.030416 - Val Loss: 0.049757\n",
      "Epoch 16/50 - Train Loss: 0.031470 - Val Loss: 0.045633\n",
      "Epoch 17/50 - Train Loss: 0.031198 - Val Loss: 0.045386\n",
      "Epoch 18/50 - Train Loss: 0.028884 - Val Loss: 0.045788\n",
      "Epoch 19/50 - Train Loss: 0.031955 - Val Loss: 0.045986\n",
      "Epoch 20/50 - Train Loss: 0.035626 - Val Loss: 0.047584\n",
      "Epoch 21/50 - Train Loss: 0.034337 - Val Loss: 0.049325\n",
      "Epoch 22/50 - Train Loss: 0.031322 - Val Loss: 0.044978\n",
      "Epoch 23/50 - Train Loss: 0.031232 - Val Loss: 0.049649\n",
      "Epoch 24/50 - Train Loss: 0.033597 - Val Loss: 0.046392\n",
      "Epoch 25/50 - Train Loss: 0.031425 - Val Loss: 0.045474\n",
      "Epoch 26/50 - Train Loss: 0.027869 - Val Loss: 0.046693\n",
      "Epoch 27/50 - Train Loss: 0.030670 - Val Loss: 0.047613\n",
      "Epoch 28/50 - Train Loss: 0.030850 - Val Loss: 0.056778\n",
      "Epoch 29/50 - Train Loss: 0.035364 - Val Loss: 0.045382\n",
      "Epoch 30/50 - Train Loss: 0.031601 - Val Loss: 0.046070\n",
      "Epoch 31/50 - Train Loss: 0.029047 - Val Loss: 0.053210\n",
      "Epoch 32/50 - Train Loss: 0.030991 - Val Loss: 0.044798\n",
      "Epoch 33/50 - Train Loss: 0.032955 - Val Loss: 0.045222\n",
      "Epoch 34/50 - Train Loss: 0.029452 - Val Loss: 0.047869\n",
      "Epoch 35/50 - Train Loss: 0.031712 - Val Loss: 0.051017\n",
      "Epoch 36/50 - Train Loss: 0.029487 - Val Loss: 0.044292\n",
      "Epoch 37/50 - Train Loss: 0.027596 - Val Loss: 0.045002\n",
      "Epoch 38/50 - Train Loss: 0.031832 - Val Loss: 0.047681\n",
      "Epoch 39/50 - Train Loss: 0.029892 - Val Loss: 0.054187\n",
      "Epoch 40/50 - Train Loss: 0.030280 - Val Loss: 0.048725\n",
      "Epoch 41/50 - Train Loss: 0.029845 - Val Loss: 0.045940\n",
      "Epoch 42/50 - Train Loss: 0.031484 - Val Loss: 0.049337\n",
      "Epoch 43/50 - Train Loss: 0.033438 - Val Loss: 0.051982\n",
      "Epoch 44/50 - Train Loss: 0.031645 - Val Loss: 0.044738\n",
      "Epoch 45/50 - Train Loss: 0.031972 - Val Loss: 0.052218\n",
      "Epoch 46/50 - Train Loss: 0.030856 - Val Loss: 0.053246\n",
      "Epoch 47/50 - Train Loss: 0.030085 - Val Loss: 0.047724\n",
      "Epoch 48/50 - Train Loss: 0.031103 - Val Loss: 0.051774\n",
      "Epoch 49/50 - Train Loss: 0.030978 - Val Loss: 0.046248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:29:46,864] Trial 42 finished with value: 0.04429168005784353 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 169, 'lr': 0.006667888443475602, 'weight_decay': 2.4030019291155775e-06, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028941 - Val Loss: 0.045896\n",
      "Epoch 1/50 - Train Loss: 0.064202 - Val Loss: 0.055072\n",
      "Epoch 2/50 - Train Loss: 0.035945 - Val Loss: 0.050515\n",
      "Epoch 3/50 - Train Loss: 0.033100 - Val Loss: 0.048351\n",
      "Epoch 4/50 - Train Loss: 0.031656 - Val Loss: 0.052420\n",
      "Epoch 5/50 - Train Loss: 0.032876 - Val Loss: 0.045126\n",
      "Epoch 6/50 - Train Loss: 0.031495 - Val Loss: 0.051197\n",
      "Epoch 7/50 - Train Loss: 0.035360 - Val Loss: 0.045461\n",
      "Epoch 8/50 - Train Loss: 0.028929 - Val Loss: 0.048328\n",
      "Epoch 9/50 - Train Loss: 0.030036 - Val Loss: 0.048111\n",
      "Epoch 10/50 - Train Loss: 0.033151 - Val Loss: 0.044620\n",
      "Epoch 11/50 - Train Loss: 0.031116 - Val Loss: 0.051112\n",
      "Epoch 12/50 - Train Loss: 0.032201 - Val Loss: 0.051075\n",
      "Epoch 13/50 - Train Loss: 0.030955 - Val Loss: 0.050946\n",
      "Epoch 14/50 - Train Loss: 0.040248 - Val Loss: 0.091155\n",
      "Epoch 15/50 - Train Loss: 0.035856 - Val Loss: 0.067307\n",
      "Epoch 16/50 - Train Loss: 0.031855 - Val Loss: 0.053135\n",
      "Epoch 17/50 - Train Loss: 0.032290 - Val Loss: 0.045990\n",
      "Epoch 18/50 - Train Loss: 0.033404 - Val Loss: 0.054275\n",
      "Epoch 19/50 - Train Loss: 0.034122 - Val Loss: 0.046845\n",
      "Epoch 20/50 - Train Loss: 0.032043 - Val Loss: 0.045091\n",
      "Epoch 21/50 - Train Loss: 0.031329 - Val Loss: 0.053061\n",
      "Epoch 22/50 - Train Loss: 0.030413 - Val Loss: 0.051059\n",
      "Epoch 23/50 - Train Loss: 0.030606 - Val Loss: 0.045660\n",
      "Epoch 24/50 - Train Loss: 0.032170 - Val Loss: 0.056520\n",
      "Epoch 25/50 - Train Loss: 0.032663 - Val Loss: 0.045629\n",
      "Epoch 26/50 - Train Loss: 0.029603 - Val Loss: 0.044382\n",
      "Epoch 27/50 - Train Loss: 0.029426 - Val Loss: 0.044719\n",
      "Epoch 28/50 - Train Loss: 0.029466 - Val Loss: 0.044864\n",
      "Epoch 29/50 - Train Loss: 0.033239 - Val Loss: 0.049701\n",
      "Epoch 30/50 - Train Loss: 0.030422 - Val Loss: 0.043947\n",
      "Epoch 31/50 - Train Loss: 0.029327 - Val Loss: 0.049823\n",
      "Epoch 32/50 - Train Loss: 0.029311 - Val Loss: 0.043619\n",
      "Epoch 33/50 - Train Loss: 0.030588 - Val Loss: 0.046963\n",
      "Epoch 34/50 - Train Loss: 0.028481 - Val Loss: 0.051009\n",
      "Epoch 35/50 - Train Loss: 0.030187 - Val Loss: 0.044125\n",
      "Epoch 36/50 - Train Loss: 0.033350 - Val Loss: 0.043863\n",
      "Epoch 37/50 - Train Loss: 0.029004 - Val Loss: 0.046559\n",
      "Epoch 38/50 - Train Loss: 0.032298 - Val Loss: 0.049410\n",
      "Epoch 39/50 - Train Loss: 0.032927 - Val Loss: 0.044110\n",
      "Epoch 40/50 - Train Loss: 0.031558 - Val Loss: 0.044908\n",
      "Epoch 41/50 - Train Loss: 0.031308 - Val Loss: 0.048118\n",
      "Epoch 42/50 - Train Loss: 0.031162 - Val Loss: 0.044166\n",
      "Epoch 43/50 - Train Loss: 0.029109 - Val Loss: 0.047383\n",
      "Epoch 44/50 - Train Loss: 0.033281 - Val Loss: 0.068780\n",
      "Epoch 45/50 - Train Loss: 0.033621 - Val Loss: 0.056812\n",
      "Epoch 46/50 - Train Loss: 0.033335 - Val Loss: 0.052171\n",
      "Epoch 47/50 - Train Loss: 0.030716 - Val Loss: 0.047115\n",
      "Epoch 48/50 - Train Loss: 0.031259 - Val Loss: 0.044624\n",
      "Epoch 49/50 - Train Loss: 0.030540 - Val Loss: 0.046592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:30:25,017] Trial 43 finished with value: 0.04361868773897489 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 147, 'lr': 0.007602243609736394, 'weight_decay': 7.393727066502742e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030289 - Val Loss: 0.045477\n",
      "Epoch 1/50 - Train Loss: 0.176405 - Val Loss: 0.070318\n",
      "Epoch 2/50 - Train Loss: 0.048328 - Val Loss: 0.074232\n",
      "Epoch 3/50 - Train Loss: 0.037307 - Val Loss: 0.051993\n",
      "Epoch 4/50 - Train Loss: 0.038275 - Val Loss: 0.050550\n",
      "Epoch 5/50 - Train Loss: 0.032453 - Val Loss: 0.060007\n",
      "Epoch 6/50 - Train Loss: 0.033683 - Val Loss: 0.048323\n",
      "Epoch 7/50 - Train Loss: 0.034565 - Val Loss: 0.046379\n",
      "Epoch 8/50 - Train Loss: 0.035185 - Val Loss: 0.048926\n",
      "Epoch 9/50 - Train Loss: 0.033001 - Val Loss: 0.048234\n",
      "Epoch 10/50 - Train Loss: 0.033673 - Val Loss: 0.046582\n",
      "Epoch 11/50 - Train Loss: 0.034565 - Val Loss: 0.044740\n",
      "Epoch 12/50 - Train Loss: 0.032702 - Val Loss: 0.044358\n",
      "Epoch 13/50 - Train Loss: 0.034342 - Val Loss: 0.046258\n",
      "Epoch 14/50 - Train Loss: 0.032504 - Val Loss: 0.050359\n",
      "Epoch 15/50 - Train Loss: 0.032364 - Val Loss: 0.048598\n",
      "Epoch 16/50 - Train Loss: 0.031725 - Val Loss: 0.044854\n",
      "Epoch 17/50 - Train Loss: 0.034109 - Val Loss: 0.051497\n",
      "Epoch 18/50 - Train Loss: 0.034364 - Val Loss: 0.050649\n",
      "Epoch 19/50 - Train Loss: 0.035279 - Val Loss: 0.044317\n",
      "Epoch 20/50 - Train Loss: 0.032268 - Val Loss: 0.044794\n",
      "Epoch 21/50 - Train Loss: 0.030906 - Val Loss: 0.048150\n",
      "Epoch 22/50 - Train Loss: 0.030700 - Val Loss: 0.044107\n",
      "Epoch 23/50 - Train Loss: 0.031994 - Val Loss: 0.044170\n",
      "Epoch 24/50 - Train Loss: 0.031899 - Val Loss: 0.045709\n",
      "Epoch 25/50 - Train Loss: 0.033415 - Val Loss: 0.058068\n",
      "Epoch 26/50 - Train Loss: 0.031124 - Val Loss: 0.048777\n",
      "Epoch 27/50 - Train Loss: 0.032038 - Val Loss: 0.051675\n",
      "Epoch 28/50 - Train Loss: 0.033405 - Val Loss: 0.050384\n",
      "Epoch 29/50 - Train Loss: 0.031615 - Val Loss: 0.044687\n",
      "Epoch 30/50 - Train Loss: 0.029027 - Val Loss: 0.044320\n",
      "Epoch 31/50 - Train Loss: 0.029558 - Val Loss: 0.045527\n",
      "Epoch 32/50 - Train Loss: 0.034465 - Val Loss: 0.044344\n",
      "Epoch 33/50 - Train Loss: 0.029817 - Val Loss: 0.052510\n",
      "Epoch 34/50 - Train Loss: 0.031127 - Val Loss: 0.044937\n",
      "Epoch 35/50 - Train Loss: 0.031105 - Val Loss: 0.044752\n",
      "Epoch 36/50 - Train Loss: 0.032710 - Val Loss: 0.044934\n",
      "Epoch 37/50 - Train Loss: 0.031455 - Val Loss: 0.044229\n",
      "Epoch 38/50 - Train Loss: 0.030840 - Val Loss: 0.046092\n",
      "Epoch 39/50 - Train Loss: 0.030428 - Val Loss: 0.054676\n",
      "Epoch 40/50 - Train Loss: 0.030466 - Val Loss: 0.049003\n",
      "Epoch 41/50 - Train Loss: 0.033160 - Val Loss: 0.045965\n",
      "Epoch 42/50 - Train Loss: 0.032232 - Val Loss: 0.044172\n",
      "Epoch 43/50 - Train Loss: 0.028823 - Val Loss: 0.044149\n",
      "Epoch 44/50 - Train Loss: 0.033963 - Val Loss: 0.047336\n",
      "Epoch 45/50 - Train Loss: 0.034556 - Val Loss: 0.045692\n",
      "Epoch 46/50 - Train Loss: 0.032528 - Val Loss: 0.044661\n",
      "Epoch 47/50 - Train Loss: 0.027801 - Val Loss: 0.044957\n",
      "Epoch 48/50 - Train Loss: 0.031033 - Val Loss: 0.044799\n",
      "Epoch 49/50 - Train Loss: 0.029941 - Val Loss: 0.044897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:31:02,184] Trial 44 finished with value: 0.044107203682263695 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 152, 'lr': 0.007920456264645102, 'weight_decay': 8.438603554672822e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032150 - Val Loss: 0.050641\n",
      "Epoch 1/50 - Train Loss: 0.075337 - Val Loss: 0.120830\n",
      "Epoch 2/50 - Train Loss: 0.042509 - Val Loss: 0.063947\n",
      "Epoch 3/50 - Train Loss: 0.035114 - Val Loss: 0.049489\n",
      "Epoch 4/50 - Train Loss: 0.034282 - Val Loss: 0.048935\n",
      "Epoch 5/50 - Train Loss: 0.032873 - Val Loss: 0.046510\n",
      "Epoch 6/50 - Train Loss: 0.030907 - Val Loss: 0.045776\n",
      "Epoch 7/50 - Train Loss: 0.032883 - Val Loss: 0.045444\n",
      "Epoch 8/50 - Train Loss: 0.031115 - Val Loss: 0.047003\n",
      "Epoch 9/50 - Train Loss: 0.030868 - Val Loss: 0.046134\n",
      "Epoch 10/50 - Train Loss: 0.032782 - Val Loss: 0.054692\n",
      "Epoch 11/50 - Train Loss: 0.031640 - Val Loss: 0.058975\n",
      "Epoch 12/50 - Train Loss: 0.033067 - Val Loss: 0.046122\n",
      "Epoch 13/50 - Train Loss: 0.032292 - Val Loss: 0.044506\n",
      "Epoch 14/50 - Train Loss: 0.029643 - Val Loss: 0.045581\n",
      "Epoch 15/50 - Train Loss: 0.029331 - Val Loss: 0.045106\n",
      "Epoch 16/50 - Train Loss: 0.033098 - Val Loss: 0.047410\n",
      "Epoch 17/50 - Train Loss: 0.030050 - Val Loss: 0.045140\n",
      "Epoch 18/50 - Train Loss: 0.028408 - Val Loss: 0.044243\n",
      "Epoch 19/50 - Train Loss: 0.032026 - Val Loss: 0.044603\n",
      "Epoch 20/50 - Train Loss: 0.030367 - Val Loss: 0.045167\n",
      "Epoch 21/50 - Train Loss: 0.030172 - Val Loss: 0.048027\n",
      "Epoch 22/50 - Train Loss: 0.030324 - Val Loss: 0.044607\n",
      "Epoch 23/50 - Train Loss: 0.033350 - Val Loss: 0.047442\n",
      "Epoch 24/50 - Train Loss: 0.028990 - Val Loss: 0.046072\n",
      "Epoch 25/50 - Train Loss: 0.032917 - Val Loss: 0.044839\n",
      "Epoch 26/50 - Train Loss: 0.032191 - Val Loss: 0.044623\n",
      "Epoch 27/50 - Train Loss: 0.031211 - Val Loss: 0.045657\n",
      "Epoch 28/50 - Train Loss: 0.032983 - Val Loss: 0.044573\n",
      "Epoch 29/50 - Train Loss: 0.031776 - Val Loss: 0.045281\n",
      "Epoch 30/50 - Train Loss: 0.029623 - Val Loss: 0.045359\n",
      "Epoch 31/50 - Train Loss: 0.032047 - Val Loss: 0.046452\n",
      "Epoch 32/50 - Train Loss: 0.031855 - Val Loss: 0.048625\n",
      "Epoch 33/50 - Train Loss: 0.029979 - Val Loss: 0.044474\n",
      "Epoch 34/50 - Train Loss: 0.031785 - Val Loss: 0.044843\n",
      "Epoch 35/50 - Train Loss: 0.028913 - Val Loss: 0.045742\n",
      "Epoch 36/50 - Train Loss: 0.031645 - Val Loss: 0.048289\n",
      "Epoch 37/50 - Train Loss: 0.029793 - Val Loss: 0.045814\n",
      "Epoch 38/50 - Train Loss: 0.029932 - Val Loss: 0.044602\n",
      "Epoch 39/50 - Train Loss: 0.029531 - Val Loss: 0.044160\n",
      "Epoch 40/50 - Train Loss: 0.028496 - Val Loss: 0.049580\n",
      "Epoch 41/50 - Train Loss: 0.032319 - Val Loss: 0.051391\n",
      "Epoch 42/50 - Train Loss: 0.031616 - Val Loss: 0.046285\n",
      "Epoch 43/50 - Train Loss: 0.032528 - Val Loss: 0.047269\n",
      "Epoch 44/50 - Train Loss: 0.027728 - Val Loss: 0.044527\n",
      "Epoch 45/50 - Train Loss: 0.031023 - Val Loss: 0.045648\n",
      "Epoch 46/50 - Train Loss: 0.031991 - Val Loss: 0.055750\n",
      "Epoch 47/50 - Train Loss: 0.031732 - Val Loss: 0.044323\n",
      "Epoch 48/50 - Train Loss: 0.026432 - Val Loss: 0.044007\n",
      "Epoch 49/50 - Train Loss: 0.029797 - Val Loss: 0.046287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:31:25,774] Trial 45 finished with value: 0.04400727773706118 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 125, 'lr': 0.0022088284243677444, 'weight_decay': 2.931534619944952e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033018 - Val Loss: 0.045201\n",
      "Epoch 1/50 - Train Loss: 0.080719 - Val Loss: 0.071945\n",
      "Epoch 2/50 - Train Loss: 0.043045 - Val Loss: 0.094871\n",
      "Epoch 3/50 - Train Loss: 0.037809 - Val Loss: 0.052231\n",
      "Epoch 4/50 - Train Loss: 0.039698 - Val Loss: 0.070415\n",
      "Epoch 5/50 - Train Loss: 0.036910 - Val Loss: 0.050445\n",
      "Epoch 6/50 - Train Loss: 0.030907 - Val Loss: 0.052919\n",
      "Epoch 7/50 - Train Loss: 0.035396 - Val Loss: 0.052706\n",
      "Epoch 8/50 - Train Loss: 0.027628 - Val Loss: 0.049665\n",
      "Epoch 9/50 - Train Loss: 0.035403 - Val Loss: 0.050951\n",
      "Epoch 10/50 - Train Loss: 0.025412 - Val Loss: 0.048140\n",
      "Epoch 11/50 - Train Loss: 0.028828 - Val Loss: 0.046696\n",
      "Epoch 12/50 - Train Loss: 0.028732 - Val Loss: 0.050120\n",
      "Epoch 13/50 - Train Loss: 0.033697 - Val Loss: 0.046067\n",
      "Epoch 14/50 - Train Loss: 0.024617 - Val Loss: 0.046340\n",
      "Epoch 15/50 - Train Loss: 0.033297 - Val Loss: 0.048761\n",
      "Epoch 16/50 - Train Loss: 0.035712 - Val Loss: 0.047778\n",
      "Epoch 17/50 - Train Loss: 0.032235 - Val Loss: 0.050063\n",
      "Epoch 18/50 - Train Loss: 0.028176 - Val Loss: 0.048439\n",
      "Epoch 19/50 - Train Loss: 0.032399 - Val Loss: 0.049667\n",
      "Epoch 20/50 - Train Loss: 0.035339 - Val Loss: 0.045145\n",
      "Epoch 21/50 - Train Loss: 0.034262 - Val Loss: 0.055092\n",
      "Epoch 22/50 - Train Loss: 0.029754 - Val Loss: 0.044889\n",
      "Epoch 23/50 - Train Loss: 0.033034 - Val Loss: 0.046662\n",
      "Epoch 24/50 - Train Loss: 0.025371 - Val Loss: 0.045566\n",
      "Epoch 25/50 - Train Loss: 0.032298 - Val Loss: 0.045435\n",
      "Epoch 26/50 - Train Loss: 0.027418 - Val Loss: 0.050417\n",
      "Epoch 27/50 - Train Loss: 0.033848 - Val Loss: 0.046022\n",
      "Epoch 28/50 - Train Loss: 0.031518 - Val Loss: 0.053628\n",
      "Epoch 29/50 - Train Loss: 0.031568 - Val Loss: 0.046325\n",
      "Epoch 30/50 - Train Loss: 0.030751 - Val Loss: 0.050248\n",
      "Epoch 31/50 - Train Loss: 0.029288 - Val Loss: 0.045399\n",
      "Epoch 32/50 - Train Loss: 0.029277 - Val Loss: 0.045979\n",
      "Epoch 33/50 - Train Loss: 0.033542 - Val Loss: 0.045138\n",
      "Epoch 34/50 - Train Loss: 0.031068 - Val Loss: 0.047900\n",
      "Epoch 35/50 - Train Loss: 0.027275 - Val Loss: 0.045322\n",
      "Epoch 36/50 - Train Loss: 0.033227 - Val Loss: 0.048409\n",
      "Epoch 37/50 - Train Loss: 0.026802 - Val Loss: 0.045551\n",
      "Epoch 38/50 - Train Loss: 0.031212 - Val Loss: 0.048752\n",
      "Epoch 39/50 - Train Loss: 0.030570 - Val Loss: 0.045221\n",
      "Epoch 40/50 - Train Loss: 0.034088 - Val Loss: 0.048098\n",
      "Epoch 41/50 - Train Loss: 0.032002 - Val Loss: 0.046749\n",
      "Epoch 42/50 - Train Loss: 0.028308 - Val Loss: 0.044831\n",
      "Epoch 43/50 - Train Loss: 0.023657 - Val Loss: 0.044617\n",
      "Epoch 44/50 - Train Loss: 0.030755 - Val Loss: 0.045226\n",
      "Epoch 45/50 - Train Loss: 0.033357 - Val Loss: 0.044634\n",
      "Epoch 46/50 - Train Loss: 0.031278 - Val Loss: 0.049390\n",
      "Epoch 47/50 - Train Loss: 0.026908 - Val Loss: 0.046090\n",
      "Epoch 48/50 - Train Loss: 0.032773 - Val Loss: 0.049634\n",
      "Epoch 49/50 - Train Loss: 0.033404 - Val Loss: 0.047867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:31:41,658] Trial 46 finished with value: 0.04461698979139328 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 143, 'lr': 0.0035172234034231073, 'weight_decay': 3.512534989329147e-06, 'batch_size': 32}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030766 - Val Loss: 0.048660\n",
      "Epoch 1/50 - Train Loss: 0.170525 - Val Loss: 0.057759\n",
      "Epoch 2/50 - Train Loss: 0.044208 - Val Loss: 0.061268\n",
      "Epoch 3/50 - Train Loss: 0.036275 - Val Loss: 0.062997\n",
      "Epoch 4/50 - Train Loss: 0.037856 - Val Loss: 0.055366\n",
      "Epoch 5/50 - Train Loss: 0.034090 - Val Loss: 0.051966\n",
      "Epoch 6/50 - Train Loss: 0.032428 - Val Loss: 0.059028\n",
      "Epoch 7/50 - Train Loss: 0.030001 - Val Loss: 0.047742\n",
      "Epoch 8/50 - Train Loss: 0.032333 - Val Loss: 0.045713\n",
      "Epoch 9/50 - Train Loss: 0.033683 - Val Loss: 0.046627\n",
      "Epoch 10/50 - Train Loss: 0.032876 - Val Loss: 0.045662\n",
      "Epoch 11/50 - Train Loss: 0.033825 - Val Loss: 0.045080\n",
      "Epoch 12/50 - Train Loss: 0.031194 - Val Loss: 0.055157\n",
      "Epoch 13/50 - Train Loss: 0.031898 - Val Loss: 0.046694\n",
      "Epoch 14/50 - Train Loss: 0.030798 - Val Loss: 0.046098\n",
      "Epoch 15/50 - Train Loss: 0.030446 - Val Loss: 0.044917\n",
      "Epoch 16/50 - Train Loss: 0.031543 - Val Loss: 0.045627\n",
      "Epoch 17/50 - Train Loss: 0.027060 - Val Loss: 0.044432\n",
      "Epoch 18/50 - Train Loss: 0.029847 - Val Loss: 0.050827\n",
      "Epoch 19/50 - Train Loss: 0.031862 - Val Loss: 0.045082\n",
      "Epoch 20/50 - Train Loss: 0.029995 - Val Loss: 0.045549\n",
      "Epoch 21/50 - Train Loss: 0.029547 - Val Loss: 0.044310\n",
      "Epoch 22/50 - Train Loss: 0.028400 - Val Loss: 0.044300\n",
      "Epoch 23/50 - Train Loss: 0.028601 - Val Loss: 0.045148\n",
      "Epoch 24/50 - Train Loss: 0.028733 - Val Loss: 0.046255\n",
      "Epoch 25/50 - Train Loss: 0.035402 - Val Loss: 0.050543\n",
      "Epoch 26/50 - Train Loss: 0.031173 - Val Loss: 0.044620\n",
      "Epoch 27/50 - Train Loss: 0.031992 - Val Loss: 0.051064\n",
      "Epoch 28/50 - Train Loss: 0.033255 - Val Loss: 0.046690\n",
      "Epoch 29/50 - Train Loss: 0.030903 - Val Loss: 0.045332\n",
      "Epoch 30/50 - Train Loss: 0.031282 - Val Loss: 0.044875\n",
      "Epoch 31/50 - Train Loss: 0.029391 - Val Loss: 0.046466\n",
      "Epoch 32/50 - Train Loss: 0.030479 - Val Loss: 0.045332\n",
      "Epoch 33/50 - Train Loss: 0.030568 - Val Loss: 0.044276\n",
      "Epoch 34/50 - Train Loss: 0.032126 - Val Loss: 0.044510\n",
      "Epoch 35/50 - Train Loss: 0.030796 - Val Loss: 0.052715\n",
      "Epoch 36/50 - Train Loss: 0.031900 - Val Loss: 0.045106\n",
      "Epoch 37/50 - Train Loss: 0.030620 - Val Loss: 0.045316\n",
      "Epoch 38/50 - Train Loss: 0.031882 - Val Loss: 0.044558\n",
      "Epoch 39/50 - Train Loss: 0.035259 - Val Loss: 0.045581\n",
      "Epoch 40/50 - Train Loss: 0.029382 - Val Loss: 0.044661\n",
      "Epoch 41/50 - Train Loss: 0.031381 - Val Loss: 0.048446\n",
      "Epoch 42/50 - Train Loss: 0.029307 - Val Loss: 0.044662\n",
      "Epoch 43/50 - Train Loss: 0.030244 - Val Loss: 0.044888\n",
      "Epoch 44/50 - Train Loss: 0.027653 - Val Loss: 0.044942\n",
      "Epoch 45/50 - Train Loss: 0.028996 - Val Loss: 0.045504\n",
      "Epoch 46/50 - Train Loss: 0.028656 - Val Loss: 0.048913\n",
      "Epoch 47/50 - Train Loss: 0.030897 - Val Loss: 0.045331\n",
      "Epoch 48/50 - Train Loss: 0.030897 - Val Loss: 0.044868\n",
      "Epoch 49/50 - Train Loss: 0.030123 - Val Loss: 0.050462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:32:15,517] Trial 47 finished with value: 0.044275913387537 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 189, 'lr': 0.008160885557546302, 'weight_decay': 1.0212920196157085e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032254 - Val Loss: 0.045452\n",
      "Epoch 1/50 - Train Loss: 0.118118 - Val Loss: 0.239395\n",
      "Epoch 2/50 - Train Loss: 0.097478 - Val Loss: 0.204878\n",
      "Epoch 3/50 - Train Loss: 0.082217 - Val Loss: 0.166290\n",
      "Epoch 4/50 - Train Loss: 0.058806 - Val Loss: 0.127840\n",
      "Epoch 5/50 - Train Loss: 0.047185 - Val Loss: 0.096431\n",
      "Epoch 6/50 - Train Loss: 0.044695 - Val Loss: 0.084746\n",
      "Epoch 7/50 - Train Loss: 0.043790 - Val Loss: 0.084225\n",
      "Epoch 8/50 - Train Loss: 0.044070 - Val Loss: 0.086789\n",
      "Epoch 9/50 - Train Loss: 0.044026 - Val Loss: 0.082212\n",
      "Epoch 10/50 - Train Loss: 0.041944 - Val Loss: 0.083379\n",
      "Epoch 11/50 - Train Loss: 0.042107 - Val Loss: 0.082960\n",
      "Epoch 12/50 - Train Loss: 0.039530 - Val Loss: 0.079202\n",
      "Epoch 13/50 - Train Loss: 0.039067 - Val Loss: 0.075362\n",
      "Epoch 14/50 - Train Loss: 0.038972 - Val Loss: 0.072102\n",
      "Epoch 15/50 - Train Loss: 0.036723 - Val Loss: 0.070276\n",
      "Epoch 16/50 - Train Loss: 0.036950 - Val Loss: 0.069509\n",
      "Epoch 17/50 - Train Loss: 0.035005 - Val Loss: 0.063698\n",
      "Epoch 18/50 - Train Loss: 0.035912 - Val Loss: 0.059839\n",
      "Epoch 19/50 - Train Loss: 0.035818 - Val Loss: 0.061285\n",
      "Epoch 20/50 - Train Loss: 0.034715 - Val Loss: 0.054713\n",
      "Epoch 21/50 - Train Loss: 0.033905 - Val Loss: 0.057727\n",
      "Epoch 22/50 - Train Loss: 0.036810 - Val Loss: 0.054327\n",
      "Epoch 23/50 - Train Loss: 0.033708 - Val Loss: 0.054296\n",
      "Epoch 24/50 - Train Loss: 0.029422 - Val Loss: 0.050656\n",
      "Epoch 25/50 - Train Loss: 0.032940 - Val Loss: 0.052388\n",
      "Epoch 26/50 - Train Loss: 0.030315 - Val Loss: 0.050290\n",
      "Epoch 27/50 - Train Loss: 0.032555 - Val Loss: 0.050659\n",
      "Epoch 28/50 - Train Loss: 0.033268 - Val Loss: 0.049730\n",
      "Epoch 29/50 - Train Loss: 0.035277 - Val Loss: 0.048466\n",
      "Epoch 30/50 - Train Loss: 0.033151 - Val Loss: 0.050843\n",
      "Epoch 31/50 - Train Loss: 0.033435 - Val Loss: 0.049895\n",
      "Epoch 32/50 - Train Loss: 0.029016 - Val Loss: 0.048839\n",
      "Epoch 33/50 - Train Loss: 0.032425 - Val Loss: 0.048949\n",
      "Epoch 34/50 - Train Loss: 0.031641 - Val Loss: 0.047459\n",
      "Epoch 35/50 - Train Loss: 0.029518 - Val Loss: 0.049419\n",
      "Epoch 36/50 - Train Loss: 0.030752 - Val Loss: 0.047921\n",
      "Epoch 37/50 - Train Loss: 0.032718 - Val Loss: 0.048646\n",
      "Epoch 38/50 - Train Loss: 0.033943 - Val Loss: 0.048071\n",
      "Epoch 39/50 - Train Loss: 0.031940 - Val Loss: 0.049074\n",
      "Epoch 40/50 - Train Loss: 0.029350 - Val Loss: 0.047233\n",
      "Epoch 41/50 - Train Loss: 0.032615 - Val Loss: 0.048561\n",
      "Epoch 42/50 - Train Loss: 0.031412 - Val Loss: 0.048348\n",
      "Epoch 43/50 - Train Loss: 0.032046 - Val Loss: 0.047818\n",
      "Epoch 44/50 - Train Loss: 0.030195 - Val Loss: 0.047915\n",
      "Epoch 45/50 - Train Loss: 0.032563 - Val Loss: 0.048270\n",
      "Epoch 46/50 - Train Loss: 0.029499 - Val Loss: 0.047780\n",
      "Epoch 47/50 - Train Loss: 0.031052 - Val Loss: 0.047525\n",
      "Epoch 48/50 - Train Loss: 0.031672 - Val Loss: 0.047624\n",
      "Epoch 49/50 - Train Loss: 0.032390 - Val Loss: 0.047873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:33:26,383] Trial 48 finished with value: 0.047232598066329956 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 172, 'lr': 6.935304172038043e-05, 'weight_decay': 1.247974146438778e-06, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029194 - Val Loss: 0.048073\n",
      "Epoch 1/50 - Train Loss: 0.165111 - Val Loss: 0.176796\n",
      "Epoch 2/50 - Train Loss: 0.056864 - Val Loss: 0.085424\n",
      "Epoch 3/50 - Train Loss: 0.051275 - Val Loss: 0.072231\n",
      "Epoch 4/50 - Train Loss: 0.070780 - Val Loss: 0.074480\n",
      "Epoch 5/50 - Train Loss: 0.053773 - Val Loss: 0.091055\n",
      "Epoch 6/50 - Train Loss: 0.040005 - Val Loss: 0.107201\n",
      "Epoch 7/50 - Train Loss: 0.043918 - Val Loss: 0.111520\n",
      "Epoch 8/50 - Train Loss: 0.046012 - Val Loss: 0.102971\n",
      "Epoch 9/50 - Train Loss: 0.041801 - Val Loss: 0.085251\n",
      "Epoch 10/50 - Train Loss: 0.044056 - Val Loss: 0.068411\n",
      "Epoch 11/50 - Train Loss: 0.044533 - Val Loss: 0.059842\n",
      "Epoch 12/50 - Train Loss: 0.044391 - Val Loss: 0.058093\n",
      "Epoch 13/50 - Train Loss: 0.035497 - Val Loss: 0.059922\n",
      "Epoch 14/50 - Train Loss: 0.041303 - Val Loss: 0.063722\n",
      "Epoch 15/50 - Train Loss: 0.034112 - Val Loss: 0.065380\n",
      "Epoch 16/50 - Train Loss: 0.033959 - Val Loss: 0.062563\n",
      "Epoch 17/50 - Train Loss: 0.040912 - Val Loss: 0.056804\n",
      "Epoch 18/50 - Train Loss: 0.031205 - Val Loss: 0.052376\n",
      "Epoch 19/50 - Train Loss: 0.040389 - Val Loss: 0.051040\n",
      "Epoch 20/50 - Train Loss: 0.038409 - Val Loss: 0.051256\n",
      "Epoch 21/50 - Train Loss: 0.040268 - Val Loss: 0.052062\n",
      "Epoch 22/50 - Train Loss: 0.032826 - Val Loss: 0.051676\n",
      "Epoch 23/50 - Train Loss: 0.031245 - Val Loss: 0.049631\n",
      "Epoch 24/50 - Train Loss: 0.038149 - Val Loss: 0.047789\n",
      "Epoch 25/50 - Train Loss: 0.030742 - Val Loss: 0.046791\n",
      "Epoch 26/50 - Train Loss: 0.038000 - Val Loss: 0.046881\n",
      "Epoch 27/50 - Train Loss: 0.038469 - Val Loss: 0.048605\n",
      "Epoch 28/50 - Train Loss: 0.037986 - Val Loss: 0.048671\n",
      "Epoch 29/50 - Train Loss: 0.029956 - Val Loss: 0.047084\n",
      "Epoch 30/50 - Train Loss: 0.030002 - Val Loss: 0.045054\n",
      "Epoch 31/50 - Train Loss: 0.038462 - Val Loss: 0.044812\n",
      "Epoch 32/50 - Train Loss: 0.029317 - Val Loss: 0.045539\n",
      "Epoch 33/50 - Train Loss: 0.029499 - Val Loss: 0.045927\n",
      "Epoch 34/50 - Train Loss: 0.030377 - Val Loss: 0.044776\n",
      "Epoch 35/50 - Train Loss: 0.038152 - Val Loss: 0.044255\n",
      "Epoch 36/50 - Train Loss: 0.030503 - Val Loss: 0.044209\n",
      "Epoch 37/50 - Train Loss: 0.036654 - Val Loss: 0.044990\n",
      "Epoch 38/50 - Train Loss: 0.035970 - Val Loss: 0.045566\n",
      "Epoch 39/50 - Train Loss: 0.038123 - Val Loss: 0.044900\n",
      "Epoch 40/50 - Train Loss: 0.038041 - Val Loss: 0.044524\n",
      "Epoch 41/50 - Train Loss: 0.036621 - Val Loss: 0.044830\n",
      "Epoch 42/50 - Train Loss: 0.026429 - Val Loss: 0.045404\n",
      "Epoch 43/50 - Train Loss: 0.038214 - Val Loss: 0.045993\n",
      "Epoch 44/50 - Train Loss: 0.029120 - Val Loss: 0.045098\n",
      "Epoch 45/50 - Train Loss: 0.036981 - Val Loss: 0.044949\n",
      "Epoch 46/50 - Train Loss: 0.028747 - Val Loss: 0.044829\n",
      "Epoch 47/50 - Train Loss: 0.036608 - Val Loss: 0.045118\n",
      "Epoch 48/50 - Train Loss: 0.028832 - Val Loss: 0.045255\n",
      "Epoch 49/50 - Train Loss: 0.029477 - Val Loss: 0.044857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:33:39,269] Trial 49 finished with value: 0.04420863837003708 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 159, 'lr': 0.001174754444334202, 'weight_decay': 7.378543253487577e-07, 'batch_size': 64}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.036379 - Val Loss: 0.044930\n",
      "Epoch 1/50 - Train Loss: 0.154379 - Val Loss: 0.100684\n",
      "Epoch 2/50 - Train Loss: 0.052620 - Val Loss: 0.069739\n",
      "Epoch 3/50 - Train Loss: 0.043513 - Val Loss: 0.084685\n",
      "Epoch 4/50 - Train Loss: 0.038906 - Val Loss: 0.071133\n",
      "Epoch 5/50 - Train Loss: 0.036853 - Val Loss: 0.061288\n",
      "Epoch 6/50 - Train Loss: 0.032876 - Val Loss: 0.055278\n",
      "Epoch 7/50 - Train Loss: 0.034111 - Val Loss: 0.053515\n",
      "Epoch 8/50 - Train Loss: 0.034120 - Val Loss: 0.050422\n",
      "Epoch 9/50 - Train Loss: 0.032380 - Val Loss: 0.048804\n",
      "Epoch 10/50 - Train Loss: 0.032330 - Val Loss: 0.047357\n",
      "Epoch 11/50 - Train Loss: 0.032418 - Val Loss: 0.047741\n",
      "Epoch 12/50 - Train Loss: 0.030785 - Val Loss: 0.046487\n",
      "Epoch 13/50 - Train Loss: 0.031798 - Val Loss: 0.047893\n",
      "Epoch 14/50 - Train Loss: 0.030139 - Val Loss: 0.045674\n",
      "Epoch 15/50 - Train Loss: 0.031648 - Val Loss: 0.046150\n",
      "Epoch 16/50 - Train Loss: 0.029871 - Val Loss: 0.047098\n",
      "Epoch 17/50 - Train Loss: 0.029872 - Val Loss: 0.045359\n",
      "Epoch 18/50 - Train Loss: 0.032088 - Val Loss: 0.045879\n",
      "Epoch 19/50 - Train Loss: 0.032898 - Val Loss: 0.045914\n",
      "Epoch 20/50 - Train Loss: 0.029731 - Val Loss: 0.045156\n",
      "Epoch 21/50 - Train Loss: 0.032987 - Val Loss: 0.048217\n",
      "Epoch 22/50 - Train Loss: 0.032140 - Val Loss: 0.045663\n",
      "Epoch 23/50 - Train Loss: 0.029654 - Val Loss: 0.045151\n",
      "Epoch 24/50 - Train Loss: 0.028413 - Val Loss: 0.045589\n",
      "Epoch 25/50 - Train Loss: 0.031232 - Val Loss: 0.045270\n",
      "Epoch 26/50 - Train Loss: 0.028588 - Val Loss: 0.046311\n",
      "Epoch 27/50 - Train Loss: 0.031402 - Val Loss: 0.046454\n",
      "Epoch 28/50 - Train Loss: 0.031660 - Val Loss: 0.046844\n",
      "Epoch 29/50 - Train Loss: 0.033104 - Val Loss: 0.047135\n",
      "Epoch 30/50 - Train Loss: 0.030919 - Val Loss: 0.045330\n",
      "Epoch 31/50 - Train Loss: 0.032022 - Val Loss: 0.044829\n",
      "Epoch 32/50 - Train Loss: 0.033175 - Val Loss: 0.050406\n",
      "Epoch 33/50 - Train Loss: 0.030695 - Val Loss: 0.045066\n",
      "Epoch 34/50 - Train Loss: 0.031032 - Val Loss: 0.045271\n",
      "Epoch 35/50 - Train Loss: 0.029751 - Val Loss: 0.045692\n",
      "Epoch 36/50 - Train Loss: 0.031096 - Val Loss: 0.045180\n",
      "Epoch 37/50 - Train Loss: 0.030624 - Val Loss: 0.046234\n",
      "Epoch 38/50 - Train Loss: 0.030931 - Val Loss: 0.045184\n",
      "Epoch 39/50 - Train Loss: 0.031268 - Val Loss: 0.046805\n",
      "Epoch 40/50 - Train Loss: 0.026988 - Val Loss: 0.045095\n",
      "Epoch 41/50 - Train Loss: 0.032436 - Val Loss: 0.046069\n",
      "Epoch 42/50 - Train Loss: 0.034415 - Val Loss: 0.045348\n",
      "Epoch 43/50 - Train Loss: 0.033404 - Val Loss: 0.046677\n",
      "Epoch 44/50 - Train Loss: 0.028003 - Val Loss: 0.045177\n",
      "Epoch 45/50 - Train Loss: 0.031282 - Val Loss: 0.045509\n",
      "Epoch 46/50 - Train Loss: 0.029877 - Val Loss: 0.046600\n",
      "Epoch 47/50 - Train Loss: 0.029552 - Val Loss: 0.044991\n",
      "Epoch 48/50 - Train Loss: 0.028940 - Val Loss: 0.045491\n",
      "Epoch 49/50 - Train Loss: 0.028709 - Val Loss: 0.046722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:34:08,637] Trial 50 finished with value: 0.04455909878015518 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 135, 'lr': 0.0005360424946592028, 'weight_decay': 1.9497547096388465e-06, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027778 - Val Loss: 0.044559\n",
      "Epoch 1/50 - Train Loss: 0.062363 - Val Loss: 0.067302\n",
      "Epoch 2/50 - Train Loss: 0.037243 - Val Loss: 0.050049\n",
      "Epoch 3/50 - Train Loss: 0.034922 - Val Loss: 0.050272\n",
      "Epoch 4/50 - Train Loss: 0.033589 - Val Loss: 0.050210\n",
      "Epoch 5/50 - Train Loss: 0.030359 - Val Loss: 0.045693\n",
      "Epoch 6/50 - Train Loss: 0.034264 - Val Loss: 0.047933\n",
      "Epoch 7/50 - Train Loss: 0.031302 - Val Loss: 0.047686\n",
      "Epoch 8/50 - Train Loss: 0.029646 - Val Loss: 0.045422\n",
      "Epoch 9/50 - Train Loss: 0.033842 - Val Loss: 0.046103\n",
      "Epoch 10/50 - Train Loss: 0.031296 - Val Loss: 0.049794\n",
      "Epoch 11/50 - Train Loss: 0.033852 - Val Loss: 0.049357\n",
      "Epoch 12/50 - Train Loss: 0.031851 - Val Loss: 0.047485\n",
      "Epoch 13/50 - Train Loss: 0.031579 - Val Loss: 0.044863\n",
      "Epoch 14/50 - Train Loss: 0.030527 - Val Loss: 0.048790\n",
      "Epoch 15/50 - Train Loss: 0.031529 - Val Loss: 0.045276\n",
      "Epoch 16/50 - Train Loss: 0.030378 - Val Loss: 0.045628\n",
      "Epoch 17/50 - Train Loss: 0.030503 - Val Loss: 0.048119\n",
      "Epoch 18/50 - Train Loss: 0.032180 - Val Loss: 0.045081\n",
      "Epoch 19/50 - Train Loss: 0.029298 - Val Loss: 0.045726\n",
      "Epoch 20/50 - Train Loss: 0.031084 - Val Loss: 0.044925\n",
      "Epoch 21/50 - Train Loss: 0.029866 - Val Loss: 0.045217\n",
      "Epoch 22/50 - Train Loss: 0.033123 - Val Loss: 0.057473\n",
      "Epoch 23/50 - Train Loss: 0.033179 - Val Loss: 0.059151\n",
      "Epoch 24/50 - Train Loss: 0.034158 - Val Loss: 0.045390\n",
      "Epoch 25/50 - Train Loss: 0.031135 - Val Loss: 0.045205\n",
      "Epoch 26/50 - Train Loss: 0.030623 - Val Loss: 0.044942\n",
      "Epoch 27/50 - Train Loss: 0.031706 - Val Loss: 0.045293\n",
      "Epoch 28/50 - Train Loss: 0.029790 - Val Loss: 0.045133\n",
      "Epoch 29/50 - Train Loss: 0.027829 - Val Loss: 0.044751\n",
      "Epoch 30/50 - Train Loss: 0.031251 - Val Loss: 0.046129\n",
      "Epoch 31/50 - Train Loss: 0.030622 - Val Loss: 0.046317\n",
      "Epoch 32/50 - Train Loss: 0.030843 - Val Loss: 0.052524\n",
      "Epoch 33/50 - Train Loss: 0.032301 - Val Loss: 0.045468\n",
      "Epoch 34/50 - Train Loss: 0.029945 - Val Loss: 0.045731\n",
      "Epoch 35/50 - Train Loss: 0.031151 - Val Loss: 0.045321\n",
      "Epoch 36/50 - Train Loss: 0.029893 - Val Loss: 0.043947\n",
      "Epoch 37/50 - Train Loss: 0.029784 - Val Loss: 0.044127\n",
      "Epoch 38/50 - Train Loss: 0.029610 - Val Loss: 0.044485\n",
      "Epoch 39/50 - Train Loss: 0.029178 - Val Loss: 0.044648\n",
      "Epoch 40/50 - Train Loss: 0.027839 - Val Loss: 0.043751\n",
      "Epoch 41/50 - Train Loss: 0.030962 - Val Loss: 0.044281\n",
      "Epoch 42/50 - Train Loss: 0.029691 - Val Loss: 0.044453\n",
      "Epoch 43/50 - Train Loss: 0.031864 - Val Loss: 0.045035\n",
      "Epoch 44/50 - Train Loss: 0.030016 - Val Loss: 0.045924\n",
      "Epoch 45/50 - Train Loss: 0.027594 - Val Loss: 0.047322\n",
      "Epoch 46/50 - Train Loss: 0.031719 - Val Loss: 0.046618\n",
      "Epoch 47/50 - Train Loss: 0.032760 - Val Loss: 0.044866\n",
      "Epoch 48/50 - Train Loss: 0.032262 - Val Loss: 0.044529\n",
      "Epoch 49/50 - Train Loss: 0.032124 - Val Loss: 0.044641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:34:28,720] Trial 51 finished with value: 0.0437513900299867 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 105, 'lr': 0.004703698803690761, 'weight_decay': 3.6825593925629753e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028028 - Val Loss: 0.045577\n",
      "Epoch 1/50 - Train Loss: 0.058282 - Val Loss: 0.069824\n",
      "Epoch 2/50 - Train Loss: 0.037351 - Val Loss: 0.058072\n",
      "Epoch 3/50 - Train Loss: 0.033524 - Val Loss: 0.060514\n",
      "Epoch 4/50 - Train Loss: 0.033280 - Val Loss: 0.052190\n",
      "Epoch 5/50 - Train Loss: 0.032275 - Val Loss: 0.049498\n",
      "Epoch 6/50 - Train Loss: 0.032539 - Val Loss: 0.048021\n",
      "Epoch 7/50 - Train Loss: 0.028470 - Val Loss: 0.046988\n",
      "Epoch 8/50 - Train Loss: 0.032716 - Val Loss: 0.050476\n",
      "Epoch 9/50 - Train Loss: 0.031407 - Val Loss: 0.051414\n",
      "Epoch 10/50 - Train Loss: 0.031803 - Val Loss: 0.046358\n",
      "Epoch 11/50 - Train Loss: 0.031402 - Val Loss: 0.047318\n",
      "Epoch 12/50 - Train Loss: 0.030818 - Val Loss: 0.046496\n",
      "Epoch 13/50 - Train Loss: 0.030276 - Val Loss: 0.047368\n",
      "Epoch 14/50 - Train Loss: 0.028129 - Val Loss: 0.045796\n",
      "Epoch 15/50 - Train Loss: 0.029211 - Val Loss: 0.046126\n",
      "Epoch 16/50 - Train Loss: 0.029599 - Val Loss: 0.046615\n",
      "Epoch 17/50 - Train Loss: 0.033692 - Val Loss: 0.054568\n",
      "Epoch 18/50 - Train Loss: 0.036705 - Val Loss: 0.058855\n",
      "Epoch 19/50 - Train Loss: 0.033195 - Val Loss: 0.055595\n",
      "Epoch 20/50 - Train Loss: 0.030655 - Val Loss: 0.050321\n",
      "Epoch 21/50 - Train Loss: 0.031297 - Val Loss: 0.046615\n",
      "Epoch 22/50 - Train Loss: 0.028104 - Val Loss: 0.045685\n",
      "Epoch 23/50 - Train Loss: 0.028505 - Val Loss: 0.045884\n",
      "Epoch 24/50 - Train Loss: 0.029842 - Val Loss: 0.045663\n",
      "Epoch 25/50 - Train Loss: 0.031733 - Val Loss: 0.047736\n",
      "Epoch 26/50 - Train Loss: 0.032536 - Val Loss: 0.048345\n",
      "Epoch 27/50 - Train Loss: 0.030614 - Val Loss: 0.046916\n",
      "Epoch 28/50 - Train Loss: 0.028963 - Val Loss: 0.052243\n",
      "Epoch 29/50 - Train Loss: 0.030360 - Val Loss: 0.070206\n",
      "Epoch 30/50 - Train Loss: 0.033385 - Val Loss: 0.049741\n",
      "Epoch 31/50 - Train Loss: 0.033046 - Val Loss: 0.045357\n",
      "Epoch 32/50 - Train Loss: 0.032023 - Val Loss: 0.046294\n",
      "Epoch 33/50 - Train Loss: 0.030774 - Val Loss: 0.045387\n",
      "Epoch 34/50 - Train Loss: 0.028138 - Val Loss: 0.046447\n",
      "Epoch 35/50 - Train Loss: 0.030009 - Val Loss: 0.045778\n",
      "Epoch 36/50 - Train Loss: 0.029721 - Val Loss: 0.045850\n",
      "Epoch 37/50 - Train Loss: 0.027862 - Val Loss: 0.046080\n",
      "Epoch 38/50 - Train Loss: 0.026324 - Val Loss: 0.046070\n",
      "Epoch 39/50 - Train Loss: 0.030616 - Val Loss: 0.045455\n",
      "Epoch 40/50 - Train Loss: 0.028215 - Val Loss: 0.048757\n",
      "Epoch 41/50 - Train Loss: 0.028047 - Val Loss: 0.045774\n",
      "Epoch 42/50 - Train Loss: 0.030017 - Val Loss: 0.046151\n",
      "Epoch 43/50 - Train Loss: 0.030983 - Val Loss: 0.046968\n",
      "Epoch 44/50 - Train Loss: 0.029979 - Val Loss: 0.047102\n",
      "Epoch 45/50 - Train Loss: 0.029360 - Val Loss: 0.045036\n",
      "Epoch 46/50 - Train Loss: 0.029289 - Val Loss: 0.045523\n",
      "Epoch 47/50 - Train Loss: 0.029256 - Val Loss: 0.045003\n",
      "Epoch 48/50 - Train Loss: 0.028769 - Val Loss: 0.052187\n",
      "Epoch 49/50 - Train Loss: 0.031249 - Val Loss: 0.051180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:34:48,208] Trial 52 finished with value: 0.04500274360179901 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 113, 'lr': 0.0026755249540763113, 'weight_decay': 2.876690556599391e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031359 - Val Loss: 0.046580\n",
      "Epoch 1/50 - Train Loss: 0.072970 - Val Loss: 0.065565\n",
      "Epoch 2/50 - Train Loss: 0.038773 - Val Loss: 0.058782\n",
      "Epoch 3/50 - Train Loss: 0.031213 - Val Loss: 0.053487\n",
      "Epoch 4/50 - Train Loss: 0.033646 - Val Loss: 0.052416\n",
      "Epoch 5/50 - Train Loss: 0.030770 - Val Loss: 0.047329\n",
      "Epoch 6/50 - Train Loss: 0.029285 - Val Loss: 0.048565\n",
      "Epoch 7/50 - Train Loss: 0.031279 - Val Loss: 0.052100\n",
      "Epoch 8/50 - Train Loss: 0.029606 - Val Loss: 0.049377\n",
      "Epoch 9/50 - Train Loss: 0.031746 - Val Loss: 0.047095\n",
      "Epoch 10/50 - Train Loss: 0.032485 - Val Loss: 0.046276\n",
      "Epoch 11/50 - Train Loss: 0.031871 - Val Loss: 0.049807\n",
      "Epoch 12/50 - Train Loss: 0.033072 - Val Loss: 0.053679\n",
      "Epoch 13/50 - Train Loss: 0.032847 - Val Loss: 0.056113\n",
      "Epoch 14/50 - Train Loss: 0.031712 - Val Loss: 0.049035\n",
      "Epoch 15/50 - Train Loss: 0.030717 - Val Loss: 0.044569\n",
      "Epoch 16/50 - Train Loss: 0.029850 - Val Loss: 0.048344\n",
      "Epoch 17/50 - Train Loss: 0.032146 - Val Loss: 0.052067\n",
      "Epoch 18/50 - Train Loss: 0.032014 - Val Loss: 0.049237\n",
      "Epoch 19/50 - Train Loss: 0.030270 - Val Loss: 0.059276\n",
      "Epoch 20/50 - Train Loss: 0.030264 - Val Loss: 0.047016\n",
      "Epoch 21/50 - Train Loss: 0.031957 - Val Loss: 0.045258\n",
      "Epoch 22/50 - Train Loss: 0.032732 - Val Loss: 0.044950\n",
      "Epoch 23/50 - Train Loss: 0.031890 - Val Loss: 0.045065\n",
      "Epoch 24/50 - Train Loss: 0.032715 - Val Loss: 0.045981\n",
      "Epoch 25/50 - Train Loss: 0.032597 - Val Loss: 0.051270\n",
      "Epoch 26/50 - Train Loss: 0.028643 - Val Loss: 0.045126\n",
      "Epoch 27/50 - Train Loss: 0.030461 - Val Loss: 0.045945\n",
      "Epoch 28/50 - Train Loss: 0.032016 - Val Loss: 0.046833\n",
      "Epoch 29/50 - Train Loss: 0.031360 - Val Loss: 0.050818\n",
      "Epoch 30/50 - Train Loss: 0.032209 - Val Loss: 0.044919\n",
      "Epoch 31/50 - Train Loss: 0.036206 - Val Loss: 0.046078\n",
      "Epoch 32/50 - Train Loss: 0.030579 - Val Loss: 0.051236\n",
      "Epoch 33/50 - Train Loss: 0.033006 - Val Loss: 0.047138\n",
      "Epoch 34/50 - Train Loss: 0.030287 - Val Loss: 0.044377\n",
      "Epoch 35/50 - Train Loss: 0.028627 - Val Loss: 0.044595\n",
      "Epoch 36/50 - Train Loss: 0.031573 - Val Loss: 0.045898\n",
      "Epoch 37/50 - Train Loss: 0.031986 - Val Loss: 0.046078\n",
      "Epoch 38/50 - Train Loss: 0.035452 - Val Loss: 0.055656\n",
      "Epoch 39/50 - Train Loss: 0.030444 - Val Loss: 0.044979\n",
      "Epoch 40/50 - Train Loss: 0.027668 - Val Loss: 0.044500\n",
      "Epoch 41/50 - Train Loss: 0.028931 - Val Loss: 0.045378\n",
      "Epoch 42/50 - Train Loss: 0.029374 - Val Loss: 0.045477\n",
      "Epoch 43/50 - Train Loss: 0.032103 - Val Loss: 0.044548\n",
      "Epoch 44/50 - Train Loss: 0.028742 - Val Loss: 0.044765\n",
      "Epoch 45/50 - Train Loss: 0.031832 - Val Loss: 0.044868\n",
      "Epoch 46/50 - Train Loss: 0.031463 - Val Loss: 0.044818\n",
      "Epoch 47/50 - Train Loss: 0.028904 - Val Loss: 0.044472\n",
      "Epoch 48/50 - Train Loss: 0.031811 - Val Loss: 0.045118\n",
      "Epoch 49/50 - Train Loss: 0.032434 - Val Loss: 0.046104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:35:07,672] Trial 53 finished with value: 0.0443765843907992 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 80, 'lr': 0.006212162458156622, 'weight_decay': 6.592904186472535e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032633 - Val Loss: 0.044746\n",
      "Epoch 1/50 - Train Loss: 0.062845 - Val Loss: 0.057312\n",
      "Epoch 2/50 - Train Loss: 0.038062 - Val Loss: 0.066958\n",
      "Epoch 3/50 - Train Loss: 0.037480 - Val Loss: 0.055466\n",
      "Epoch 4/50 - Train Loss: 0.031083 - Val Loss: 0.046203\n",
      "Epoch 5/50 - Train Loss: 0.031616 - Val Loss: 0.045870\n",
      "Epoch 6/50 - Train Loss: 0.031665 - Val Loss: 0.049725\n",
      "Epoch 7/50 - Train Loss: 0.031114 - Val Loss: 0.049244\n",
      "Epoch 8/50 - Train Loss: 0.033085 - Val Loss: 0.045043\n",
      "Epoch 9/50 - Train Loss: 0.032556 - Val Loss: 0.048063\n",
      "Epoch 10/50 - Train Loss: 0.029728 - Val Loss: 0.045748\n",
      "Epoch 11/50 - Train Loss: 0.030608 - Val Loss: 0.050476\n",
      "Epoch 12/50 - Train Loss: 0.030209 - Val Loss: 0.051472\n",
      "Epoch 13/50 - Train Loss: 0.031573 - Val Loss: 0.047177\n",
      "Epoch 14/50 - Train Loss: 0.030890 - Val Loss: 0.045420\n",
      "Epoch 15/50 - Train Loss: 0.031169 - Val Loss: 0.049176\n",
      "Epoch 16/50 - Train Loss: 0.033518 - Val Loss: 0.045575\n",
      "Epoch 17/50 - Train Loss: 0.032774 - Val Loss: 0.046035\n",
      "Epoch 18/50 - Train Loss: 0.033819 - Val Loss: 0.053573\n",
      "Epoch 19/50 - Train Loss: 0.032430 - Val Loss: 0.045414\n",
      "Epoch 20/50 - Train Loss: 0.032843 - Val Loss: 0.052106\n",
      "Epoch 21/50 - Train Loss: 0.031249 - Val Loss: 0.044906\n",
      "Epoch 22/50 - Train Loss: 0.031391 - Val Loss: 0.045268\n",
      "Epoch 23/50 - Train Loss: 0.030156 - Val Loss: 0.051120\n",
      "Epoch 24/50 - Train Loss: 0.032028 - Val Loss: 0.045249\n",
      "Epoch 25/50 - Train Loss: 0.029639 - Val Loss: 0.044901\n",
      "Epoch 26/50 - Train Loss: 0.032824 - Val Loss: 0.048752\n",
      "Epoch 27/50 - Train Loss: 0.032888 - Val Loss: 0.044639\n",
      "Epoch 28/50 - Train Loss: 0.030893 - Val Loss: 0.046780\n",
      "Epoch 29/50 - Train Loss: 0.029403 - Val Loss: 0.047472\n",
      "Epoch 30/50 - Train Loss: 0.030342 - Val Loss: 0.047813\n",
      "Epoch 31/50 - Train Loss: 0.028242 - Val Loss: 0.044783\n",
      "Epoch 32/50 - Train Loss: 0.029811 - Val Loss: 0.050609\n",
      "Epoch 33/50 - Train Loss: 0.027439 - Val Loss: 0.044757\n",
      "Epoch 34/50 - Train Loss: 0.028828 - Val Loss: 0.045396\n",
      "Epoch 35/50 - Train Loss: 0.031165 - Val Loss: 0.045480\n",
      "Epoch 36/50 - Train Loss: 0.031364 - Val Loss: 0.047789\n",
      "Epoch 37/50 - Train Loss: 0.029175 - Val Loss: 0.045827\n",
      "Epoch 38/50 - Train Loss: 0.029495 - Val Loss: 0.046006\n",
      "Epoch 39/50 - Train Loss: 0.027380 - Val Loss: 0.046669\n",
      "Epoch 40/50 - Train Loss: 0.028755 - Val Loss: 0.053674\n",
      "Epoch 41/50 - Train Loss: 0.030251 - Val Loss: 0.045119\n",
      "Epoch 42/50 - Train Loss: 0.029907 - Val Loss: 0.046177\n",
      "Epoch 43/50 - Train Loss: 0.031561 - Val Loss: 0.050578\n",
      "Epoch 44/50 - Train Loss: 0.034022 - Val Loss: 0.046419\n",
      "Epoch 45/50 - Train Loss: 0.035217 - Val Loss: 0.044926\n",
      "Epoch 46/50 - Train Loss: 0.031566 - Val Loss: 0.045059\n",
      "Epoch 47/50 - Train Loss: 0.031694 - Val Loss: 0.045548\n",
      "Epoch 48/50 - Train Loss: 0.030326 - Val Loss: 0.045145\n",
      "Epoch 49/50 - Train Loss: 0.026262 - Val Loss: 0.048061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:35:45,844] Trial 54 finished with value: 0.044638662288586296 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 133, 'lr': 0.004261832898833212, 'weight_decay': 3.2975381965356764e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030894 - Val Loss: 0.059079\n",
      "Epoch 1/50 - Train Loss: 0.091068 - Val Loss: 0.068285\n",
      "Epoch 2/50 - Train Loss: 0.043981 - Val Loss: 0.063505\n",
      "Epoch 3/50 - Train Loss: 0.039216 - Val Loss: 0.052803\n",
      "Epoch 4/50 - Train Loss: 0.038985 - Val Loss: 0.054642\n",
      "Epoch 5/50 - Train Loss: 0.032175 - Val Loss: 0.049192\n",
      "Epoch 6/50 - Train Loss: 0.033623 - Val Loss: 0.051017\n",
      "Epoch 7/50 - Train Loss: 0.033132 - Val Loss: 0.049116\n",
      "Epoch 8/50 - Train Loss: 0.032764 - Val Loss: 0.048412\n",
      "Epoch 9/50 - Train Loss: 0.031805 - Val Loss: 0.046396\n",
      "Epoch 10/50 - Train Loss: 0.029878 - Val Loss: 0.046124\n",
      "Epoch 11/50 - Train Loss: 0.031714 - Val Loss: 0.046314\n",
      "Epoch 12/50 - Train Loss: 0.031075 - Val Loss: 0.046208\n",
      "Epoch 13/50 - Train Loss: 0.034871 - Val Loss: 0.051135\n",
      "Epoch 14/50 - Train Loss: 0.033107 - Val Loss: 0.054493\n",
      "Epoch 15/50 - Train Loss: 0.034310 - Val Loss: 0.047528\n",
      "Epoch 16/50 - Train Loss: 0.031374 - Val Loss: 0.046745\n",
      "Epoch 17/50 - Train Loss: 0.031727 - Val Loss: 0.045175\n",
      "Epoch 18/50 - Train Loss: 0.031337 - Val Loss: 0.052720\n",
      "Epoch 19/50 - Train Loss: 0.033349 - Val Loss: 0.053649\n",
      "Epoch 20/50 - Train Loss: 0.032976 - Val Loss: 0.048022\n",
      "Epoch 21/50 - Train Loss: 0.033311 - Val Loss: 0.045954\n",
      "Epoch 22/50 - Train Loss: 0.030874 - Val Loss: 0.045995\n",
      "Epoch 23/50 - Train Loss: 0.032455 - Val Loss: 0.049247\n",
      "Epoch 24/50 - Train Loss: 0.030614 - Val Loss: 0.045209\n",
      "Epoch 25/50 - Train Loss: 0.029626 - Val Loss: 0.046252\n",
      "Epoch 26/50 - Train Loss: 0.029205 - Val Loss: 0.045022\n",
      "Epoch 27/50 - Train Loss: 0.033927 - Val Loss: 0.049626\n",
      "Epoch 28/50 - Train Loss: 0.028924 - Val Loss: 0.045884\n",
      "Epoch 29/50 - Train Loss: 0.030245 - Val Loss: 0.045172\n",
      "Epoch 30/50 - Train Loss: 0.029056 - Val Loss: 0.045796\n",
      "Epoch 31/50 - Train Loss: 0.030282 - Val Loss: 0.048056\n",
      "Epoch 32/50 - Train Loss: 0.029760 - Val Loss: 0.045249\n",
      "Epoch 33/50 - Train Loss: 0.032862 - Val Loss: 0.050160\n",
      "Epoch 34/50 - Train Loss: 0.031594 - Val Loss: 0.045700\n",
      "Epoch 35/50 - Train Loss: 0.031755 - Val Loss: 0.045520\n",
      "Epoch 36/50 - Train Loss: 0.033170 - Val Loss: 0.045536\n",
      "Epoch 37/50 - Train Loss: 0.033544 - Val Loss: 0.046234\n",
      "Epoch 38/50 - Train Loss: 0.033171 - Val Loss: 0.048481\n",
      "Epoch 39/50 - Train Loss: 0.029550 - Val Loss: 0.047800\n",
      "Epoch 40/50 - Train Loss: 0.030763 - Val Loss: 0.052583\n",
      "Epoch 41/50 - Train Loss: 0.033841 - Val Loss: 0.053692\n",
      "Epoch 42/50 - Train Loss: 0.032313 - Val Loss: 0.052399\n",
      "Epoch 43/50 - Train Loss: 0.030310 - Val Loss: 0.045848\n",
      "Epoch 44/50 - Train Loss: 0.028058 - Val Loss: 0.047463\n",
      "Epoch 45/50 - Train Loss: 0.031872 - Val Loss: 0.046037\n",
      "Epoch 46/50 - Train Loss: 0.026517 - Val Loss: 0.046713\n",
      "Epoch 47/50 - Train Loss: 0.027973 - Val Loss: 0.045812\n",
      "Epoch 48/50 - Train Loss: 0.030088 - Val Loss: 0.046965\n",
      "Epoch 49/50 - Train Loss: 0.030842 - Val Loss: 0.049930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:36:14,717] Trial 55 finished with value: 0.045022214452425637 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 151, 'lr': 0.008713205957883932, 'weight_decay': 4.6361347008201875e-08, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029620 - Val Loss: 0.046082\n",
      "Epoch 1/50 - Train Loss: 0.137385 - Val Loss: 0.097425\n",
      "Epoch 2/50 - Train Loss: 0.051290 - Val Loss: 0.122703\n",
      "Epoch 3/50 - Train Loss: 0.054048 - Val Loss: 0.082742\n",
      "Epoch 4/50 - Train Loss: 0.048278 - Val Loss: 0.071289\n",
      "Epoch 5/50 - Train Loss: 0.041075 - Val Loss: 0.079818\n",
      "Epoch 6/50 - Train Loss: 0.045920 - Val Loss: 0.086647\n",
      "Epoch 7/50 - Train Loss: 0.044359 - Val Loss: 0.069059\n",
      "Epoch 8/50 - Train Loss: 0.035404 - Val Loss: 0.066699\n",
      "Epoch 9/50 - Train Loss: 0.040917 - Val Loss: 0.068441\n",
      "Epoch 10/50 - Train Loss: 0.032702 - Val Loss: 0.056329\n",
      "Epoch 11/50 - Train Loss: 0.032788 - Val Loss: 0.055741\n",
      "Epoch 12/50 - Train Loss: 0.040334 - Val Loss: 0.055716\n",
      "Epoch 13/50 - Train Loss: 0.031668 - Val Loss: 0.052338\n",
      "Epoch 14/50 - Train Loss: 0.031284 - Val Loss: 0.049871\n",
      "Epoch 15/50 - Train Loss: 0.030610 - Val Loss: 0.052445\n",
      "Epoch 16/50 - Train Loss: 0.029841 - Val Loss: 0.049170\n",
      "Epoch 17/50 - Train Loss: 0.037685 - Val Loss: 0.051492\n",
      "Epoch 18/50 - Train Loss: 0.030641 - Val Loss: 0.047965\n",
      "Epoch 19/50 - Train Loss: 0.037350 - Val Loss: 0.049455\n",
      "Epoch 20/50 - Train Loss: 0.038890 - Val Loss: 0.048667\n",
      "Epoch 21/50 - Train Loss: 0.029734 - Val Loss: 0.047388\n",
      "Epoch 22/50 - Train Loss: 0.038501 - Val Loss: 0.054459\n",
      "Epoch 23/50 - Train Loss: 0.030941 - Val Loss: 0.049378\n",
      "Epoch 24/50 - Train Loss: 0.029311 - Val Loss: 0.047219\n",
      "Epoch 25/50 - Train Loss: 0.029817 - Val Loss: 0.051961\n",
      "Epoch 26/50 - Train Loss: 0.030118 - Val Loss: 0.048352\n",
      "Epoch 27/50 - Train Loss: 0.037467 - Val Loss: 0.046824\n",
      "Epoch 28/50 - Train Loss: 0.037737 - Val Loss: 0.050508\n",
      "Epoch 29/50 - Train Loss: 0.036883 - Val Loss: 0.048919\n",
      "Epoch 30/50 - Train Loss: 0.029325 - Val Loss: 0.047747\n",
      "Epoch 31/50 - Train Loss: 0.036188 - Val Loss: 0.050279\n",
      "Epoch 32/50 - Train Loss: 0.035697 - Val Loss: 0.049177\n",
      "Epoch 33/50 - Train Loss: 0.030879 - Val Loss: 0.047580\n",
      "Epoch 34/50 - Train Loss: 0.029491 - Val Loss: 0.048331\n",
      "Epoch 35/50 - Train Loss: 0.028967 - Val Loss: 0.046878\n",
      "Epoch 36/50 - Train Loss: 0.028936 - Val Loss: 0.046501\n",
      "Epoch 37/50 - Train Loss: 0.029138 - Val Loss: 0.047533\n",
      "Epoch 38/50 - Train Loss: 0.029377 - Val Loss: 0.047296\n",
      "Epoch 39/50 - Train Loss: 0.037135 - Val Loss: 0.047804\n",
      "Epoch 40/50 - Train Loss: 0.037852 - Val Loss: 0.049446\n",
      "Epoch 41/50 - Train Loss: 0.037059 - Val Loss: 0.047264\n",
      "Epoch 42/50 - Train Loss: 0.037017 - Val Loss: 0.049563\n",
      "Epoch 43/50 - Train Loss: 0.029059 - Val Loss: 0.047751\n",
      "Epoch 44/50 - Train Loss: 0.029095 - Val Loss: 0.046057\n",
      "Epoch 45/50 - Train Loss: 0.030202 - Val Loss: 0.048446\n",
      "Epoch 46/50 - Train Loss: 0.038810 - Val Loss: 0.045949\n",
      "Epoch 47/50 - Train Loss: 0.029446 - Val Loss: 0.048604\n",
      "Epoch 48/50 - Train Loss: 0.036079 - Val Loss: 0.048229\n",
      "Epoch 49/50 - Train Loss: 0.029522 - Val Loss: 0.047670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:36:26,576] Trial 56 finished with value: 0.04594868794083595 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 162, 'lr': 0.005618007657011628, 'weight_decay': 0.00034540970787704883, 'batch_size': 64}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.036315 - Val Loss: 0.048814\n",
      "Epoch 1/50 - Train Loss: 0.097548 - Val Loss: 0.084302\n",
      "Epoch 2/50 - Train Loss: 0.060424 - Val Loss: 0.086671\n",
      "Epoch 3/50 - Train Loss: 0.042403 - Val Loss: 0.110936\n",
      "Epoch 4/50 - Train Loss: 0.044448 - Val Loss: 0.084557\n",
      "Epoch 5/50 - Train Loss: 0.038238 - Val Loss: 0.060978\n",
      "Epoch 6/50 - Train Loss: 0.041893 - Val Loss: 0.058472\n",
      "Epoch 7/50 - Train Loss: 0.034982 - Val Loss: 0.074404\n",
      "Epoch 8/50 - Train Loss: 0.033515 - Val Loss: 0.063893\n",
      "Epoch 9/50 - Train Loss: 0.032301 - Val Loss: 0.051036\n",
      "Epoch 10/50 - Train Loss: 0.039763 - Val Loss: 0.052791\n",
      "Epoch 11/50 - Train Loss: 0.031860 - Val Loss: 0.054273\n",
      "Epoch 12/50 - Train Loss: 0.030057 - Val Loss: 0.048899\n",
      "Epoch 13/50 - Train Loss: 0.037944 - Val Loss: 0.052030\n",
      "Epoch 14/50 - Train Loss: 0.033692 - Val Loss: 0.051002\n",
      "Epoch 15/50 - Train Loss: 0.031222 - Val Loss: 0.047647\n",
      "Epoch 16/50 - Train Loss: 0.033347 - Val Loss: 0.046957\n",
      "Epoch 17/50 - Train Loss: 0.029273 - Val Loss: 0.052550\n",
      "Epoch 18/50 - Train Loss: 0.035250 - Val Loss: 0.047182\n",
      "Epoch 19/50 - Train Loss: 0.028261 - Val Loss: 0.046571\n",
      "Epoch 20/50 - Train Loss: 0.033342 - Val Loss: 0.051801\n",
      "Epoch 21/50 - Train Loss: 0.033984 - Val Loss: 0.047547\n",
      "Epoch 22/50 - Train Loss: 0.028989 - Val Loss: 0.046218\n",
      "Epoch 23/50 - Train Loss: 0.032607 - Val Loss: 0.050203\n",
      "Epoch 24/50 - Train Loss: 0.033579 - Val Loss: 0.046897\n",
      "Epoch 25/50 - Train Loss: 0.033271 - Val Loss: 0.046595\n",
      "Epoch 26/50 - Train Loss: 0.030106 - Val Loss: 0.046102\n",
      "Epoch 27/50 - Train Loss: 0.033454 - Val Loss: 0.047967\n",
      "Epoch 28/50 - Train Loss: 0.031144 - Val Loss: 0.049005\n",
      "Epoch 29/50 - Train Loss: 0.030857 - Val Loss: 0.045844\n",
      "Epoch 30/50 - Train Loss: 0.029590 - Val Loss: 0.047913\n",
      "Epoch 31/50 - Train Loss: 0.033346 - Val Loss: 0.046766\n",
      "Epoch 32/50 - Train Loss: 0.031615 - Val Loss: 0.045299\n",
      "Epoch 33/50 - Train Loss: 0.034551 - Val Loss: 0.047098\n",
      "Epoch 34/50 - Train Loss: 0.031926 - Val Loss: 0.045823\n",
      "Epoch 35/50 - Train Loss: 0.028580 - Val Loss: 0.047137\n",
      "Epoch 36/50 - Train Loss: 0.030960 - Val Loss: 0.048599\n",
      "Epoch 37/50 - Train Loss: 0.029448 - Val Loss: 0.047658\n",
      "Epoch 38/50 - Train Loss: 0.027523 - Val Loss: 0.045763\n",
      "Epoch 39/50 - Train Loss: 0.030049 - Val Loss: 0.047293\n",
      "Epoch 40/50 - Train Loss: 0.034787 - Val Loss: 0.046632\n",
      "Epoch 41/50 - Train Loss: 0.032272 - Val Loss: 0.046568\n",
      "Epoch 42/50 - Train Loss: 0.032546 - Val Loss: 0.046892\n",
      "Epoch 43/50 - Train Loss: 0.029533 - Val Loss: 0.046198\n",
      "Epoch 44/50 - Train Loss: 0.036424 - Val Loss: 0.048716\n",
      "Epoch 45/50 - Train Loss: 0.034475 - Val Loss: 0.046503\n",
      "Epoch 46/50 - Train Loss: 0.030174 - Val Loss: 0.045872\n",
      "Epoch 47/50 - Train Loss: 0.031993 - Val Loss: 0.047457\n",
      "Epoch 48/50 - Train Loss: 0.033573 - Val Loss: 0.046538\n",
      "Epoch 49/50 - Train Loss: 0.031630 - Val Loss: 0.046267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:36:43,264] Trial 57 finished with value: 0.045299287885427475 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 98, 'lr': 0.0029340683041082644, 'weight_decay': 2.0383136771390015e-07, 'batch_size': 32}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032647 - Val Loss: 0.048020\n",
      "Epoch 1/50 - Train Loss: 0.098165 - Val Loss: 0.114743\n",
      "Epoch 2/50 - Train Loss: 0.048926 - Val Loss: 0.056631\n",
      "Epoch 3/50 - Train Loss: 0.040722 - Val Loss: 0.057427\n",
      "Epoch 4/50 - Train Loss: 0.035066 - Val Loss: 0.050521\n",
      "Epoch 5/50 - Train Loss: 0.033570 - Val Loss: 0.046994\n",
      "Epoch 6/50 - Train Loss: 0.031253 - Val Loss: 0.047302\n",
      "Epoch 7/50 - Train Loss: 0.034362 - Val Loss: 0.047057\n",
      "Epoch 8/50 - Train Loss: 0.030110 - Val Loss: 0.045491\n",
      "Epoch 9/50 - Train Loss: 0.029957 - Val Loss: 0.053748\n",
      "Epoch 10/50 - Train Loss: 0.034332 - Val Loss: 0.058074\n",
      "Epoch 11/50 - Train Loss: 0.032922 - Val Loss: 0.048660\n",
      "Epoch 12/50 - Train Loss: 0.031280 - Val Loss: 0.049098\n",
      "Epoch 13/50 - Train Loss: 0.032322 - Val Loss: 0.048920\n",
      "Epoch 14/50 - Train Loss: 0.031675 - Val Loss: 0.050295\n",
      "Epoch 15/50 - Train Loss: 0.030604 - Val Loss: 0.046567\n",
      "Epoch 16/50 - Train Loss: 0.029025 - Val Loss: 0.050989\n",
      "Epoch 17/50 - Train Loss: 0.030321 - Val Loss: 0.051366\n",
      "Epoch 18/50 - Train Loss: 0.030697 - Val Loss: 0.045078\n",
      "Epoch 19/50 - Train Loss: 0.031443 - Val Loss: 0.046539\n",
      "Epoch 20/50 - Train Loss: 0.034703 - Val Loss: 0.045634\n",
      "Epoch 21/50 - Train Loss: 0.029751 - Val Loss: 0.046173\n",
      "Epoch 22/50 - Train Loss: 0.030425 - Val Loss: 0.045451\n",
      "Epoch 23/50 - Train Loss: 0.031720 - Val Loss: 0.046711\n",
      "Epoch 24/50 - Train Loss: 0.028079 - Val Loss: 0.047856\n",
      "Epoch 25/50 - Train Loss: 0.034211 - Val Loss: 0.045903\n",
      "Epoch 26/50 - Train Loss: 0.031472 - Val Loss: 0.045453\n",
      "Epoch 27/50 - Train Loss: 0.033202 - Val Loss: 0.049283\n",
      "Epoch 28/50 - Train Loss: 0.031218 - Val Loss: 0.056169\n",
      "Epoch 29/50 - Train Loss: 0.030283 - Val Loss: 0.047226\n",
      "Epoch 30/50 - Train Loss: 0.031056 - Val Loss: 0.045056\n",
      "Epoch 31/50 - Train Loss: 0.030491 - Val Loss: 0.045201\n",
      "Epoch 32/50 - Train Loss: 0.030575 - Val Loss: 0.045441\n",
      "Epoch 33/50 - Train Loss: 0.032122 - Val Loss: 0.045565\n",
      "Epoch 34/50 - Train Loss: 0.031426 - Val Loss: 0.045666\n",
      "Epoch 35/50 - Train Loss: 0.030958 - Val Loss: 0.046638\n",
      "Epoch 36/50 - Train Loss: 0.030067 - Val Loss: 0.046416\n",
      "Epoch 37/50 - Train Loss: 0.030385 - Val Loss: 0.046326\n",
      "Epoch 38/50 - Train Loss: 0.030202 - Val Loss: 0.045997\n",
      "Epoch 39/50 - Train Loss: 0.029218 - Val Loss: 0.047722\n",
      "Epoch 40/50 - Train Loss: 0.032156 - Val Loss: 0.045466\n",
      "Epoch 41/50 - Train Loss: 0.029111 - Val Loss: 0.046709\n",
      "Epoch 42/50 - Train Loss: 0.029295 - Val Loss: 0.046381\n",
      "Epoch 43/50 - Train Loss: 0.031431 - Val Loss: 0.045445\n",
      "Epoch 44/50 - Train Loss: 0.029893 - Val Loss: 0.046831\n",
      "Epoch 45/50 - Train Loss: 0.029599 - Val Loss: 0.045722\n",
      "Epoch 46/50 - Train Loss: 0.031585 - Val Loss: 0.045050\n",
      "Epoch 47/50 - Train Loss: 0.029639 - Val Loss: 0.045692\n",
      "Epoch 48/50 - Train Loss: 0.028601 - Val Loss: 0.045780\n",
      "Epoch 49/50 - Train Loss: 0.029598 - Val Loss: 0.046060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:37:13,800] Trial 58 finished with value: 0.045049965381622314 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 125, 'lr': 0.0017763982922444133, 'weight_decay': 7.100496273318776e-06, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029406 - Val Loss: 0.045117\n",
      "Epoch 1/50 - Train Loss: 0.049177 - Val Loss: 0.105803\n",
      "Epoch 2/50 - Train Loss: 0.045247 - Val Loss: 0.051530\n",
      "Epoch 3/50 - Train Loss: 0.033913 - Val Loss: 0.066089\n",
      "Epoch 4/50 - Train Loss: 0.034372 - Val Loss: 0.048984\n",
      "Epoch 5/50 - Train Loss: 0.036050 - Val Loss: 0.049157\n",
      "Epoch 6/50 - Train Loss: 0.033284 - Val Loss: 0.048884\n",
      "Epoch 7/50 - Train Loss: 0.031344 - Val Loss: 0.045060\n",
      "Epoch 8/50 - Train Loss: 0.035384 - Val Loss: 0.049903\n",
      "Epoch 9/50 - Train Loss: 0.032642 - Val Loss: 0.051971\n",
      "Epoch 10/50 - Train Loss: 0.031580 - Val Loss: 0.046531\n",
      "Epoch 11/50 - Train Loss: 0.033504 - Val Loss: 0.051000\n",
      "Epoch 12/50 - Train Loss: 0.030224 - Val Loss: 0.050728\n",
      "Epoch 13/50 - Train Loss: 0.031376 - Val Loss: 0.044876\n",
      "Epoch 14/50 - Train Loss: 0.032942 - Val Loss: 0.047127\n",
      "Epoch 15/50 - Train Loss: 0.030462 - Val Loss: 0.045979\n",
      "Epoch 16/50 - Train Loss: 0.027229 - Val Loss: 0.045918\n",
      "Epoch 17/50 - Train Loss: 0.031164 - Val Loss: 0.046926\n",
      "Epoch 18/50 - Train Loss: 0.029941 - Val Loss: 0.048243\n",
      "Epoch 19/50 - Train Loss: 0.033451 - Val Loss: 0.048374\n",
      "Epoch 20/50 - Train Loss: 0.026987 - Val Loss: 0.045256\n",
      "Epoch 21/50 - Train Loss: 0.029340 - Val Loss: 0.045583\n",
      "Epoch 22/50 - Train Loss: 0.027372 - Val Loss: 0.045951\n",
      "Epoch 23/50 - Train Loss: 0.030670 - Val Loss: 0.048419\n",
      "Epoch 24/50 - Train Loss: 0.031388 - Val Loss: 0.046526\n",
      "Epoch 25/50 - Train Loss: 0.033457 - Val Loss: 0.044382\n",
      "Epoch 26/50 - Train Loss: 0.028598 - Val Loss: 0.045701\n",
      "Epoch 27/50 - Train Loss: 0.034647 - Val Loss: 0.047657\n",
      "Epoch 28/50 - Train Loss: 0.032138 - Val Loss: 0.049810\n",
      "Epoch 29/50 - Train Loss: 0.030314 - Val Loss: 0.047160\n",
      "Epoch 30/50 - Train Loss: 0.029922 - Val Loss: 0.045448\n",
      "Epoch 31/50 - Train Loss: 0.032873 - Val Loss: 0.044522\n",
      "Epoch 32/50 - Train Loss: 0.029816 - Val Loss: 0.047856\n",
      "Epoch 33/50 - Train Loss: 0.031312 - Val Loss: 0.050529\n",
      "Epoch 34/50 - Train Loss: 0.031027 - Val Loss: 0.045557\n",
      "Epoch 35/50 - Train Loss: 0.027592 - Val Loss: 0.044446\n",
      "Epoch 36/50 - Train Loss: 0.030971 - Val Loss: 0.045908\n",
      "Epoch 37/50 - Train Loss: 0.035272 - Val Loss: 0.048861\n",
      "Epoch 38/50 - Train Loss: 0.031119 - Val Loss: 0.047878\n",
      "Epoch 39/50 - Train Loss: 0.029314 - Val Loss: 0.046893\n",
      "Epoch 40/50 - Train Loss: 0.033043 - Val Loss: 0.044184\n",
      "Epoch 41/50 - Train Loss: 0.032730 - Val Loss: 0.045568\n",
      "Epoch 42/50 - Train Loss: 0.027294 - Val Loss: 0.046981\n",
      "Epoch 43/50 - Train Loss: 0.030928 - Val Loss: 0.045797\n",
      "Epoch 44/50 - Train Loss: 0.028075 - Val Loss: 0.045140\n",
      "Epoch 45/50 - Train Loss: 0.026708 - Val Loss: 0.044880\n",
      "Epoch 46/50 - Train Loss: 0.031149 - Val Loss: 0.047772\n",
      "Epoch 47/50 - Train Loss: 0.028208 - Val Loss: 0.050324\n",
      "Epoch 48/50 - Train Loss: 0.029125 - Val Loss: 0.044905\n",
      "Epoch 49/50 - Train Loss: 0.028939 - Val Loss: 0.046458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:37:36,293] Trial 59 finished with value: 0.04418381117284298 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 106, 'lr': 0.0035593267733766685, 'weight_decay': 1.3837190001434318e-06, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029656 - Val Loss: 0.050826\n",
      "Epoch 1/50 - Train Loss: 0.060781 - Val Loss: 0.051012\n",
      "Epoch 2/50 - Train Loss: 0.035033 - Val Loss: 0.049284\n",
      "Epoch 3/50 - Train Loss: 0.032856 - Val Loss: 0.045633\n",
      "Epoch 4/50 - Train Loss: 0.032813 - Val Loss: 0.048982\n",
      "Epoch 5/50 - Train Loss: 0.031004 - Val Loss: 0.047367\n",
      "Epoch 6/50 - Train Loss: 0.032289 - Val Loss: 0.045328\n",
      "Epoch 7/50 - Train Loss: 0.034405 - Val Loss: 0.050938\n",
      "Epoch 8/50 - Train Loss: 0.033791 - Val Loss: 0.049068\n",
      "Epoch 9/50 - Train Loss: 0.031429 - Val Loss: 0.051983\n",
      "Epoch 10/50 - Train Loss: 0.031348 - Val Loss: 0.053683\n",
      "Epoch 11/50 - Train Loss: 0.032767 - Val Loss: 0.045949\n",
      "Epoch 12/50 - Train Loss: 0.032051 - Val Loss: 0.045172\n",
      "Epoch 13/50 - Train Loss: 0.034194 - Val Loss: 0.053635\n",
      "Epoch 14/50 - Train Loss: 0.032850 - Val Loss: 0.050926\n",
      "Epoch 15/50 - Train Loss: 0.033124 - Val Loss: 0.046084\n",
      "Epoch 16/50 - Train Loss: 0.029888 - Val Loss: 0.049734\n",
      "Epoch 17/50 - Train Loss: 0.032070 - Val Loss: 0.053790\n",
      "Epoch 18/50 - Train Loss: 0.032540 - Val Loss: 0.065626\n",
      "Epoch 19/50 - Train Loss: 0.034659 - Val Loss: 0.046141\n",
      "Epoch 20/50 - Train Loss: 0.031438 - Val Loss: 0.044495\n",
      "Epoch 21/50 - Train Loss: 0.031680 - Val Loss: 0.046558\n",
      "Epoch 22/50 - Train Loss: 0.029898 - Val Loss: 0.045071\n",
      "Epoch 23/50 - Train Loss: 0.031197 - Val Loss: 0.045876\n",
      "Epoch 24/50 - Train Loss: 0.029550 - Val Loss: 0.046018\n",
      "Epoch 25/50 - Train Loss: 0.032180 - Val Loss: 0.047522\n",
      "Epoch 26/50 - Train Loss: 0.031444 - Val Loss: 0.046736\n",
      "Epoch 27/50 - Train Loss: 0.029486 - Val Loss: 0.045669\n",
      "Epoch 28/50 - Train Loss: 0.032246 - Val Loss: 0.049738\n",
      "Epoch 29/50 - Train Loss: 0.030497 - Val Loss: 0.048553\n",
      "Epoch 30/50 - Train Loss: 0.030967 - Val Loss: 0.045637\n",
      "Epoch 31/50 - Train Loss: 0.027641 - Val Loss: 0.047326\n",
      "Epoch 32/50 - Train Loss: 0.028328 - Val Loss: 0.045110\n",
      "Epoch 33/50 - Train Loss: 0.029413 - Val Loss: 0.045959\n",
      "Epoch 34/50 - Train Loss: 0.029193 - Val Loss: 0.046441\n",
      "Epoch 35/50 - Train Loss: 0.029367 - Val Loss: 0.047169\n",
      "Epoch 36/50 - Train Loss: 0.030378 - Val Loss: 0.057042\n",
      "Epoch 37/50 - Train Loss: 0.031414 - Val Loss: 0.046444\n",
      "Epoch 38/50 - Train Loss: 0.031716 - Val Loss: 0.047255\n",
      "Epoch 39/50 - Train Loss: 0.031689 - Val Loss: 0.049626\n",
      "Epoch 40/50 - Train Loss: 0.029086 - Val Loss: 0.048464\n",
      "Epoch 41/50 - Train Loss: 0.029894 - Val Loss: 0.047422\n",
      "Epoch 42/50 - Train Loss: 0.032375 - Val Loss: 0.047715\n",
      "Epoch 43/50 - Train Loss: 0.029989 - Val Loss: 0.048189\n",
      "Epoch 44/50 - Train Loss: 0.029233 - Val Loss: 0.049478\n",
      "Epoch 45/50 - Train Loss: 0.028502 - Val Loss: 0.047440\n",
      "Epoch 46/50 - Train Loss: 0.030173 - Val Loss: 0.048459\n",
      "Epoch 47/50 - Train Loss: 0.029750 - Val Loss: 0.047430\n",
      "Epoch 48/50 - Train Loss: 0.026345 - Val Loss: 0.047406\n",
      "Epoch 49/50 - Train Loss: 0.029637 - Val Loss: 0.046769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:38:13,232] Trial 60 finished with value: 0.04449542239308357 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 143, 'lr': 0.004535251691922147, 'weight_decay': 1.3434948692628426e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028660 - Val Loss: 0.044857\n",
      "Epoch 1/50 - Train Loss: 0.058956 - Val Loss: 0.059428\n",
      "Epoch 2/50 - Train Loss: 0.037929 - Val Loss: 0.049062\n",
      "Epoch 3/50 - Train Loss: 0.034368 - Val Loss: 0.047771\n",
      "Epoch 4/50 - Train Loss: 0.031269 - Val Loss: 0.049513\n",
      "Epoch 5/50 - Train Loss: 0.033219 - Val Loss: 0.047806\n",
      "Epoch 6/50 - Train Loss: 0.033571 - Val Loss: 0.044903\n",
      "Epoch 7/50 - Train Loss: 0.033212 - Val Loss: 0.046005\n",
      "Epoch 8/50 - Train Loss: 0.035596 - Val Loss: 0.044918\n",
      "Epoch 9/50 - Train Loss: 0.032096 - Val Loss: 0.044786\n",
      "Epoch 10/50 - Train Loss: 0.032020 - Val Loss: 0.044926\n",
      "Epoch 11/50 - Train Loss: 0.032609 - Val Loss: 0.051523\n",
      "Epoch 12/50 - Train Loss: 0.031367 - Val Loss: 0.046837\n",
      "Epoch 13/50 - Train Loss: 0.031166 - Val Loss: 0.044891\n",
      "Epoch 14/50 - Train Loss: 0.031339 - Val Loss: 0.048465\n",
      "Epoch 15/50 - Train Loss: 0.032436 - Val Loss: 0.045140\n",
      "Epoch 16/50 - Train Loss: 0.034810 - Val Loss: 0.044375\n",
      "Epoch 17/50 - Train Loss: 0.030834 - Val Loss: 0.051159\n",
      "Epoch 18/50 - Train Loss: 0.029374 - Val Loss: 0.045295\n",
      "Epoch 19/50 - Train Loss: 0.031260 - Val Loss: 0.044695\n",
      "Epoch 20/50 - Train Loss: 0.029469 - Val Loss: 0.044897\n",
      "Epoch 21/50 - Train Loss: 0.030552 - Val Loss: 0.060325\n",
      "Epoch 22/50 - Train Loss: 0.032528 - Val Loss: 0.062879\n",
      "Epoch 23/50 - Train Loss: 0.029238 - Val Loss: 0.046135\n",
      "Epoch 24/50 - Train Loss: 0.028555 - Val Loss: 0.044131\n",
      "Epoch 25/50 - Train Loss: 0.028080 - Val Loss: 0.046817\n",
      "Epoch 26/50 - Train Loss: 0.036959 - Val Loss: 0.045415\n",
      "Epoch 27/50 - Train Loss: 0.032682 - Val Loss: 0.050200\n",
      "Epoch 28/50 - Train Loss: 0.030900 - Val Loss: 0.047398\n",
      "Epoch 29/50 - Train Loss: 0.031709 - Val Loss: 0.044988\n",
      "Epoch 30/50 - Train Loss: 0.029022 - Val Loss: 0.045671\n",
      "Epoch 31/50 - Train Loss: 0.032048 - Val Loss: 0.044297\n",
      "Epoch 32/50 - Train Loss: 0.030238 - Val Loss: 0.044985\n",
      "Epoch 33/50 - Train Loss: 0.028433 - Val Loss: 0.045733\n",
      "Epoch 34/50 - Train Loss: 0.033495 - Val Loss: 0.043900\n",
      "Epoch 35/50 - Train Loss: 0.028848 - Val Loss: 0.044973\n",
      "Epoch 36/50 - Train Loss: 0.027583 - Val Loss: 0.044612\n",
      "Epoch 37/50 - Train Loss: 0.028889 - Val Loss: 0.044696\n",
      "Epoch 38/50 - Train Loss: 0.030474 - Val Loss: 0.044276\n",
      "Epoch 39/50 - Train Loss: 0.030002 - Val Loss: 0.045510\n",
      "Epoch 40/50 - Train Loss: 0.027894 - Val Loss: 0.048733\n",
      "Epoch 41/50 - Train Loss: 0.030013 - Val Loss: 0.048748\n",
      "Epoch 42/50 - Train Loss: 0.030167 - Val Loss: 0.045066\n",
      "Epoch 43/50 - Train Loss: 0.029941 - Val Loss: 0.044563\n",
      "Epoch 44/50 - Train Loss: 0.028367 - Val Loss: 0.045366\n",
      "Epoch 45/50 - Train Loss: 0.029177 - Val Loss: 0.044029\n",
      "Epoch 46/50 - Train Loss: 0.028957 - Val Loss: 0.044739\n",
      "Epoch 47/50 - Train Loss: 0.029323 - Val Loss: 0.046566\n",
      "Epoch 48/50 - Train Loss: 0.027633 - Val Loss: 0.045661\n",
      "Epoch 49/50 - Train Loss: 0.032152 - Val Loss: 0.051463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:38:37,366] Trial 61 finished with value: 0.043899515022834144 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 96, 'lr': 0.0060884378990644, 'weight_decay': 5.692151883392809e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028357 - Val Loss: 0.044943\n",
      "Epoch 1/50 - Train Loss: 0.049290 - Val Loss: 0.055401\n",
      "Epoch 2/50 - Train Loss: 0.034098 - Val Loss: 0.049140\n",
      "Epoch 3/50 - Train Loss: 0.034110 - Val Loss: 0.050103\n",
      "Epoch 4/50 - Train Loss: 0.032267 - Val Loss: 0.046694\n",
      "Epoch 5/50 - Train Loss: 0.030219 - Val Loss: 0.047066\n",
      "Epoch 6/50 - Train Loss: 0.033076 - Val Loss: 0.062379\n",
      "Epoch 7/50 - Train Loss: 0.036648 - Val Loss: 0.059591\n",
      "Epoch 8/50 - Train Loss: 0.033582 - Val Loss: 0.052269\n",
      "Epoch 9/50 - Train Loss: 0.032465 - Val Loss: 0.045060\n",
      "Epoch 10/50 - Train Loss: 0.030471 - Val Loss: 0.045287\n",
      "Epoch 11/50 - Train Loss: 0.031456 - Val Loss: 0.048306\n",
      "Epoch 12/50 - Train Loss: 0.030943 - Val Loss: 0.044725\n",
      "Epoch 13/50 - Train Loss: 0.030336 - Val Loss: 0.044197\n",
      "Epoch 14/50 - Train Loss: 0.034449 - Val Loss: 0.045008\n",
      "Epoch 15/50 - Train Loss: 0.032130 - Val Loss: 0.048176\n",
      "Epoch 16/50 - Train Loss: 0.032687 - Val Loss: 0.048452\n",
      "Epoch 17/50 - Train Loss: 0.030388 - Val Loss: 0.049221\n",
      "Epoch 18/50 - Train Loss: 0.031320 - Val Loss: 0.044608\n",
      "Epoch 19/50 - Train Loss: 0.029793 - Val Loss: 0.046048\n",
      "Epoch 20/50 - Train Loss: 0.029307 - Val Loss: 0.044629\n",
      "Epoch 21/50 - Train Loss: 0.035078 - Val Loss: 0.046359\n",
      "Epoch 22/50 - Train Loss: 0.031509 - Val Loss: 0.044076\n",
      "Epoch 23/50 - Train Loss: 0.033244 - Val Loss: 0.044373\n",
      "Epoch 24/50 - Train Loss: 0.026836 - Val Loss: 0.048322\n",
      "Epoch 25/50 - Train Loss: 0.030340 - Val Loss: 0.043953\n",
      "Epoch 26/50 - Train Loss: 0.029511 - Val Loss: 0.044432\n",
      "Epoch 27/50 - Train Loss: 0.030526 - Val Loss: 0.043940\n",
      "Epoch 28/50 - Train Loss: 0.030697 - Val Loss: 0.047126\n",
      "Epoch 29/50 - Train Loss: 0.029360 - Val Loss: 0.053818\n",
      "Epoch 30/50 - Train Loss: 0.034923 - Val Loss: 0.059440\n",
      "Epoch 31/50 - Train Loss: 0.031947 - Val Loss: 0.048052\n",
      "Epoch 32/50 - Train Loss: 0.030369 - Val Loss: 0.044915\n",
      "Epoch 33/50 - Train Loss: 0.030518 - Val Loss: 0.046885\n",
      "Epoch 34/50 - Train Loss: 0.030772 - Val Loss: 0.066123\n",
      "Epoch 35/50 - Train Loss: 0.031354 - Val Loss: 0.049381\n",
      "Epoch 36/50 - Train Loss: 0.032900 - Val Loss: 0.049561\n",
      "Epoch 37/50 - Train Loss: 0.031519 - Val Loss: 0.047426\n",
      "Epoch 38/50 - Train Loss: 0.030590 - Val Loss: 0.046354\n",
      "Epoch 39/50 - Train Loss: 0.026189 - Val Loss: 0.045956\n",
      "Epoch 40/50 - Train Loss: 0.030654 - Val Loss: 0.047034\n",
      "Epoch 41/50 - Train Loss: 0.028205 - Val Loss: 0.045014\n",
      "Epoch 42/50 - Train Loss: 0.031309 - Val Loss: 0.048034\n",
      "Epoch 43/50 - Train Loss: 0.030597 - Val Loss: 0.049039\n",
      "Epoch 44/50 - Train Loss: 0.030436 - Val Loss: 0.045403\n",
      "Epoch 45/50 - Train Loss: 0.028808 - Val Loss: 0.044720\n",
      "Epoch 46/50 - Train Loss: 0.031690 - Val Loss: 0.046873\n",
      "Epoch 47/50 - Train Loss: 0.029953 - Val Loss: 0.048162\n",
      "Epoch 48/50 - Train Loss: 0.028513 - Val Loss: 0.044603\n",
      "Epoch 49/50 - Train Loss: 0.027195 - Val Loss: 0.048030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:39:00,601] Trial 62 finished with value: 0.04394016166528066 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 105, 'lr': 0.007893161491457907, 'weight_decay': 3.7998712672032355e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027983 - Val Loss: 0.050109\n",
      "Epoch 1/50 - Train Loss: 0.062492 - Val Loss: 0.111381\n",
      "Epoch 2/50 - Train Loss: 0.041190 - Val Loss: 0.074324\n",
      "Epoch 3/50 - Train Loss: 0.032937 - Val Loss: 0.051541\n",
      "Epoch 4/50 - Train Loss: 0.034406 - Val Loss: 0.050074\n",
      "Epoch 5/50 - Train Loss: 0.036572 - Val Loss: 0.058116\n",
      "Epoch 6/50 - Train Loss: 0.033756 - Val Loss: 0.057151\n",
      "Epoch 7/50 - Train Loss: 0.031550 - Val Loss: 0.051005\n",
      "Epoch 8/50 - Train Loss: 0.032529 - Val Loss: 0.049328\n",
      "Epoch 9/50 - Train Loss: 0.031558 - Val Loss: 0.051027\n",
      "Epoch 10/50 - Train Loss: 0.032023 - Val Loss: 0.059451\n",
      "Epoch 11/50 - Train Loss: 0.030352 - Val Loss: 0.054942\n",
      "Epoch 12/50 - Train Loss: 0.032333 - Val Loss: 0.059245\n",
      "Epoch 13/50 - Train Loss: 0.032323 - Val Loss: 0.049442\n",
      "Epoch 14/50 - Train Loss: 0.030550 - Val Loss: 0.047011\n",
      "Epoch 15/50 - Train Loss: 0.031598 - Val Loss: 0.047521\n",
      "Epoch 16/50 - Train Loss: 0.031195 - Val Loss: 0.048269\n",
      "Epoch 17/50 - Train Loss: 0.033794 - Val Loss: 0.048283\n",
      "Epoch 18/50 - Train Loss: 0.032437 - Val Loss: 0.045162\n",
      "Epoch 19/50 - Train Loss: 0.029009 - Val Loss: 0.044940\n",
      "Epoch 20/50 - Train Loss: 0.029427 - Val Loss: 0.048822\n",
      "Epoch 21/50 - Train Loss: 0.032583 - Val Loss: 0.053153\n",
      "Epoch 22/50 - Train Loss: 0.031068 - Val Loss: 0.045504\n",
      "Epoch 23/50 - Train Loss: 0.027993 - Val Loss: 0.047199\n",
      "Epoch 24/50 - Train Loss: 0.032210 - Val Loss: 0.051109\n",
      "Epoch 25/50 - Train Loss: 0.029626 - Val Loss: 0.049034\n",
      "Epoch 26/50 - Train Loss: 0.030557 - Val Loss: 0.047872\n",
      "Epoch 27/50 - Train Loss: 0.029805 - Val Loss: 0.046110\n",
      "Epoch 28/50 - Train Loss: 0.027080 - Val Loss: 0.044892\n",
      "Epoch 29/50 - Train Loss: 0.031621 - Val Loss: 0.046089\n",
      "Epoch 30/50 - Train Loss: 0.032925 - Val Loss: 0.050990\n",
      "Epoch 31/50 - Train Loss: 0.027913 - Val Loss: 0.046421\n",
      "Epoch 32/50 - Train Loss: 0.029895 - Val Loss: 0.044117\n",
      "Epoch 33/50 - Train Loss: 0.029783 - Val Loss: 0.050218\n",
      "Epoch 34/50 - Train Loss: 0.030619 - Val Loss: 0.047904\n",
      "Epoch 35/50 - Train Loss: 0.029889 - Val Loss: 0.045040\n",
      "Epoch 36/50 - Train Loss: 0.030727 - Val Loss: 0.044754\n",
      "Epoch 37/50 - Train Loss: 0.029840 - Val Loss: 0.046067\n",
      "Epoch 38/50 - Train Loss: 0.031142 - Val Loss: 0.046969\n",
      "Epoch 39/50 - Train Loss: 0.031354 - Val Loss: 0.047871\n",
      "Epoch 40/50 - Train Loss: 0.032190 - Val Loss: 0.047571\n",
      "Epoch 41/50 - Train Loss: 0.031721 - Val Loss: 0.045177\n",
      "Epoch 42/50 - Train Loss: 0.028509 - Val Loss: 0.044874\n",
      "Epoch 43/50 - Train Loss: 0.030810 - Val Loss: 0.044660\n",
      "Epoch 44/50 - Train Loss: 0.030136 - Val Loss: 0.047660\n",
      "Epoch 45/50 - Train Loss: 0.031620 - Val Loss: 0.047387\n",
      "Epoch 46/50 - Train Loss: 0.029121 - Val Loss: 0.044863\n",
      "Epoch 47/50 - Train Loss: 0.028338 - Val Loss: 0.044391\n",
      "Epoch 48/50 - Train Loss: 0.031074 - Val Loss: 0.046711\n",
      "Epoch 49/50 - Train Loss: 0.031351 - Val Loss: 0.047685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:39:20,480] Trial 63 finished with value: 0.04411739483475685 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 120, 'lr': 0.002576255882100503, 'weight_decay': 2.7848067642093166e-06, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030280 - Val Loss: 0.046410\n",
      "Epoch 1/50 - Train Loss: 0.086929 - Val Loss: 0.124540\n",
      "Epoch 2/50 - Train Loss: 0.043820 - Val Loss: 0.073822\n",
      "Epoch 3/50 - Train Loss: 0.035554 - Val Loss: 0.061850\n",
      "Epoch 4/50 - Train Loss: 0.034056 - Val Loss: 0.048803\n",
      "Epoch 5/50 - Train Loss: 0.032948 - Val Loss: 0.060444\n",
      "Epoch 6/50 - Train Loss: 0.035735 - Val Loss: 0.069729\n",
      "Epoch 7/50 - Train Loss: 0.035201 - Val Loss: 0.050722\n",
      "Epoch 8/50 - Train Loss: 0.030209 - Val Loss: 0.051104\n",
      "Epoch 9/50 - Train Loss: 0.033017 - Val Loss: 0.051397\n",
      "Epoch 10/50 - Train Loss: 0.031041 - Val Loss: 0.053818\n",
      "Epoch 11/50 - Train Loss: 0.029806 - Val Loss: 0.046497\n",
      "Epoch 12/50 - Train Loss: 0.030176 - Val Loss: 0.047028\n",
      "Epoch 13/50 - Train Loss: 0.029172 - Val Loss: 0.046386\n",
      "Epoch 14/50 - Train Loss: 0.032177 - Val Loss: 0.046932\n",
      "Epoch 15/50 - Train Loss: 0.033491 - Val Loss: 0.044863\n",
      "Epoch 16/50 - Train Loss: 0.029999 - Val Loss: 0.044664\n",
      "Epoch 17/50 - Train Loss: 0.032090 - Val Loss: 0.047575\n",
      "Epoch 18/50 - Train Loss: 0.030899 - Val Loss: 0.045770\n",
      "Epoch 19/50 - Train Loss: 0.033573 - Val Loss: 0.052120\n",
      "Epoch 20/50 - Train Loss: 0.029946 - Val Loss: 0.047178\n",
      "Epoch 21/50 - Train Loss: 0.028969 - Val Loss: 0.044441\n",
      "Epoch 22/50 - Train Loss: 0.030055 - Val Loss: 0.047772\n",
      "Epoch 23/50 - Train Loss: 0.030645 - Val Loss: 0.047241\n",
      "Epoch 24/50 - Train Loss: 0.032017 - Val Loss: 0.048260\n",
      "Epoch 25/50 - Train Loss: 0.029066 - Val Loss: 0.044716\n",
      "Epoch 26/50 - Train Loss: 0.030923 - Val Loss: 0.049727\n",
      "Epoch 27/50 - Train Loss: 0.030181 - Val Loss: 0.045508\n",
      "Epoch 28/50 - Train Loss: 0.030635 - Val Loss: 0.045980\n",
      "Epoch 29/50 - Train Loss: 0.029114 - Val Loss: 0.044477\n",
      "Epoch 30/50 - Train Loss: 0.031631 - Val Loss: 0.046441\n",
      "Epoch 31/50 - Train Loss: 0.030073 - Val Loss: 0.045198\n",
      "Epoch 32/50 - Train Loss: 0.029407 - Val Loss: 0.045543\n",
      "Epoch 33/50 - Train Loss: 0.027886 - Val Loss: 0.045964\n",
      "Epoch 34/50 - Train Loss: 0.029458 - Val Loss: 0.046654\n",
      "Epoch 35/50 - Train Loss: 0.029150 - Val Loss: 0.045070\n",
      "Epoch 36/50 - Train Loss: 0.029866 - Val Loss: 0.046154\n",
      "Epoch 37/50 - Train Loss: 0.029550 - Val Loss: 0.044307\n",
      "Epoch 38/50 - Train Loss: 0.032336 - Val Loss: 0.059730\n",
      "Epoch 39/50 - Train Loss: 0.032494 - Val Loss: 0.045292\n",
      "Epoch 40/50 - Train Loss: 0.031336 - Val Loss: 0.043810\n",
      "Epoch 41/50 - Train Loss: 0.028464 - Val Loss: 0.044363\n",
      "Epoch 42/50 - Train Loss: 0.029931 - Val Loss: 0.048899\n",
      "Epoch 43/50 - Train Loss: 0.029794 - Val Loss: 0.044508\n",
      "Epoch 44/50 - Train Loss: 0.029032 - Val Loss: 0.043601\n",
      "Epoch 45/50 - Train Loss: 0.031047 - Val Loss: 0.044235\n",
      "Epoch 46/50 - Train Loss: 0.030390 - Val Loss: 0.049391\n",
      "Epoch 47/50 - Train Loss: 0.029033 - Val Loss: 0.044023\n",
      "Epoch 48/50 - Train Loss: 0.028617 - Val Loss: 0.045407\n",
      "Epoch 49/50 - Train Loss: 0.029765 - Val Loss: 0.045415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:39:55,045] Trial 64 finished with value: 0.04360143095254898 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 87, 'lr': 0.0049841066948095765, 'weight_decay': 4.416853064385256e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026859 - Val Loss: 0.046547\n",
      "Epoch 1/50 - Train Loss: 0.060435 - Val Loss: 0.069861\n",
      "Epoch 2/50 - Train Loss: 0.047283 - Val Loss: 0.088701\n",
      "Epoch 3/50 - Train Loss: 0.044102 - Val Loss: 0.068257\n",
      "Epoch 4/50 - Train Loss: 0.037065 - Val Loss: 0.061810\n",
      "Epoch 5/50 - Train Loss: 0.031601 - Val Loss: 0.071361\n",
      "Epoch 6/50 - Train Loss: 0.033277 - Val Loss: 0.058181\n",
      "Epoch 7/50 - Train Loss: 0.033748 - Val Loss: 0.048154\n",
      "Epoch 8/50 - Train Loss: 0.031142 - Val Loss: 0.045844\n",
      "Epoch 9/50 - Train Loss: 0.033768 - Val Loss: 0.046265\n",
      "Epoch 10/50 - Train Loss: 0.031521 - Val Loss: 0.045805\n",
      "Epoch 11/50 - Train Loss: 0.033585 - Val Loss: 0.045493\n",
      "Epoch 12/50 - Train Loss: 0.034117 - Val Loss: 0.052834\n",
      "Epoch 13/50 - Train Loss: 0.034060 - Val Loss: 0.051808\n",
      "Epoch 14/50 - Train Loss: 0.033059 - Val Loss: 0.046491\n",
      "Epoch 15/50 - Train Loss: 0.027020 - Val Loss: 0.047283\n",
      "Epoch 16/50 - Train Loss: 0.032114 - Val Loss: 0.048048\n",
      "Epoch 17/50 - Train Loss: 0.032284 - Val Loss: 0.050039\n",
      "Epoch 18/50 - Train Loss: 0.036856 - Val Loss: 0.046223\n",
      "Epoch 19/50 - Train Loss: 0.033399 - Val Loss: 0.045176\n",
      "Epoch 20/50 - Train Loss: 0.029928 - Val Loss: 0.045551\n",
      "Epoch 21/50 - Train Loss: 0.030425 - Val Loss: 0.045123\n",
      "Epoch 22/50 - Train Loss: 0.030554 - Val Loss: 0.050022\n",
      "Epoch 23/50 - Train Loss: 0.028481 - Val Loss: 0.046183\n",
      "Epoch 24/50 - Train Loss: 0.030563 - Val Loss: 0.044623\n",
      "Epoch 25/50 - Train Loss: 0.030562 - Val Loss: 0.049135\n",
      "Epoch 26/50 - Train Loss: 0.031811 - Val Loss: 0.043974\n",
      "Epoch 27/50 - Train Loss: 0.030654 - Val Loss: 0.045353\n",
      "Epoch 28/50 - Train Loss: 0.032512 - Val Loss: 0.044941\n",
      "Epoch 29/50 - Train Loss: 0.029248 - Val Loss: 0.051115\n",
      "Epoch 30/50 - Train Loss: 0.029405 - Val Loss: 0.045621\n",
      "Epoch 31/50 - Train Loss: 0.033454 - Val Loss: 0.044306\n",
      "Epoch 32/50 - Train Loss: 0.031869 - Val Loss: 0.045778\n",
      "Epoch 33/50 - Train Loss: 0.029292 - Val Loss: 0.046200\n",
      "Epoch 34/50 - Train Loss: 0.030479 - Val Loss: 0.044644\n",
      "Epoch 35/50 - Train Loss: 0.029492 - Val Loss: 0.047930\n",
      "Epoch 36/50 - Train Loss: 0.029383 - Val Loss: 0.045390\n",
      "Epoch 37/50 - Train Loss: 0.031641 - Val Loss: 0.045236\n",
      "Epoch 38/50 - Train Loss: 0.029010 - Val Loss: 0.045384\n",
      "Epoch 39/50 - Train Loss: 0.030168 - Val Loss: 0.044347\n",
      "Epoch 40/50 - Train Loss: 0.030790 - Val Loss: 0.046540\n",
      "Epoch 41/50 - Train Loss: 0.027197 - Val Loss: 0.044531\n",
      "Epoch 42/50 - Train Loss: 0.030480 - Val Loss: 0.045692\n",
      "Epoch 43/50 - Train Loss: 0.033453 - Val Loss: 0.047890\n",
      "Epoch 44/50 - Train Loss: 0.030710 - Val Loss: 0.045775\n",
      "Epoch 45/50 - Train Loss: 0.028588 - Val Loss: 0.045870\n",
      "Epoch 46/50 - Train Loss: 0.031662 - Val Loss: 0.045457\n",
      "Epoch 47/50 - Train Loss: 0.029014 - Val Loss: 0.052061\n",
      "Epoch 48/50 - Train Loss: 0.030586 - Val Loss: 0.044180\n",
      "Epoch 49/50 - Train Loss: 0.025553 - Val Loss: 0.044846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:40:18,085] Trial 65 finished with value: 0.043973926454782486 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 71, 'lr': 0.004595184034638663, 'weight_decay': 2.262801625004933e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030712 - Val Loss: 0.044831\n",
      "Epoch 1/50 - Train Loss: 0.112202 - Val Loss: 0.071912\n",
      "Epoch 2/50 - Train Loss: 0.045008 - Val Loss: 0.073024\n",
      "Epoch 3/50 - Train Loss: 0.040865 - Val Loss: 0.058508\n",
      "Epoch 4/50 - Train Loss: 0.037281 - Val Loss: 0.051392\n",
      "Epoch 5/50 - Train Loss: 0.035244 - Val Loss: 0.055578\n",
      "Epoch 6/50 - Train Loss: 0.033382 - Val Loss: 0.046471\n",
      "Epoch 7/50 - Train Loss: 0.033116 - Val Loss: 0.049547\n",
      "Epoch 8/50 - Train Loss: 0.031597 - Val Loss: 0.046564\n",
      "Epoch 9/50 - Train Loss: 0.035117 - Val Loss: 0.048061\n",
      "Epoch 10/50 - Train Loss: 0.031802 - Val Loss: 0.052760\n",
      "Epoch 11/50 - Train Loss: 0.032820 - Val Loss: 0.049259\n",
      "Epoch 12/50 - Train Loss: 0.033505 - Val Loss: 0.047103\n",
      "Epoch 13/50 - Train Loss: 0.032491 - Val Loss: 0.061179\n",
      "Epoch 14/50 - Train Loss: 0.032711 - Val Loss: 0.044807\n",
      "Epoch 15/50 - Train Loss: 0.031212 - Val Loss: 0.046026\n",
      "Epoch 16/50 - Train Loss: 0.033598 - Val Loss: 0.050872\n",
      "Epoch 17/50 - Train Loss: 0.032679 - Val Loss: 0.046068\n",
      "Epoch 18/50 - Train Loss: 0.034440 - Val Loss: 0.046020\n",
      "Epoch 19/50 - Train Loss: 0.032747 - Val Loss: 0.061396\n",
      "Epoch 20/50 - Train Loss: 0.031594 - Val Loss: 0.044559\n",
      "Epoch 21/50 - Train Loss: 0.032270 - Val Loss: 0.045152\n",
      "Epoch 22/50 - Train Loss: 0.034979 - Val Loss: 0.053599\n",
      "Epoch 23/50 - Train Loss: 0.036199 - Val Loss: 0.056803\n",
      "Epoch 24/50 - Train Loss: 0.032901 - Val Loss: 0.044942\n",
      "Epoch 25/50 - Train Loss: 0.029289 - Val Loss: 0.045838\n",
      "Epoch 26/50 - Train Loss: 0.030363 - Val Loss: 0.049374\n",
      "Epoch 27/50 - Train Loss: 0.031063 - Val Loss: 0.045317\n",
      "Epoch 28/50 - Train Loss: 0.031686 - Val Loss: 0.049008\n",
      "Epoch 29/50 - Train Loss: 0.032252 - Val Loss: 0.045869\n",
      "Epoch 30/50 - Train Loss: 0.027906 - Val Loss: 0.051826\n",
      "Epoch 31/50 - Train Loss: 0.034472 - Val Loss: 0.045177\n",
      "Epoch 32/50 - Train Loss: 0.033778 - Val Loss: 0.044993\n",
      "Epoch 33/50 - Train Loss: 0.030822 - Val Loss: 0.045823\n",
      "Epoch 34/50 - Train Loss: 0.031607 - Val Loss: 0.048621\n",
      "Epoch 35/50 - Train Loss: 0.030769 - Val Loss: 0.045687\n",
      "Epoch 36/50 - Train Loss: 0.032585 - Val Loss: 0.058598\n",
      "Epoch 37/50 - Train Loss: 0.031930 - Val Loss: 0.053581\n",
      "Epoch 38/50 - Train Loss: 0.033774 - Val Loss: 0.045115\n",
      "Epoch 39/50 - Train Loss: 0.028496 - Val Loss: 0.045227\n",
      "Epoch 40/50 - Train Loss: 0.029909 - Val Loss: 0.049825\n",
      "Epoch 41/50 - Train Loss: 0.031804 - Val Loss: 0.045591\n",
      "Epoch 42/50 - Train Loss: 0.029991 - Val Loss: 0.045074\n",
      "Epoch 43/50 - Train Loss: 0.030707 - Val Loss: 0.053550\n",
      "Epoch 44/50 - Train Loss: 0.032071 - Val Loss: 0.048371\n",
      "Epoch 45/50 - Train Loss: 0.030961 - Val Loss: 0.053751\n",
      "Epoch 46/50 - Train Loss: 0.030927 - Val Loss: 0.050009\n",
      "Epoch 47/50 - Train Loss: 0.033788 - Val Loss: 0.054806\n",
      "Epoch 48/50 - Train Loss: 0.030843 - Val Loss: 0.050269\n",
      "Epoch 49/50 - Train Loss: 0.029984 - Val Loss: 0.045514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:40:48,209] Trial 66 finished with value: 0.04455863684415817 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 86, 'lr': 0.009444330064169034, 'weight_decay': 1.1153549251713874e-06, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031499 - Val Loss: 0.045899\n",
      "Epoch 1/50 - Train Loss: 0.188767 - Val Loss: 0.085843\n",
      "Epoch 2/50 - Train Loss: 0.086327 - Val Loss: 0.118483\n",
      "Epoch 3/50 - Train Loss: 0.051798 - Val Loss: 0.141156\n",
      "Epoch 4/50 - Train Loss: 0.056474 - Val Loss: 0.110583\n",
      "Epoch 5/50 - Train Loss: 0.042388 - Val Loss: 0.068639\n",
      "Epoch 6/50 - Train Loss: 0.042259 - Val Loss: 0.059710\n",
      "Epoch 7/50 - Train Loss: 0.052026 - Val Loss: 0.070483\n",
      "Epoch 8/50 - Train Loss: 0.044708 - Val Loss: 0.091503\n",
      "Epoch 9/50 - Train Loss: 0.042765 - Val Loss: 0.089088\n",
      "Epoch 10/50 - Train Loss: 0.038680 - Val Loss: 0.063473\n",
      "Epoch 11/50 - Train Loss: 0.041691 - Val Loss: 0.049704\n",
      "Epoch 12/50 - Train Loss: 0.036586 - Val Loss: 0.051329\n",
      "Epoch 13/50 - Train Loss: 0.042007 - Val Loss: 0.067087\n",
      "Epoch 14/50 - Train Loss: 0.041799 - Val Loss: 0.059746\n",
      "Epoch 15/50 - Train Loss: 0.032461 - Val Loss: 0.047678\n",
      "Epoch 16/50 - Train Loss: 0.034725 - Val Loss: 0.048638\n",
      "Epoch 17/50 - Train Loss: 0.031916 - Val Loss: 0.060350\n",
      "Epoch 18/50 - Train Loss: 0.040328 - Val Loss: 0.056249\n",
      "Epoch 19/50 - Train Loss: 0.039472 - Val Loss: 0.047848\n",
      "Epoch 20/50 - Train Loss: 0.030147 - Val Loss: 0.046471\n",
      "Epoch 21/50 - Train Loss: 0.030638 - Val Loss: 0.046957\n",
      "Epoch 22/50 - Train Loss: 0.029764 - Val Loss: 0.049352\n",
      "Epoch 23/50 - Train Loss: 0.038161 - Val Loss: 0.048232\n",
      "Epoch 24/50 - Train Loss: 0.030157 - Val Loss: 0.045708\n",
      "Epoch 25/50 - Train Loss: 0.038236 - Val Loss: 0.045500\n",
      "Epoch 26/50 - Train Loss: 0.036724 - Val Loss: 0.046977\n",
      "Epoch 27/50 - Train Loss: 0.038598 - Val Loss: 0.047160\n",
      "Epoch 28/50 - Train Loss: 0.028793 - Val Loss: 0.044928\n",
      "Epoch 29/50 - Train Loss: 0.037771 - Val Loss: 0.044973\n",
      "Epoch 30/50 - Train Loss: 0.028875 - Val Loss: 0.047277\n",
      "Epoch 31/50 - Train Loss: 0.037708 - Val Loss: 0.046728\n",
      "Epoch 32/50 - Train Loss: 0.036776 - Val Loss: 0.045155\n",
      "Epoch 33/50 - Train Loss: 0.037319 - Val Loss: 0.046745\n",
      "Epoch 34/50 - Train Loss: 0.037413 - Val Loss: 0.048464\n",
      "Epoch 35/50 - Train Loss: 0.030675 - Val Loss: 0.045785\n",
      "Epoch 36/50 - Train Loss: 0.029189 - Val Loss: 0.044969\n",
      "Epoch 37/50 - Train Loss: 0.036842 - Val Loss: 0.046942\n",
      "Epoch 38/50 - Train Loss: 0.029520 - Val Loss: 0.046399\n",
      "Epoch 39/50 - Train Loss: 0.028988 - Val Loss: 0.045229\n",
      "Epoch 40/50 - Train Loss: 0.036661 - Val Loss: 0.045621\n",
      "Epoch 41/50 - Train Loss: 0.028632 - Val Loss: 0.046266\n",
      "Epoch 42/50 - Train Loss: 0.030341 - Val Loss: 0.044886\n",
      "Epoch 43/50 - Train Loss: 0.028779 - Val Loss: 0.045421\n",
      "Epoch 44/50 - Train Loss: 0.028782 - Val Loss: 0.049459\n",
      "Epoch 45/50 - Train Loss: 0.028872 - Val Loss: 0.045137\n",
      "Epoch 46/50 - Train Loss: 0.036673 - Val Loss: 0.045493\n",
      "Epoch 47/50 - Train Loss: 0.028248 - Val Loss: 0.048829\n",
      "Epoch 48/50 - Train Loss: 0.035703 - Val Loss: 0.046598\n",
      "Epoch 49/50 - Train Loss: 0.030084 - Val Loss: 0.045773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:41:03,991] Trial 67 finished with value: 0.04488631337881088 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 200, 'lr': 0.00345196993890809, 'weight_decay': 7.531563853677491e-07, 'batch_size': 64}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.035715 - Val Loss: 0.049159\n",
      "Epoch 1/50 - Train Loss: 0.069514 - Val Loss: 0.074773\n",
      "Epoch 2/50 - Train Loss: 0.047011 - Val Loss: 0.092351\n",
      "Epoch 3/50 - Train Loss: 0.039922 - Val Loss: 0.073159\n",
      "Epoch 4/50 - Train Loss: 0.040478 - Val Loss: 0.077076\n",
      "Epoch 5/50 - Train Loss: 0.038204 - Val Loss: 0.066239\n",
      "Epoch 6/50 - Train Loss: 0.040575 - Val Loss: 0.052825\n",
      "Epoch 7/50 - Train Loss: 0.040493 - Val Loss: 0.053650\n",
      "Epoch 8/50 - Train Loss: 0.035884 - Val Loss: 0.053080\n",
      "Epoch 9/50 - Train Loss: 0.035167 - Val Loss: 0.047519\n",
      "Epoch 10/50 - Train Loss: 0.033355 - Val Loss: 0.047669\n",
      "Epoch 11/50 - Train Loss: 0.032004 - Val Loss: 0.061149\n",
      "Epoch 12/50 - Train Loss: 0.033638 - Val Loss: 0.046741\n",
      "Epoch 13/50 - Train Loss: 0.036787 - Val Loss: 0.050505\n",
      "Epoch 14/50 - Train Loss: 0.030694 - Val Loss: 0.045222\n",
      "Epoch 15/50 - Train Loss: 0.034395 - Val Loss: 0.046469\n",
      "Epoch 16/50 - Train Loss: 0.035209 - Val Loss: 0.048397\n",
      "Epoch 17/50 - Train Loss: 0.030546 - Val Loss: 0.047371\n",
      "Epoch 18/50 - Train Loss: 0.033600 - Val Loss: 0.046667\n",
      "Epoch 19/50 - Train Loss: 0.033403 - Val Loss: 0.045308\n",
      "Epoch 20/50 - Train Loss: 0.033554 - Val Loss: 0.045608\n",
      "Epoch 21/50 - Train Loss: 0.030056 - Val Loss: 0.047238\n",
      "Epoch 22/50 - Train Loss: 0.033060 - Val Loss: 0.044693\n",
      "Epoch 23/50 - Train Loss: 0.031474 - Val Loss: 0.046424\n",
      "Epoch 24/50 - Train Loss: 0.033311 - Val Loss: 0.049636\n",
      "Epoch 25/50 - Train Loss: 0.030104 - Val Loss: 0.044386\n",
      "Epoch 26/50 - Train Loss: 0.031320 - Val Loss: 0.045449\n",
      "Epoch 27/50 - Train Loss: 0.029936 - Val Loss: 0.044802\n",
      "Epoch 28/50 - Train Loss: 0.029790 - Val Loss: 0.052602\n",
      "Epoch 29/50 - Train Loss: 0.028998 - Val Loss: 0.047606\n",
      "Epoch 30/50 - Train Loss: 0.030224 - Val Loss: 0.045172\n",
      "Epoch 31/50 - Train Loss: 0.032546 - Val Loss: 0.044871\n",
      "Epoch 32/50 - Train Loss: 0.034966 - Val Loss: 0.045165\n",
      "Epoch 33/50 - Train Loss: 0.032990 - Val Loss: 0.048836\n",
      "Epoch 34/50 - Train Loss: 0.031258 - Val Loss: 0.047226\n",
      "Epoch 35/50 - Train Loss: 0.030775 - Val Loss: 0.045384\n",
      "Epoch 36/50 - Train Loss: 0.031760 - Val Loss: 0.045589\n",
      "Epoch 37/50 - Train Loss: 0.027101 - Val Loss: 0.044393\n",
      "Epoch 38/50 - Train Loss: 0.032559 - Val Loss: 0.045609\n",
      "Epoch 39/50 - Train Loss: 0.030453 - Val Loss: 0.045108\n",
      "Epoch 40/50 - Train Loss: 0.031751 - Val Loss: 0.044743\n",
      "Epoch 41/50 - Train Loss: 0.028907 - Val Loss: 0.044829\n",
      "Epoch 42/50 - Train Loss: 0.029631 - Val Loss: 0.044481\n",
      "Epoch 43/50 - Train Loss: 0.027042 - Val Loss: 0.044149\n",
      "Epoch 44/50 - Train Loss: 0.031735 - Val Loss: 0.045281\n",
      "Epoch 45/50 - Train Loss: 0.027716 - Val Loss: 0.045350\n",
      "Epoch 46/50 - Train Loss: 0.031929 - Val Loss: 0.045117\n",
      "Epoch 47/50 - Train Loss: 0.026484 - Val Loss: 0.045741\n",
      "Epoch 48/50 - Train Loss: 0.030219 - Val Loss: 0.044609\n",
      "Epoch 49/50 - Train Loss: 0.032335 - Val Loss: 0.045823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:41:21,792] Trial 68 finished with value: 0.04414888843894005 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 48, 'lr': 0.0064067734070937, 'weight_decay': 1.515809579979432e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029346 - Val Loss: 0.047712\n",
      "Epoch 1/50 - Train Loss: 0.074685 - Val Loss: 0.103745\n",
      "Epoch 2/50 - Train Loss: 0.041255 - Val Loss: 0.067545\n",
      "Epoch 3/50 - Train Loss: 0.037852 - Val Loss: 0.062621\n",
      "Epoch 4/50 - Train Loss: 0.034278 - Val Loss: 0.046047\n",
      "Epoch 5/50 - Train Loss: 0.031662 - Val Loss: 0.045771\n",
      "Epoch 6/50 - Train Loss: 0.033040 - Val Loss: 0.051756\n",
      "Epoch 7/50 - Train Loss: 0.033006 - Val Loss: 0.046249\n",
      "Epoch 8/50 - Train Loss: 0.030799 - Val Loss: 0.050623\n",
      "Epoch 9/50 - Train Loss: 0.031316 - Val Loss: 0.049526\n",
      "Epoch 10/50 - Train Loss: 0.031625 - Val Loss: 0.046052\n",
      "Epoch 11/50 - Train Loss: 0.033315 - Val Loss: 0.053716\n",
      "Epoch 12/50 - Train Loss: 0.030213 - Val Loss: 0.044735\n",
      "Epoch 13/50 - Train Loss: 0.028798 - Val Loss: 0.048186\n",
      "Epoch 14/50 - Train Loss: 0.032363 - Val Loss: 0.046822\n",
      "Epoch 15/50 - Train Loss: 0.030082 - Val Loss: 0.056846\n",
      "Epoch 16/50 - Train Loss: 0.033751 - Val Loss: 0.045336\n",
      "Epoch 17/50 - Train Loss: 0.031372 - Val Loss: 0.058555\n",
      "Epoch 18/50 - Train Loss: 0.031330 - Val Loss: 0.045111\n",
      "Epoch 19/50 - Train Loss: 0.030601 - Val Loss: 0.048933\n",
      "Epoch 20/50 - Train Loss: 0.031929 - Val Loss: 0.045382\n",
      "Epoch 21/50 - Train Loss: 0.028657 - Val Loss: 0.051584\n",
      "Epoch 22/50 - Train Loss: 0.031497 - Val Loss: 0.044376\n",
      "Epoch 23/50 - Train Loss: 0.032223 - Val Loss: 0.044969\n",
      "Epoch 24/50 - Train Loss: 0.032516 - Val Loss: 0.044093\n",
      "Epoch 25/50 - Train Loss: 0.034479 - Val Loss: 0.058544\n",
      "Epoch 26/50 - Train Loss: 0.030956 - Val Loss: 0.053135\n",
      "Epoch 27/50 - Train Loss: 0.030047 - Val Loss: 0.044494\n",
      "Epoch 28/50 - Train Loss: 0.031008 - Val Loss: 0.045482\n",
      "Epoch 29/50 - Train Loss: 0.027396 - Val Loss: 0.043468\n",
      "Epoch 30/50 - Train Loss: 0.029623 - Val Loss: 0.045590\n",
      "Epoch 31/50 - Train Loss: 0.029193 - Val Loss: 0.047734\n",
      "Epoch 32/50 - Train Loss: 0.030486 - Val Loss: 0.045462\n",
      "Epoch 33/50 - Train Loss: 0.030360 - Val Loss: 0.053214\n",
      "Epoch 34/50 - Train Loss: 0.027629 - Val Loss: 0.059359\n",
      "Epoch 35/50 - Train Loss: 0.029299 - Val Loss: 0.051131\n",
      "Epoch 36/50 - Train Loss: 0.029383 - Val Loss: 0.045380\n",
      "Epoch 37/50 - Train Loss: 0.030508 - Val Loss: 0.044234\n",
      "Epoch 38/50 - Train Loss: 0.031088 - Val Loss: 0.045375\n",
      "Epoch 39/50 - Train Loss: 0.028966 - Val Loss: 0.044228\n",
      "Epoch 40/50 - Train Loss: 0.031566 - Val Loss: 0.052568\n",
      "Epoch 41/50 - Train Loss: 0.028426 - Val Loss: 0.048084\n",
      "Epoch 42/50 - Train Loss: 0.029076 - Val Loss: 0.046229\n",
      "Epoch 43/50 - Train Loss: 0.029873 - Val Loss: 0.049152\n",
      "Epoch 44/50 - Train Loss: 0.028731 - Val Loss: 0.044105\n",
      "Epoch 45/50 - Train Loss: 0.031133 - Val Loss: 0.045081\n",
      "Epoch 46/50 - Train Loss: 0.028715 - Val Loss: 0.049323\n",
      "Epoch 47/50 - Train Loss: 0.030363 - Val Loss: 0.044374\n",
      "Epoch 48/50 - Train Loss: 0.029516 - Val Loss: 0.052573\n",
      "Epoch 49/50 - Train Loss: 0.030159 - Val Loss: 0.048806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:42:14,691] Trial 69 finished with value: 0.04346813509861628 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 179, 'lr': 0.00796306897875724, 'weight_decay': 7.614832381441386e-08, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030514 - Val Loss: 0.045971\n",
      "Epoch 1/50 - Train Loss: 0.196697 - Val Loss: 0.275313\n",
      "Epoch 2/50 - Train Loss: 0.093561 - Val Loss: 0.106915\n",
      "Epoch 3/50 - Train Loss: 0.062301 - Val Loss: 0.070225\n",
      "Epoch 4/50 - Train Loss: 0.046638 - Val Loss: 0.121526\n",
      "Epoch 5/50 - Train Loss: 0.046046 - Val Loss: 0.075611\n",
      "Epoch 6/50 - Train Loss: 0.043892 - Val Loss: 0.051998\n",
      "Epoch 7/50 - Train Loss: 0.041540 - Val Loss: 0.062015\n",
      "Epoch 8/50 - Train Loss: 0.041402 - Val Loss: 0.060081\n",
      "Epoch 9/50 - Train Loss: 0.037630 - Val Loss: 0.048165\n",
      "Epoch 10/50 - Train Loss: 0.033842 - Val Loss: 0.051936\n",
      "Epoch 11/50 - Train Loss: 0.032590 - Val Loss: 0.047711\n",
      "Epoch 12/50 - Train Loss: 0.036935 - Val Loss: 0.059167\n",
      "Epoch 13/50 - Train Loss: 0.033007 - Val Loss: 0.047463\n",
      "Epoch 14/50 - Train Loss: 0.034841 - Val Loss: 0.052891\n",
      "Epoch 15/50 - Train Loss: 0.037818 - Val Loss: 0.046396\n",
      "Epoch 16/50 - Train Loss: 0.036044 - Val Loss: 0.050807\n",
      "Epoch 17/50 - Train Loss: 0.037327 - Val Loss: 0.047840\n",
      "Epoch 18/50 - Train Loss: 0.033251 - Val Loss: 0.051228\n",
      "Epoch 19/50 - Train Loss: 0.028725 - Val Loss: 0.046749\n",
      "Epoch 20/50 - Train Loss: 0.033641 - Val Loss: 0.050183\n",
      "Epoch 21/50 - Train Loss: 0.034470 - Val Loss: 0.046207\n",
      "Epoch 22/50 - Train Loss: 0.028974 - Val Loss: 0.047792\n",
      "Epoch 23/50 - Train Loss: 0.033753 - Val Loss: 0.045375\n",
      "Epoch 24/50 - Train Loss: 0.036965 - Val Loss: 0.056701\n",
      "Epoch 25/50 - Train Loss: 0.037337 - Val Loss: 0.050260\n",
      "Epoch 26/50 - Train Loss: 0.032209 - Val Loss: 0.047115\n",
      "Epoch 27/50 - Train Loss: 0.035918 - Val Loss: 0.046600\n",
      "Epoch 28/50 - Train Loss: 0.036111 - Val Loss: 0.047433\n",
      "Epoch 29/50 - Train Loss: 0.032302 - Val Loss: 0.047205\n",
      "Epoch 30/50 - Train Loss: 0.029681 - Val Loss: 0.047041\n",
      "Epoch 31/50 - Train Loss: 0.034809 - Val Loss: 0.046860\n",
      "Epoch 32/50 - Train Loss: 0.033135 - Val Loss: 0.046174\n",
      "Epoch 33/50 - Train Loss: 0.028739 - Val Loss: 0.047181\n",
      "Epoch 34/50 - Train Loss: 0.031950 - Val Loss: 0.045956\n",
      "Epoch 35/50 - Train Loss: 0.030594 - Val Loss: 0.046170\n",
      "Epoch 36/50 - Train Loss: 0.028611 - Val Loss: 0.045082\n",
      "Epoch 37/50 - Train Loss: 0.029208 - Val Loss: 0.050455\n",
      "Epoch 38/50 - Train Loss: 0.031376 - Val Loss: 0.045348\n",
      "Epoch 39/50 - Train Loss: 0.031685 - Val Loss: 0.045373\n",
      "Epoch 40/50 - Train Loss: 0.030648 - Val Loss: 0.049305\n",
      "Epoch 41/50 - Train Loss: 0.031306 - Val Loss: 0.046790\n",
      "Epoch 42/50 - Train Loss: 0.028451 - Val Loss: 0.049505\n",
      "Epoch 43/50 - Train Loss: 0.034577 - Val Loss: 0.044675\n",
      "Epoch 44/50 - Train Loss: 0.028981 - Val Loss: 0.045546\n",
      "Epoch 45/50 - Train Loss: 0.036260 - Val Loss: 0.045782\n",
      "Epoch 46/50 - Train Loss: 0.030489 - Val Loss: 0.053072\n",
      "Epoch 47/50 - Train Loss: 0.027878 - Val Loss: 0.046365\n",
      "Epoch 48/50 - Train Loss: 0.032940 - Val Loss: 0.045564\n",
      "Epoch 49/50 - Train Loss: 0.032556 - Val Loss: 0.045935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:42:37,053] Trial 70 finished with value: 0.044675275683403015 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 178, 'lr': 0.007710972634856202, 'weight_decay': 7.162777370131778e-08, 'batch_size': 32}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026536 - Val Loss: 0.046161\n",
      "Epoch 1/50 - Train Loss: 0.060092 - Val Loss: 0.103162\n",
      "Epoch 2/50 - Train Loss: 0.038311 - Val Loss: 0.048317\n",
      "Epoch 3/50 - Train Loss: 0.031891 - Val Loss: 0.051389\n",
      "Epoch 4/50 - Train Loss: 0.035291 - Val Loss: 0.046013\n",
      "Epoch 5/50 - Train Loss: 0.032627 - Val Loss: 0.045017\n",
      "Epoch 6/50 - Train Loss: 0.031043 - Val Loss: 0.056598\n",
      "Epoch 7/50 - Train Loss: 0.032927 - Val Loss: 0.054317\n",
      "Epoch 8/50 - Train Loss: 0.030522 - Val Loss: 0.064885\n",
      "Epoch 9/50 - Train Loss: 0.033361 - Val Loss: 0.045418\n",
      "Epoch 10/50 - Train Loss: 0.030799 - Val Loss: 0.047971\n",
      "Epoch 11/50 - Train Loss: 0.026754 - Val Loss: 0.044534\n",
      "Epoch 12/50 - Train Loss: 0.031114 - Val Loss: 0.049953\n",
      "Epoch 13/50 - Train Loss: 0.030835 - Val Loss: 0.048150\n",
      "Epoch 14/50 - Train Loss: 0.031151 - Val Loss: 0.057310\n",
      "Epoch 15/50 - Train Loss: 0.031713 - Val Loss: 0.046630\n",
      "Epoch 16/50 - Train Loss: 0.029775 - Val Loss: 0.045294\n",
      "Epoch 17/50 - Train Loss: 0.031166 - Val Loss: 0.048375\n",
      "Epoch 18/50 - Train Loss: 0.032026 - Val Loss: 0.044612\n",
      "Epoch 19/50 - Train Loss: 0.032405 - Val Loss: 0.045360\n",
      "Epoch 20/50 - Train Loss: 0.032499 - Val Loss: 0.064499\n",
      "Epoch 21/50 - Train Loss: 0.031054 - Val Loss: 0.055939\n",
      "Epoch 22/50 - Train Loss: 0.033440 - Val Loss: 0.052307\n",
      "Epoch 23/50 - Train Loss: 0.030862 - Val Loss: 0.053191\n",
      "Epoch 24/50 - Train Loss: 0.033211 - Val Loss: 0.044937\n",
      "Epoch 25/50 - Train Loss: 0.031393 - Val Loss: 0.047242\n",
      "Epoch 26/50 - Train Loss: 0.033002 - Val Loss: 0.053762\n",
      "Epoch 27/50 - Train Loss: 0.027717 - Val Loss: 0.045473\n",
      "Epoch 28/50 - Train Loss: 0.031152 - Val Loss: 0.052320\n",
      "Epoch 29/50 - Train Loss: 0.032109 - Val Loss: 0.045869\n",
      "Epoch 30/50 - Train Loss: 0.032051 - Val Loss: 0.049134\n",
      "Epoch 31/50 - Train Loss: 0.029549 - Val Loss: 0.051202\n",
      "Epoch 32/50 - Train Loss: 0.027437 - Val Loss: 0.045024\n",
      "Epoch 33/50 - Train Loss: 0.030094 - Val Loss: 0.049757\n",
      "Epoch 34/50 - Train Loss: 0.030384 - Val Loss: 0.045355\n",
      "Epoch 35/50 - Train Loss: 0.031054 - Val Loss: 0.046003\n",
      "Epoch 36/50 - Train Loss: 0.033231 - Val Loss: 0.045005\n",
      "Epoch 37/50 - Train Loss: 0.029757 - Val Loss: 0.044428\n",
      "Epoch 38/50 - Train Loss: 0.031082 - Val Loss: 0.044760\n",
      "Epoch 39/50 - Train Loss: 0.031770 - Val Loss: 0.045003\n",
      "Epoch 40/50 - Train Loss: 0.033335 - Val Loss: 0.047654\n",
      "Epoch 41/50 - Train Loss: 0.031096 - Val Loss: 0.046717\n",
      "Epoch 42/50 - Train Loss: 0.028948 - Val Loss: 0.046700\n",
      "Epoch 43/50 - Train Loss: 0.029288 - Val Loss: 0.044774\n",
      "Epoch 44/50 - Train Loss: 0.030358 - Val Loss: 0.044253\n",
      "Epoch 45/50 - Train Loss: 0.029045 - Val Loss: 0.045712\n",
      "Epoch 46/50 - Train Loss: 0.030419 - Val Loss: 0.044712\n",
      "Epoch 47/50 - Train Loss: 0.030315 - Val Loss: 0.045068\n",
      "Epoch 48/50 - Train Loss: 0.029518 - Val Loss: 0.050259\n",
      "Epoch 49/50 - Train Loss: 0.029837 - Val Loss: 0.045837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:43:27,701] Trial 71 finished with value: 0.044253108402093254 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 189, 'lr': 0.00562815394399951, 'weight_decay': 1.4706784707775628e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030742 - Val Loss: 0.054802\n",
      "Epoch 1/50 - Train Loss: 0.100207 - Val Loss: 0.088237\n",
      "Epoch 2/50 - Train Loss: 0.041834 - Val Loss: 0.076427\n",
      "Epoch 3/50 - Train Loss: 0.034865 - Val Loss: 0.051065\n",
      "Epoch 4/50 - Train Loss: 0.036132 - Val Loss: 0.067172\n",
      "Epoch 5/50 - Train Loss: 0.035123 - Val Loss: 0.046310\n",
      "Epoch 6/50 - Train Loss: 0.034452 - Val Loss: 0.058149\n",
      "Epoch 7/50 - Train Loss: 0.033813 - Val Loss: 0.045477\n",
      "Epoch 8/50 - Train Loss: 0.033969 - Val Loss: 0.051630\n",
      "Epoch 9/50 - Train Loss: 0.031511 - Val Loss: 0.045820\n",
      "Epoch 10/50 - Train Loss: 0.031775 - Val Loss: 0.047185\n",
      "Epoch 11/50 - Train Loss: 0.030289 - Val Loss: 0.048319\n",
      "Epoch 12/50 - Train Loss: 0.031097 - Val Loss: 0.051390\n",
      "Epoch 13/50 - Train Loss: 0.030959 - Val Loss: 0.046140\n",
      "Epoch 14/50 - Train Loss: 0.034922 - Val Loss: 0.044686\n",
      "Epoch 15/50 - Train Loss: 0.032555 - Val Loss: 0.056268\n",
      "Epoch 16/50 - Train Loss: 0.031340 - Val Loss: 0.045593\n",
      "Epoch 17/50 - Train Loss: 0.031233 - Val Loss: 0.044566\n",
      "Epoch 18/50 - Train Loss: 0.033571 - Val Loss: 0.050207\n",
      "Epoch 19/50 - Train Loss: 0.031138 - Val Loss: 0.050725\n",
      "Epoch 20/50 - Train Loss: 0.032353 - Val Loss: 0.045319\n",
      "Epoch 21/50 - Train Loss: 0.030796 - Val Loss: 0.044760\n",
      "Epoch 22/50 - Train Loss: 0.034352 - Val Loss: 0.049448\n",
      "Epoch 23/50 - Train Loss: 0.033700 - Val Loss: 0.058050\n",
      "Epoch 24/50 - Train Loss: 0.032058 - Val Loss: 0.054971\n",
      "Epoch 25/50 - Train Loss: 0.029611 - Val Loss: 0.044664\n",
      "Epoch 26/50 - Train Loss: 0.030921 - Val Loss: 0.044913\n",
      "Epoch 27/50 - Train Loss: 0.032337 - Val Loss: 0.052030\n",
      "Epoch 28/50 - Train Loss: 0.032280 - Val Loss: 0.056175\n",
      "Epoch 29/50 - Train Loss: 0.033623 - Val Loss: 0.048216\n",
      "Epoch 30/50 - Train Loss: 0.030837 - Val Loss: 0.045509\n",
      "Epoch 31/50 - Train Loss: 0.033773 - Val Loss: 0.044317\n",
      "Epoch 32/50 - Train Loss: 0.035006 - Val Loss: 0.050612\n",
      "Epoch 33/50 - Train Loss: 0.032368 - Val Loss: 0.046719\n",
      "Epoch 34/50 - Train Loss: 0.029916 - Val Loss: 0.044075\n",
      "Epoch 35/50 - Train Loss: 0.029982 - Val Loss: 0.044591\n",
      "Epoch 36/50 - Train Loss: 0.030606 - Val Loss: 0.044663\n",
      "Epoch 37/50 - Train Loss: 0.031697 - Val Loss: 0.048013\n",
      "Epoch 38/50 - Train Loss: 0.029614 - Val Loss: 0.045829\n",
      "Epoch 39/50 - Train Loss: 0.029514 - Val Loss: 0.045735\n",
      "Epoch 40/50 - Train Loss: 0.035508 - Val Loss: 0.045285\n",
      "Epoch 41/50 - Train Loss: 0.032820 - Val Loss: 0.044294\n",
      "Epoch 42/50 - Train Loss: 0.031279 - Val Loss: 0.044525\n",
      "Epoch 43/50 - Train Loss: 0.032033 - Val Loss: 0.045696\n",
      "Epoch 44/50 - Train Loss: 0.029284 - Val Loss: 0.043993\n",
      "Epoch 45/50 - Train Loss: 0.032643 - Val Loss: 0.052157\n",
      "Epoch 46/50 - Train Loss: 0.030411 - Val Loss: 0.045911\n",
      "Epoch 47/50 - Train Loss: 0.029856 - Val Loss: 0.044197\n",
      "Epoch 48/50 - Train Loss: 0.031110 - Val Loss: 0.046593\n",
      "Epoch 49/50 - Train Loss: 0.029185 - Val Loss: 0.047183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:44:17,489] Trial 72 finished with value: 0.04399285217126211 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 166, 'lr': 0.009611475981585887, 'weight_decay': 4.3766352208416874e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028222 - Val Loss: 0.045208\n",
      "Epoch 1/50 - Train Loss: 0.066968 - Val Loss: 0.061013\n",
      "Epoch 2/50 - Train Loss: 0.044012 - Val Loss: 0.069684\n",
      "Epoch 3/50 - Train Loss: 0.036785 - Val Loss: 0.057252\n",
      "Epoch 4/50 - Train Loss: 0.032520 - Val Loss: 0.059450\n",
      "Epoch 5/50 - Train Loss: 0.032864 - Val Loss: 0.045607\n",
      "Epoch 6/50 - Train Loss: 0.033955 - Val Loss: 0.049495\n",
      "Epoch 7/50 - Train Loss: 0.034299 - Val Loss: 0.045599\n",
      "Epoch 8/50 - Train Loss: 0.035299 - Val Loss: 0.047135\n",
      "Epoch 9/50 - Train Loss: 0.032029 - Val Loss: 0.044992\n",
      "Epoch 10/50 - Train Loss: 0.030606 - Val Loss: 0.045129\n",
      "Epoch 11/50 - Train Loss: 0.032062 - Val Loss: 0.047487\n",
      "Epoch 12/50 - Train Loss: 0.029122 - Val Loss: 0.045299\n",
      "Epoch 13/50 - Train Loss: 0.031329 - Val Loss: 0.044427\n",
      "Epoch 14/50 - Train Loss: 0.030926 - Val Loss: 0.047294\n",
      "Epoch 15/50 - Train Loss: 0.030524 - Val Loss: 0.045288\n",
      "Epoch 16/50 - Train Loss: 0.032501 - Val Loss: 0.046707\n",
      "Epoch 17/50 - Train Loss: 0.028743 - Val Loss: 0.045493\n",
      "Epoch 18/50 - Train Loss: 0.028991 - Val Loss: 0.048671\n",
      "Epoch 19/50 - Train Loss: 0.031030 - Val Loss: 0.044950\n",
      "Epoch 20/50 - Train Loss: 0.028618 - Val Loss: 0.043940\n",
      "Epoch 21/50 - Train Loss: 0.027777 - Val Loss: 0.047078\n",
      "Epoch 22/50 - Train Loss: 0.032186 - Val Loss: 0.047302\n",
      "Epoch 23/50 - Train Loss: 0.029968 - Val Loss: 0.044950\n",
      "Epoch 24/50 - Train Loss: 0.030882 - Val Loss: 0.047212\n",
      "Epoch 25/50 - Train Loss: 0.030456 - Val Loss: 0.045777\n",
      "Epoch 26/50 - Train Loss: 0.032737 - Val Loss: 0.044675\n",
      "Epoch 27/50 - Train Loss: 0.033103 - Val Loss: 0.047954\n",
      "Epoch 28/50 - Train Loss: 0.029623 - Val Loss: 0.045947\n",
      "Epoch 29/50 - Train Loss: 0.029806 - Val Loss: 0.044128\n",
      "Epoch 30/50 - Train Loss: 0.027408 - Val Loss: 0.044716\n",
      "Epoch 31/50 - Train Loss: 0.031224 - Val Loss: 0.043990\n",
      "Epoch 32/50 - Train Loss: 0.032287 - Val Loss: 0.045276\n",
      "Epoch 33/50 - Train Loss: 0.029348 - Val Loss: 0.046844\n",
      "Epoch 34/50 - Train Loss: 0.028616 - Val Loss: 0.048508\n",
      "Epoch 35/50 - Train Loss: 0.028637 - Val Loss: 0.047458\n",
      "Epoch 36/50 - Train Loss: 0.031500 - Val Loss: 0.047706\n",
      "Epoch 37/50 - Train Loss: 0.028625 - Val Loss: 0.045344\n",
      "Epoch 38/50 - Train Loss: 0.027580 - Val Loss: 0.046601\n",
      "Epoch 39/50 - Train Loss: 0.030030 - Val Loss: 0.045085\n",
      "Epoch 40/50 - Train Loss: 0.030638 - Val Loss: 0.045122\n",
      "Epoch 41/50 - Train Loss: 0.025620 - Val Loss: 0.045526\n",
      "Epoch 42/50 - Train Loss: 0.030149 - Val Loss: 0.047749\n",
      "Epoch 43/50 - Train Loss: 0.031146 - Val Loss: 0.046709\n",
      "Epoch 44/50 - Train Loss: 0.028860 - Val Loss: 0.046899\n",
      "Epoch 45/50 - Train Loss: 0.026983 - Val Loss: 0.044452\n",
      "Epoch 46/50 - Train Loss: 0.029514 - Val Loss: 0.045310\n",
      "Epoch 47/50 - Train Loss: 0.030154 - Val Loss: 0.049480\n",
      "Epoch 48/50 - Train Loss: 0.028984 - Val Loss: 0.045577\n",
      "Epoch 49/50 - Train Loss: 0.027205 - Val Loss: 0.046996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:45:11,822] Trial 73 finished with value: 0.04393988475203514 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 150, 'lr': 0.004936505952499843, 'weight_decay': 9.480979470596743e-08, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031237 - Val Loss: 0.045294\n",
      "Epoch 1/50 - Train Loss: 0.082692 - Val Loss: 0.091073\n",
      "Epoch 2/50 - Train Loss: 0.042052 - Val Loss: 0.088662\n",
      "Epoch 3/50 - Train Loss: 0.039976 - Val Loss: 0.056241\n",
      "Epoch 4/50 - Train Loss: 0.036073 - Val Loss: 0.051627\n",
      "Epoch 5/50 - Train Loss: 0.031715 - Val Loss: 0.050510\n",
      "Epoch 6/50 - Train Loss: 0.031803 - Val Loss: 0.047918\n",
      "Epoch 7/50 - Train Loss: 0.032445 - Val Loss: 0.051880\n",
      "Epoch 8/50 - Train Loss: 0.034065 - Val Loss: 0.046700\n",
      "Epoch 9/50 - Train Loss: 0.032537 - Val Loss: 0.047487\n",
      "Epoch 10/50 - Train Loss: 0.033920 - Val Loss: 0.045457\n",
      "Epoch 11/50 - Train Loss: 0.030502 - Val Loss: 0.045244\n",
      "Epoch 12/50 - Train Loss: 0.034021 - Val Loss: 0.057541\n",
      "Epoch 13/50 - Train Loss: 0.032813 - Val Loss: 0.044624\n",
      "Epoch 14/50 - Train Loss: 0.036601 - Val Loss: 0.048739\n",
      "Epoch 15/50 - Train Loss: 0.033507 - Val Loss: 0.045341\n",
      "Epoch 16/50 - Train Loss: 0.031323 - Val Loss: 0.050097\n",
      "Epoch 17/50 - Train Loss: 0.029986 - Val Loss: 0.048461\n",
      "Epoch 18/50 - Train Loss: 0.030754 - Val Loss: 0.052774\n",
      "Epoch 19/50 - Train Loss: 0.032591 - Val Loss: 0.045660\n",
      "Epoch 20/50 - Train Loss: 0.029940 - Val Loss: 0.046227\n",
      "Epoch 21/50 - Train Loss: 0.030169 - Val Loss: 0.045411\n",
      "Epoch 22/50 - Train Loss: 0.031045 - Val Loss: 0.048682\n",
      "Epoch 23/50 - Train Loss: 0.034151 - Val Loss: 0.053800\n",
      "Epoch 24/50 - Train Loss: 0.032745 - Val Loss: 0.048757\n",
      "Epoch 25/50 - Train Loss: 0.031693 - Val Loss: 0.055581\n",
      "Epoch 26/50 - Train Loss: 0.031646 - Val Loss: 0.048721\n",
      "Epoch 27/50 - Train Loss: 0.031017 - Val Loss: 0.051697\n",
      "Epoch 28/50 - Train Loss: 0.033922 - Val Loss: 0.048363\n",
      "Epoch 29/50 - Train Loss: 0.029270 - Val Loss: 0.045083\n",
      "Epoch 30/50 - Train Loss: 0.029026 - Val Loss: 0.045554\n",
      "Epoch 31/50 - Train Loss: 0.032215 - Val Loss: 0.047320\n",
      "Epoch 32/50 - Train Loss: 0.030796 - Val Loss: 0.045040\n",
      "Epoch 33/50 - Train Loss: 0.030165 - Val Loss: 0.045503\n",
      "Epoch 34/50 - Train Loss: 0.031597 - Val Loss: 0.045232\n",
      "Epoch 35/50 - Train Loss: 0.031003 - Val Loss: 0.046299\n",
      "Epoch 36/50 - Train Loss: 0.028777 - Val Loss: 0.045074\n",
      "Epoch 37/50 - Train Loss: 0.029611 - Val Loss: 0.046705\n",
      "Epoch 38/50 - Train Loss: 0.030491 - Val Loss: 0.045331\n",
      "Epoch 39/50 - Train Loss: 0.029106 - Val Loss: 0.045443\n",
      "Epoch 40/50 - Train Loss: 0.030152 - Val Loss: 0.047270\n",
      "Epoch 41/50 - Train Loss: 0.027761 - Val Loss: 0.046757\n",
      "Epoch 42/50 - Train Loss: 0.031538 - Val Loss: 0.046906\n",
      "Epoch 43/50 - Train Loss: 0.028433 - Val Loss: 0.045574\n",
      "Epoch 44/50 - Train Loss: 0.030278 - Val Loss: 0.045635\n",
      "Epoch 45/50 - Train Loss: 0.030833 - Val Loss: 0.045514\n",
      "Epoch 46/50 - Train Loss: 0.030038 - Val Loss: 0.053054\n",
      "Epoch 47/50 - Train Loss: 0.029560 - Val Loss: 0.045259\n",
      "Epoch 48/50 - Train Loss: 0.030354 - Val Loss: 0.045365\n",
      "Epoch 49/50 - Train Loss: 0.027852 - Val Loss: 0.046274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:46:19,751] Trial 74 finished with value: 0.044624172151088715 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 140, 'lr': 0.007499240533184368, 'weight_decay': 2.292737415657406e-07, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028752 - Val Loss: 0.049707\n",
      "Epoch 1/50 - Train Loss: 0.057708 - Val Loss: 0.069746\n",
      "Epoch 2/50 - Train Loss: 0.043769 - Val Loss: 0.067374\n",
      "Epoch 3/50 - Train Loss: 0.039306 - Val Loss: 0.055183\n",
      "Epoch 4/50 - Train Loss: 0.037584 - Val Loss: 0.054362\n",
      "Epoch 5/50 - Train Loss: 0.035237 - Val Loss: 0.048222\n",
      "Epoch 6/50 - Train Loss: 0.033547 - Val Loss: 0.050485\n",
      "Epoch 7/50 - Train Loss: 0.035550 - Val Loss: 0.075012\n",
      "Epoch 8/50 - Train Loss: 0.033493 - Val Loss: 0.055422\n",
      "Epoch 9/50 - Train Loss: 0.032323 - Val Loss: 0.048175\n",
      "Epoch 10/50 - Train Loss: 0.031968 - Val Loss: 0.047806\n",
      "Epoch 11/50 - Train Loss: 0.031268 - Val Loss: 0.047870\n",
      "Epoch 12/50 - Train Loss: 0.032849 - Val Loss: 0.047603\n",
      "Epoch 13/50 - Train Loss: 0.030735 - Val Loss: 0.045012\n",
      "Epoch 14/50 - Train Loss: 0.032807 - Val Loss: 0.048503\n",
      "Epoch 15/50 - Train Loss: 0.030598 - Val Loss: 0.045249\n",
      "Epoch 16/50 - Train Loss: 0.034929 - Val Loss: 0.053619\n",
      "Epoch 17/50 - Train Loss: 0.034138 - Val Loss: 0.044879\n",
      "Epoch 18/50 - Train Loss: 0.034838 - Val Loss: 0.049858\n",
      "Epoch 19/50 - Train Loss: 0.031456 - Val Loss: 0.044274\n",
      "Epoch 20/50 - Train Loss: 0.032396 - Val Loss: 0.044749\n",
      "Epoch 21/50 - Train Loss: 0.032834 - Val Loss: 0.044592\n",
      "Epoch 22/50 - Train Loss: 0.031433 - Val Loss: 0.045879\n",
      "Epoch 23/50 - Train Loss: 0.032529 - Val Loss: 0.048229\n",
      "Epoch 24/50 - Train Loss: 0.033453 - Val Loss: 0.048410\n",
      "Epoch 25/50 - Train Loss: 0.029522 - Val Loss: 0.044561\n",
      "Epoch 26/50 - Train Loss: 0.031810 - Val Loss: 0.044725\n",
      "Epoch 27/50 - Train Loss: 0.029964 - Val Loss: 0.045521\n",
      "Epoch 28/50 - Train Loss: 0.033187 - Val Loss: 0.048000\n",
      "Epoch 29/50 - Train Loss: 0.030761 - Val Loss: 0.045882\n",
      "Epoch 30/50 - Train Loss: 0.031176 - Val Loss: 0.056209\n",
      "Epoch 31/50 - Train Loss: 0.032488 - Val Loss: 0.051763\n",
      "Epoch 32/50 - Train Loss: 0.031148 - Val Loss: 0.045301\n",
      "Epoch 33/50 - Train Loss: 0.030774 - Val Loss: 0.051267\n",
      "Epoch 34/50 - Train Loss: 0.028233 - Val Loss: 0.044971\n",
      "Epoch 35/50 - Train Loss: 0.028009 - Val Loss: 0.045930\n",
      "Epoch 36/50 - Train Loss: 0.028731 - Val Loss: 0.048285\n",
      "Epoch 37/50 - Train Loss: 0.032464 - Val Loss: 0.049023\n",
      "Epoch 38/50 - Train Loss: 0.032156 - Val Loss: 0.047322\n",
      "Epoch 39/50 - Train Loss: 0.030306 - Val Loss: 0.051099\n",
      "Epoch 40/50 - Train Loss: 0.028823 - Val Loss: 0.046233\n",
      "Epoch 41/50 - Train Loss: 0.028418 - Val Loss: 0.047761\n",
      "Epoch 42/50 - Train Loss: 0.029504 - Val Loss: 0.045269\n",
      "Epoch 43/50 - Train Loss: 0.029869 - Val Loss: 0.046593\n",
      "Epoch 44/50 - Train Loss: 0.031146 - Val Loss: 0.048739\n",
      "Epoch 45/50 - Train Loss: 0.030833 - Val Loss: 0.060913\n",
      "Epoch 46/50 - Train Loss: 0.031734 - Val Loss: 0.048615\n",
      "Epoch 47/50 - Train Loss: 0.033224 - Val Loss: 0.045609\n",
      "Epoch 48/50 - Train Loss: 0.033813 - Val Loss: 0.048099\n",
      "Epoch 49/50 - Train Loss: 0.031329 - Val Loss: 0.047759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:47:05,306] Trial 75 finished with value: 0.04427408551176389 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 119, 'lr': 0.003967061723467917, 'weight_decay': 1.9357370357538273e-06, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031582 - Val Loss: 0.051074\n",
      "Epoch 1/50 - Train Loss: 0.051308 - Val Loss: 0.060952\n",
      "Epoch 2/50 - Train Loss: 0.035239 - Val Loss: 0.049859\n",
      "Epoch 3/50 - Train Loss: 0.032483 - Val Loss: 0.050636\n",
      "Epoch 4/50 - Train Loss: 0.031487 - Val Loss: 0.049151\n",
      "Epoch 5/50 - Train Loss: 0.035677 - Val Loss: 0.049392\n",
      "Epoch 6/50 - Train Loss: 0.032016 - Val Loss: 0.046440\n",
      "Epoch 7/50 - Train Loss: 0.032169 - Val Loss: 0.051090\n",
      "Epoch 8/50 - Train Loss: 0.031705 - Val Loss: 0.044962\n",
      "Epoch 9/50 - Train Loss: 0.029135 - Val Loss: 0.045001\n",
      "Epoch 10/50 - Train Loss: 0.031416 - Val Loss: 0.046279\n",
      "Epoch 11/50 - Train Loss: 0.032808 - Val Loss: 0.046157\n",
      "Epoch 12/50 - Train Loss: 0.033588 - Val Loss: 0.051705\n",
      "Epoch 13/50 - Train Loss: 0.031341 - Val Loss: 0.050221\n",
      "Epoch 14/50 - Train Loss: 0.033666 - Val Loss: 0.046452\n",
      "Epoch 15/50 - Train Loss: 0.029823 - Val Loss: 0.044867\n",
      "Epoch 16/50 - Train Loss: 0.031759 - Val Loss: 0.048383\n",
      "Epoch 17/50 - Train Loss: 0.029726 - Val Loss: 0.045504\n",
      "Epoch 18/50 - Train Loss: 0.031602 - Val Loss: 0.045954\n",
      "Epoch 19/50 - Train Loss: 0.030813 - Val Loss: 0.055748\n",
      "Epoch 20/50 - Train Loss: 0.031006 - Val Loss: 0.049739\n",
      "Epoch 21/50 - Train Loss: 0.034703 - Val Loss: 0.045567\n",
      "Epoch 22/50 - Train Loss: 0.030617 - Val Loss: 0.046543\n",
      "Epoch 23/50 - Train Loss: 0.027734 - Val Loss: 0.046354\n",
      "Epoch 24/50 - Train Loss: 0.028873 - Val Loss: 0.061541\n",
      "Epoch 25/50 - Train Loss: 0.032044 - Val Loss: 0.046869\n",
      "Epoch 26/50 - Train Loss: 0.028872 - Val Loss: 0.058899\n",
      "Epoch 27/50 - Train Loss: 0.030760 - Val Loss: 0.050508\n",
      "Epoch 28/50 - Train Loss: 0.031167 - Val Loss: 0.054446\n",
      "Epoch 29/50 - Train Loss: 0.030399 - Val Loss: 0.052429\n",
      "Epoch 30/50 - Train Loss: 0.031026 - Val Loss: 0.046789\n",
      "Epoch 31/50 - Train Loss: 0.030705 - Val Loss: 0.049340\n",
      "Epoch 32/50 - Train Loss: 0.032113 - Val Loss: 0.046109\n",
      "Epoch 33/50 - Train Loss: 0.033331 - Val Loss: 0.045589\n",
      "Epoch 34/50 - Train Loss: 0.032040 - Val Loss: 0.046207\n",
      "Epoch 35/50 - Train Loss: 0.031359 - Val Loss: 0.048200\n",
      "Epoch 36/50 - Train Loss: 0.031265 - Val Loss: 0.050508\n",
      "Epoch 37/50 - Train Loss: 0.028451 - Val Loss: 0.045449\n",
      "Epoch 38/50 - Train Loss: 0.029122 - Val Loss: 0.046393\n",
      "Epoch 39/50 - Train Loss: 0.030117 - Val Loss: 0.048331\n",
      "Epoch 40/50 - Train Loss: 0.029264 - Val Loss: 0.050664\n",
      "Epoch 41/50 - Train Loss: 0.029655 - Val Loss: 0.044849\n",
      "Epoch 42/50 - Train Loss: 0.031287 - Val Loss: 0.045445\n",
      "Epoch 43/50 - Train Loss: 0.028501 - Val Loss: 0.045818\n",
      "Epoch 44/50 - Train Loss: 0.028586 - Val Loss: 0.047243\n",
      "Epoch 45/50 - Train Loss: 0.029889 - Val Loss: 0.046022\n",
      "Epoch 46/50 - Train Loss: 0.028723 - Val Loss: 0.047148\n",
      "Epoch 47/50 - Train Loss: 0.031677 - Val Loss: 0.052037\n",
      "Epoch 48/50 - Train Loss: 0.031057 - Val Loss: 0.045966\n",
      "Epoch 49/50 - Train Loss: 0.028597 - Val Loss: 0.045380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:47:42,280] Trial 76 finished with value: 0.04484883571664492 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 131, 'lr': 0.002336629606298899, 'weight_decay': 6.139132644610516e-08, 'batch_size': 8}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030437 - Val Loss: 0.046988\n",
      "Epoch 1/50 - Train Loss: 0.107114 - Val Loss: 0.229505\n",
      "Epoch 2/50 - Train Loss: 0.080987 - Val Loss: 0.070215\n",
      "Epoch 3/50 - Train Loss: 0.053556 - Val Loss: 0.070900\n",
      "Epoch 4/50 - Train Loss: 0.039457 - Val Loss: 0.058599\n",
      "Epoch 5/50 - Train Loss: 0.034726 - Val Loss: 0.053060\n",
      "Epoch 6/50 - Train Loss: 0.035777 - Val Loss: 0.047916\n",
      "Epoch 7/50 - Train Loss: 0.038263 - Val Loss: 0.049304\n",
      "Epoch 8/50 - Train Loss: 0.034063 - Val Loss: 0.049520\n",
      "Epoch 9/50 - Train Loss: 0.031520 - Val Loss: 0.047140\n",
      "Epoch 10/50 - Train Loss: 0.032029 - Val Loss: 0.045800\n",
      "Epoch 11/50 - Train Loss: 0.029850 - Val Loss: 0.050426\n",
      "Epoch 12/50 - Train Loss: 0.032214 - Val Loss: 0.044981\n",
      "Epoch 13/50 - Train Loss: 0.034503 - Val Loss: 0.047072\n",
      "Epoch 14/50 - Train Loss: 0.030340 - Val Loss: 0.046804\n",
      "Epoch 15/50 - Train Loss: 0.027651 - Val Loss: 0.047149\n",
      "Epoch 16/50 - Train Loss: 0.034808 - Val Loss: 0.044166\n",
      "Epoch 17/50 - Train Loss: 0.028768 - Val Loss: 0.048826\n",
      "Epoch 18/50 - Train Loss: 0.031213 - Val Loss: 0.045509\n",
      "Epoch 19/50 - Train Loss: 0.027959 - Val Loss: 0.045735\n",
      "Epoch 20/50 - Train Loss: 0.031054 - Val Loss: 0.046899\n",
      "Epoch 21/50 - Train Loss: 0.032269 - Val Loss: 0.043794\n",
      "Epoch 22/50 - Train Loss: 0.029827 - Val Loss: 0.047228\n",
      "Epoch 23/50 - Train Loss: 0.033138 - Val Loss: 0.046284\n",
      "Epoch 24/50 - Train Loss: 0.029912 - Val Loss: 0.045233\n",
      "Epoch 25/50 - Train Loss: 0.029691 - Val Loss: 0.044961\n",
      "Epoch 26/50 - Train Loss: 0.031362 - Val Loss: 0.046270\n",
      "Epoch 27/50 - Train Loss: 0.034175 - Val Loss: 0.047079\n",
      "Epoch 28/50 - Train Loss: 0.029660 - Val Loss: 0.044088\n",
      "Epoch 29/50 - Train Loss: 0.029766 - Val Loss: 0.043332\n",
      "Epoch 30/50 - Train Loss: 0.028893 - Val Loss: 0.043893\n",
      "Epoch 31/50 - Train Loss: 0.029419 - Val Loss: 0.044836\n",
      "Epoch 32/50 - Train Loss: 0.029395 - Val Loss: 0.044356\n",
      "Epoch 33/50 - Train Loss: 0.032266 - Val Loss: 0.043325\n",
      "Epoch 34/50 - Train Loss: 0.027618 - Val Loss: 0.048833\n",
      "Epoch 35/50 - Train Loss: 0.034116 - Val Loss: 0.045168\n",
      "Epoch 36/50 - Train Loss: 0.032370 - Val Loss: 0.050272\n",
      "Epoch 37/50 - Train Loss: 0.029896 - Val Loss: 0.045189\n",
      "Epoch 38/50 - Train Loss: 0.029672 - Val Loss: 0.043137\n",
      "Epoch 39/50 - Train Loss: 0.029605 - Val Loss: 0.046329\n",
      "Epoch 40/50 - Train Loss: 0.031407 - Val Loss: 0.044176\n",
      "Epoch 41/50 - Train Loss: 0.027926 - Val Loss: 0.044304\n",
      "Epoch 42/50 - Train Loss: 0.031139 - Val Loss: 0.043991\n",
      "Epoch 43/50 - Train Loss: 0.030918 - Val Loss: 0.045230\n",
      "Epoch 44/50 - Train Loss: 0.027980 - Val Loss: 0.043267\n",
      "Epoch 45/50 - Train Loss: 0.031652 - Val Loss: 0.043167\n",
      "Epoch 46/50 - Train Loss: 0.027483 - Val Loss: 0.046526\n",
      "Epoch 47/50 - Train Loss: 0.030398 - Val Loss: 0.046931\n",
      "Epoch 48/50 - Train Loss: 0.030862 - Val Loss: 0.046142\n",
      "Epoch 49/50 - Train Loss: 0.031561 - Val Loss: 0.047237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:48:09,738] Trial 77 finished with value: 0.04313663952052593 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 226, 'lr': 0.003319098293959013, 'weight_decay': 3.5310944722768497e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032895 - Val Loss: 0.048253\n",
      "Epoch 1/50 - Train Loss: 0.105883 - Val Loss: 0.217423\n",
      "Epoch 2/50 - Train Loss: 0.053681 - Val Loss: 0.059931\n",
      "Epoch 3/50 - Train Loss: 0.055061 - Val Loss: 0.096842\n",
      "Epoch 4/50 - Train Loss: 0.038975 - Val Loss: 0.048575\n",
      "Epoch 5/50 - Train Loss: 0.037219 - Val Loss: 0.049894\n",
      "Epoch 6/50 - Train Loss: 0.032937 - Val Loss: 0.050779\n",
      "Epoch 7/50 - Train Loss: 0.034407 - Val Loss: 0.046008\n",
      "Epoch 8/50 - Train Loss: 0.028759 - Val Loss: 0.050628\n",
      "Epoch 9/50 - Train Loss: 0.031962 - Val Loss: 0.045568\n",
      "Epoch 10/50 - Train Loss: 0.033528 - Val Loss: 0.046219\n",
      "Epoch 11/50 - Train Loss: 0.031082 - Val Loss: 0.054963\n",
      "Epoch 12/50 - Train Loss: 0.031838 - Val Loss: 0.044728\n",
      "Epoch 13/50 - Train Loss: 0.030867 - Val Loss: 0.051040\n",
      "Epoch 14/50 - Train Loss: 0.032960 - Val Loss: 0.046555\n",
      "Epoch 15/50 - Train Loss: 0.032455 - Val Loss: 0.044499\n",
      "Epoch 16/50 - Train Loss: 0.030523 - Val Loss: 0.049158\n",
      "Epoch 17/50 - Train Loss: 0.027457 - Val Loss: 0.045823\n",
      "Epoch 18/50 - Train Loss: 0.031734 - Val Loss: 0.048842\n",
      "Epoch 19/50 - Train Loss: 0.030110 - Val Loss: 0.047912\n",
      "Epoch 20/50 - Train Loss: 0.033850 - Val Loss: 0.044491\n",
      "Epoch 21/50 - Train Loss: 0.033280 - Val Loss: 0.048038\n",
      "Epoch 22/50 - Train Loss: 0.029760 - Val Loss: 0.049527\n",
      "Epoch 23/50 - Train Loss: 0.026874 - Val Loss: 0.044050\n",
      "Epoch 24/50 - Train Loss: 0.029446 - Val Loss: 0.049782\n",
      "Epoch 25/50 - Train Loss: 0.032557 - Val Loss: 0.051993\n",
      "Epoch 26/50 - Train Loss: 0.030585 - Val Loss: 0.045446\n",
      "Epoch 27/50 - Train Loss: 0.032730 - Val Loss: 0.048568\n",
      "Epoch 28/50 - Train Loss: 0.034169 - Val Loss: 0.051613\n",
      "Epoch 29/50 - Train Loss: 0.033276 - Val Loss: 0.050098\n",
      "Epoch 30/50 - Train Loss: 0.033034 - Val Loss: 0.044480\n",
      "Epoch 31/50 - Train Loss: 0.026902 - Val Loss: 0.045199\n",
      "Epoch 32/50 - Train Loss: 0.030136 - Val Loss: 0.044842\n",
      "Epoch 33/50 - Train Loss: 0.029122 - Val Loss: 0.045918\n",
      "Epoch 34/50 - Train Loss: 0.029074 - Val Loss: 0.048819\n",
      "Epoch 35/50 - Train Loss: 0.035917 - Val Loss: 0.047470\n",
      "Epoch 36/50 - Train Loss: 0.031144 - Val Loss: 0.046755\n",
      "Epoch 37/50 - Train Loss: 0.031994 - Val Loss: 0.046690\n",
      "Epoch 38/50 - Train Loss: 0.029887 - Val Loss: 0.048649\n",
      "Epoch 39/50 - Train Loss: 0.029350 - Val Loss: 0.046134\n",
      "Epoch 40/50 - Train Loss: 0.030187 - Val Loss: 0.050234\n",
      "Epoch 41/50 - Train Loss: 0.032894 - Val Loss: 0.049090\n",
      "Epoch 42/50 - Train Loss: 0.031447 - Val Loss: 0.056016\n",
      "Epoch 43/50 - Train Loss: 0.028012 - Val Loss: 0.046659\n",
      "Epoch 44/50 - Train Loss: 0.031743 - Val Loss: 0.046999\n",
      "Epoch 45/50 - Train Loss: 0.029401 - Val Loss: 0.049747\n",
      "Epoch 46/50 - Train Loss: 0.028974 - Val Loss: 0.044141\n",
      "Epoch 47/50 - Train Loss: 0.028639 - Val Loss: 0.045917\n",
      "Epoch 48/50 - Train Loss: 0.033212 - Val Loss: 0.045451\n",
      "Epoch 49/50 - Train Loss: 0.031558 - Val Loss: 0.045320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:48:43,416] Trial 78 finished with value: 0.044049955904483795 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 239, 'lr': 0.0030397150279410797, 'weight_decay': 9.788076154009533e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032458 - Val Loss: 0.045169\n",
      "Epoch 1/50 - Train Loss: 0.156972 - Val Loss: 0.132596\n",
      "Epoch 2/50 - Train Loss: 0.052636 - Val Loss: 0.097020\n",
      "Epoch 3/50 - Train Loss: 0.036740 - Val Loss: 0.056018\n",
      "Epoch 4/50 - Train Loss: 0.034006 - Val Loss: 0.053957\n",
      "Epoch 5/50 - Train Loss: 0.031203 - Val Loss: 0.054562\n",
      "Epoch 6/50 - Train Loss: 0.032052 - Val Loss: 0.054605\n",
      "Epoch 7/50 - Train Loss: 0.034335 - Val Loss: 0.045373\n",
      "Epoch 8/50 - Train Loss: 0.035915 - Val Loss: 0.044698\n",
      "Epoch 9/50 - Train Loss: 0.029215 - Val Loss: 0.050964\n",
      "Epoch 10/50 - Train Loss: 0.034405 - Val Loss: 0.055453\n",
      "Epoch 11/50 - Train Loss: 0.029010 - Val Loss: 0.044659\n",
      "Epoch 12/50 - Train Loss: 0.031840 - Val Loss: 0.047195\n",
      "Epoch 13/50 - Train Loss: 0.032025 - Val Loss: 0.050912\n",
      "Epoch 14/50 - Train Loss: 0.032189 - Val Loss: 0.044971\n",
      "Epoch 15/50 - Train Loss: 0.033355 - Val Loss: 0.045137\n",
      "Epoch 16/50 - Train Loss: 0.025636 - Val Loss: 0.044266\n",
      "Epoch 17/50 - Train Loss: 0.031595 - Val Loss: 0.049039\n",
      "Epoch 18/50 - Train Loss: 0.031035 - Val Loss: 0.045408\n",
      "Epoch 19/50 - Train Loss: 0.034604 - Val Loss: 0.047379\n",
      "Epoch 20/50 - Train Loss: 0.032699 - Val Loss: 0.047039\n",
      "Epoch 21/50 - Train Loss: 0.030698 - Val Loss: 0.046012\n",
      "Epoch 22/50 - Train Loss: 0.027006 - Val Loss: 0.045609\n",
      "Epoch 23/50 - Train Loss: 0.033420 - Val Loss: 0.048198\n",
      "Epoch 24/50 - Train Loss: 0.029762 - Val Loss: 0.045110\n",
      "Epoch 25/50 - Train Loss: 0.031205 - Val Loss: 0.045140\n",
      "Epoch 26/50 - Train Loss: 0.035823 - Val Loss: 0.045430\n",
      "Epoch 27/50 - Train Loss: 0.033627 - Val Loss: 0.046021\n",
      "Epoch 28/50 - Train Loss: 0.032775 - Val Loss: 0.045120\n",
      "Epoch 29/50 - Train Loss: 0.030865 - Val Loss: 0.057408\n",
      "Epoch 30/50 - Train Loss: 0.031348 - Val Loss: 0.052800\n",
      "Epoch 31/50 - Train Loss: 0.035583 - Val Loss: 0.044586\n",
      "Epoch 32/50 - Train Loss: 0.030280 - Val Loss: 0.044550\n",
      "Epoch 33/50 - Train Loss: 0.027549 - Val Loss: 0.050359\n",
      "Epoch 34/50 - Train Loss: 0.030536 - Val Loss: 0.046178\n",
      "Epoch 35/50 - Train Loss: 0.030249 - Val Loss: 0.045613\n",
      "Epoch 36/50 - Train Loss: 0.032620 - Val Loss: 0.051822\n",
      "Epoch 37/50 - Train Loss: 0.032621 - Val Loss: 0.046461\n",
      "Epoch 38/50 - Train Loss: 0.030257 - Val Loss: 0.045150\n",
      "Epoch 39/50 - Train Loss: 0.029546 - Val Loss: 0.046270\n",
      "Epoch 40/50 - Train Loss: 0.034354 - Val Loss: 0.046715\n",
      "Epoch 41/50 - Train Loss: 0.029994 - Val Loss: 0.050125\n",
      "Epoch 42/50 - Train Loss: 0.028196 - Val Loss: 0.055404\n",
      "Epoch 43/50 - Train Loss: 0.030407 - Val Loss: 0.048555\n",
      "Epoch 44/50 - Train Loss: 0.028846 - Val Loss: 0.046326\n",
      "Epoch 45/50 - Train Loss: 0.032424 - Val Loss: 0.047665\n",
      "Epoch 46/50 - Train Loss: 0.032733 - Val Loss: 0.049773\n",
      "Epoch 47/50 - Train Loss: 0.032679 - Val Loss: 0.054426\n",
      "Epoch 48/50 - Train Loss: 0.031298 - Val Loss: 0.047580\n",
      "Epoch 49/50 - Train Loss: 0.031702 - Val Loss: 0.049706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:49:14,568] Trial 79 finished with value: 0.044266074895858765 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 226, 'lr': 0.006863467158933373, 'weight_decay': 5.42063169974906e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028644 - Val Loss: 0.053550\n",
      "Epoch 1/50 - Train Loss: 0.108242 - Val Loss: 0.065914\n",
      "Epoch 2/50 - Train Loss: 0.047301 - Val Loss: 0.061424\n",
      "Epoch 3/50 - Train Loss: 0.038771 - Val Loss: 0.065161\n",
      "Epoch 4/50 - Train Loss: 0.033782 - Val Loss: 0.050736\n",
      "Epoch 5/50 - Train Loss: 0.032062 - Val Loss: 0.046680\n",
      "Epoch 6/50 - Train Loss: 0.033678 - Val Loss: 0.057394\n",
      "Epoch 7/50 - Train Loss: 0.034974 - Val Loss: 0.047125\n",
      "Epoch 8/50 - Train Loss: 0.031294 - Val Loss: 0.049646\n",
      "Epoch 9/50 - Train Loss: 0.033270 - Val Loss: 0.059195\n",
      "Epoch 10/50 - Train Loss: 0.033230 - Val Loss: 0.048930\n",
      "Epoch 11/50 - Train Loss: 0.035098 - Val Loss: 0.045153\n",
      "Epoch 12/50 - Train Loss: 0.031128 - Val Loss: 0.048447\n",
      "Epoch 13/50 - Train Loss: 0.031267 - Val Loss: 0.062773\n",
      "Epoch 14/50 - Train Loss: 0.037132 - Val Loss: 0.048066\n",
      "Epoch 15/50 - Train Loss: 0.035250 - Val Loss: 0.044064\n",
      "Epoch 16/50 - Train Loss: 0.031470 - Val Loss: 0.050547\n",
      "Epoch 17/50 - Train Loss: 0.032530 - Val Loss: 0.046924\n",
      "Epoch 18/50 - Train Loss: 0.035247 - Val Loss: 0.043664\n",
      "Epoch 19/50 - Train Loss: 0.033142 - Val Loss: 0.046835\n",
      "Epoch 20/50 - Train Loss: 0.028201 - Val Loss: 0.045503\n",
      "Epoch 21/50 - Train Loss: 0.032319 - Val Loss: 0.049568\n",
      "Epoch 22/50 - Train Loss: 0.031099 - Val Loss: 0.047024\n",
      "Epoch 23/50 - Train Loss: 0.027674 - Val Loss: 0.043600\n",
      "Epoch 24/50 - Train Loss: 0.033574 - Val Loss: 0.046986\n",
      "Epoch 25/50 - Train Loss: 0.032692 - Val Loss: 0.044376\n",
      "Epoch 26/50 - Train Loss: 0.033429 - Val Loss: 0.054669\n",
      "Epoch 27/50 - Train Loss: 0.033274 - Val Loss: 0.055501\n",
      "Epoch 28/50 - Train Loss: 0.035204 - Val Loss: 0.046263\n",
      "Epoch 29/50 - Train Loss: 0.030045 - Val Loss: 0.045760\n",
      "Epoch 30/50 - Train Loss: 0.027386 - Val Loss: 0.044454\n",
      "Epoch 31/50 - Train Loss: 0.030219 - Val Loss: 0.043957\n",
      "Epoch 32/50 - Train Loss: 0.028586 - Val Loss: 0.045489\n",
      "Epoch 33/50 - Train Loss: 0.029914 - Val Loss: 0.057486\n",
      "Epoch 34/50 - Train Loss: 0.031621 - Val Loss: 0.049957\n",
      "Epoch 35/50 - Train Loss: 0.029098 - Val Loss: 0.044047\n",
      "Epoch 36/50 - Train Loss: 0.031652 - Val Loss: 0.043351\n",
      "Epoch 37/50 - Train Loss: 0.031223 - Val Loss: 0.047545\n",
      "Epoch 38/50 - Train Loss: 0.028013 - Val Loss: 0.043487\n",
      "Epoch 39/50 - Train Loss: 0.026775 - Val Loss: 0.043262\n",
      "Epoch 40/50 - Train Loss: 0.031046 - Val Loss: 0.044807\n",
      "Epoch 41/50 - Train Loss: 0.028005 - Val Loss: 0.044073\n",
      "Epoch 42/50 - Train Loss: 0.030837 - Val Loss: 0.047891\n",
      "Epoch 43/50 - Train Loss: 0.030278 - Val Loss: 0.044544\n",
      "Epoch 44/50 - Train Loss: 0.031241 - Val Loss: 0.044405\n",
      "Epoch 45/50 - Train Loss: 0.029755 - Val Loss: 0.057766\n",
      "Epoch 46/50 - Train Loss: 0.034440 - Val Loss: 0.044651\n",
      "Epoch 47/50 - Train Loss: 0.031148 - Val Loss: 0.046144\n",
      "Epoch 48/50 - Train Loss: 0.030660 - Val Loss: 0.068093\n",
      "Epoch 49/50 - Train Loss: 0.035553 - Val Loss: 0.046446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:49:50,151] Trial 80 finished with value: 0.043262261897325516 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 220, 'lr': 0.005796426104669904, 'weight_decay': 2.7986856099980432e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.034023 - Val Loss: 0.043927\n",
      "Epoch 1/50 - Train Loss: 0.089792 - Val Loss: 0.078348\n",
      "Epoch 2/50 - Train Loss: 0.053101 - Val Loss: 0.099648\n",
      "Epoch 3/50 - Train Loss: 0.040073 - Val Loss: 0.048926\n",
      "Epoch 4/50 - Train Loss: 0.036852 - Val Loss: 0.050556\n",
      "Epoch 5/50 - Train Loss: 0.033649 - Val Loss: 0.055831\n",
      "Epoch 6/50 - Train Loss: 0.032116 - Val Loss: 0.050141\n",
      "Epoch 7/50 - Train Loss: 0.030927 - Val Loss: 0.047038\n",
      "Epoch 8/50 - Train Loss: 0.030787 - Val Loss: 0.045432\n",
      "Epoch 9/50 - Train Loss: 0.032203 - Val Loss: 0.046578\n",
      "Epoch 10/50 - Train Loss: 0.033971 - Val Loss: 0.049062\n",
      "Epoch 11/50 - Train Loss: 0.031763 - Val Loss: 0.049750\n",
      "Epoch 12/50 - Train Loss: 0.033328 - Val Loss: 0.047019\n",
      "Epoch 13/50 - Train Loss: 0.033830 - Val Loss: 0.049652\n",
      "Epoch 14/50 - Train Loss: 0.031355 - Val Loss: 0.044405\n",
      "Epoch 15/50 - Train Loss: 0.029994 - Val Loss: 0.052859\n",
      "Epoch 16/50 - Train Loss: 0.030315 - Val Loss: 0.054686\n",
      "Epoch 17/50 - Train Loss: 0.034468 - Val Loss: 0.046283\n",
      "Epoch 18/50 - Train Loss: 0.034337 - Val Loss: 0.045175\n",
      "Epoch 19/50 - Train Loss: 0.027900 - Val Loss: 0.048166\n",
      "Epoch 20/50 - Train Loss: 0.034254 - Val Loss: 0.054898\n",
      "Epoch 21/50 - Train Loss: 0.031430 - Val Loss: 0.047501\n",
      "Epoch 22/50 - Train Loss: 0.032090 - Val Loss: 0.050485\n",
      "Epoch 23/50 - Train Loss: 0.031415 - Val Loss: 0.046390\n",
      "Epoch 24/50 - Train Loss: 0.033248 - Val Loss: 0.044772\n",
      "Epoch 25/50 - Train Loss: 0.033164 - Val Loss: 0.044844\n",
      "Epoch 26/50 - Train Loss: 0.031388 - Val Loss: 0.053762\n",
      "Epoch 27/50 - Train Loss: 0.029332 - Val Loss: 0.046200\n",
      "Epoch 28/50 - Train Loss: 0.029763 - Val Loss: 0.048573\n",
      "Epoch 29/50 - Train Loss: 0.031476 - Val Loss: 0.051159\n",
      "Epoch 30/50 - Train Loss: 0.028635 - Val Loss: 0.044760\n",
      "Epoch 31/50 - Train Loss: 0.031075 - Val Loss: 0.047507\n",
      "Epoch 32/50 - Train Loss: 0.032423 - Val Loss: 0.046766\n",
      "Epoch 33/50 - Train Loss: 0.027819 - Val Loss: 0.046107\n",
      "Epoch 34/50 - Train Loss: 0.029742 - Val Loss: 0.044713\n",
      "Epoch 35/50 - Train Loss: 0.027425 - Val Loss: 0.045597\n",
      "Epoch 36/50 - Train Loss: 0.032133 - Val Loss: 0.044019\n",
      "Epoch 37/50 - Train Loss: 0.029183 - Val Loss: 0.052220\n",
      "Epoch 38/50 - Train Loss: 0.030104 - Val Loss: 0.046527\n",
      "Epoch 39/50 - Train Loss: 0.031631 - Val Loss: 0.044696\n",
      "Epoch 40/50 - Train Loss: 0.032374 - Val Loss: 0.044519\n",
      "Epoch 41/50 - Train Loss: 0.028041 - Val Loss: 0.044534\n",
      "Epoch 42/50 - Train Loss: 0.030918 - Val Loss: 0.045250\n",
      "Epoch 43/50 - Train Loss: 0.028911 - Val Loss: 0.044748\n",
      "Epoch 44/50 - Train Loss: 0.032250 - Val Loss: 0.044325\n",
      "Epoch 45/50 - Train Loss: 0.031476 - Val Loss: 0.044506\n",
      "Epoch 46/50 - Train Loss: 0.031726 - Val Loss: 0.054791\n",
      "Epoch 47/50 - Train Loss: 0.032788 - Val Loss: 0.048914\n",
      "Epoch 48/50 - Train Loss: 0.029257 - Val Loss: 0.045004\n",
      "Epoch 49/50 - Train Loss: 0.033811 - Val Loss: 0.045762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:50:23,746] Trial 81 finished with value: 0.044018588960170746 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 231, 'lr': 0.005667559350019928, 'weight_decay': 2.6669335569582415e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030774 - Val Loss: 0.049308\n",
      "Epoch 1/50 - Train Loss: 0.601064 - Val Loss: 0.122552\n",
      "Epoch 2/50 - Train Loss: 0.048910 - Val Loss: 0.086366\n",
      "Epoch 3/50 - Train Loss: 0.043909 - Val Loss: 0.078226\n",
      "Epoch 4/50 - Train Loss: 0.038118 - Val Loss: 0.065931\n",
      "Epoch 5/50 - Train Loss: 0.032997 - Val Loss: 0.051315\n",
      "Epoch 6/50 - Train Loss: 0.034179 - Val Loss: 0.065653\n",
      "Epoch 7/50 - Train Loss: 0.034340 - Val Loss: 0.061931\n",
      "Epoch 8/50 - Train Loss: 0.031019 - Val Loss: 0.049841\n",
      "Epoch 9/50 - Train Loss: 0.034642 - Val Loss: 0.050435\n",
      "Epoch 10/50 - Train Loss: 0.032825 - Val Loss: 0.049305\n",
      "Epoch 11/50 - Train Loss: 0.030560 - Val Loss: 0.045304\n",
      "Epoch 12/50 - Train Loss: 0.032721 - Val Loss: 0.052292\n",
      "Epoch 13/50 - Train Loss: 0.033608 - Val Loss: 0.061023\n",
      "Epoch 14/50 - Train Loss: 0.031861 - Val Loss: 0.045693\n",
      "Epoch 15/50 - Train Loss: 0.029688 - Val Loss: 0.046374\n",
      "Epoch 16/50 - Train Loss: 0.029553 - Val Loss: 0.044620\n",
      "Epoch 17/50 - Train Loss: 0.028296 - Val Loss: 0.046965\n",
      "Epoch 18/50 - Train Loss: 0.032182 - Val Loss: 0.044494\n",
      "Epoch 19/50 - Train Loss: 0.031810 - Val Loss: 0.047318\n",
      "Epoch 20/50 - Train Loss: 0.030316 - Val Loss: 0.057774\n",
      "Epoch 21/50 - Train Loss: 0.032930 - Val Loss: 0.051787\n",
      "Epoch 22/50 - Train Loss: 0.031613 - Val Loss: 0.048859\n",
      "Epoch 23/50 - Train Loss: 0.033288 - Val Loss: 0.045153\n",
      "Epoch 24/50 - Train Loss: 0.032409 - Val Loss: 0.043519\n",
      "Epoch 25/50 - Train Loss: 0.033593 - Val Loss: 0.046165\n",
      "Epoch 26/50 - Train Loss: 0.030011 - Val Loss: 0.050903\n",
      "Epoch 27/50 - Train Loss: 0.032585 - Val Loss: 0.047397\n",
      "Epoch 28/50 - Train Loss: 0.028979 - Val Loss: 0.056473\n",
      "Epoch 29/50 - Train Loss: 0.030442 - Val Loss: 0.046066\n",
      "Epoch 30/50 - Train Loss: 0.030844 - Val Loss: 0.044442\n",
      "Epoch 31/50 - Train Loss: 0.030998 - Val Loss: 0.044849\n",
      "Epoch 32/50 - Train Loss: 0.029836 - Val Loss: 0.045218\n",
      "Epoch 33/50 - Train Loss: 0.030575 - Val Loss: 0.049858\n",
      "Epoch 34/50 - Train Loss: 0.032253 - Val Loss: 0.044052\n",
      "Epoch 35/50 - Train Loss: 0.030865 - Val Loss: 0.044978\n",
      "Epoch 36/50 - Train Loss: 0.030070 - Val Loss: 0.044457\n",
      "Epoch 37/50 - Train Loss: 0.034067 - Val Loss: 0.044786\n",
      "Epoch 38/50 - Train Loss: 0.026191 - Val Loss: 0.044192\n",
      "Epoch 39/50 - Train Loss: 0.030729 - Val Loss: 0.044289\n",
      "Epoch 40/50 - Train Loss: 0.027610 - Val Loss: 0.055088\n",
      "Epoch 41/50 - Train Loss: 0.033554 - Val Loss: 0.057473\n",
      "Epoch 42/50 - Train Loss: 0.032597 - Val Loss: 0.046454\n",
      "Epoch 43/50 - Train Loss: 0.029892 - Val Loss: 0.045963\n",
      "Epoch 44/50 - Train Loss: 0.028626 - Val Loss: 0.045176\n",
      "Epoch 45/50 - Train Loss: 0.029979 - Val Loss: 0.045074\n",
      "Epoch 46/50 - Train Loss: 0.029435 - Val Loss: 0.049792\n",
      "Epoch 47/50 - Train Loss: 0.032834 - Val Loss: 0.053913\n",
      "Epoch 48/50 - Train Loss: 0.030752 - Val Loss: 0.045083\n",
      "Epoch 49/50 - Train Loss: 0.026872 - Val Loss: 0.044088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:50:58,708] Trial 82 finished with value: 0.04351876117289066 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 256, 'lr': 0.008422040090206478, 'weight_decay': 1.5547891540045797e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030725 - Val Loss: 0.046285\n",
      "Epoch 1/50 - Train Loss: 0.056214 - Val Loss: 0.057193\n",
      "Epoch 2/50 - Train Loss: 0.044990 - Val Loss: 0.049829\n",
      "Epoch 3/50 - Train Loss: 0.037211 - Val Loss: 0.061356\n",
      "Epoch 4/50 - Train Loss: 0.036534 - Val Loss: 0.063184\n",
      "Epoch 5/50 - Train Loss: 0.036813 - Val Loss: 0.049612\n",
      "Epoch 6/50 - Train Loss: 0.033090 - Val Loss: 0.053129\n",
      "Epoch 7/50 - Train Loss: 0.032589 - Val Loss: 0.045579\n",
      "Epoch 8/50 - Train Loss: 0.030986 - Val Loss: 0.051923\n",
      "Epoch 9/50 - Train Loss: 0.033221 - Val Loss: 0.047111\n",
      "Epoch 10/50 - Train Loss: 0.032672 - Val Loss: 0.047395\n",
      "Epoch 11/50 - Train Loss: 0.028398 - Val Loss: 0.045020\n",
      "Epoch 12/50 - Train Loss: 0.032271 - Val Loss: 0.051523\n",
      "Epoch 13/50 - Train Loss: 0.030662 - Val Loss: 0.052592\n",
      "Epoch 14/50 - Train Loss: 0.031967 - Val Loss: 0.047707\n",
      "Epoch 15/50 - Train Loss: 0.026818 - Val Loss: 0.044761\n",
      "Epoch 16/50 - Train Loss: 0.034156 - Val Loss: 0.044517\n",
      "Epoch 17/50 - Train Loss: 0.033434 - Val Loss: 0.044250\n",
      "Epoch 18/50 - Train Loss: 0.028634 - Val Loss: 0.045454\n",
      "Epoch 19/50 - Train Loss: 0.027078 - Val Loss: 0.043948\n",
      "Epoch 20/50 - Train Loss: 0.030520 - Val Loss: 0.044758\n",
      "Epoch 21/50 - Train Loss: 0.032013 - Val Loss: 0.046777\n",
      "Epoch 22/50 - Train Loss: 0.029855 - Val Loss: 0.043796\n",
      "Epoch 23/50 - Train Loss: 0.032640 - Val Loss: 0.044381\n",
      "Epoch 24/50 - Train Loss: 0.031685 - Val Loss: 0.049505\n",
      "Epoch 25/50 - Train Loss: 0.029626 - Val Loss: 0.088192\n",
      "Epoch 26/50 - Train Loss: 0.040303 - Val Loss: 0.046011\n",
      "Epoch 27/50 - Train Loss: 0.035062 - Val Loss: 0.043926\n",
      "Epoch 28/50 - Train Loss: 0.033200 - Val Loss: 0.056511\n",
      "Epoch 29/50 - Train Loss: 0.031596 - Val Loss: 0.048800\n",
      "Epoch 30/50 - Train Loss: 0.027328 - Val Loss: 0.044087\n",
      "Epoch 31/50 - Train Loss: 0.034392 - Val Loss: 0.044749\n",
      "Epoch 32/50 - Train Loss: 0.032192 - Val Loss: 0.047896\n",
      "Epoch 33/50 - Train Loss: 0.028249 - Val Loss: 0.051075\n",
      "Epoch 34/50 - Train Loss: 0.030796 - Val Loss: 0.046106\n",
      "Epoch 35/50 - Train Loss: 0.029490 - Val Loss: 0.043789\n",
      "Epoch 36/50 - Train Loss: 0.026174 - Val Loss: 0.044854\n",
      "Epoch 37/50 - Train Loss: 0.032287 - Val Loss: 0.052140\n",
      "Epoch 38/50 - Train Loss: 0.029833 - Val Loss: 0.044030\n",
      "Epoch 39/50 - Train Loss: 0.032166 - Val Loss: 0.043781\n",
      "Epoch 40/50 - Train Loss: 0.030160 - Val Loss: 0.047610\n",
      "Epoch 41/50 - Train Loss: 0.029445 - Val Loss: 0.043894\n",
      "Epoch 42/50 - Train Loss: 0.032936 - Val Loss: 0.044275\n",
      "Epoch 43/50 - Train Loss: 0.028911 - Val Loss: 0.047770\n",
      "Epoch 44/50 - Train Loss: 0.029396 - Val Loss: 0.045683\n",
      "Epoch 45/50 - Train Loss: 0.028162 - Val Loss: 0.045435\n",
      "Epoch 46/50 - Train Loss: 0.030971 - Val Loss: 0.044393\n",
      "Epoch 47/50 - Train Loss: 0.034188 - Val Loss: 0.049496\n",
      "Epoch 48/50 - Train Loss: 0.031627 - Val Loss: 0.046255\n",
      "Epoch 49/50 - Train Loss: 0.026347 - Val Loss: 0.045317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:51:30,068] Trial 83 finished with value: 0.0437813475728035 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 208, 'lr': 0.003727095051539527, 'weight_decay': 1.796021601536675e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028822 - Val Loss: 0.043832\n",
      "Epoch 1/50 - Train Loss: 0.522072 - Val Loss: 0.145456\n",
      "Epoch 2/50 - Train Loss: 0.047277 - Val Loss: 0.064996\n",
      "Epoch 3/50 - Train Loss: 0.040299 - Val Loss: 0.050391\n",
      "Epoch 4/50 - Train Loss: 0.033965 - Val Loss: 0.052103\n",
      "Epoch 5/50 - Train Loss: 0.035494 - Val Loss: 0.048449\n",
      "Epoch 6/50 - Train Loss: 0.047666 - Val Loss: 0.073234\n",
      "Epoch 7/50 - Train Loss: 0.053427 - Val Loss: 0.072970\n",
      "Epoch 8/50 - Train Loss: 0.039673 - Val Loss: 0.064504\n",
      "Epoch 9/50 - Train Loss: 0.041638 - Val Loss: 0.046797\n",
      "Epoch 10/50 - Train Loss: 0.031864 - Val Loss: 0.052413\n",
      "Epoch 11/50 - Train Loss: 0.030397 - Val Loss: 0.045280\n",
      "Epoch 12/50 - Train Loss: 0.030579 - Val Loss: 0.050278\n",
      "Epoch 13/50 - Train Loss: 0.032842 - Val Loss: 0.044921\n",
      "Epoch 14/50 - Train Loss: 0.032242 - Val Loss: 0.045437\n",
      "Epoch 15/50 - Train Loss: 0.032941 - Val Loss: 0.049282\n",
      "Epoch 16/50 - Train Loss: 0.028017 - Val Loss: 0.054862\n",
      "Epoch 17/50 - Train Loss: 0.029980 - Val Loss: 0.048826\n",
      "Epoch 18/50 - Train Loss: 0.030199 - Val Loss: 0.048199\n",
      "Epoch 19/50 - Train Loss: 0.032442 - Val Loss: 0.050717\n",
      "Epoch 20/50 - Train Loss: 0.031013 - Val Loss: 0.046203\n",
      "Epoch 21/50 - Train Loss: 0.032359 - Val Loss: 0.046566\n",
      "Epoch 22/50 - Train Loss: 0.030118 - Val Loss: 0.052228\n",
      "Epoch 23/50 - Train Loss: 0.029205 - Val Loss: 0.046734\n",
      "Epoch 24/50 - Train Loss: 0.034471 - Val Loss: 0.047950\n",
      "Epoch 25/50 - Train Loss: 0.031026 - Val Loss: 0.044344\n",
      "Epoch 26/50 - Train Loss: 0.031582 - Val Loss: 0.043901\n",
      "Epoch 27/50 - Train Loss: 0.030751 - Val Loss: 0.052906\n",
      "Epoch 28/50 - Train Loss: 0.032932 - Val Loss: 0.071269\n",
      "Epoch 29/50 - Train Loss: 0.035271 - Val Loss: 0.060020\n",
      "Epoch 30/50 - Train Loss: 0.036472 - Val Loss: 0.051649\n",
      "Epoch 31/50 - Train Loss: 0.036419 - Val Loss: 0.052651\n",
      "Epoch 32/50 - Train Loss: 0.029181 - Val Loss: 0.054385\n",
      "Epoch 33/50 - Train Loss: 0.033288 - Val Loss: 0.045213\n",
      "Epoch 34/50 - Train Loss: 0.034645 - Val Loss: 0.044122\n",
      "Epoch 35/50 - Train Loss: 0.030954 - Val Loss: 0.066790\n",
      "Epoch 36/50 - Train Loss: 0.033715 - Val Loss: 0.048215\n",
      "Epoch 37/50 - Train Loss: 0.035847 - Val Loss: 0.046073\n",
      "Epoch 38/50 - Train Loss: 0.031741 - Val Loss: 0.046030\n",
      "Epoch 39/50 - Train Loss: 0.029667 - Val Loss: 0.053237\n",
      "Epoch 40/50 - Train Loss: 0.028285 - Val Loss: 0.045338\n",
      "Epoch 41/50 - Train Loss: 0.025084 - Val Loss: 0.045374\n",
      "Epoch 42/50 - Train Loss: 0.026519 - Val Loss: 0.044545\n",
      "Epoch 43/50 - Train Loss: 0.028769 - Val Loss: 0.045221\n",
      "Epoch 44/50 - Train Loss: 0.029205 - Val Loss: 0.046965\n",
      "Epoch 45/50 - Train Loss: 0.031639 - Val Loss: 0.045692\n",
      "Epoch 46/50 - Train Loss: 0.026833 - Val Loss: 0.044560\n",
      "Epoch 47/50 - Train Loss: 0.029047 - Val Loss: 0.060981\n",
      "Epoch 48/50 - Train Loss: 0.032264 - Val Loss: 0.046919\n",
      "Epoch 49/50 - Train Loss: 0.026291 - Val Loss: 0.044659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:52:05,699] Trial 84 finished with value: 0.04390106536448002 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 255, 'lr': 0.009636796386425093, 'weight_decay': 1.159506910809757e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028871 - Val Loss: 0.045673\n",
      "Epoch 1/50 - Train Loss: 0.216759 - Val Loss: 0.208664\n",
      "Epoch 2/50 - Train Loss: 0.067020 - Val Loss: 0.084476\n",
      "Epoch 3/50 - Train Loss: 0.048003 - Val Loss: 0.079903\n",
      "Epoch 4/50 - Train Loss: 0.037582 - Val Loss: 0.084385\n",
      "Epoch 5/50 - Train Loss: 0.039330 - Val Loss: 0.057732\n",
      "Epoch 6/50 - Train Loss: 0.034455 - Val Loss: 0.048463\n",
      "Epoch 7/50 - Train Loss: 0.036523 - Val Loss: 0.057738\n",
      "Epoch 8/50 - Train Loss: 0.036056 - Val Loss: 0.059894\n",
      "Epoch 9/50 - Train Loss: 0.035272 - Val Loss: 0.047328\n",
      "Epoch 10/50 - Train Loss: 0.033227 - Val Loss: 0.055330\n",
      "Epoch 11/50 - Train Loss: 0.033834 - Val Loss: 0.047329\n",
      "Epoch 12/50 - Train Loss: 0.029952 - Val Loss: 0.045545\n",
      "Epoch 13/50 - Train Loss: 0.032266 - Val Loss: 0.046715\n",
      "Epoch 14/50 - Train Loss: 0.027466 - Val Loss: 0.048806\n",
      "Epoch 15/50 - Train Loss: 0.031405 - Val Loss: 0.049726\n",
      "Epoch 16/50 - Train Loss: 0.030432 - Val Loss: 0.045746\n",
      "Epoch 17/50 - Train Loss: 0.027625 - Val Loss: 0.047431\n",
      "Epoch 18/50 - Train Loss: 0.034588 - Val Loss: 0.045855\n",
      "Epoch 19/50 - Train Loss: 0.032875 - Val Loss: 0.046157\n",
      "Epoch 20/50 - Train Loss: 0.032448 - Val Loss: 0.046073\n",
      "Epoch 21/50 - Train Loss: 0.032064 - Val Loss: 0.047863\n",
      "Epoch 22/50 - Train Loss: 0.026698 - Val Loss: 0.045360\n",
      "Epoch 23/50 - Train Loss: 0.027781 - Val Loss: 0.044656\n",
      "Epoch 24/50 - Train Loss: 0.029412 - Val Loss: 0.046511\n",
      "Epoch 25/50 - Train Loss: 0.031842 - Val Loss: 0.044691\n",
      "Epoch 26/50 - Train Loss: 0.031574 - Val Loss: 0.045400\n",
      "Epoch 27/50 - Train Loss: 0.033507 - Val Loss: 0.043881\n",
      "Epoch 28/50 - Train Loss: 0.030540 - Val Loss: 0.045625\n",
      "Epoch 29/50 - Train Loss: 0.031933 - Val Loss: 0.045312\n",
      "Epoch 30/50 - Train Loss: 0.034286 - Val Loss: 0.048206\n",
      "Epoch 31/50 - Train Loss: 0.032075 - Val Loss: 0.044581\n",
      "Epoch 32/50 - Train Loss: 0.033954 - Val Loss: 0.046110\n",
      "Epoch 33/50 - Train Loss: 0.031277 - Val Loss: 0.045122\n",
      "Epoch 34/50 - Train Loss: 0.032238 - Val Loss: 0.045066\n",
      "Epoch 35/50 - Train Loss: 0.030185 - Val Loss: 0.048907\n",
      "Epoch 36/50 - Train Loss: 0.030786 - Val Loss: 0.045529\n",
      "Epoch 37/50 - Train Loss: 0.032631 - Val Loss: 0.044094\n",
      "Epoch 38/50 - Train Loss: 0.031120 - Val Loss: 0.044974\n",
      "Epoch 39/50 - Train Loss: 0.029696 - Val Loss: 0.044050\n",
      "Epoch 40/50 - Train Loss: 0.030430 - Val Loss: 0.046155\n",
      "Epoch 41/50 - Train Loss: 0.033016 - Val Loss: 0.049534\n",
      "Epoch 42/50 - Train Loss: 0.028637 - Val Loss: 0.045834\n",
      "Epoch 43/50 - Train Loss: 0.029837 - Val Loss: 0.046500\n",
      "Epoch 44/50 - Train Loss: 0.028211 - Val Loss: 0.044960\n",
      "Epoch 45/50 - Train Loss: 0.026801 - Val Loss: 0.045386\n",
      "Epoch 46/50 - Train Loss: 0.032862 - Val Loss: 0.045670\n",
      "Epoch 47/50 - Train Loss: 0.032657 - Val Loss: 0.045302\n",
      "Epoch 48/50 - Train Loss: 0.030964 - Val Loss: 0.044739\n",
      "Epoch 49/50 - Train Loss: 0.032092 - Val Loss: 0.045893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:52:39,999] Trial 85 finished with value: 0.043880634009838104 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 248, 'lr': 0.006548295162456248, 'weight_decay': 3.725116925526248e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028259 - Val Loss: 0.045117\n",
      "Epoch 1/50 - Train Loss: 0.173458 - Val Loss: 0.294442\n",
      "Epoch 2/50 - Train Loss: 0.105929 - Val Loss: 0.089557\n",
      "Epoch 3/50 - Train Loss: 0.051194 - Val Loss: 0.101993\n",
      "Epoch 4/50 - Train Loss: 0.043233 - Val Loss: 0.058688\n",
      "Epoch 5/50 - Train Loss: 0.039102 - Val Loss: 0.072493\n",
      "Epoch 6/50 - Train Loss: 0.037848 - Val Loss: 0.046068\n",
      "Epoch 7/50 - Train Loss: 0.028966 - Val Loss: 0.050559\n",
      "Epoch 8/50 - Train Loss: 0.033931 - Val Loss: 0.045072\n",
      "Epoch 9/50 - Train Loss: 0.031457 - Val Loss: 0.053505\n",
      "Epoch 10/50 - Train Loss: 0.033320 - Val Loss: 0.053250\n",
      "Epoch 11/50 - Train Loss: 0.032303 - Val Loss: 0.055769\n",
      "Epoch 12/50 - Train Loss: 0.032234 - Val Loss: 0.045153\n",
      "Epoch 13/50 - Train Loss: 0.032467 - Val Loss: 0.045029\n",
      "Epoch 14/50 - Train Loss: 0.030565 - Val Loss: 0.045700\n",
      "Epoch 15/50 - Train Loss: 0.034878 - Val Loss: 0.044176\n",
      "Epoch 16/50 - Train Loss: 0.031829 - Val Loss: 0.048354\n",
      "Epoch 17/50 - Train Loss: 0.033431 - Val Loss: 0.054243\n",
      "Epoch 18/50 - Train Loss: 0.030343 - Val Loss: 0.050961\n",
      "Epoch 19/50 - Train Loss: 0.028821 - Val Loss: 0.047253\n",
      "Epoch 20/50 - Train Loss: 0.029030 - Val Loss: 0.046209\n",
      "Epoch 21/50 - Train Loss: 0.030600 - Val Loss: 0.044086\n",
      "Epoch 22/50 - Train Loss: 0.031599 - Val Loss: 0.046669\n",
      "Epoch 23/50 - Train Loss: 0.032290 - Val Loss: 0.051609\n",
      "Epoch 24/50 - Train Loss: 0.033012 - Val Loss: 0.051982\n",
      "Epoch 25/50 - Train Loss: 0.030180 - Val Loss: 0.047533\n",
      "Epoch 26/50 - Train Loss: 0.027856 - Val Loss: 0.045913\n",
      "Epoch 27/50 - Train Loss: 0.031306 - Val Loss: 0.044444\n",
      "Epoch 28/50 - Train Loss: 0.029818 - Val Loss: 0.045538\n",
      "Epoch 29/50 - Train Loss: 0.030124 - Val Loss: 0.047037\n",
      "Epoch 30/50 - Train Loss: 0.031885 - Val Loss: 0.064409\n",
      "Epoch 31/50 - Train Loss: 0.034379 - Val Loss: 0.058015\n",
      "Epoch 32/50 - Train Loss: 0.031019 - Val Loss: 0.044163\n",
      "Epoch 33/50 - Train Loss: 0.030451 - Val Loss: 0.046155\n",
      "Epoch 34/50 - Train Loss: 0.031348 - Val Loss: 0.053594\n",
      "Epoch 35/50 - Train Loss: 0.032772 - Val Loss: 0.053273\n",
      "Epoch 36/50 - Train Loss: 0.032297 - Val Loss: 0.044513\n",
      "Epoch 37/50 - Train Loss: 0.026967 - Val Loss: 0.044930\n",
      "Epoch 38/50 - Train Loss: 0.031442 - Val Loss: 0.045466\n",
      "Epoch 39/50 - Train Loss: 0.030560 - Val Loss: 0.045737\n",
      "Epoch 40/50 - Train Loss: 0.029269 - Val Loss: 0.044431\n",
      "Epoch 41/50 - Train Loss: 0.029483 - Val Loss: 0.044625\n",
      "Epoch 42/50 - Train Loss: 0.034630 - Val Loss: 0.049425\n",
      "Epoch 43/50 - Train Loss: 0.029339 - Val Loss: 0.045396\n",
      "Epoch 44/50 - Train Loss: 0.030524 - Val Loss: 0.044746\n",
      "Epoch 45/50 - Train Loss: 0.034280 - Val Loss: 0.054139\n",
      "Epoch 46/50 - Train Loss: 0.030951 - Val Loss: 0.050728\n",
      "Epoch 47/50 - Train Loss: 0.031465 - Val Loss: 0.047484\n",
      "Epoch 48/50 - Train Loss: 0.029075 - Val Loss: 0.045913\n",
      "Epoch 49/50 - Train Loss: 0.031705 - Val Loss: 0.044683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:53:12,253] Trial 86 finished with value: 0.04408622719347477 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 217, 'lr': 0.005121889695130552, 'weight_decay': 2.331905151770857e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028945 - Val Loss: 0.047102\n",
      "Epoch 1/50 - Train Loss: 0.194101 - Val Loss: 0.162814\n",
      "Epoch 2/50 - Train Loss: 0.055884 - Val Loss: 0.114961\n",
      "Epoch 3/50 - Train Loss: 0.045738 - Val Loss: 0.060043\n",
      "Epoch 4/50 - Train Loss: 0.041430 - Val Loss: 0.080196\n",
      "Epoch 5/50 - Train Loss: 0.037952 - Val Loss: 0.052003\n",
      "Epoch 6/50 - Train Loss: 0.037754 - Val Loss: 0.047575\n",
      "Epoch 7/50 - Train Loss: 0.032936 - Val Loss: 0.046880\n",
      "Epoch 8/50 - Train Loss: 0.033524 - Val Loss: 0.048781\n",
      "Epoch 9/50 - Train Loss: 0.034515 - Val Loss: 0.058751\n",
      "Epoch 10/50 - Train Loss: 0.033596 - Val Loss: 0.067255\n",
      "Epoch 11/50 - Train Loss: 0.037820 - Val Loss: 0.056396\n",
      "Epoch 12/50 - Train Loss: 0.031146 - Val Loss: 0.052408\n",
      "Epoch 13/50 - Train Loss: 0.034868 - Val Loss: 0.044679\n",
      "Epoch 14/50 - Train Loss: 0.033678 - Val Loss: 0.046453\n",
      "Epoch 15/50 - Train Loss: 0.031572 - Val Loss: 0.044951\n",
      "Epoch 16/50 - Train Loss: 0.032647 - Val Loss: 0.044506\n",
      "Epoch 17/50 - Train Loss: 0.032762 - Val Loss: 0.045063\n",
      "Epoch 18/50 - Train Loss: 0.032130 - Val Loss: 0.045101\n",
      "Epoch 19/50 - Train Loss: 0.035812 - Val Loss: 0.052476\n",
      "Epoch 20/50 - Train Loss: 0.034543 - Val Loss: 0.052907\n",
      "Epoch 21/50 - Train Loss: 0.032383 - Val Loss: 0.044707\n",
      "Epoch 22/50 - Train Loss: 0.032155 - Val Loss: 0.048628\n",
      "Epoch 23/50 - Train Loss: 0.030711 - Val Loss: 0.046678\n",
      "Epoch 24/50 - Train Loss: 0.032475 - Val Loss: 0.045493\n",
      "Epoch 25/50 - Train Loss: 0.029865 - Val Loss: 0.047140\n",
      "Epoch 26/50 - Train Loss: 0.032300 - Val Loss: 0.045633\n",
      "Epoch 27/50 - Train Loss: 0.030018 - Val Loss: 0.048013\n",
      "Epoch 28/50 - Train Loss: 0.031387 - Val Loss: 0.046541\n",
      "Epoch 29/50 - Train Loss: 0.032201 - Val Loss: 0.049185\n",
      "Epoch 30/50 - Train Loss: 0.030669 - Val Loss: 0.044800\n",
      "Epoch 31/50 - Train Loss: 0.030117 - Val Loss: 0.056556\n",
      "Epoch 32/50 - Train Loss: 0.028163 - Val Loss: 0.048885\n",
      "Epoch 33/50 - Train Loss: 0.031215 - Val Loss: 0.047735\n",
      "Epoch 34/50 - Train Loss: 0.027834 - Val Loss: 0.044930\n",
      "Epoch 35/50 - Train Loss: 0.028903 - Val Loss: 0.045109\n",
      "Epoch 36/50 - Train Loss: 0.030488 - Val Loss: 0.044329\n",
      "Epoch 37/50 - Train Loss: 0.033429 - Val Loss: 0.054274\n",
      "Epoch 38/50 - Train Loss: 0.031100 - Val Loss: 0.047909\n",
      "Epoch 39/50 - Train Loss: 0.034709 - Val Loss: 0.047247\n",
      "Epoch 40/50 - Train Loss: 0.030948 - Val Loss: 0.046981\n",
      "Epoch 41/50 - Train Loss: 0.029612 - Val Loss: 0.046008\n",
      "Epoch 42/50 - Train Loss: 0.028291 - Val Loss: 0.045284\n",
      "Epoch 43/50 - Train Loss: 0.030648 - Val Loss: 0.044552\n",
      "Epoch 44/50 - Train Loss: 0.031393 - Val Loss: 0.046529\n",
      "Epoch 45/50 - Train Loss: 0.027833 - Val Loss: 0.051061\n",
      "Epoch 46/50 - Train Loss: 0.031947 - Val Loss: 0.045562\n",
      "Epoch 47/50 - Train Loss: 0.029279 - Val Loss: 0.045836\n",
      "Epoch 48/50 - Train Loss: 0.032628 - Val Loss: 0.044340\n",
      "Epoch 49/50 - Train Loss: 0.031059 - Val Loss: 0.059256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:53:47,007] Trial 87 finished with value: 0.04432902671396732 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 231, 'lr': 0.007686388420622263, 'weight_decay': 4.8948781693614115e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033142 - Val Loss: 0.046344\n",
      "Epoch 1/50 - Train Loss: 0.159246 - Val Loss: 0.204629\n",
      "Epoch 2/50 - Train Loss: 0.065046 - Val Loss: 0.067129\n",
      "Epoch 3/50 - Train Loss: 0.052675 - Val Loss: 0.093672\n",
      "Epoch 4/50 - Train Loss: 0.040893 - Val Loss: 0.068718\n",
      "Epoch 5/50 - Train Loss: 0.039342 - Val Loss: 0.056935\n",
      "Epoch 6/50 - Train Loss: 0.035845 - Val Loss: 0.050409\n",
      "Epoch 7/50 - Train Loss: 0.037436 - Val Loss: 0.047822\n",
      "Epoch 8/50 - Train Loss: 0.031933 - Val Loss: 0.047436\n",
      "Epoch 9/50 - Train Loss: 0.033093 - Val Loss: 0.055099\n",
      "Epoch 10/50 - Train Loss: 0.033784 - Val Loss: 0.051002\n",
      "Epoch 11/50 - Train Loss: 0.032397 - Val Loss: 0.062151\n",
      "Epoch 12/50 - Train Loss: 0.033038 - Val Loss: 0.071658\n",
      "Epoch 13/50 - Train Loss: 0.034769 - Val Loss: 0.048353\n",
      "Epoch 14/50 - Train Loss: 0.030898 - Val Loss: 0.045029\n",
      "Epoch 15/50 - Train Loss: 0.033845 - Val Loss: 0.050486\n",
      "Epoch 16/50 - Train Loss: 0.031667 - Val Loss: 0.044962\n",
      "Epoch 17/50 - Train Loss: 0.029105 - Val Loss: 0.044295\n",
      "Epoch 18/50 - Train Loss: 0.029139 - Val Loss: 0.052071\n",
      "Epoch 19/50 - Train Loss: 0.033513 - Val Loss: 0.046537\n",
      "Epoch 20/50 - Train Loss: 0.032138 - Val Loss: 0.046371\n",
      "Epoch 21/50 - Train Loss: 0.033593 - Val Loss: 0.044730\n",
      "Epoch 22/50 - Train Loss: 0.032673 - Val Loss: 0.049901\n",
      "Epoch 23/50 - Train Loss: 0.031333 - Val Loss: 0.047702\n",
      "Epoch 24/50 - Train Loss: 0.032853 - Val Loss: 0.051221\n",
      "Epoch 25/50 - Train Loss: 0.034190 - Val Loss: 0.045500\n",
      "Epoch 26/50 - Train Loss: 0.030581 - Val Loss: 0.045572\n",
      "Epoch 27/50 - Train Loss: 0.032448 - Val Loss: 0.044415\n",
      "Epoch 28/50 - Train Loss: 0.034195 - Val Loss: 0.044251\n",
      "Epoch 29/50 - Train Loss: 0.028739 - Val Loss: 0.044810\n",
      "Epoch 30/50 - Train Loss: 0.031668 - Val Loss: 0.045838\n",
      "Epoch 31/50 - Train Loss: 0.030626 - Val Loss: 0.054733\n",
      "Epoch 32/50 - Train Loss: 0.034210 - Val Loss: 0.045522\n",
      "Epoch 33/50 - Train Loss: 0.031305 - Val Loss: 0.045262\n",
      "Epoch 34/50 - Train Loss: 0.031091 - Val Loss: 0.044743\n",
      "Epoch 35/50 - Train Loss: 0.028319 - Val Loss: 0.049649\n",
      "Epoch 36/50 - Train Loss: 0.030716 - Val Loss: 0.051182\n",
      "Epoch 37/50 - Train Loss: 0.027327 - Val Loss: 0.043813\n",
      "Epoch 38/50 - Train Loss: 0.028579 - Val Loss: 0.044157\n",
      "Epoch 39/50 - Train Loss: 0.028470 - Val Loss: 0.044468\n",
      "Epoch 40/50 - Train Loss: 0.030012 - Val Loss: 0.049261\n",
      "Epoch 41/50 - Train Loss: 0.029551 - Val Loss: 0.050502\n",
      "Epoch 42/50 - Train Loss: 0.029564 - Val Loss: 0.045120\n",
      "Epoch 43/50 - Train Loss: 0.031723 - Val Loss: 0.044488\n",
      "Epoch 44/50 - Train Loss: 0.026557 - Val Loss: 0.043742\n",
      "Epoch 45/50 - Train Loss: 0.029958 - Val Loss: 0.046451\n",
      "Epoch 46/50 - Train Loss: 0.031878 - Val Loss: 0.050959\n",
      "Epoch 47/50 - Train Loss: 0.028914 - Val Loss: 0.044444\n",
      "Epoch 48/50 - Train Loss: 0.028758 - Val Loss: 0.044693\n",
      "Epoch 49/50 - Train Loss: 0.026475 - Val Loss: 0.044064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:54:27,968] Trial 88 finished with value: 0.04374230466783047 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 248, 'lr': 0.004125316430565467, 'weight_decay': 1.0915655978268081e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029447 - Val Loss: 0.045798\n",
      "Epoch 1/50 - Train Loss: 0.075551 - Val Loss: 0.110382\n",
      "Epoch 2/50 - Train Loss: 0.047392 - Val Loss: 0.075877\n",
      "Epoch 3/50 - Train Loss: 0.040764 - Val Loss: 0.054821\n",
      "Epoch 4/50 - Train Loss: 0.035260 - Val Loss: 0.061338\n",
      "Epoch 5/50 - Train Loss: 0.035352 - Val Loss: 0.048913\n",
      "Epoch 6/50 - Train Loss: 0.032013 - Val Loss: 0.047138\n",
      "Epoch 7/50 - Train Loss: 0.030725 - Val Loss: 0.055904\n",
      "Epoch 8/50 - Train Loss: 0.031331 - Val Loss: 0.044595\n",
      "Epoch 9/50 - Train Loss: 0.031874 - Val Loss: 0.044608\n",
      "Epoch 10/50 - Train Loss: 0.035627 - Val Loss: 0.051072\n",
      "Epoch 11/50 - Train Loss: 0.032848 - Val Loss: 0.050133\n",
      "Epoch 12/50 - Train Loss: 0.029076 - Val Loss: 0.053138\n",
      "Epoch 13/50 - Train Loss: 0.033347 - Val Loss: 0.045037\n",
      "Epoch 14/50 - Train Loss: 0.032044 - Val Loss: 0.043965\n",
      "Epoch 15/50 - Train Loss: 0.027399 - Val Loss: 0.046621\n",
      "Epoch 16/50 - Train Loss: 0.029030 - Val Loss: 0.046719\n",
      "Epoch 17/50 - Train Loss: 0.031666 - Val Loss: 0.043738\n",
      "Epoch 18/50 - Train Loss: 0.030643 - Val Loss: 0.043914\n",
      "Epoch 19/50 - Train Loss: 0.032708 - Val Loss: 0.046003\n",
      "Epoch 20/50 - Train Loss: 0.029778 - Val Loss: 0.051587\n",
      "Epoch 21/50 - Train Loss: 0.030384 - Val Loss: 0.046415\n",
      "Epoch 22/50 - Train Loss: 0.029868 - Val Loss: 0.044180\n",
      "Epoch 23/50 - Train Loss: 0.034407 - Val Loss: 0.045253\n",
      "Epoch 24/50 - Train Loss: 0.032543 - Val Loss: 0.047519\n",
      "Epoch 25/50 - Train Loss: 0.029797 - Val Loss: 0.044191\n",
      "Epoch 26/50 - Train Loss: 0.029397 - Val Loss: 0.044254\n",
      "Epoch 27/50 - Train Loss: 0.029055 - Val Loss: 0.044625\n",
      "Epoch 28/50 - Train Loss: 0.033202 - Val Loss: 0.044298\n",
      "Epoch 29/50 - Train Loss: 0.033212 - Val Loss: 0.044322\n",
      "Epoch 30/50 - Train Loss: 0.036519 - Val Loss: 0.045987\n",
      "Epoch 31/50 - Train Loss: 0.033067 - Val Loss: 0.047885\n",
      "Epoch 32/50 - Train Loss: 0.036129 - Val Loss: 0.049882\n",
      "Epoch 33/50 - Train Loss: 0.031962 - Val Loss: 0.050337\n",
      "Epoch 34/50 - Train Loss: 0.027847 - Val Loss: 0.048420\n",
      "Epoch 35/50 - Train Loss: 0.031475 - Val Loss: 0.047093\n",
      "Epoch 36/50 - Train Loss: 0.030672 - Val Loss: 0.047046\n",
      "Epoch 37/50 - Train Loss: 0.027742 - Val Loss: 0.045128\n",
      "Epoch 38/50 - Train Loss: 0.030997 - Val Loss: 0.045714\n",
      "Epoch 39/50 - Train Loss: 0.030172 - Val Loss: 0.044732\n",
      "Epoch 40/50 - Train Loss: 0.027412 - Val Loss: 0.045692\n",
      "Epoch 41/50 - Train Loss: 0.033153 - Val Loss: 0.047019\n",
      "Epoch 42/50 - Train Loss: 0.031078 - Val Loss: 0.049434\n",
      "Epoch 43/50 - Train Loss: 0.029935 - Val Loss: 0.045078\n",
      "Epoch 44/50 - Train Loss: 0.027332 - Val Loss: 0.049041\n",
      "Epoch 45/50 - Train Loss: 0.030102 - Val Loss: 0.049187\n",
      "Epoch 46/50 - Train Loss: 0.028713 - Val Loss: 0.044765\n",
      "Epoch 47/50 - Train Loss: 0.031511 - Val Loss: 0.047441\n",
      "Epoch 48/50 - Train Loss: 0.031752 - Val Loss: 0.046404\n",
      "Epoch 49/50 - Train Loss: 0.030582 - Val Loss: 0.045193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:54:59,360] Trial 89 finished with value: 0.043737515807151794 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 197, 'lr': 0.0031187952474819305, 'weight_decay': 1.3417971476873093e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026886 - Val Loss: 0.044900\n",
      "Epoch 1/50 - Train Loss: 0.113078 - Val Loss: 0.073655\n",
      "Epoch 2/50 - Train Loss: 0.043568 - Val Loss: 0.073155\n",
      "Epoch 3/50 - Train Loss: 0.038869 - Val Loss: 0.075171\n",
      "Epoch 4/50 - Train Loss: 0.037449 - Val Loss: 0.049669\n",
      "Epoch 5/50 - Train Loss: 0.037457 - Val Loss: 0.051442\n",
      "Epoch 6/50 - Train Loss: 0.034949 - Val Loss: 0.057996\n",
      "Epoch 7/50 - Train Loss: 0.035489 - Val Loss: 0.047364\n",
      "Epoch 8/50 - Train Loss: 0.035593 - Val Loss: 0.060922\n",
      "Epoch 9/50 - Train Loss: 0.032342 - Val Loss: 0.047163\n",
      "Epoch 10/50 - Train Loss: 0.029332 - Val Loss: 0.046373\n",
      "Epoch 11/50 - Train Loss: 0.031712 - Val Loss: 0.047362\n",
      "Epoch 12/50 - Train Loss: 0.034689 - Val Loss: 0.045082\n",
      "Epoch 13/50 - Train Loss: 0.030684 - Val Loss: 0.050418\n",
      "Epoch 14/50 - Train Loss: 0.029749 - Val Loss: 0.044456\n",
      "Epoch 15/50 - Train Loss: 0.034292 - Val Loss: 0.044807\n",
      "Epoch 16/50 - Train Loss: 0.031276 - Val Loss: 0.046655\n",
      "Epoch 17/50 - Train Loss: 0.032359 - Val Loss: 0.045037\n",
      "Epoch 18/50 - Train Loss: 0.032792 - Val Loss: 0.048583\n",
      "Epoch 19/50 - Train Loss: 0.033143 - Val Loss: 0.050723\n",
      "Epoch 20/50 - Train Loss: 0.035049 - Val Loss: 0.044958\n",
      "Epoch 21/50 - Train Loss: 0.037308 - Val Loss: 0.051119\n",
      "Epoch 22/50 - Train Loss: 0.030759 - Val Loss: 0.059178\n",
      "Epoch 23/50 - Train Loss: 0.034286 - Val Loss: 0.045907\n",
      "Epoch 24/50 - Train Loss: 0.032026 - Val Loss: 0.048265\n",
      "Epoch 25/50 - Train Loss: 0.030405 - Val Loss: 0.046490\n",
      "Epoch 26/50 - Train Loss: 0.032798 - Val Loss: 0.044579\n",
      "Epoch 27/50 - Train Loss: 0.033836 - Val Loss: 0.046464\n",
      "Epoch 28/50 - Train Loss: 0.030994 - Val Loss: 0.057207\n",
      "Epoch 29/50 - Train Loss: 0.031568 - Val Loss: 0.044108\n",
      "Epoch 30/50 - Train Loss: 0.031912 - Val Loss: 0.044911\n",
      "Epoch 31/50 - Train Loss: 0.029835 - Val Loss: 0.049920\n",
      "Epoch 32/50 - Train Loss: 0.031039 - Val Loss: 0.044820\n",
      "Epoch 33/50 - Train Loss: 0.032183 - Val Loss: 0.044022\n",
      "Epoch 34/50 - Train Loss: 0.026178 - Val Loss: 0.044001\n",
      "Epoch 35/50 - Train Loss: 0.029650 - Val Loss: 0.044679\n",
      "Epoch 36/50 - Train Loss: 0.026606 - Val Loss: 0.048116\n",
      "Epoch 37/50 - Train Loss: 0.029668 - Val Loss: 0.043705\n",
      "Epoch 38/50 - Train Loss: 0.032334 - Val Loss: 0.043891\n",
      "Epoch 39/50 - Train Loss: 0.031455 - Val Loss: 0.048880\n",
      "Epoch 40/50 - Train Loss: 0.030735 - Val Loss: 0.045877\n",
      "Epoch 41/50 - Train Loss: 0.031781 - Val Loss: 0.046012\n",
      "Epoch 42/50 - Train Loss: 0.028757 - Val Loss: 0.043939\n",
      "Epoch 43/50 - Train Loss: 0.029879 - Val Loss: 0.046034\n",
      "Epoch 44/50 - Train Loss: 0.032747 - Val Loss: 0.048208\n",
      "Epoch 45/50 - Train Loss: 0.030909 - Val Loss: 0.048454\n",
      "Epoch 46/50 - Train Loss: 0.031589 - Val Loss: 0.043844\n",
      "Epoch 47/50 - Train Loss: 0.031934 - Val Loss: 0.045422\n",
      "Epoch 48/50 - Train Loss: 0.032251 - Val Loss: 0.043776\n",
      "Epoch 49/50 - Train Loss: 0.029598 - Val Loss: 0.043981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:55:40,416] Trial 90 finished with value: 0.0437045581638813 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 227, 'lr': 0.0013717488679082226, 'weight_decay': 2.1453210400446705e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028598 - Val Loss: 0.045640\n",
      "Epoch 1/50 - Train Loss: 0.096129 - Val Loss: 0.082785\n",
      "Epoch 2/50 - Train Loss: 0.041888 - Val Loss: 0.079040\n",
      "Epoch 3/50 - Train Loss: 0.036024 - Val Loss: 0.048330\n",
      "Epoch 4/50 - Train Loss: 0.030681 - Val Loss: 0.048463\n",
      "Epoch 5/50 - Train Loss: 0.031257 - Val Loss: 0.045767\n",
      "Epoch 6/50 - Train Loss: 0.029392 - Val Loss: 0.047147\n",
      "Epoch 7/50 - Train Loss: 0.032113 - Val Loss: 0.047508\n",
      "Epoch 8/50 - Train Loss: 0.034392 - Val Loss: 0.049506\n",
      "Epoch 9/50 - Train Loss: 0.030743 - Val Loss: 0.045476\n",
      "Epoch 10/50 - Train Loss: 0.032040 - Val Loss: 0.051459\n",
      "Epoch 11/50 - Train Loss: 0.032601 - Val Loss: 0.048142\n",
      "Epoch 12/50 - Train Loss: 0.032552 - Val Loss: 0.048339\n",
      "Epoch 13/50 - Train Loss: 0.033659 - Val Loss: 0.044655\n",
      "Epoch 14/50 - Train Loss: 0.030873 - Val Loss: 0.044264\n",
      "Epoch 15/50 - Train Loss: 0.027621 - Val Loss: 0.053316\n",
      "Epoch 16/50 - Train Loss: 0.033383 - Val Loss: 0.045524\n",
      "Epoch 17/50 - Train Loss: 0.034986 - Val Loss: 0.058285\n",
      "Epoch 18/50 - Train Loss: 0.033959 - Val Loss: 0.054623\n",
      "Epoch 19/50 - Train Loss: 0.029228 - Val Loss: 0.054035\n",
      "Epoch 20/50 - Train Loss: 0.036505 - Val Loss: 0.044987\n",
      "Epoch 21/50 - Train Loss: 0.030909 - Val Loss: 0.044789\n",
      "Epoch 22/50 - Train Loss: 0.034810 - Val Loss: 0.044861\n",
      "Epoch 23/50 - Train Loss: 0.030705 - Val Loss: 0.044429\n",
      "Epoch 24/50 - Train Loss: 0.027507 - Val Loss: 0.049921\n",
      "Epoch 25/50 - Train Loss: 0.032790 - Val Loss: 0.044953\n",
      "Epoch 26/50 - Train Loss: 0.029290 - Val Loss: 0.045069\n",
      "Epoch 27/50 - Train Loss: 0.035706 - Val Loss: 0.045363\n",
      "Epoch 28/50 - Train Loss: 0.033173 - Val Loss: 0.064283\n",
      "Epoch 29/50 - Train Loss: 0.035191 - Val Loss: 0.045041\n",
      "Epoch 30/50 - Train Loss: 0.034871 - Val Loss: 0.048665\n",
      "Epoch 31/50 - Train Loss: 0.034740 - Val Loss: 0.050831\n",
      "Epoch 32/50 - Train Loss: 0.027536 - Val Loss: 0.044825\n",
      "Epoch 33/50 - Train Loss: 0.029693 - Val Loss: 0.044000\n",
      "Epoch 34/50 - Train Loss: 0.033164 - Val Loss: 0.046176\n",
      "Epoch 35/50 - Train Loss: 0.028838 - Val Loss: 0.044057\n",
      "Epoch 36/50 - Train Loss: 0.029763 - Val Loss: 0.048608\n",
      "Epoch 37/50 - Train Loss: 0.029325 - Val Loss: 0.049837\n",
      "Epoch 38/50 - Train Loss: 0.031172 - Val Loss: 0.046716\n",
      "Epoch 39/50 - Train Loss: 0.027329 - Val Loss: 0.045333\n",
      "Epoch 40/50 - Train Loss: 0.032877 - Val Loss: 0.046546\n",
      "Epoch 41/50 - Train Loss: 0.033151 - Val Loss: 0.051992\n",
      "Epoch 42/50 - Train Loss: 0.035115 - Val Loss: 0.049324\n",
      "Epoch 43/50 - Train Loss: 0.032024 - Val Loss: 0.047150\n",
      "Epoch 44/50 - Train Loss: 0.030509 - Val Loss: 0.048239\n",
      "Epoch 45/50 - Train Loss: 0.029171 - Val Loss: 0.045022\n",
      "Epoch 46/50 - Train Loss: 0.030054 - Val Loss: 0.045519\n",
      "Epoch 47/50 - Train Loss: 0.027559 - Val Loss: 0.045512\n",
      "Epoch 48/50 - Train Loss: 0.032362 - Val Loss: 0.046342\n",
      "Epoch 49/50 - Train Loss: 0.029686 - Val Loss: 0.053726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:56:07,326] Trial 91 finished with value: 0.04399961605668068 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 147, 'lr': 0.00866315304843269, 'weight_decay': 1.5903658877464732e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031876 - Val Loss: 0.048016\n",
      "Epoch 1/50 - Train Loss: 0.097240 - Val Loss: 0.063432\n",
      "Epoch 2/50 - Train Loss: 0.051564 - Val Loss: 0.137473\n",
      "Epoch 3/50 - Train Loss: 0.049082 - Val Loss: 0.053629\n",
      "Epoch 4/50 - Train Loss: 0.039806 - Val Loss: 0.048810\n",
      "Epoch 5/50 - Train Loss: 0.037727 - Val Loss: 0.055858\n",
      "Epoch 6/50 - Train Loss: 0.036478 - Val Loss: 0.051619\n",
      "Epoch 7/50 - Train Loss: 0.035226 - Val Loss: 0.050655\n",
      "Epoch 8/50 - Train Loss: 0.032165 - Val Loss: 0.046898\n",
      "Epoch 9/50 - Train Loss: 0.030416 - Val Loss: 0.046953\n",
      "Epoch 10/50 - Train Loss: 0.032989 - Val Loss: 0.043891\n",
      "Epoch 11/50 - Train Loss: 0.031253 - Val Loss: 0.044138\n",
      "Epoch 12/50 - Train Loss: 0.030789 - Val Loss: 0.049409\n",
      "Epoch 13/50 - Train Loss: 0.028861 - Val Loss: 0.046952\n",
      "Epoch 14/50 - Train Loss: 0.030184 - Val Loss: 0.043618\n",
      "Epoch 15/50 - Train Loss: 0.031853 - Val Loss: 0.051042\n",
      "Epoch 16/50 - Train Loss: 0.032192 - Val Loss: 0.060596\n",
      "Epoch 17/50 - Train Loss: 0.031957 - Val Loss: 0.045706\n",
      "Epoch 18/50 - Train Loss: 0.030717 - Val Loss: 0.046605\n",
      "Epoch 19/50 - Train Loss: 0.028624 - Val Loss: 0.044018\n",
      "Epoch 20/50 - Train Loss: 0.032983 - Val Loss: 0.043905\n",
      "Epoch 21/50 - Train Loss: 0.030620 - Val Loss: 0.051366\n",
      "Epoch 22/50 - Train Loss: 0.028089 - Val Loss: 0.046527\n",
      "Epoch 23/50 - Train Loss: 0.025703 - Val Loss: 0.043506\n",
      "Epoch 24/50 - Train Loss: 0.029398 - Val Loss: 0.050691\n",
      "Epoch 25/50 - Train Loss: 0.030641 - Val Loss: 0.045835\n",
      "Epoch 26/50 - Train Loss: 0.032003 - Val Loss: 0.044027\n",
      "Epoch 27/50 - Train Loss: 0.031454 - Val Loss: 0.043588\n",
      "Epoch 28/50 - Train Loss: 0.028232 - Val Loss: 0.044559\n",
      "Epoch 29/50 - Train Loss: 0.030298 - Val Loss: 0.049018\n",
      "Epoch 30/50 - Train Loss: 0.031334 - Val Loss: 0.047291\n",
      "Epoch 31/50 - Train Loss: 0.028695 - Val Loss: 0.045294\n",
      "Epoch 32/50 - Train Loss: 0.030760 - Val Loss: 0.043458\n",
      "Epoch 33/50 - Train Loss: 0.028099 - Val Loss: 0.043812\n",
      "Epoch 34/50 - Train Loss: 0.032640 - Val Loss: 0.047910\n",
      "Epoch 35/50 - Train Loss: 0.031921 - Val Loss: 0.043910\n",
      "Epoch 36/50 - Train Loss: 0.030621 - Val Loss: 0.044307\n",
      "Epoch 37/50 - Train Loss: 0.032157 - Val Loss: 0.044975\n",
      "Epoch 38/50 - Train Loss: 0.029683 - Val Loss: 0.043794\n",
      "Epoch 39/50 - Train Loss: 0.030678 - Val Loss: 0.044529\n",
      "Epoch 40/50 - Train Loss: 0.033739 - Val Loss: 0.044352\n",
      "Epoch 41/50 - Train Loss: 0.030665 - Val Loss: 0.055243\n",
      "Epoch 42/50 - Train Loss: 0.032702 - Val Loss: 0.046817\n",
      "Epoch 43/50 - Train Loss: 0.029794 - Val Loss: 0.044058\n",
      "Epoch 44/50 - Train Loss: 0.032182 - Val Loss: 0.044384\n",
      "Epoch 45/50 - Train Loss: 0.031390 - Val Loss: 0.044397\n",
      "Epoch 46/50 - Train Loss: 0.027243 - Val Loss: 0.049448\n",
      "Epoch 47/50 - Train Loss: 0.033105 - Val Loss: 0.047162\n",
      "Epoch 48/50 - Train Loss: 0.032189 - Val Loss: 0.046652\n",
      "Epoch 49/50 - Train Loss: 0.029878 - Val Loss: 0.050341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:56:31,984] Trial 92 finished with value: 0.04345806501805782 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 159, 'lr': 0.007027482337667604, 'weight_decay': 2.65220605857773e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028895 - Val Loss: 0.045077\n",
      "Epoch 1/50 - Train Loss: 0.118908 - Val Loss: 0.136772\n",
      "Epoch 2/50 - Train Loss: 0.047428 - Val Loss: 0.087688\n",
      "Epoch 3/50 - Train Loss: 0.042264 - Val Loss: 0.054001\n",
      "Epoch 4/50 - Train Loss: 0.038146 - Val Loss: 0.052816\n",
      "Epoch 5/50 - Train Loss: 0.036907 - Val Loss: 0.060774\n",
      "Epoch 6/50 - Train Loss: 0.033802 - Val Loss: 0.047523\n",
      "Epoch 7/50 - Train Loss: 0.035994 - Val Loss: 0.046407\n",
      "Epoch 8/50 - Train Loss: 0.034575 - Val Loss: 0.046516\n",
      "Epoch 9/50 - Train Loss: 0.031116 - Val Loss: 0.045347\n",
      "Epoch 10/50 - Train Loss: 0.035356 - Val Loss: 0.048123\n",
      "Epoch 11/50 - Train Loss: 0.033696 - Val Loss: 0.047326\n",
      "Epoch 12/50 - Train Loss: 0.032973 - Val Loss: 0.046684\n",
      "Epoch 13/50 - Train Loss: 0.032193 - Val Loss: 0.046491\n",
      "Epoch 14/50 - Train Loss: 0.029666 - Val Loss: 0.049221\n",
      "Epoch 15/50 - Train Loss: 0.030212 - Val Loss: 0.056910\n",
      "Epoch 16/50 - Train Loss: 0.032495 - Val Loss: 0.045829\n",
      "Epoch 17/50 - Train Loss: 0.029204 - Val Loss: 0.046748\n",
      "Epoch 18/50 - Train Loss: 0.028735 - Val Loss: 0.044894\n",
      "Epoch 19/50 - Train Loss: 0.031836 - Val Loss: 0.047109\n",
      "Epoch 20/50 - Train Loss: 0.034836 - Val Loss: 0.050194\n",
      "Epoch 21/50 - Train Loss: 0.030813 - Val Loss: 0.044750\n",
      "Epoch 22/50 - Train Loss: 0.030568 - Val Loss: 0.047138\n",
      "Epoch 23/50 - Train Loss: 0.030414 - Val Loss: 0.047437\n",
      "Epoch 24/50 - Train Loss: 0.032280 - Val Loss: 0.048222\n",
      "Epoch 25/50 - Train Loss: 0.032837 - Val Loss: 0.045622\n",
      "Epoch 26/50 - Train Loss: 0.032232 - Val Loss: 0.047027\n",
      "Epoch 27/50 - Train Loss: 0.029845 - Val Loss: 0.045441\n",
      "Epoch 28/50 - Train Loss: 0.030845 - Val Loss: 0.044752\n",
      "Epoch 29/50 - Train Loss: 0.034150 - Val Loss: 0.045294\n",
      "Epoch 30/50 - Train Loss: 0.029715 - Val Loss: 0.045767\n",
      "Epoch 31/50 - Train Loss: 0.030439 - Val Loss: 0.044849\n",
      "Epoch 32/50 - Train Loss: 0.031992 - Val Loss: 0.045020\n",
      "Epoch 33/50 - Train Loss: 0.030711 - Val Loss: 0.046789\n",
      "Epoch 34/50 - Train Loss: 0.030593 - Val Loss: 0.046571\n",
      "Epoch 35/50 - Train Loss: 0.033667 - Val Loss: 0.049089\n",
      "Epoch 36/50 - Train Loss: 0.032764 - Val Loss: 0.063555\n",
      "Epoch 37/50 - Train Loss: 0.034069 - Val Loss: 0.054578\n",
      "Epoch 38/50 - Train Loss: 0.033151 - Val Loss: 0.045323\n",
      "Epoch 39/50 - Train Loss: 0.029620 - Val Loss: 0.045534\n",
      "Epoch 40/50 - Train Loss: 0.032078 - Val Loss: 0.058532\n",
      "Epoch 41/50 - Train Loss: 0.034850 - Val Loss: 0.055733\n",
      "Epoch 42/50 - Train Loss: 0.033864 - Val Loss: 0.056557\n",
      "Epoch 43/50 - Train Loss: 0.035612 - Val Loss: 0.059098\n",
      "Epoch 44/50 - Train Loss: 0.032006 - Val Loss: 0.046294\n",
      "Epoch 45/50 - Train Loss: 0.034197 - Val Loss: 0.045106\n",
      "Epoch 46/50 - Train Loss: 0.030852 - Val Loss: 0.051041\n",
      "Epoch 47/50 - Train Loss: 0.032848 - Val Loss: 0.044539\n",
      "Epoch 48/50 - Train Loss: 0.029753 - Val Loss: 0.044332\n",
      "Epoch 49/50 - Train Loss: 0.032087 - Val Loss: 0.050871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:56:56,038] Trial 93 finished with value: 0.04433174803853035 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 208, 'lr': 0.006057563530048507, 'weight_decay': 2.647244103646226e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.034671 - Val Loss: 0.046173\n",
      "Epoch 1/50 - Train Loss: 0.076307 - Val Loss: 0.131345\n",
      "Epoch 2/50 - Train Loss: 0.049582 - Val Loss: 0.085002\n",
      "Epoch 3/50 - Train Loss: 0.040807 - Val Loss: 0.053093\n",
      "Epoch 4/50 - Train Loss: 0.037464 - Val Loss: 0.068738\n",
      "Epoch 5/50 - Train Loss: 0.036761 - Val Loss: 0.048648\n",
      "Epoch 6/50 - Train Loss: 0.031102 - Val Loss: 0.045899\n",
      "Epoch 7/50 - Train Loss: 0.028069 - Val Loss: 0.045383\n",
      "Epoch 8/50 - Train Loss: 0.031227 - Val Loss: 0.045605\n",
      "Epoch 9/50 - Train Loss: 0.031154 - Val Loss: 0.047988\n",
      "Epoch 10/50 - Train Loss: 0.028673 - Val Loss: 0.051748\n",
      "Epoch 11/50 - Train Loss: 0.033052 - Val Loss: 0.046522\n",
      "Epoch 12/50 - Train Loss: 0.032837 - Val Loss: 0.046261\n",
      "Epoch 13/50 - Train Loss: 0.034020 - Val Loss: 0.058460\n",
      "Epoch 14/50 - Train Loss: 0.030857 - Val Loss: 0.047688\n",
      "Epoch 15/50 - Train Loss: 0.030795 - Val Loss: 0.047022\n",
      "Epoch 16/50 - Train Loss: 0.030714 - Val Loss: 0.045413\n",
      "Epoch 17/50 - Train Loss: 0.035782 - Val Loss: 0.046191\n",
      "Epoch 18/50 - Train Loss: 0.036598 - Val Loss: 0.054588\n",
      "Epoch 19/50 - Train Loss: 0.032696 - Val Loss: 0.050039\n",
      "Epoch 20/50 - Train Loss: 0.033233 - Val Loss: 0.045098\n",
      "Epoch 21/50 - Train Loss: 0.030436 - Val Loss: 0.044875\n",
      "Epoch 22/50 - Train Loss: 0.031692 - Val Loss: 0.046842\n",
      "Epoch 23/50 - Train Loss: 0.035727 - Val Loss: 0.058033\n",
      "Epoch 24/50 - Train Loss: 0.034107 - Val Loss: 0.048934\n",
      "Epoch 25/50 - Train Loss: 0.031187 - Val Loss: 0.044512\n",
      "Epoch 26/50 - Train Loss: 0.032921 - Val Loss: 0.045387\n",
      "Epoch 27/50 - Train Loss: 0.027374 - Val Loss: 0.045122\n",
      "Epoch 28/50 - Train Loss: 0.033353 - Val Loss: 0.044580\n",
      "Epoch 29/50 - Train Loss: 0.034277 - Val Loss: 0.044719\n",
      "Epoch 30/50 - Train Loss: 0.027855 - Val Loss: 0.050908\n",
      "Epoch 31/50 - Train Loss: 0.031265 - Val Loss: 0.045665\n",
      "Epoch 32/50 - Train Loss: 0.031219 - Val Loss: 0.044604\n",
      "Epoch 33/50 - Train Loss: 0.031684 - Val Loss: 0.048111\n",
      "Epoch 34/50 - Train Loss: 0.029272 - Val Loss: 0.044414\n",
      "Epoch 35/50 - Train Loss: 0.033521 - Val Loss: 0.044631\n",
      "Epoch 36/50 - Train Loss: 0.028875 - Val Loss: 0.045299\n",
      "Epoch 37/50 - Train Loss: 0.030673 - Val Loss: 0.045401\n",
      "Epoch 38/50 - Train Loss: 0.033558 - Val Loss: 0.057217\n",
      "Epoch 39/50 - Train Loss: 0.032972 - Val Loss: 0.048588\n",
      "Epoch 40/50 - Train Loss: 0.030521 - Val Loss: 0.044075\n",
      "Epoch 41/50 - Train Loss: 0.029279 - Val Loss: 0.045359\n",
      "Epoch 42/50 - Train Loss: 0.029461 - Val Loss: 0.045133\n",
      "Epoch 43/50 - Train Loss: 0.029804 - Val Loss: 0.046021\n",
      "Epoch 44/50 - Train Loss: 0.032253 - Val Loss: 0.046836\n",
      "Epoch 45/50 - Train Loss: 0.030505 - Val Loss: 0.051701\n",
      "Epoch 46/50 - Train Loss: 0.034147 - Val Loss: 0.044938\n",
      "Epoch 47/50 - Train Loss: 0.028942 - Val Loss: 0.044658\n",
      "Epoch 48/50 - Train Loss: 0.032435 - Val Loss: 0.047642\n",
      "Epoch 49/50 - Train Loss: 0.033876 - Val Loss: 0.056440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:57:24,106] Trial 94 finished with value: 0.044075172394514084 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 175, 'lr': 0.00485002254618581, 'weight_decay': 3.027675130175354e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032079 - Val Loss: 0.046596\n",
      "Epoch 1/50 - Train Loss: 0.152064 - Val Loss: 0.318634\n",
      "Epoch 2/50 - Train Loss: 0.156076 - Val Loss: 0.298341\n",
      "Epoch 3/50 - Train Loss: 0.130800 - Val Loss: 0.278028\n",
      "Epoch 4/50 - Train Loss: 0.108277 - Val Loss: 0.259029\n",
      "Epoch 5/50 - Train Loss: 0.108949 - Val Loss: 0.240009\n",
      "Epoch 6/50 - Train Loss: 0.091528 - Val Loss: 0.221733\n",
      "Epoch 7/50 - Train Loss: 0.087953 - Val Loss: 0.204320\n",
      "Epoch 8/50 - Train Loss: 0.081440 - Val Loss: 0.186972\n",
      "Epoch 9/50 - Train Loss: 0.069959 - Val Loss: 0.170811\n",
      "Epoch 10/50 - Train Loss: 0.065167 - Val Loss: 0.156835\n",
      "Epoch 11/50 - Train Loss: 0.059422 - Val Loss: 0.143268\n",
      "Epoch 12/50 - Train Loss: 0.052666 - Val Loss: 0.131160\n",
      "Epoch 13/50 - Train Loss: 0.050578 - Val Loss: 0.120957\n",
      "Epoch 14/50 - Train Loss: 0.046484 - Val Loss: 0.112585\n",
      "Epoch 15/50 - Train Loss: 0.046361 - Val Loss: 0.106180\n",
      "Epoch 16/50 - Train Loss: 0.043352 - Val Loss: 0.101067\n",
      "Epoch 17/50 - Train Loss: 0.043807 - Val Loss: 0.097460\n",
      "Epoch 18/50 - Train Loss: 0.043406 - Val Loss: 0.094589\n",
      "Epoch 19/50 - Train Loss: 0.044592 - Val Loss: 0.092508\n",
      "Epoch 20/50 - Train Loss: 0.045220 - Val Loss: 0.090319\n",
      "Epoch 21/50 - Train Loss: 0.042130 - Val Loss: 0.089474\n",
      "Epoch 22/50 - Train Loss: 0.044478 - Val Loss: 0.089228\n",
      "Epoch 23/50 - Train Loss: 0.041882 - Val Loss: 0.088470\n",
      "Epoch 24/50 - Train Loss: 0.039463 - Val Loss: 0.087817\n",
      "Epoch 25/50 - Train Loss: 0.041458 - Val Loss: 0.086767\n",
      "Epoch 26/50 - Train Loss: 0.039351 - Val Loss: 0.085088\n",
      "Epoch 27/50 - Train Loss: 0.040166 - Val Loss: 0.084194\n",
      "Epoch 28/50 - Train Loss: 0.040590 - Val Loss: 0.083328\n",
      "Epoch 29/50 - Train Loss: 0.038418 - Val Loss: 0.082657\n",
      "Epoch 30/50 - Train Loss: 0.040001 - Val Loss: 0.081988\n",
      "Epoch 31/50 - Train Loss: 0.041515 - Val Loss: 0.081849\n",
      "Epoch 32/50 - Train Loss: 0.040868 - Val Loss: 0.081714\n",
      "Epoch 33/50 - Train Loss: 0.043002 - Val Loss: 0.081333\n",
      "Epoch 34/50 - Train Loss: 0.038596 - Val Loss: 0.080835\n",
      "Epoch 35/50 - Train Loss: 0.041047 - Val Loss: 0.080044\n",
      "Epoch 36/50 - Train Loss: 0.039829 - Val Loss: 0.078691\n",
      "Epoch 37/50 - Train Loss: 0.039187 - Val Loss: 0.077583\n",
      "Epoch 38/50 - Train Loss: 0.039540 - Val Loss: 0.076784\n",
      "Epoch 39/50 - Train Loss: 0.037947 - Val Loss: 0.076655\n",
      "Epoch 40/50 - Train Loss: 0.037599 - Val Loss: 0.076054\n",
      "Epoch 41/50 - Train Loss: 0.038826 - Val Loss: 0.074448\n",
      "Epoch 42/50 - Train Loss: 0.036560 - Val Loss: 0.073578\n",
      "Epoch 43/50 - Train Loss: 0.039277 - Val Loss: 0.072574\n",
      "Epoch 44/50 - Train Loss: 0.038370 - Val Loss: 0.071976\n",
      "Epoch 45/50 - Train Loss: 0.037320 - Val Loss: 0.071396\n",
      "Epoch 46/50 - Train Loss: 0.034417 - Val Loss: 0.070639\n",
      "Epoch 47/50 - Train Loss: 0.035987 - Val Loss: 0.069584\n",
      "Epoch 48/50 - Train Loss: 0.037345 - Val Loss: 0.069128\n",
      "Epoch 49/50 - Train Loss: 0.035272 - Val Loss: 0.068137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:57:48,532] Trial 95 finished with value: 0.06749477982521057 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 158, 'lr': 3.6836992420026804e-05, 'weight_decay': 1.7894487759848372e-07, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.038400 - Val Loss: 0.067495\n",
      "Epoch 1/50 - Train Loss: 0.095487 - Val Loss: 0.138456\n",
      "Epoch 2/50 - Train Loss: 0.049769 - Val Loss: 0.070783\n",
      "Epoch 3/50 - Train Loss: 0.044949 - Val Loss: 0.070737\n",
      "Epoch 4/50 - Train Loss: 0.044584 - Val Loss: 0.090288\n",
      "Epoch 5/50 - Train Loss: 0.041593 - Val Loss: 0.084189\n",
      "Epoch 6/50 - Train Loss: 0.039336 - Val Loss: 0.068732\n",
      "Epoch 7/50 - Train Loss: 0.039720 - Val Loss: 0.068512\n",
      "Epoch 8/50 - Train Loss: 0.036389 - Val Loss: 0.067805\n",
      "Epoch 9/50 - Train Loss: 0.038682 - Val Loss: 0.060052\n",
      "Epoch 10/50 - Train Loss: 0.031378 - Val Loss: 0.058264\n",
      "Epoch 11/50 - Train Loss: 0.033204 - Val Loss: 0.056356\n",
      "Epoch 12/50 - Train Loss: 0.031346 - Val Loss: 0.050645\n",
      "Epoch 13/50 - Train Loss: 0.036537 - Val Loss: 0.054077\n",
      "Epoch 14/50 - Train Loss: 0.032508 - Val Loss: 0.051104\n",
      "Epoch 15/50 - Train Loss: 0.033173 - Val Loss: 0.050254\n",
      "Epoch 16/50 - Train Loss: 0.033853 - Val Loss: 0.050133\n",
      "Epoch 17/50 - Train Loss: 0.033441 - Val Loss: 0.047884\n",
      "Epoch 18/50 - Train Loss: 0.033290 - Val Loss: 0.050667\n",
      "Epoch 19/50 - Train Loss: 0.033366 - Val Loss: 0.048205\n",
      "Epoch 20/50 - Train Loss: 0.030692 - Val Loss: 0.049556\n",
      "Epoch 21/50 - Train Loss: 0.033394 - Val Loss: 0.046212\n",
      "Epoch 22/50 - Train Loss: 0.031029 - Val Loss: 0.047700\n",
      "Epoch 23/50 - Train Loss: 0.033829 - Val Loss: 0.046551\n",
      "Epoch 24/50 - Train Loss: 0.029694 - Val Loss: 0.047501\n",
      "Epoch 25/50 - Train Loss: 0.031156 - Val Loss: 0.046842\n",
      "Epoch 26/50 - Train Loss: 0.030680 - Val Loss: 0.045167\n",
      "Epoch 27/50 - Train Loss: 0.031982 - Val Loss: 0.048732\n",
      "Epoch 28/50 - Train Loss: 0.033682 - Val Loss: 0.045221\n",
      "Epoch 29/50 - Train Loss: 0.031281 - Val Loss: 0.047492\n",
      "Epoch 30/50 - Train Loss: 0.030578 - Val Loss: 0.045875\n",
      "Epoch 31/50 - Train Loss: 0.030428 - Val Loss: 0.046859\n",
      "Epoch 32/50 - Train Loss: 0.030676 - Val Loss: 0.046011\n",
      "Epoch 33/50 - Train Loss: 0.030718 - Val Loss: 0.045513\n",
      "Epoch 34/50 - Train Loss: 0.032461 - Val Loss: 0.046823\n",
      "Epoch 35/50 - Train Loss: 0.028819 - Val Loss: 0.045070\n",
      "Epoch 36/50 - Train Loss: 0.028574 - Val Loss: 0.046106\n",
      "Epoch 37/50 - Train Loss: 0.029510 - Val Loss: 0.044954\n",
      "Epoch 38/50 - Train Loss: 0.027971 - Val Loss: 0.044372\n",
      "Epoch 39/50 - Train Loss: 0.029973 - Val Loss: 0.044814\n",
      "Epoch 40/50 - Train Loss: 0.031458 - Val Loss: 0.044582\n",
      "Epoch 41/50 - Train Loss: 0.033352 - Val Loss: 0.045477\n",
      "Epoch 42/50 - Train Loss: 0.032196 - Val Loss: 0.045921\n",
      "Epoch 43/50 - Train Loss: 0.031438 - Val Loss: 0.045843\n",
      "Epoch 44/50 - Train Loss: 0.029672 - Val Loss: 0.044857\n",
      "Epoch 45/50 - Train Loss: 0.028987 - Val Loss: 0.044823\n",
      "Epoch 46/50 - Train Loss: 0.035712 - Val Loss: 0.045030\n",
      "Epoch 47/50 - Train Loss: 0.034408 - Val Loss: 0.045831\n",
      "Epoch 48/50 - Train Loss: 0.030727 - Val Loss: 0.045440\n",
      "Epoch 49/50 - Train Loss: 0.032019 - Val Loss: 0.045622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:58:16,291] Trial 96 finished with value: 0.044277630746364594 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 165, 'lr': 0.00032985919055584346, 'weight_decay': 8.077878294099842e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029171 - Val Loss: 0.044278\n",
      "Epoch 1/50 - Train Loss: 0.230775 - Val Loss: 0.133737\n",
      "Epoch 2/50 - Train Loss: 0.075223 - Val Loss: 0.208332\n",
      "Epoch 3/50 - Train Loss: 0.093901 - Val Loss: 0.128756\n",
      "Epoch 4/50 - Train Loss: 0.044704 - Val Loss: 0.062440\n",
      "Epoch 5/50 - Train Loss: 0.056042 - Val Loss: 0.057807\n",
      "Epoch 6/50 - Train Loss: 0.044222 - Val Loss: 0.092325\n",
      "Epoch 7/50 - Train Loss: 0.049375 - Val Loss: 0.074677\n",
      "Epoch 8/50 - Train Loss: 0.035131 - Val Loss: 0.049727\n",
      "Epoch 9/50 - Train Loss: 0.043026 - Val Loss: 0.049019\n",
      "Epoch 10/50 - Train Loss: 0.041825 - Val Loss: 0.054973\n",
      "Epoch 11/50 - Train Loss: 0.032666 - Val Loss: 0.053496\n",
      "Epoch 12/50 - Train Loss: 0.039156 - Val Loss: 0.048108\n",
      "Epoch 13/50 - Train Loss: 0.033149 - Val Loss: 0.047826\n",
      "Epoch 14/50 - Train Loss: 0.039598 - Val Loss: 0.048310\n",
      "Epoch 15/50 - Train Loss: 0.039135 - Val Loss: 0.051648\n",
      "Epoch 16/50 - Train Loss: 0.039942 - Val Loss: 0.049547\n",
      "Epoch 17/50 - Train Loss: 0.039665 - Val Loss: 0.046877\n",
      "Epoch 18/50 - Train Loss: 0.032066 - Val Loss: 0.047046\n",
      "Epoch 19/50 - Train Loss: 0.038032 - Val Loss: 0.051980\n",
      "Epoch 20/50 - Train Loss: 0.037955 - Val Loss: 0.051785\n",
      "Epoch 21/50 - Train Loss: 0.038542 - Val Loss: 0.047659\n",
      "Epoch 22/50 - Train Loss: 0.038244 - Val Loss: 0.047408\n",
      "Epoch 23/50 - Train Loss: 0.030602 - Val Loss: 0.048832\n",
      "Epoch 24/50 - Train Loss: 0.038289 - Val Loss: 0.049388\n",
      "Epoch 25/50 - Train Loss: 0.029669 - Val Loss: 0.046279\n",
      "Epoch 26/50 - Train Loss: 0.037500 - Val Loss: 0.046421\n",
      "Epoch 27/50 - Train Loss: 0.031055 - Val Loss: 0.047138\n",
      "Epoch 28/50 - Train Loss: 0.029971 - Val Loss: 0.046558\n",
      "Epoch 29/50 - Train Loss: 0.037340 - Val Loss: 0.046306\n",
      "Epoch 30/50 - Train Loss: 0.037358 - Val Loss: 0.048750\n",
      "Epoch 31/50 - Train Loss: 0.029629 - Val Loss: 0.048485\n",
      "Epoch 32/50 - Train Loss: 0.038103 - Val Loss: 0.047639\n",
      "Epoch 33/50 - Train Loss: 0.029012 - Val Loss: 0.047354\n",
      "Epoch 34/50 - Train Loss: 0.031088 - Val Loss: 0.048220\n",
      "Epoch 35/50 - Train Loss: 0.036527 - Val Loss: 0.047982\n",
      "Epoch 36/50 - Train Loss: 0.029377 - Val Loss: 0.046762\n",
      "Epoch 37/50 - Train Loss: 0.029042 - Val Loss: 0.046598\n",
      "Epoch 38/50 - Train Loss: 0.029170 - Val Loss: 0.046302\n",
      "Epoch 39/50 - Train Loss: 0.029183 - Val Loss: 0.046153\n",
      "Epoch 40/50 - Train Loss: 0.028894 - Val Loss: 0.046903\n",
      "Epoch 41/50 - Train Loss: 0.036700 - Val Loss: 0.048422\n",
      "Epoch 42/50 - Train Loss: 0.037175 - Val Loss: 0.048209\n",
      "Epoch 43/50 - Train Loss: 0.038688 - Val Loss: 0.049122\n",
      "Epoch 44/50 - Train Loss: 0.028781 - Val Loss: 0.048547\n",
      "Epoch 45/50 - Train Loss: 0.035471 - Val Loss: 0.049426\n",
      "Epoch 46/50 - Train Loss: 0.036912 - Val Loss: 0.050476\n",
      "Epoch 47/50 - Train Loss: 0.028546 - Val Loss: 0.047280\n",
      "Epoch 48/50 - Train Loss: 0.038560 - Val Loss: 0.047507\n",
      "Epoch 49/50 - Train Loss: 0.030359 - Val Loss: 0.048420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:58:28,601] Trial 97 finished with value: 0.04615285247564316 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 185, 'lr': 0.00864404914602026, 'weight_decay': 6.021596330397949e-07, 'batch_size': 64}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.036343 - Val Loss: 0.047853\n",
      "Epoch 1/50 - Train Loss: 0.120963 - Val Loss: 0.180976\n",
      "Epoch 2/50 - Train Loss: 0.059961 - Val Loss: 0.062794\n",
      "Epoch 3/50 - Train Loss: 0.042416 - Val Loss: 0.105804\n",
      "Epoch 4/50 - Train Loss: 0.046395 - Val Loss: 0.056509\n",
      "Epoch 5/50 - Train Loss: 0.039954 - Val Loss: 0.063473\n",
      "Epoch 6/50 - Train Loss: 0.035912 - Val Loss: 0.050565\n",
      "Epoch 7/50 - Train Loss: 0.031881 - Val Loss: 0.047164\n",
      "Epoch 8/50 - Train Loss: 0.031833 - Val Loss: 0.051894\n",
      "Epoch 9/50 - Train Loss: 0.038093 - Val Loss: 0.046336\n",
      "Epoch 10/50 - Train Loss: 0.034555 - Val Loss: 0.052649\n",
      "Epoch 11/50 - Train Loss: 0.033958 - Val Loss: 0.045701\n",
      "Epoch 12/50 - Train Loss: 0.032209 - Val Loss: 0.047187\n",
      "Epoch 13/50 - Train Loss: 0.032742 - Val Loss: 0.046572\n",
      "Epoch 14/50 - Train Loss: 0.034005 - Val Loss: 0.049070\n",
      "Epoch 15/50 - Train Loss: 0.030545 - Val Loss: 0.045776\n",
      "Epoch 16/50 - Train Loss: 0.036404 - Val Loss: 0.045467\n",
      "Epoch 17/50 - Train Loss: 0.025578 - Val Loss: 0.045835\n",
      "Epoch 18/50 - Train Loss: 0.033431 - Val Loss: 0.045460\n",
      "Epoch 19/50 - Train Loss: 0.035416 - Val Loss: 0.053424\n",
      "Epoch 20/50 - Train Loss: 0.031835 - Val Loss: 0.046175\n",
      "Epoch 21/50 - Train Loss: 0.031570 - Val Loss: 0.046611\n",
      "Epoch 22/50 - Train Loss: 0.031723 - Val Loss: 0.045620\n",
      "Epoch 23/50 - Train Loss: 0.027068 - Val Loss: 0.048786\n",
      "Epoch 24/50 - Train Loss: 0.030483 - Val Loss: 0.048054\n",
      "Epoch 25/50 - Train Loss: 0.028875 - Val Loss: 0.046241\n",
      "Epoch 26/50 - Train Loss: 0.033789 - Val Loss: 0.048015\n",
      "Epoch 27/50 - Train Loss: 0.028196 - Val Loss: 0.046740\n",
      "Epoch 28/50 - Train Loss: 0.029499 - Val Loss: 0.048546\n",
      "Epoch 29/50 - Train Loss: 0.035257 - Val Loss: 0.047345\n",
      "Epoch 30/50 - Train Loss: 0.031115 - Val Loss: 0.048954\n",
      "Epoch 31/50 - Train Loss: 0.031801 - Val Loss: 0.050649\n",
      "Epoch 32/50 - Train Loss: 0.034325 - Val Loss: 0.045002\n",
      "Epoch 33/50 - Train Loss: 0.028584 - Val Loss: 0.046181\n",
      "Epoch 34/50 - Train Loss: 0.035271 - Val Loss: 0.047653\n",
      "Epoch 35/50 - Train Loss: 0.035703 - Val Loss: 0.066392\n",
      "Epoch 36/50 - Train Loss: 0.034438 - Val Loss: 0.048918\n",
      "Epoch 37/50 - Train Loss: 0.031167 - Val Loss: 0.062639\n",
      "Epoch 38/50 - Train Loss: 0.031395 - Val Loss: 0.048014\n",
      "Epoch 39/50 - Train Loss: 0.034366 - Val Loss: 0.055768\n",
      "Epoch 40/50 - Train Loss: 0.031560 - Val Loss: 0.048018\n",
      "Epoch 41/50 - Train Loss: 0.032084 - Val Loss: 0.059190\n",
      "Epoch 42/50 - Train Loss: 0.032282 - Val Loss: 0.045910\n",
      "Epoch 43/50 - Train Loss: 0.028823 - Val Loss: 0.053255\n",
      "Epoch 44/50 - Train Loss: 0.031568 - Val Loss: 0.048089\n",
      "Epoch 45/50 - Train Loss: 0.029068 - Val Loss: 0.055638\n",
      "Epoch 46/50 - Train Loss: 0.036401 - Val Loss: 0.047565\n",
      "Epoch 47/50 - Train Loss: 0.030720 - Val Loss: 0.046234\n",
      "Epoch 48/50 - Train Loss: 0.029301 - Val Loss: 0.051912\n",
      "Epoch 49/50 - Train Loss: 0.028777 - Val Loss: 0.045822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:58:50,379] Trial 98 finished with value: 0.04500177502632141 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 195, 'lr': 0.006939369100827709, 'weight_decay': 2.9821021163883616e-07, 'batch_size': 32}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028900 - Val Loss: 0.049925\n",
      "Epoch 1/50 - Train Loss: 0.091450 - Val Loss: 0.063006\n",
      "Epoch 2/50 - Train Loss: 0.039971 - Val Loss: 0.050180\n",
      "Epoch 3/50 - Train Loss: 0.041689 - Val Loss: 0.069993\n",
      "Epoch 4/50 - Train Loss: 0.038790 - Val Loss: 0.078988\n",
      "Epoch 5/50 - Train Loss: 0.037828 - Val Loss: 0.047589\n",
      "Epoch 6/50 - Train Loss: 0.038008 - Val Loss: 0.056374\n",
      "Epoch 7/50 - Train Loss: 0.038549 - Val Loss: 0.059779\n",
      "Epoch 8/50 - Train Loss: 0.035067 - Val Loss: 0.046674\n",
      "Epoch 9/50 - Train Loss: 0.031594 - Val Loss: 0.045009\n",
      "Epoch 10/50 - Train Loss: 0.028964 - Val Loss: 0.053853\n",
      "Epoch 11/50 - Train Loss: 0.031453 - Val Loss: 0.046052\n",
      "Epoch 12/50 - Train Loss: 0.030703 - Val Loss: 0.044372\n",
      "Epoch 13/50 - Train Loss: 0.027429 - Val Loss: 0.044282\n",
      "Epoch 14/50 - Train Loss: 0.029624 - Val Loss: 0.049374\n",
      "Epoch 15/50 - Train Loss: 0.030714 - Val Loss: 0.045035\n",
      "Epoch 16/50 - Train Loss: 0.036588 - Val Loss: 0.050685\n",
      "Epoch 17/50 - Train Loss: 0.032498 - Val Loss: 0.049165\n",
      "Epoch 18/50 - Train Loss: 0.033214 - Val Loss: 0.052903\n",
      "Epoch 19/50 - Train Loss: 0.033638 - Val Loss: 0.054748\n",
      "Epoch 20/50 - Train Loss: 0.032986 - Val Loss: 0.044632\n",
      "Epoch 21/50 - Train Loss: 0.033929 - Val Loss: 0.044954\n",
      "Epoch 22/50 - Train Loss: 0.033295 - Val Loss: 0.046465\n",
      "Epoch 23/50 - Train Loss: 0.025229 - Val Loss: 0.047229\n",
      "Epoch 24/50 - Train Loss: 0.033762 - Val Loss: 0.047760\n",
      "Epoch 25/50 - Train Loss: 0.029935 - Val Loss: 0.047161\n",
      "Epoch 26/50 - Train Loss: 0.034938 - Val Loss: 0.044690\n",
      "Epoch 27/50 - Train Loss: 0.030861 - Val Loss: 0.044775\n",
      "Epoch 28/50 - Train Loss: 0.031201 - Val Loss: 0.046582\n",
      "Epoch 29/50 - Train Loss: 0.029432 - Val Loss: 0.044605\n",
      "Epoch 30/50 - Train Loss: 0.036627 - Val Loss: 0.045100\n",
      "Epoch 31/50 - Train Loss: 0.031529 - Val Loss: 0.044305\n",
      "Epoch 32/50 - Train Loss: 0.031102 - Val Loss: 0.044661\n",
      "Epoch 33/50 - Train Loss: 0.035068 - Val Loss: 0.044310\n",
      "Epoch 34/50 - Train Loss: 0.031142 - Val Loss: 0.046983\n",
      "Epoch 35/50 - Train Loss: 0.030913 - Val Loss: 0.044841\n",
      "Epoch 36/50 - Train Loss: 0.031036 - Val Loss: 0.049962\n",
      "Epoch 37/50 - Train Loss: 0.031192 - Val Loss: 0.058441\n",
      "Epoch 38/50 - Train Loss: 0.032180 - Val Loss: 0.045788\n",
      "Epoch 39/50 - Train Loss: 0.033043 - Val Loss: 0.045191\n",
      "Epoch 40/50 - Train Loss: 0.031080 - Val Loss: 0.047238\n",
      "Epoch 41/50 - Train Loss: 0.030109 - Val Loss: 0.055223\n",
      "Epoch 42/50 - Train Loss: 0.027433 - Val Loss: 0.044923\n",
      "Epoch 43/50 - Train Loss: 0.028855 - Val Loss: 0.046187\n",
      "Epoch 44/50 - Train Loss: 0.029520 - Val Loss: 0.044622\n",
      "Epoch 45/50 - Train Loss: 0.031500 - Val Loss: 0.047390\n",
      "Epoch 46/50 - Train Loss: 0.028585 - Val Loss: 0.047440\n",
      "Epoch 47/50 - Train Loss: 0.028650 - Val Loss: 0.045421\n",
      "Epoch 48/50 - Train Loss: 0.031672 - Val Loss: 0.047954\n",
      "Epoch 49/50 - Train Loss: 0.029602 - Val Loss: 0.049831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-04 14:59:20,980] Trial 99 finished with value: 0.04428194463253021 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 238, 'lr': 0.005428599088144779, 'weight_decay': 5.187452890813634e-08, 'batch_size': 16}. Best is trial 40 with value: 0.043100775529940925.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033894 - Val Loss: 0.044893\n",
      "Best Hyperparameters: {'n_hidden_layers': 1, 'n_hidden_units': 139, 'lr': 0.00434625111972206, 'weight_decay': 6.21222641225517e-07, 'batch_size': 8}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to search over\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 5)\n",
    "    n_hidden_units = trial.suggest_int(\"n_hidden_units\", 32, 256)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])  # Match the original hp['batch_sz']\n",
    "\n",
    "    # Create train & validation loaders (following the original code)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize MLP model\n",
    "    model = BasicMLP(\n",
    "        N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "        N_HIDDEN_LAYERS=n_hidden_layers,\n",
    "        N_HIDDEN_UNITS=n_hidden_units,\n",
    "        N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "        loss_function=LOSS_FUNC,\n",
    "    )\n",
    "\n",
    "    # Train and return validation loss\n",
    "    val_loss = model.train_model(train_loader, val_loader, epochs=50, lr=lr, weight_decay=weight_decay, device=device)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"mlp_hyperparameter_optimization_phy_adjusted_dist3\", storage=\"sqlite:///mlp_hyperparameter_optimization_phy.db\", load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_hidden_layers': 2,\n",
       " 'n_hidden_units': 237,\n",
       " 'lr': 0.004360166659287016,\n",
       " 'weight_decay': 5.881374833423609e-08,\n",
       " 'batch_size': 16}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best params for the physics model in previous studies\n",
    "# remove if retune\n",
    "best_params = {'n_hidden_layers': 2, 'n_hidden_units': 237, 'lr': 0.004360166659287016, 'weight_decay': 5.881374833423609e-08, 'batch_size': 16}\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.068529 - Val Loss: 0.067292\n",
      "Epoch 2/50 - Train Loss: 0.043031 - Val Loss: 0.089617\n",
      "Epoch 3/50 - Train Loss: 0.037203 - Val Loss: 0.071924\n",
      "Epoch 4/50 - Train Loss: 0.038949 - Val Loss: 0.046050\n",
      "Epoch 5/50 - Train Loss: 0.033820 - Val Loss: 0.047601\n",
      "Epoch 6/50 - Train Loss: 0.033625 - Val Loss: 0.061231\n",
      "Epoch 7/50 - Train Loss: 0.035570 - Val Loss: 0.046728\n",
      "Epoch 8/50 - Train Loss: 0.036929 - Val Loss: 0.047170\n",
      "Epoch 9/50 - Train Loss: 0.032583 - Val Loss: 0.050686\n",
      "Epoch 10/50 - Train Loss: 0.033732 - Val Loss: 0.044899\n",
      "Epoch 11/50 - Train Loss: 0.029680 - Val Loss: 0.047724\n",
      "Epoch 12/50 - Train Loss: 0.033290 - Val Loss: 0.050238\n",
      "Epoch 13/50 - Train Loss: 0.030613 - Val Loss: 0.048486\n",
      "Epoch 14/50 - Train Loss: 0.033094 - Val Loss: 0.044320\n",
      "Epoch 15/50 - Train Loss: 0.035775 - Val Loss: 0.052647\n",
      "Epoch 16/50 - Train Loss: 0.035374 - Val Loss: 0.048271\n",
      "Epoch 17/50 - Train Loss: 0.031155 - Val Loss: 0.048400\n",
      "Epoch 18/50 - Train Loss: 0.027713 - Val Loss: 0.047667\n",
      "Epoch 19/50 - Train Loss: 0.031818 - Val Loss: 0.044445\n",
      "Epoch 20/50 - Train Loss: 0.032363 - Val Loss: 0.044227\n",
      "Epoch 21/50 - Train Loss: 0.031540 - Val Loss: 0.047963\n",
      "Epoch 22/50 - Train Loss: 0.030841 - Val Loss: 0.047635\n",
      "Epoch 23/50 - Train Loss: 0.029854 - Val Loss: 0.045687\n",
      "Epoch 24/50 - Train Loss: 0.032791 - Val Loss: 0.043955\n",
      "Epoch 25/50 - Train Loss: 0.028895 - Val Loss: 0.045222\n",
      "Epoch 26/50 - Train Loss: 0.029493 - Val Loss: 0.049873\n",
      "Epoch 27/50 - Train Loss: 0.029390 - Val Loss: 0.046058\n",
      "Epoch 28/50 - Train Loss: 0.031553 - Val Loss: 0.046780\n",
      "Epoch 29/50 - Train Loss: 0.027224 - Val Loss: 0.044435\n",
      "Epoch 30/50 - Train Loss: 0.026006 - Val Loss: 0.043836\n",
      "Epoch 31/50 - Train Loss: 0.026262 - Val Loss: 0.046738\n",
      "Epoch 32/50 - Train Loss: 0.031744 - Val Loss: 0.048851\n",
      "Epoch 33/50 - Train Loss: 0.031592 - Val Loss: 0.047208\n",
      "Epoch 34/50 - Train Loss: 0.029776 - Val Loss: 0.045882\n",
      "Epoch 35/50 - Train Loss: 0.026796 - Val Loss: 0.049781\n",
      "Epoch 36/50 - Train Loss: 0.034325 - Val Loss: 0.046836\n",
      "Epoch 37/50 - Train Loss: 0.032544 - Val Loss: 0.045586\n",
      "Epoch 38/50 - Train Loss: 0.031394 - Val Loss: 0.045632\n",
      "Epoch 39/50 - Train Loss: 0.030137 - Val Loss: 0.044380\n",
      "Epoch 40/50 - Train Loss: 0.031826 - Val Loss: 0.044751\n",
      "Epoch 41/50 - Train Loss: 0.030102 - Val Loss: 0.045713\n",
      "Epoch 42/50 - Train Loss: 0.032606 - Val Loss: 0.044501\n",
      "Epoch 43/50 - Train Loss: 0.028977 - Val Loss: 0.049220\n",
      "Epoch 44/50 - Train Loss: 0.031002 - Val Loss: 0.044629\n",
      "Epoch 45/50 - Train Loss: 0.030779 - Val Loss: 0.045517\n",
      "Epoch 46/50 - Train Loss: 0.030053 - Val Loss: 0.046944\n",
      "Epoch 47/50 - Train Loss: 0.028027 - Val Loss: 0.044724\n",
      "Epoch 48/50 - Train Loss: 0.031364 - Val Loss: 0.046909\n",
      "Epoch 49/50 - Train Loss: 0.032766 - Val Loss: 0.047157\n",
      "Epoch 50/50 - Train Loss: 0.026766 - Val Loss: 0.045195\n",
      "Model saved as best_mlp_no2.pth in Model folder\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "best_model = BasicMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=best_params[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=best_params[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "    loss_function=LOSS_FUNC,\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "best_model.train_model(train_loader, val_loader, epochs=50, lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"], device=device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(best_model.state_dict(), f\"{MODEL_PATH}/best_mlp_no2_adjusted_dist.pth\")\n",
    "print(\"Model saved as best_mlp_no2.pth in Model folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO2</td>\n",
       "      <td>1.37</td>\n",
       "      <td>93.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0   min    max\n",
       "0        NO2  1.37  93.95"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "df_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 248.409109\n",
      "Test RMSE Loss: 16.273734\n",
      "Test SMAPE Loss: 39.003706%\n"
     ]
    }
   ],
   "source": [
    "best_model.load_state_dict(torch.load(f\"{MODEL_PATH}/best_mlp_no2_adjusted_dist.pth\"))\n",
    "best_model.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "mse, rmse_val, smape_val = best_model.test_model(test_loader, min_value=min_value, max_value=max_value, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 248.40910929670335, Test RMSE: 16.273734426648694, Test SMAPE: 39.00370553687767\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test MSE: {mse}, Test RMSE: {rmse_val}, Test SMAPE: {smape_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "# Iterate through the test set and collect predictions & ground truth\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_test, y_true = batch  # Get input and ground truth\n",
    "        x_test = x_test.to(\"cpu\")  # Ensure data is on CPU if needed\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = best_model(x_test)\n",
    "\n",
    "        # Store results\n",
    "        y_preds.append(y_pred.cpu())\n",
    "        y_trues.append(y_true.cpu())\n",
    "\n",
    "# Convert lists to tensors\n",
    "y_preds = torch.cat(y_preds, dim=0).numpy()\n",
    "y_trues = torch.cat(y_trues, dim=0).numpy()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_trues.flatten(), label=\"Ground Truth (NO)\", linestyle=\"-\", color=\"blue\")\n",
    "plt.scatter(range(len(y_preds.flatten())), y_preds.flatten(), label=\"Predictions\", color=\"black\", s=10)\n",
    "\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"NO Level\")\n",
    "plt.title(\"Predictions vs. Ground Truth\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
