{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n",
    "For future models, a suggestion is to embed the training/testing functions in a Model class, instead of keeping them loose from each other. (With, optimally, inheritance from a base class, etc etc, such that there is minimal code duplication.) This way, the training procedure can be easily tailored per model. In the current set-up, different functions have to be called for fully-connected networks and hierarchical networks because they handle the data differently. Another way this would be a worth investment, is for implementation of physics-informed models, which require a whole physics injection into the training procedure. In that case, tight coupling is much recommended over the current state of this file. Therefore, I'd first change the code such that it works per model and such that only functionalities independent of model type are actually independent/loosely coupled from the models, therewith facilitating scalable experimentation.\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify potential errors happening on Habrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "from modelling import *\n",
    "from modelling import GRU\n",
    "from modelling import HGRU\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HABROK = bool(0)                       # True if running on Habrok or external server\n",
    "if HABROK:\n",
    "    print(\"Successfully imported libraries into env\")\n",
    "    USER = 'habrok'\n",
    "else:\n",
    "    USER = 'tinus'\n",
    "\n",
    "if USER == 'tinus':\n",
    "    os.chdir(r\"c:\\Users\\vwold\\Documents\\Bachelor\\ICML_paper\\forecasting_smog_DL\\forecasting_smog_DL\\src\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"results/models\")\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "elif HABROK:\n",
    "    os.chdir(r\"/home1/s4372948/thesis/modelling/\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"results/models\")\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "\n",
    "torch.manual_seed(34)\n",
    "\n",
    "N_HOURS_U = 72\n",
    "N_HOURS_Y = 24\n",
    "N_HOURS_STEP = 24\n",
    "CONTAMINANTS = ['NO2', 'O3', 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u')\n",
    "train_output_frames = get_dataframes('train', 'y')\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u')\n",
    "val_output_frames = get_dataframes('val', 'y')\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u')\n",
    "test_output_frames = get_dataframes('test', 'y')\n",
    "\n",
    "print(\"Successfully loaded data\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    5,                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, all (hyper)parameters are defined. The hyperparameters are defined in\n",
    "# a dictionary, which is then passed to the model and the training functions.\n",
    "# The grid search is performed by generating all possible combinations of the\n",
    "# hyperparameters defined in the hp_space dictionary, and then performing k-fold cross\n",
    "# validation on each of these configurations. The best configuration is then returned.\n",
    "# When the search is finished, comment out the hp_space dictionary and save the best found\n",
    "# hyperparameters in the hp dictionary, and train the final model with these.\n",
    "\n",
    "hp = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : HGRU,\n",
    "    'input_units' : train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 64,\n",
    "    'branches' : 4,\n",
    "    'output_units' : train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : torch.optim.Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min',\n",
    "                          'factor' : 0.1,\n",
    "                          'patience' : 3,\n",
    "                          'cooldown' : 8,\n",
    "                          'verbose' : True},\n",
    "    'w_decay' : 1e-7,\n",
    "    'loss_fn' : torch.nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 20,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}                                   # The lr for the branched layer(s) is calculated\n",
    "                                    # based on the \"power ratio\" between the branched\n",
    "                                    # part of the network and the shared layer, which\n",
    "                                    # is *assumed* to be proportional to n_hidden_layers\n",
    "hp['lr_branch'] = hp['lr_shared'] * hp['hidden_layers']\n",
    "\n",
    "hp_space = []                       # grid search space, put in the hyperparameters\n",
    "                                    # to search over here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start hyperparameter search/training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training now\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'results/grid_search_exe_s/exe_of_HGRU_at_{current_time}.txt'\n",
    "# train_dataset_full = ConcatDataset([train_dataset, val_dataset])\n",
    "#                                     If HABROK, print to external file, else print to stdout\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(f\"Grid search execution of HGRU at {current_time}\\n\")\n",
    "#                                     # Train on the full training set\n",
    "#     model, best_hp, val_loss = grid_search(hp, hp_space, train_dataset_full, True)\n",
    "#                                     # Externally save the best model\n",
    "#     torch.save(model.state_dict(), f\"{MODEL_PATH}/results/model_HGRU.pth\")\n",
    "\n",
    "#     hp = update_dict(hp, best_hp)   # Update the hp dictionary with the best hyperparameters\n",
    "#     print_dict_vertically(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out model architecture with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    model = HGRU(hp['n_hours_u'],\n",
    "                 hp['n_hours_y'],\n",
    "                 hp['input_units'],\n",
    "                 hp['hidden_layers'],\n",
    "                 hp['hidden_units'], \n",
    "                 hp['branches'],\n",
    "                 hp['output_units'])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on complete training dataset (= train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "                                        # Train the final model on the full training set,\n",
    "                                        # save the final model, and save the losses for plotting\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nTraining on full training set...\")\n",
    "    model_final, train_losses, test_losses, shared_losses, branch_losses = \\\n",
    "        train_hierarchical(hp, train_loader, val_loader, True)\n",
    "    torch.save(model_final.state_dict(), f'{MODEL_PATH}/model_HGRU.pth')\n",
    "\n",
    "df_losses = pd.DataFrame({'L_train': train_losses, 'L_test': test_losses})\n",
    "df_losses.to_csv(f'{os.path.join(os.getcwd(), \"results/final_losses\")}/losses_HGRU_at_{current_time}.csv', \n",
    "                 sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = HGRU(hp['input_units'], hp['hidden_layers'], hp['hidden_units'],\n",
    "                     hp['branches'], hp['output_units'])\n",
    "model_final.load_state_dict(torch.load(f\"{MODEL_PATH}/model_HGRU.pth\"))\n",
    "print(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "test_error = test_hierarchical(model_final, nn.MSELoss(), test_loader)\n",
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_hierarchical(model_final, nn.MSELoss(), train_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), val_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), test_loader))\n",
    "\n",
    "print(\"\\nMSE Training set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Validation set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Test set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRMSE Training set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Validation set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Test set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")\n",
    "np.sqrt(test_hierarchical(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 5\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'NO2', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'O3', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM10', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM25', N_HOURS_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
