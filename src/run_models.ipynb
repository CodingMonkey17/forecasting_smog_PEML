{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "For running the models, we take the following steps:\n",
    "\n",
    "> * Prepare packages, setup, data\n",
    "> * Load model\n",
    "> * Define hyperparameters\n",
    "> * Train the model\n",
    "> * Evaluate the model\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify errors happening on Habrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prepare packages, setup, data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "Importing modelling package...\n",
      "\n",
      "Running __init__.py for data pipeline\n",
      "Modelling package initialized\n",
      "\n",
      "Importing libs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "print(\"Importing modelling package...\")\n",
    "from modelling import *\n",
    "\n",
    "print(\"Importing libs...\")\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    SubsetRandomSampler,\n",
    "    SequentialSampler\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = tc.cuda.is_available()\n",
    "device = tc.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Global\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HABROK = bool(0)                       # True if running on Habrok or external server\n",
    "if HABROK:\n",
    "    print(\"Successfully imported libraries into env\")\n",
    "    USER = 'habrok'\n",
    "else:\n",
    "    USER = 'tinus'\n",
    "\n",
    "if USER == 'tinus':\n",
    "    os.chdir(r\"c:\\Users\\vwold\\Documents\\Bachelor\\ICML_paper\\forecasting_smog_DL\\forecasting_smog_DL\\src\\modelling\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"models\")\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "elif HABROK:\n",
    "    os.chdir(r\"/home1/s4372948/thesis/modelling/\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"models\")\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "\n",
    "tc.manual_seed(34)\n",
    "mpl.rcParams['figure.figsize'] = (7, 3)\n",
    "\n",
    "N_HOURS_U = 72\n",
    "N_HOURS_Y = 24\n",
    "N_HOURS_STEP = 24\n",
    "CONTAMINANTS = ['NO2', 'O3', 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data and create PyTorch *Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_2017_u = pd.read_csv('../../data/data_combined/train_2017_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2017_y = pd.read_csv('../../data/data_combined/train_2017_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2018_u = pd.read_csv('../../data/data_combined/train_2018_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2018_y = pd.read_csv('../../data/data_combined/train_2018_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2020_u = pd.read_csv('../../data/data_combined/train_2020_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2020_y = pd.read_csv('../../data/data_combined/train_2020_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2021_u = pd.read_csv('../../data/data_combined/train_2021_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2021_y = pd.read_csv('../../data/data_combined/train_2021_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2022_u = pd.read_csv('../../data/data_combined/train_2022_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_train_2022_y = pd.read_csv('../../data/data_combined/train_2022_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_val_2021_u = pd.read_csv('../../data/data_combined/val_2021_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_val_2021_y = pd.read_csv('../../data/data_combined/val_2021_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_val_2022_u = pd.read_csv('../../data/data_combined/val_2022_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_val_2022_y = pd.read_csv('../../data/data_combined/val_2022_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_val_2023_u = pd.read_csv('../../data/data_combined/val_2023_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_val_2023_y = pd.read_csv('../../data/data_combined/val_2023_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_test_2021_u = pd.read_csv('../../data/data_combined/test_2021_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_test_2021_y = pd.read_csv('../../data/data_combined/test_2021_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_test_2022_u = pd.read_csv('../../data/data_combined/test_2022_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_test_2022_y = pd.read_csv('../../data/data_combined/test_2022_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_test_2023_u = pd.read_csv('../../data/data_combined/test_2023_combined_u.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')\n",
    "# df_test_2023_y = pd.read_csv('../../data/data_combined/test_2023_combined_y.csv',\n",
    "#                    index_col = 'DateTime', sep = ';', decimal = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input_frames = [\n",
    "#     df_train_2017_u,\n",
    "#     df_train_2018_u,\n",
    "#     df_train_2020_u,\n",
    "#     df_train_2021_u,\n",
    "#     df_train_2022_u\n",
    "# ]\n",
    "# train_output_frames = [\n",
    "#     df_train_2017_y,\n",
    "#     df_train_2018_y,\n",
    "#     df_train_2020_y,\n",
    "#     df_train_2021_y,\n",
    "#     df_train_2022_y\n",
    "# ]\n",
    "# val_input_frames = [\n",
    "#     df_val_2021_u,\n",
    "#     df_val_2022_u,\n",
    "#     df_val_2023_u\n",
    "# ]\n",
    "# val_output_frames = [\n",
    "#     df_val_2021_y,\n",
    "#     df_val_2022_y,\n",
    "#     df_val_2023_y\n",
    "# ]\n",
    "# test_input_frames = [\n",
    "#     df_test_2021_u,\n",
    "#     df_test_2022_u,\n",
    "#     df_test_2023_u\n",
    "# ]\n",
    "# test_output_frames = [\n",
    "#     df_test_2021_y,\n",
    "#     df_test_2022_y,\n",
    "#     df_test_2023_y\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/data_combined/train_2017_combined_u.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_input_frames \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataframes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m train_output_frames \u001b[38;5;241m=\u001b[39m get_dataframes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m val_input_frames \u001b[38;5;241m=\u001b[39m get_dataframes(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vwold\\Documents\\Bachelor\\ICML_paper\\forecasting_smog_DL\\forecasting_smog_DL\\src\\modelling\\extract.py:32\u001b[0m, in \u001b[0;36mget_dataframes\u001b[1;34m(what, UY)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m UY \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m what \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mimport_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_2017_combined_u.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     33\u001b[0m                 import_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_2018_combined_u.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     34\u001b[0m                 import_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_2020_combined_u.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     35\u001b[0m                 import_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_2021_combined_u.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     36\u001b[0m                 import_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_2022_combined_u.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m what \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [import_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_2021_combined_u.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     39\u001b[0m                 import_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_2022_combined_u.csv\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     40\u001b[0m                 import_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_2023_combined_u.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\vwold\\Documents\\Bachelor\\ICML_paper\\forecasting_smog_DL\\forecasting_smog_DL\\src\\modelling\\extract.py:15\u001b[0m, in \u001b[0;36mimport_csv\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimport_csv\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Imports a file from the data/data_combined folder\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    :param file_name: name of the file to import\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/data_combined/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDateTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/data_combined/train_2017_combined_u.csv'"
     ]
    }
   ],
   "source": [
    "train_input_frames = get_dataframes('train', 'input')\n",
    "train_output_frames = get_dataframes('train', 'output')\n",
    "val_input_frames = get_dataframes('val', 'input')\n",
    "val_output_frames = get_dataframes('val', 'output')\n",
    "test_input_frames = get_dataframes('test', 'input')\n",
    "test_output_frames = get_dataframes('test', 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Successfully loaded data\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,\n",
    "    train_output_frames,\n",
    "    5,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Branch(nn.Module):\n",
    "    \"\"\"Assisting module for the MBGRU branches: ignores all hidden \n",
    "    GRU states thereby allowing for the use of nn.Sequential\"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"Initializes the branch\"\"\"\n",
    "        super(Branch, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, out):\n",
    "        \"\"\"Defines the forward pass of the module, ignores hidden GRU states\"\"\"\n",
    "        for layer in self.layers[:-1]:  # If an instance of nn.GRU, discard hidden states\n",
    "            if isinstance(layer, nn.GRU):\n",
    "                out, _ = layer(out)\n",
    "                                        # apply dense layer to each time step, then stack,\n",
    "                                        # which yields (N_HOURS_Y, N_BATCHES, N_OUTPUT_UNITS),\n",
    "                                        # then transpose to (N_BATCHES, N_HOURS_Y, N_OUTPUT_UNITS)\n",
    "        return tc.stack([self.layers[-1](out[:, idx, :])\n",
    "                         for idx in range(-N_HOURS_Y, 0)],\n",
    "                        dim = 0).transpose(0, 1)\n",
    "\n",
    "class MBGRU(nn.Module):\n",
    "    \"\"\"Defines a Multi-Branch GRU (MBGRU)\"\"\"\n",
    "\n",
    "    def __init__(self, N_INPUT_UNITS, N_HIDDEN_LAYERS, \n",
    "                 N_HIDDEN_UNITS, N_BRANCHES, N_OUTPUT_UNITS):\n",
    "        \"\"\"Initializes the MMGRU\"\"\"\n",
    "        super(MBGRU, self).__init__()  # Initialize the superclass and data members\n",
    "        self.d_input_units = N_INPUT_UNITS\n",
    "        self.d_hidden_layers = N_HIDDEN_LAYERS\n",
    "        self.d_hidden_units = N_HIDDEN_UNITS\n",
    "        self.d_branches = N_BRANCHES\n",
    "        self.d_output_untis = N_OUTPUT_UNITS\n",
    "                                        # Branches distributed evenly\n",
    "        self.d_branch_units = self.d_output_untis // self.d_branches\n",
    "        if self.d_hidden_units % self.d_branches != 0:\n",
    "            raise ValueError(\"N_HIDDEN_UNITS must be divisible by N_BRANCHES\")\n",
    "        if self.d_output_untis % self.d_branches != 0:\n",
    "            raise ValueError(\"N_OUTPUT_UNITS must be divisible by N_BRANCHES\")\n",
    "\n",
    "                                        # Initialize input layer and shared layer\n",
    "        self.input_layer = nn.GRU(self.d_input_units, self.d_hidden_units,\n",
    "                                   batch_first = True)\n",
    "        self.shared_layer = nn.GRU(self.d_hidden_units, self.d_hidden_units,\n",
    "                                    batch_first = True)\n",
    "\n",
    "        self.branches = nn.ModuleList() # Initialize branches, using Branch module:\n",
    "        for _ in range(self.d_branches):\n",
    "            branch_layers = [nn.GRU(self.d_hidden_units,\n",
    "                                     self.d_hidden_units // self.d_branches,\n",
    "                                     batch_first = True)]\n",
    "            branch_layers.extend([nn.GRU(self.d_hidden_units // self.d_branches,\n",
    "                                          self.d_hidden_units // self.d_branches, \n",
    "                                          self.d_hidden_layers - 1,\n",
    "                                          batch_first = True)])\n",
    "            branch_layers.append(nn.Linear(self.d_hidden_units // self.d_branches,\n",
    "                                           self.d_output_untis // self.d_branches))\n",
    "            self.branches.append(Branch(branch_layers))\n",
    "\n",
    "    def forward(self, u):\n",
    "        \"\"\"Forward pass of the MBGRU\"\"\"\n",
    "        u, _ = self.input_layer(u)      # Pass input through, while discard hidden states\n",
    "        shared_output, _ = self.shared_layer(u)\n",
    "                                        # Pass shared output through branches, in each only\n",
    "                                        # selecting the last N_HOURS_Y outputs\n",
    "        return [branch(shared_output[:, -N_HOURS_Y:, :]) for branch in self.branches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define evaluation and plotting functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of denormalisation function, its helper functions, and other helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_min_max(path):\n",
    "    \"\"\"\n",
    "    Retrieves the min and max values for each contaminant from the given path by:\n",
    "    1. Reading the csv file\n",
    "    2. Creating a dictionary with contaminant names as keys and min/max values as values\n",
    "    3. Returning the dictionary\n",
    "    \"\"\"\n",
    "    df_minmax = pd.read_csv(path, sep = ';', encoding = 'UTF-8', index_col = 0)\n",
    "    min = {f'{cont}_min': df_minmax.loc[cont, 'min'] for cont in CONTAMINANTS}\n",
    "    max = {f'{cont}_max': df_minmax.loc[cont, 'max'] for cont in CONTAMINANTS}\n",
    "    return {**min, **max}\n",
    "\n",
    "\n",
    "def normalise_linear_inv(tensor, min, max):\n",
    "    \"\"\"Performs inverse linear scaling (minmax) on tensor\"\"\"\n",
    "    return (tensor * (max - min)) + min\n",
    "\n",
    "\n",
    "def denormalise(tensor_3D, path):\n",
    "    \"\"\"Helper function for denormalising the predictions\"\"\"\n",
    "    tensor_3D_copy = tensor_3D.clone().detach()\n",
    "    dict_minmax = retrieve_min_max(path)\n",
    "\n",
    "    for idx, cont in enumerate(CONTAMINANTS):\n",
    "        min_val = dict_minmax[f'{cont}_min']\n",
    "        max_val = dict_minmax[f'{cont}_max']\n",
    "                                        # take first and only batch, all\n",
    "                                        # time steps, current contaminant\n",
    "        tensor_3D_copy[:, :, idx] = normalise_linear_inv(tensor_3D[:, :, idx],\n",
    "                                                         min_val,\n",
    "                                                         max_val)\n",
    "    return tensor_3D_copy\n",
    "\n",
    "\n",
    "def get_pred_and_gt(model, dataset, idx, denorm = True):\n",
    "    \"\"\"Returns the predictions and ground truth for a given row\"\"\"\n",
    "                                        # add batch dimension\n",
    "    input_data = dataset[idx][0].unsqueeze(0)\n",
    "    ground_truth = dataset[idx][1].unsqueeze(0)\n",
    "    \n",
    "    if denorm:                          # denormalise if necessary\n",
    "        pred = denormalise(tc.cat(model(input_data), dim = 2),\n",
    "                           MINMAX_PATH).detach().numpy()\n",
    "        gt = denormalise(ground_truth, MINMAX_PATH).detach().numpy()\n",
    "    else:\n",
    "        pred = tc.cat(model(input_data), dim = 2).detach().numpy()\n",
    "        gt = ground_truth.detach().numpy()\n",
    "\n",
    "    if pred.shape[0] != gt.shape[0]:\n",
    "        raise ValueError(\"choose_plot_component_values(): pred and gt should be comparable\")\n",
    "    return pred, gt\n",
    "\n",
    "\n",
    "def get_index(list: list, component):\n",
    "    \"\"\"Returns the index of the component in the list\"\"\"\n",
    "    try:\n",
    "        return list.index(component)\n",
    "    except ValueError:\n",
    "        return \"Component not found in the list\"\n",
    "    \n",
    "\n",
    "def choose_plot_component_values(model, dataset, idx, comp, denorm = True):\n",
    "    \"\"\"Chooses the correct component values for plotting\"\"\"\n",
    "    pred, gt = get_pred_and_gt(model, dataset, idx, denorm)\n",
    "    comp_idx = get_index(CONTAMINANTS, comp)\n",
    "    return np.squeeze(pred[:, :, comp_idx]), np.squeeze(gt[:, :, comp_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define plotting functions:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.set_palette(\"dark\")\n",
    "    sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    \"\"\"Plots the train and validation losses\"\"\"\n",
    "    set_style()\n",
    "\n",
    "    sns.lineplot(x = range(len(train_losses)), y = train_losses, label = \"Rtrain\")\n",
    "    sns.lineplot(x = range(len(val_losses)), y = val_losses, label = \"Rvalidation\")\n",
    "\n",
    "    plt.title('Empirical risk \"training error\" vs Risk \"validation/testing error\"')\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"risk / empirical risk\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_flexibility(empirical_risk, risk, x_labels):\n",
    "    \"\"\"PLots risk over model flexbility\"\"\"\n",
    "    set_style()\n",
    "\n",
    "    sns.scatterplot(x = range(len(empirical_risk)), y = empirical_risk, label = \"R_emp\")\n",
    "    sns.scatterplot(x = range(len(risk)), y = risk, label = \"R\")\n",
    "\n",
    "    plt.title('Empirical risk/Risk vs Model Flexibility')\n",
    "    plt.xticks(range(len(x_labels)), x_labels)\n",
    "    plt.xlabel(\"model flexibility\")\n",
    "    plt.ylabel(\"risk / empirical risk\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_losses_normalised(losses_1, losses_2, what = \"\"):\n",
    "    \"\"\"Plots two sequences of losses, normalised by themselves\"\"\"\n",
    "    set_style()\n",
    "\n",
    "    sns.lineplot(x = range(len(losses_1)),\n",
    "                 y = normalise_linear_inv(\n",
    "                     tc.tensor(losses_1), min(losses_1), max(losses_1)\n",
    "                 ), label = \"Losses 1\")\n",
    "    sns.lineplot(x = range(len(losses_2)),\n",
    "                 y = normalise_linear_inv(\n",
    "                     tc.tensor(losses_2), min(losses_2), max(losses_2)\n",
    "                 ), label = \"Losses 2\")\n",
    "\n",
    "    plt.title(f'Losses 1 vs Losses 2 - {what}')\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss (normalised)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred_vs_gt(model, dataset, row, comp):\n",
    "    \"\"\"Plots predictions (dotted) vs ground truth (solid)\"\"\"\n",
    "    pred, gt = choose_plot_component_values(model, dataset, row, comp)\n",
    "    \n",
    "    set_style()\n",
    "    \n",
    "    sns.lineplot(x = range(N_HOURS_Y), y = pred, label = f\"{comp}_pred\", linestyle = 'dashed')\n",
    "    sns.lineplot(x = range(N_HOURS_Y), y = gt, label = f\"{comp}_true\")\n",
    "    \n",
    "    plt.title(f\"{comp} prediction vs ground truth\")\n",
    "    plt.xlabel(\"time in hrs\")\n",
    "    plt.ylabel(f\"{comp} concentration\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RNN_view(data):\n",
    "    \"\"\"\n",
    "    An RNN expects a batch or data to be formatted as:\n",
    "    [batch_size, sequence_length, n_features],\n",
    "    this function reshapes the batch to that format.\n",
    "    \"\"\"\n",
    "    return data.view(data.shape[0], N_HOURS_U, len(COMPONENTS))\n",
    "\n",
    "\n",
    "def test_MB(model, loss_fn, test_loader, denorm = False, path = None):\n",
    "    \"\"\"Evaluates on test set and returns test loss\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = np.float64(0)\n",
    "\n",
    "    with tc.no_grad():\n",
    "        for batch_test_u, batch_test_y in test_loader:\n",
    "            batch_test_u = batch_test_u.to(device)\n",
    "            batch_test_y = batch_test_y.to(device)\n",
    "\n",
    "            pred = tc.cat(model(batch_test_u), dim = 2)\n",
    "            \n",
    "            if denorm:\n",
    "                pred = denormalise(pred, path)\n",
    "                batch_test_y = denormalise(batch_test_y, path)\n",
    "            \n",
    "            test_loss += loss_fn(pred, batch_test_y).item()\n",
    "\n",
    "    return test_loss / len(test_loader)\n",
    "\n",
    "\n",
    "def test_MB_separately(model, loss_fn, test_loader, denorm = False, path = None):\n",
    "    \"\"\"Evaluates on test set and returns test loss\"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    test_losses = [np.float64(0) for _ in CONTAMINANTS]\n",
    "\n",
    "    with tc.no_grad():\n",
    "        for batch_test_u, batch_test_y in test_loader:\n",
    "            batch_test_u = batch_test_u.to(device)\n",
    "            batch_test_y = batch_test_y.to(device)\n",
    "\n",
    "            pred = tc.cat(model(batch_test_u), dim = 2)\n",
    "            if denorm:\n",
    "                pred = denormalise(pred, path)\n",
    "                batch_test_y = denormalise(batch_test_y, path)\n",
    "\n",
    "            for comp in range(len(CONTAMINANTS)):                    \n",
    "                test_losses[comp] += loss_fn(\n",
    "                    pred[:, :, comp],\n",
    "                    batch_test_y[:, :, comp]\n",
    "                ).item()\n",
    "\n",
    "    for comp in range(len(CONTAMINANTS)):\n",
    "        test_losses[comp] /= len(test_loader)\n",
    "    return {comp: loss for comp, loss in zip(CONTAMINANTS, test_losses)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define training functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mb_model(hp):\n",
    "    \"\"\"Initializes the multi-branched model\"\"\"\n",
    "    return hp['model_class'](hp['input_units'], hp['hidden_layers'], hp['hidden_units'],\n",
    "                             hp['branches'], hp['output_units']).to(device)\n",
    "\n",
    "\n",
    "def init_main_optimizer(model, hp):\n",
    "    \"\"\"Initializes the optimizer for the shared layer\"\"\"\n",
    "    return hp['Optimizer'](model.parameters(), lr = hp['lr_shared'], \n",
    "                           weight_decay = hp['w_decay'])\n",
    "\n",
    "\n",
    "def init_branch_optimizers(model, hp):\n",
    "    \"\"\"Initializes the optimizers for each branch\"\"\"\n",
    "    optimizers = []\n",
    "    for branch in model.branches:\n",
    "        optimizers.append(hp['Optimizer'](branch.parameters(), \n",
    "                                          lr = hp['lr_branch'], \n",
    "                                          weight_decay = hp['w_decay']))\n",
    "    return optimizers\n",
    "\n",
    "\n",
    "def init_main_scheduler(optimizer, hp):\n",
    "    \"\"\"Initializes the scheduler for the shared layer\"\"\"\n",
    "    return hp['scheduler'](optimizer, **hp['scheduler_kwargs'])\n",
    "\n",
    "\n",
    "def init_branch_schedulers(optimizers, hp):\n",
    "    \"\"\"Initializes the schedulers for each branch\"\"\"\n",
    "    schedulers = []\n",
    "    for optimizer in optimizers:\n",
    "        schedulers.append(hp['scheduler'](optimizer, **hp['scheduler_kwargs']))\n",
    "    return schedulers\n",
    "\n",
    "\n",
    "def init_early_stopper(hp, verbose):\n",
    "     \"\"\"Initializes early stopping object\"\"\"\n",
    "     return hp['early_stopper'](hp['patience'], verbose)\n",
    "\n",
    "\n",
    "def schedulers_epoch(main_scheduler, secondary_schedulers, val_loss = None):\n",
    "    \"\"\"Performs a scheduler step for each scheduler\"\"\"\n",
    "    main_scheduler.step(val_loss)\n",
    "    for scheduler in secondary_schedulers:\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "def print_epoch_loss(epoch, train_losses, val_losses, x = 5):\n",
    "    \"\"\"Prints the train and validation losses per x epochs\"\"\"\n",
    "    if ((epoch + 1) % x == 0) or (epoch == 0):\n",
    "        print(\"Epoch: {} \\tLtrain: {:.6f} \\tLval: {:.6f}\".format(\n",
    "              epoch + 1, train_losses[epoch], val_losses[epoch]))\n",
    "\n",
    "\n",
    "def training_epoch_shared_layer(model, optimizer, loss_fn, train_loader):\n",
    "    \"\"\"Trains the shared layer of model for one epoch\"\"\"\n",
    "                                        # Start by freezing the branches;\n",
    "    for param in model.branches.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    train_loss = np.float64(0)          # Loop over the training set;\n",
    "    for batch_train_u, batch_train_y in train_loader:\n",
    "            batch_train_u = batch_train_u.to(device)\n",
    "            batch_train_y = batch_train_y.to(device)\n",
    "                                        # Do the forward pass and calculate loss;\n",
    "            batch_preds = tc.cat(model(batch_train_u), dim = 2)\n",
    "            batch_loss = loss_fn(batch_preds, batch_train_y)\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()       # Do the backward pass and update the weights;\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "    return train_loss                   # Return new loss after one \"shared layer step\"\n",
    "\n",
    "\n",
    "def training_epoch_branches(model, optimizers, loss_fn, train_loader):\n",
    "    \"\"\"Trains the branches of model for one epoch\"\"\"\n",
    "                                        # Start by unfreezing the branches and\n",
    "                                        # freezing the shared layer;\n",
    "    for param in model.branches.parameters():\n",
    "            param.requires_grad_(True)\n",
    "    for param in model.shared_layer.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    \n",
    "    train_loss = np.float64(0)\n",
    "    model.train()                       # Loop over the training set;\n",
    "    for batch_train_u, batch_train_y in train_loader:\n",
    "        batch_train_u = batch_train_u.to(device)\n",
    "        batch_train_y = batch_train_y.to(device)\n",
    "                                        # For every batch, loop over the branches;\n",
    "        for idx, optimizer in enumerate(optimizers):\n",
    "                                        # Perform the forward pass and calculate loss;\n",
    "                                        # Index branch dimension\n",
    "            branch_batch_loss = loss_fn(\n",
    "                tc.cat(model(batch_train_u), dim = 2)[:, :, idx],\n",
    "                batch_train_y[:, :, idx] \n",
    "            )\n",
    "            train_loss += branch_batch_loss.item()\n",
    "                                        \n",
    "            optimizer.zero_grad()       # Perform the backward pass and update the weights;\n",
    "            branch_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "                                        # Finally, unfreeze the shared layer and return.\n",
    "    for param in model.shared_layer.parameters():\n",
    "            param.requires_grad_(True)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validation_epoch(model, loss_fn, val_loader):\n",
    "    \"\"\"Evaluates the model on the validation set for one epoch\"\"\"\n",
    "    val_loss = np.float64(0)\n",
    "    model.eval()                        # Set model to evaluation mode;\n",
    "    with tc.no_grad():                  # Loop over the validation set;\n",
    "        for batch_val_u, batch_val_y in val_loader:\n",
    "            batch_val_u = batch_val_u.to(device)\n",
    "            batch_val_y = batch_val_y.to(device)\n",
    "                                        # Do the forward pass, concatenate the branch outputs,\n",
    "                                        # and calculate loss;\n",
    "            batch_preds = tc.cat(model(batch_val_u), dim = 2)\n",
    "            val_loss += loss_fn(batch_preds, batch_val_y).item()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_mb(hp, train_loader, val_loader, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains a MBNN by freezing the branches and shared layer alternately:\n",
    "    1. Initialize the model, optimizers, schedulers and early stopper,\n",
    "       and determine the loss function\n",
    "    2. For each epoch:\n",
    "        a. Train the shared layer\n",
    "        b. Train the branches\n",
    "        c. Evaluate on the validation set\n",
    "        d. Perform a scheduler step\n",
    "        e. Print the average train and validation losses per epoch\n",
    "        f. Check if the early stopper should stop the training\n",
    "    3. Return the best model, and the train and validation losses\n",
    "    \"\"\"\n",
    "    model = init_mb_model(hp)\n",
    "    main_optimizer = init_main_optimizer(model, hp)\n",
    "    branch_optimizers = init_branch_optimizers(model, hp)\n",
    "    shared_scheduler = init_main_scheduler(main_optimizer, hp)\n",
    "    branch_schedulers = init_branch_schedulers(branch_optimizers, hp)\n",
    "    early_stopper = init_early_stopper(hp, verbose)\n",
    "    loss_fn = hp['loss_fn']\n",
    "    shared_losses, branch_losses = [], []\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(hp['epochs']):   # For each epoch:\n",
    "                                        # save losses per epoch for both network parts\n",
    "        shared_losses.append(training_epoch_shared_layer(\n",
    "            model, main_optimizer, loss_fn, train_loader))\n",
    "        branch_losses.append(training_epoch_branches(\n",
    "            model, branch_optimizers, loss_fn, train_loader))\n",
    "                                        # calculate training and validation loss\n",
    "        train_loss = validation_epoch(model, loss_fn, train_loader)\n",
    "        val_loss = validation_epoch(model, loss_fn, val_loader)\n",
    "        schedulers_epoch(shared_scheduler, branch_schedulers, val_loss)\n",
    "        \n",
    "                                        # save average losses per batch for each epoch\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        print_epoch_loss(epoch, train_losses, val_losses, 1) if verbose else None\n",
    "                \n",
    "        if early_stopper(val_losses[epoch], epoch, model):\n",
    "            break\n",
    "    return early_stopper.best_model, train_losses, val_losses, shared_losses, branch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of standard $k$-fold cross-validation, configurated on training a MBNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_means_unequal_lists(lists):\n",
    "    \"\"\"\n",
    "    Calculates the means of lists with unequal lengths. This is useful\n",
    "    for calculating the average train and validation losses per epoch,\n",
    "    which may have \"early stopped\" at different epoch moments.\n",
    "    \"\"\"\n",
    "    max_len = max([len(list) for list in lists])\n",
    "\n",
    "    for list in lists:                  # First, get lists to equal length with padding,\n",
    "        if len(list) < max_len:         # then, calculate the mean over the lists\n",
    "            list.extend([np.nan] * (max_len - len(list)))\n",
    "    return np.nanmean(lists, axis = 0)\n",
    "\n",
    "\n",
    "def get_idx_KFoldXV_expanding_window(fold, n_folds, dataset_len):\n",
    "    \"\"\"Calculates the indices of expanding window k-fold cross validation\"\"\"\n",
    "    fold_size = dataset_len // n_folds # integer division\n",
    "                                       # determine ending indices of:\n",
    "    if fold == n_folds - 1:            # last fold\n",
    "        train_end_idx = fold_size * fold\n",
    "        val_end_idx = dataset_len\n",
    "    else:                              # all other folds\n",
    "        train_end_idx = fold_size * (fold + 1)\n",
    "        val_end_idx = fold_size * (fold + 2)\n",
    "\n",
    "    train_indices = list(range(0, train_end_idx))\n",
    "    val_indices = list(range(train_end_idx, val_end_idx))\n",
    "    return train_indices, val_indices\n",
    "\n",
    "\n",
    "def get_idx_KFoldXV_sliding_window(fold, n_folds, dataset_len):\n",
    "    \"\"\"Calculates the indices sliding window k-fold cross validation\"\"\"\n",
    "    fold_size = dataset_len // (n_folds + 1)\n",
    "    remainder = dataset_len % (n_folds + 1)\n",
    "\n",
    "    train_start_idx = fold_size * fold + min(fold, remainder)\n",
    "    train_end_idx = fold_size * (fold + 1) + min(fold, remainder)\n",
    "    val_start_idx = train_end_idx\n",
    "    val_end_idx = val_start_idx + fold_size\n",
    "\n",
    "    train_indices = list(range(train_start_idx, train_end_idx))\n",
    "    val_indices = list(range(val_start_idx, val_end_idx))\n",
    "    return train_indices, val_indices\n",
    "    \n",
    "\n",
    "def KFoldXV_MB(hp, train_dataset, verbose = True): \n",
    "    \"\"\"\n",
    "    Performs k-fold cross validation training on a given model:\n",
    "    1. For each fold:\n",
    "        a. Get the indices for the current fold\n",
    "        b. Initialize the train and validation loaders\n",
    "        c. Train the model on the current fold\n",
    "        d. Save the train and validation losses\n",
    "        e. In case it has performed well, save the \"best\" model\n",
    "    2. Calculate the average train and validation losses per epoch\n",
    "    3. Return the best model, and the train and validation losses\n",
    "    \"\"\"\n",
    "    val_losses_kfold = []\n",
    "\n",
    "    for fold in range(hp['k_folds']):\n",
    "        print(f\"\\n\\tFold {fold + 1}/{hp['k_folds']}\") if verbose else None\n",
    "                                        # get indices for the current fold\n",
    "        train_indices, val_indices = get_idx_KFoldXV_sliding_window(\n",
    "            fold, hp['k_folds'], train_dataset.__len__())\n",
    "                                        # create the train and validation loaders,\n",
    "                                        # with random sampling for the train loader\n",
    "        train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], \n",
    "                            sampler = SubsetRandomSampler(train_indices))\n",
    "        val_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], \n",
    "                            sampler = SequentialSampler(val_indices))\n",
    "                                        # Train new model on the current fold\n",
    "        _, _, val_losses, _, _ = train_mb(hp, train_loader, val_loader, verbose)\n",
    "        val_losses_kfold.append(val_losses)\n",
    "\n",
    "    return np.mean([losses[-1] for losses in val_losses_kfold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an ordinary grid search through the given hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_by_keys(dict, secondary_dict):\n",
    "    \"\"\"\n",
    "    Filters a dictionary by the keys of another dictionary. For example:\n",
    "    dict = {'a': 1, 'b': 2, 'c': 3} and secondary_dict = {'a': 0, 'c': 0}\n",
    "    returns {'a': 1, 'c': 3}\n",
    "    \"\"\"\n",
    "    return {k: dict[k] for k in secondary_dict if k in dict}\n",
    "\n",
    "\n",
    "def update_dict(dict, secondary_dict):\n",
    "    \"\"\"\n",
    "    Updates a dictionary with the keys of another dictionary. For example:\n",
    "    dict = {'a': 1, 'b': 2, 'c': 3} and secondary_dict = {'a': 0, 'c': 0}\n",
    "    returns {'a': 0, 'b': 2, 'c': 0}\n",
    "    \"\"\"\n",
    "    return {**dict, **secondary_dict}\n",
    "\n",
    "\n",
    "def print_dict_vertically(d):\n",
    "    \"\"\"Prints a dictionary formatted vertically\"\"\"\n",
    "    max_key_len = max(len(key) for key in d.keys())\n",
    "    for key, value in d.items():\n",
    "        print(f\"{key:{max_key_len}}: {value}\")\n",
    "\n",
    "\n",
    "def print_dict_vertically_root(d):\n",
    "    \"\"\"Prints a dictionary formatted vertically\"\"\"\n",
    "    max_key_len = max(len(key) for key in d.keys())\n",
    "    for key, value in d.items():\n",
    "        print(f\"{key:{max_key_len}}: {np.sqrt(value)}\")\n",
    "\n",
    "\n",
    "def ensure_integers(configs):\n",
    "    \"\"\"Ensures the hidden_layers and hidden_units are op type int\"\"\"\n",
    "    for config in configs:\n",
    "        config['hidden_layers'] = int(config['hidden_layers'])\n",
    "        config['hidden_units'] = int(config['hidden_units'])\n",
    "    return configs\n",
    "\n",
    "# https://stackoverflow.com/questions/798854/all-combinations-of-a-list-of-lists\n",
    "def gen_configs(hp_space):\n",
    "    \"\"\"Helper function to initiate the configuration generator\"\"\"\n",
    "    keys = list(hp_space.keys())\n",
    "    values = list(hp_space.values())\n",
    "    configs = [dict(zip(keys, combi))   # Generate all possible combinations, see link^ for source\n",
    "                    for combi in np.array(np.meshgrid(*values)).T.reshape(-1, len(values))]\n",
    "    return ensure_integers(configs)\n",
    "\n",
    "\n",
    "def print_current_config(config):\n",
    "    \"\"\"Prints the current configuration\"\"\"\n",
    "    print(\"CURRENT CONFIGURATION:\")\n",
    "    print_dict_vertically(config)\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_end_of_grid_search(best_hp, best_val_loss):\n",
    "    \"\"\"Prints the results of the grid search\"\"\"\n",
    "    print(f\"##### Best average validation loss so far: {best_val_loss:.6f} #####\")\n",
    "    print(\"With configuration:\")\n",
    "    print_dict_vertically(best_hp)\n",
    "    print()\n",
    "\n",
    "\n",
    "def grid_search(hp, hp_space, train_dataset, verbose = False):\n",
    "    \"\"\"Perform a grid seach through hyperparameter space\"\"\"\n",
    "    best_hp, best_val_loss = {}, np.inf\n",
    "                                        # For each hyperparameter configuration:\n",
    "    for config in gen_configs(hp_space):\n",
    "        config_dict = update_dict(hp, config)\n",
    "        if verbose:\n",
    "            print_current_config(config_dict)\n",
    "                                        # train model with current configuration\n",
    "        current_config_loss = KFoldXV_MB(config_dict, train_dataset, verbose)\n",
    "        if current_config_loss < best_val_loss:\n",
    "            best_hp = config_dict.copy()\n",
    "            best_val_loss = current_config_loss\n",
    "        if verbose:\n",
    "            print(f\"Average final validation loss: {current_config_loss:.6f}\")\n",
    "            print_end_of_grid_search(best_hp, best_val_loss)\n",
    "\n",
    "    if not verbose:                      # print once when not verbose\n",
    "        print_end_of_grid_search(best_hp, best_val_loss)\n",
    "    \n",
    "    return best_hp, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and searchable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, all (hyper)parameters are defined. The hyperparameters are defined in\n",
    "# a dictionary, which is then passed to the model and the training functions.\n",
    "# The grid search is performed by generating all possible combinations of the\n",
    "# hyperparameters defined in the hp_space dictionary, and then performing k-fold cross\n",
    "# validation on each of these configurations. The best configuration is then returned.\n",
    "# When the search is finished, comment out the hp_space dictionary and save the best found\n",
    "# hyperparameters in the hp dictionary, and train the final model with these.\n",
    "\n",
    "hp = {\n",
    "    'model_class' : MBGRU,\n",
    "    'input_units' : train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 64,\n",
    "    'branches' : 4,\n",
    "    'output_units' : train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min', 'factor' : 0.1,\n",
    "                          'patience' : 3, 'cooldown' : 8, 'verbose' : True},\n",
    "    'w_decay' : 1e-7,\n",
    "    'loss_fn' : nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 20,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}                                   # The lr for the branched layer(s) is calculated\n",
    "                                    # based on the \"power ratio\" between the branched\n",
    "                                    # part of the network and the shared layer, which\n",
    "                                    # is *assumed* to be proportional to n_hidden_layers\n",
    "hp['lr_branch'] = hp['lr_shared'] * hp['hidden_layers']\n",
    "\n",
    "# hp_space = {\n",
    "#     'hidden_layers' : [2, 3, 4, 5, 6, 7, 8],\n",
    "#     'hidden_units' : [16, 32, 48, 64, 80, 96, 112, 128],\n",
    "#     'w_decay' : [1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by performing a grid search and saving the optimally configurated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training now\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'grid_search_exe_s/exe_of_MBGRU_at_{current_time}.txt'\n",
    "# train_dataset_full = ConcatDataset([train_dataset, val_dataset])\n",
    "                                    # If HABROK, print to external file, else print to stdout\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(f\"Grid search execution of MBGRU at {current_time}\\n\")\n",
    "#                                     # Train on the full training set\n",
    "#     model, best_hp, val_loss = grid_search(hp, hp_space, train_dataset_full, True)\n",
    "#                                     # Externally save the best model\n",
    "#     tc.save(model.state_dict(), f\"{MODEL_PATH}\\model_MBGRU.pth\")\n",
    "\n",
    "#     hp = update_dict(hp, best_hp)   # Update the hp dictionary with the best hyperparameters\n",
    "#     print_dict_vertically(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out model architecture with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing model:\n",
      "MBGRU(\n",
      "  (input_layer): GRU(10, 64, batch_first=True)\n",
      "  (shared_layer): GRU(64, 64, batch_first=True)\n",
      "  (branches): ModuleList(\n",
      "    (0-3): 4 x Branch(\n",
      "      (layers): ModuleList(\n",
      "        (0): GRU(64, 16, batch_first=True)\n",
      "        (1): GRU(16, 16, num_layers=3, batch_first=True)\n",
      "        (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    model = MBGRU(hp['input_units'], hp['hidden_layers'], hp['hidden_units'], \n",
    "                   hp['branches'], hp['output_units'])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on complete training dataset (= train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "                                        # Train the final model on the full training set,\n",
    "                                        # save the final model, and save the losses for plotting\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(\"\\nTraining on full training set...\")\n",
    "#     model_final, train_losses, test_losses, shared_losses, branch_losses = \\\n",
    "#         train_mb(hp, train_loader, val_loader, True)\n",
    "#     tc.save(model_final.state_dict(), f'{MODEL_PATH}\\model_MBGRU.pth')\n",
    "\n",
    "# df_losses = pd.DataFrame({'L_train': train_losses, 'L_test': test_losses})\n",
    "# df_losses.to_csv(f'{os.path.join(os.getcwd(), \"final_losses\")}\\losses_MBGRU_at_{current_time}.csv', \n",
    "#                  sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\vwold\\\\Documents\\\\Bachelor\\\\ICML_paper\\\\forecasting_smog_DL\\\\forecasting_smog_DL\\\\src\\\\modelling\\\\models\\\\model_MBGRU.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_final \u001b[38;5;241m=\u001b[39m MBGRU(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_units\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_units\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      2\u001b[0m                      hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbranches\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_units\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m model_final\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel_MBGRU.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_final)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\vwold\\\\Documents\\\\Bachelor\\\\ICML_paper\\\\forecasting_smog_DL\\\\forecasting_smog_DL\\\\src\\\\modelling\\\\models\\\\model_MBGRU.pth'"
     ]
    }
   ],
   "source": [
    "model_final = MBGRU(hp['input_units'], hp['hidden_layers'], hp['hidden_units'],\n",
    "                     hp['branches'], hp['output_units'])\n",
    "model_final.load_state_dict(tc.load(f\"{MODEL_PATH}\\model_MBGRU.pth\"))\n",
    "print(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "test_error = test_MB(model_final, nn.MSELoss(), test_loader)\n",
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_MB(model_final, nn.MSELoss(), train_loader))\n",
    "print(test_MB(model_final, nn.MSELoss(), val_loader))\n",
    "print(test_MB(model_final, nn.MSELoss(), test_loader))\n",
    "\n",
    "print(\"\\nMSE Training set:\")\n",
    "print_dict_vertically(\n",
    "    test_MB_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Validation set:\")\n",
    "print_dict_vertically(\n",
    "    test_MB_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Test set:\")\n",
    "print_dict_vertically(\n",
    "    test_MB_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRMSE Training set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_MB_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Validation set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_MB_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Test set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_MB_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")\n",
    "np.sqrt(test_MB(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 5\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'NO2')\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'O3')\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM10')\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM25')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
