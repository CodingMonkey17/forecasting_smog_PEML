{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "For running the models, we take the following steps:\n",
    "\n",
    "> * Prepare packages, setup, data\n",
    "> * Load model\n",
    "> * Define hyperparameters\n",
    "> * Train the model\n",
    "> * Evaluate the model\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify potential errors happening on Habrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prepare packages, setup, data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "Importing modelling package...\n",
      "\n",
      "Running __init__.py for data pipeline\n",
      "Modelling package initialized\n",
      "\n",
      "Importing libs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "print(\"Importing modelling package...\")\n",
    "from modelling import *\n",
    "from modelling import GRU\n",
    "from modelling import HGRU\n",
    "\n",
    "#! Kijk welke imports weg kunnen!\n",
    "print(\"Importing libs...\")\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    SubsetRandomSampler,\n",
    "    SequentialSampler\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = tc.cuda.is_available()\n",
    "device = tc.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Global\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HABROK = bool(0)                       # True if running on Habrok or external server\n",
    "if HABROK:\n",
    "    print(\"Successfully imported libraries into env\")\n",
    "    USER = 'habrok'\n",
    "else:\n",
    "    USER = 'tinus'\n",
    "\n",
    "if USER == 'tinus':\n",
    "    os.chdir(r\"c:\\Users\\vwold\\Documents\\Bachelor\\ICML_paper\\forecasting_smog_DL\\forecasting_smog_DL\\src\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"results/models\") #! TODO get these two out and to somewhere else, or gone in general\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "elif HABROK:\n",
    "    os.chdir(r\"/home1/s4372948/thesis/modelling/\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"results/models\")\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "\n",
    "tc.manual_seed(34)\n",
    "mpl.rcParams['figure.figsize'] = (7, 3)\n",
    "\n",
    "N_HOURS_U = 72\n",
    "N_HOURS_Y = 24\n",
    "N_HOURS_STEP = 24\n",
    "CONTAMINANTS = ['NO2', 'O3', 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data and create PyTorch *Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u')\n",
    "train_output_frames = get_dataframes('train', 'y')\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u')\n",
    "val_output_frames = get_dataframes('val', 'y')\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u')\n",
    "test_output_frames = get_dataframes('test', 'y')\n",
    "\n",
    "print(\"Successfully loaded data\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    5,                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define evaluation and plotting functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define training functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an ordinary grid search through the given hyperparameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and searchable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, all (hyper)parameters are defined. The hyperparameters are defined in\n",
    "# a dictionary, which is then passed to the model and the training functions.\n",
    "# The grid search is performed by generating all possible combinations of the\n",
    "# hyperparameters defined in the hp_space dictionary, and then performing k-fold cross\n",
    "# validation on each of these configurations. The best configuration is then returned.\n",
    "# When the search is finished, comment out the hp_space dictionary and save the best found\n",
    "# hyperparameters in the hp dictionary, and train the final model with these.\n",
    "\n",
    "#! TODO voeg nieuwe constructors van Model classes toe aan hp_space en ook de model initialisatie\n",
    "\n",
    "hp = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : HGRU,\n",
    "    'input_units' : train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 64,\n",
    "    'branches' : 4,\n",
    "    'output_units' : train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min', 'factor' : 0.1,\n",
    "                          'patience' : 3, 'cooldown' : 8, 'verbose' : True},\n",
    "    'w_decay' : 1e-7,\n",
    "    'loss_fn' : nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 20,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}                                   # The lr for the branched layer(s) is calculated\n",
    "                                    # based on the \"power ratio\" between the branched\n",
    "                                    # part of the network and the shared layer, which\n",
    "                                    # is *assumed* to be proportional to n_hidden_layers\n",
    "hp['lr_branch'] = hp['lr_shared'] * hp['hidden_layers']\n",
    "\n",
    "# hp_space = {\n",
    "#     'hidden_layers' : [2, 3, 4, 5, 6, 7, 8],\n",
    "#     'hidden_units' : [16, 32, 48, 64, 80, 96, 112, 128],\n",
    "#     'w_decay' : [1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by performing a grid search and saving the optimally configurated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training now\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'results/grid_search_exe_s/exe_of_HGRU_at_{current_time}.txt'\n",
    "# train_dataset_full = ConcatDataset([train_dataset, val_dataset])\n",
    "#                                     If HABROK, print to external file, else print to stdout\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(f\"Grid search execution of HGRU at {current_time}\\n\")\n",
    "#                                     # Train on the full training set\n",
    "#     model, best_hp, val_loss = grid_search(hp, hp_space, train_dataset_full, True)\n",
    "#                                     # Externally save the best model\n",
    "#     tc.save(model.state_dict(), f\"{MODEL_PATH}/results/model_HGRU.pth\")\n",
    "\n",
    "#     hp = update_dict(hp, best_hp)   # Update the hp dictionary with the best hyperparameters\n",
    "#     print_dict_vertically(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out model architecture with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing model:\n",
      "HGRU(\n",
      "  (input_layer): GRU(10, 64, batch_first=True)\n",
      "  (shared_layer): GRU(64, 64, batch_first=True)\n",
      "  (branches): ModuleList(\n",
      "    (0-3): 4 x Branch(\n",
      "      (layers): ModuleList(\n",
      "        (0): GRU(64, 16, batch_first=True)\n",
      "        (1): GRU(16, 16, num_layers=3, batch_first=True)\n",
      "        (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    model = HGRU(hp['n_hours_u'],\n",
    "                 hp['n_hours_y'],\n",
    "                 hp['input_units'],\n",
    "                 hp['hidden_layers'],\n",
    "                 hp['hidden_units'], \n",
    "                 hp['branches'],\n",
    "                 hp['output_units'])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on complete training dataset (= train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on full training set...\n",
      "An exception of type <class 'KeyboardInterrupt'> occurred with value \n",
      "Traceback: <traceback object at 0x00000184E38F4C00>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PrintManager(stdout_location, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, HABROK):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining on full training set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     model_final, train_losses, test_losses, shared_losses, branch_losses \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 9\u001b[0m         \u001b[43mtrain_mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     tc\u001b[38;5;241m.\u001b[39msave(model_final\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_HGRU.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m df_losses \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_train\u001b[39m\u001b[38;5;124m'\u001b[39m: train_losses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_test\u001b[39m\u001b[38;5;124m'\u001b[39m: test_losses})\n",
      "Cell \u001b[1;32mIn[8], line 161\u001b[0m, in \u001b[0;36mtrain_mb\u001b[1;34m(hp, train_loader, val_loader, verbose)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):   \u001b[38;5;66;03m# For each epoch:\u001b[39;00m\n\u001b[0;32m    158\u001b[0m                                     \u001b[38;5;66;03m# save losses per epoch for both network parts\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     shared_losses\u001b[38;5;241m.\u001b[39mappend(training_epoch_shared_layer(\n\u001b[0;32m    160\u001b[0m         model, main_optimizer, loss_fn, train_loader))\n\u001b[1;32m--> 161\u001b[0m     branch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtraining_epoch_branches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch_optimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    163\u001b[0m                                     \u001b[38;5;66;03m# calculate training and validation loss\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m validation_epoch(model, loss_fn, train_loader)\n",
      "Cell \u001b[1;32mIn[8], line 110\u001b[0m, in \u001b[0;36mtraining_epoch_branches\u001b[1;34m(model, optimizers, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m    107\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m branch_batch_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()       \u001b[38;5;66;03m# Perform the backward pass and update the weights;\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43mbranch_batch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    112\u001b[0m                             \u001b[38;5;66;03m# Finally, unfreeze the shared layer and return.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "                                        # Train the final model on the full training set,\n",
    "                                        # save the final model, and save the losses for plotting\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nTraining on full training set...\")\n",
    "    model_final, train_losses, test_losses, shared_losses, branch_losses = \\\n",
    "        train_hierarchical(hp, train_loader, val_loader, True)\n",
    "    tc.save(model_final.state_dict(), f'{MODEL_PATH}/model_HGRU.pth')\n",
    "\n",
    "df_losses = pd.DataFrame({'L_train': train_losses, 'L_test': test_losses})\n",
    "df_losses.to_csv(f'{os.path.join(os.getcwd(), \"results/final_losses\")}/losses_HGRU_at_{current_time}.csv', \n",
    "                 sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\vwold\\\\Documents\\\\Bachelor\\\\ICML_paper\\\\forecasting_smog_DL\\\\forecasting_smog_DL\\\\src\\\\models\\\\model_MBGRU.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_final \u001b[38;5;241m=\u001b[39m MBGRU(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_units\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_units\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      2\u001b[0m                      hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbranches\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_units\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m model_final\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel_MBGRU.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_final)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\vwold\\\\Documents\\\\Bachelor\\\\ICML_paper\\\\forecasting_smog_DL\\\\forecasting_smog_DL\\\\src\\\\models\\\\model_MBGRU.pth'"
     ]
    }
   ],
   "source": [
    "model_final = HGRU(hp['input_units'], hp['hidden_layers'], hp['hidden_units'],\n",
    "                     hp['branches'], hp['output_units'])\n",
    "model_final.load_state_dict(tc.load(f\"{MODEL_PATH}/model_HGRU.pth\"))\n",
    "print(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "test_error = test_hierarchical(model_final, nn.MSELoss(), test_loader)\n",
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_hierarchical(model_final, nn.MSELoss(), train_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), val_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), test_loader))\n",
    "\n",
    "print(\"\\nMSE Training set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Validation set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Test set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRMSE Training set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Validation set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Test set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")\n",
    "np.sqrt(test_hierarchical(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 5\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'NO2', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'O3', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM10', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM25', N_HOURS_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
