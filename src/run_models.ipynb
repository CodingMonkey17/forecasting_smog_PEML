{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "For running the models, we take the following steps:\n",
    "\n",
    "> * Prepare packages, setup, data\n",
    "> * Load model\n",
    "> * Define hyperparameters\n",
    "> * Train the model\n",
    "> * Evaluate the model\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify potential errors happening on Habrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prepare packages, setup, data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "Importing modelling package...\n",
      "\n",
      "Running __init__.py for data pipeline\n",
      "Modelling package initialized\n",
      "\n",
      "Importing libs...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "print(\"Importing modelling package...\")\n",
    "from modelling import *\n",
    "from modelling import GRU\n",
    "from modelling import HGRU\n",
    "\n",
    "print(\"Importing libs...\")\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    "    SubsetRandomSampler,\n",
    "    SequentialSampler\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = tc.cuda.is_available()\n",
    "device = tc.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Global\" variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HABROK = bool(0)                       # True if running on Habrok or external server\n",
    "if HABROK:\n",
    "    print(\"Successfully imported libraries into env\")\n",
    "    USER = 'habrok'\n",
    "else:\n",
    "    USER = 'tinus'\n",
    "\n",
    "if USER == 'tinus':\n",
    "    os.chdir(r\"c:\\Users\\vwold\\Documents\\Bachelor\\ICML_paper\\forecasting_smog_DL\\forecasting_smog_DL\\src\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"models\") #! TODO get these two out and to somewhere else, or gone in general\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "elif HABROK:\n",
    "    os.chdir(r\"/home1/s4372948/thesis/modelling/\")\n",
    "    MODEL_PATH = os.path.join(os.getcwd(), \"models\")\n",
    "    MINMAX_PATH = \"../data/data_combined/contaminant_minmax.csv\"\n",
    "\n",
    "tc.manual_seed(34)\n",
    "mpl.rcParams['figure.figsize'] = (7, 3)\n",
    "\n",
    "N_HOURS_U = 72\n",
    "N_HOURS_Y = 24\n",
    "N_HOURS_STEP = 24\n",
    "CONTAMINANTS = ['NO2', 'O3', 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data and create PyTorch *Datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u')\n",
    "train_output_frames = get_dataframes('train', 'y')\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u')\n",
    "val_output_frames = get_dataframes('val', 'y')\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u')\n",
    "test_output_frames = get_dataframes('test', 'y')\n",
    "\n",
    "print(\"Successfully loaded data\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    5,                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define evaluation and plotting functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define training functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mb_model(hp):\n",
    "    \"\"\"\n",
    "    Inits multi-branched, or hierarchical, model,\n",
    "    by calling its constructor with the hyperparameters\n",
    "    \"\"\"\n",
    "    return hp['model_class'](hp['n_hours_u'],\n",
    "                             hp['n_hours_y'],\n",
    "                             hp['input_units'],\n",
    "                             hp['hidden_layers'],\n",
    "                             hp['hidden_units'],\n",
    "                             hp['branches'],\n",
    "                             hp['output_units']\n",
    "                             ).to(device)\n",
    "\n",
    "\n",
    "def init_main_optimizer(model, hp):\n",
    "    \"\"\"Initializes the optimizer for the shared layer\"\"\"\n",
    "    return hp['Optimizer'](model.parameters(), lr = hp['lr_shared'], \n",
    "                           weight_decay = hp['w_decay'])\n",
    "\n",
    "\n",
    "def init_branch_optimizers(model, hp):\n",
    "    \"\"\"Initializes the optimizers for each branch\"\"\"\n",
    "    optimizers = []\n",
    "    for branch in model.branches:\n",
    "        optimizers.append(hp['Optimizer'](branch.parameters(), \n",
    "                                          lr = hp['lr_branch'], \n",
    "                                          weight_decay = hp['w_decay']))\n",
    "    return optimizers\n",
    "\n",
    "\n",
    "def init_main_scheduler(optimizer, hp):\n",
    "    \"\"\"Initializes the scheduler for the shared layer\"\"\"\n",
    "    return hp['scheduler'](optimizer, **hp['scheduler_kwargs'])\n",
    "\n",
    "\n",
    "def init_branch_schedulers(optimizers, hp):\n",
    "    \"\"\"Initializes the schedulers for each branch\"\"\"\n",
    "    schedulers = []\n",
    "    for optimizer in optimizers:\n",
    "        schedulers.append(hp['scheduler'](optimizer, **hp['scheduler_kwargs']))\n",
    "    return schedulers\n",
    "\n",
    "\n",
    "def init_early_stopper(hp, verbose):\n",
    "     \"\"\"Initializes early stopping object\"\"\"\n",
    "     return hp['early_stopper'](hp['patience'], verbose)\n",
    "\n",
    "\n",
    "def schedulers_epoch(main_scheduler, secondary_schedulers, val_loss = None):\n",
    "    \"\"\"Performs a scheduler step for each scheduler\"\"\"\n",
    "    main_scheduler.step(val_loss)\n",
    "    for scheduler in secondary_schedulers:\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "def print_epoch_loss(epoch, train_losses, val_losses, x = 5):\n",
    "    \"\"\"Prints the train and validation losses per x epochs\"\"\"\n",
    "    if ((epoch + 1) % x == 0) or (epoch == 0):\n",
    "        print(\"Epoch: {} \\tLtrain: {:.6f} \\tLval: {:.6f}\".format(\n",
    "              epoch + 1, train_losses[epoch], val_losses[epoch]))\n",
    "\n",
    "\n",
    "def training_epoch_shared_layer(model, optimizer, loss_fn, train_loader):\n",
    "    \"\"\"Trains the shared layer of model for one epoch\"\"\"\n",
    "                                        # Start by freezing the branches;\n",
    "    for param in model.branches.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    train_loss = np.float64(0)          # Loop over the training set;\n",
    "    for batch_train_u, batch_train_y in train_loader:\n",
    "            batch_train_u = batch_train_u.to(device)\n",
    "            batch_train_y = batch_train_y.to(device)\n",
    "                                        # Do the forward pass and calculate loss;\n",
    "            batch_preds = tc.cat(model(batch_train_u), dim = 2)\n",
    "            batch_loss = loss_fn(batch_preds, batch_train_y)\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()       # Do the backward pass and update the weights;\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "    return train_loss                   # Return new loss after one \"shared layer step\"\n",
    "\n",
    "\n",
    "def training_epoch_branches(model, optimizers, loss_fn, train_loader):\n",
    "    \"\"\"Trains the branches of model for one epoch\"\"\"\n",
    "                                        # Start by unfreezing the branches and\n",
    "                                        # freezing the shared layer;\n",
    "    for param in model.branches.parameters():\n",
    "            param.requires_grad_(True)\n",
    "    for param in model.shared_layer.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    \n",
    "    train_loss = np.float64(0)\n",
    "    model.train()                       # Loop over the training set;\n",
    "    for batch_train_u, batch_train_y in train_loader:\n",
    "        batch_train_u = batch_train_u.to(device)\n",
    "        batch_train_y = batch_train_y.to(device)\n",
    "                                        # For every batch, loop over the branches;\n",
    "        for idx, optimizer in enumerate(optimizers):\n",
    "                                        # Perform the forward pass and calculate loss;\n",
    "                                        # Index branch dimension\n",
    "            branch_batch_loss = loss_fn(\n",
    "                tc.cat(model(batch_train_u), dim = 2)[:, :, idx],\n",
    "                batch_train_y[:, :, idx] \n",
    "            )\n",
    "            train_loss += branch_batch_loss.item()\n",
    "                                        \n",
    "            optimizer.zero_grad()       # Perform the backward pass and update the weights;\n",
    "            branch_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "                                        # Finally, unfreeze the shared layer and return.\n",
    "    for param in model.shared_layer.parameters():\n",
    "            param.requires_grad_(True)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validation_epoch(model, loss_fn, val_loader):\n",
    "    \"\"\"Evaluates the model on the validation set for one epoch\"\"\"\n",
    "    val_loss = np.float64(0)\n",
    "    model.eval()                        # Set model to evaluation mode;\n",
    "    with tc.no_grad():                  # Loop over the validation set;\n",
    "        for batch_val_u, batch_val_y in val_loader:\n",
    "            batch_val_u = batch_val_u.to(device)\n",
    "            batch_val_y = batch_val_y.to(device)\n",
    "                                        # Do the forward pass, concatenate the branch outputs,\n",
    "                                        # and calculate loss;\n",
    "            batch_preds = tc.cat(model(batch_val_u), dim = 2)\n",
    "            val_loss += loss_fn(batch_preds, batch_val_y).item()\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_mb(hp, train_loader, val_loader, verbose = False):\n",
    "    \"\"\"\n",
    "    Trains a MBNN by freezing the branches and shared layer alternately:\n",
    "    1. Initialize the model, optimizers, schedulers and early stopper,\n",
    "       and determine the loss function\n",
    "    2. For each epoch:\n",
    "        a. Train the shared layer\n",
    "        b. Train the branches\n",
    "        c. Evaluate on the validation set\n",
    "        d. Perform a scheduler step\n",
    "        e. Print the average train and validation losses per epoch\n",
    "        f. Check if the early stopper should stop the training\n",
    "    3. Return the best model, and the train and validation losses\n",
    "    \"\"\"\n",
    "    model = init_mb_model(hp)\n",
    "    main_optimizer = init_main_optimizer(model, hp)\n",
    "    branch_optimizers = init_branch_optimizers(model, hp)\n",
    "    shared_scheduler = init_main_scheduler(main_optimizer, hp)\n",
    "    branch_schedulers = init_branch_schedulers(branch_optimizers, hp)\n",
    "    early_stopper = init_early_stopper(hp, verbose)\n",
    "    loss_fn = hp['loss_fn']\n",
    "    shared_losses, branch_losses = [], []\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(hp['epochs']):   # For each epoch:\n",
    "                                        # save losses per epoch for both network parts\n",
    "        shared_losses.append(training_epoch_shared_layer(\n",
    "            model, main_optimizer, loss_fn, train_loader))\n",
    "        branch_losses.append(training_epoch_branches(\n",
    "            model, branch_optimizers, loss_fn, train_loader))\n",
    "                                        # calculate training and validation loss\n",
    "        train_loss = validation_epoch(model, loss_fn, train_loader)\n",
    "        val_loss = validation_epoch(model, loss_fn, val_loader)\n",
    "        schedulers_epoch(shared_scheduler, branch_schedulers, val_loss)\n",
    "        \n",
    "                                        # save average losses per batch for each epoch\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        print_epoch_loss(epoch, train_losses, val_losses, 1) if verbose else None\n",
    "                \n",
    "        if early_stopper(val_losses[epoch], epoch, model):\n",
    "            break\n",
    "    return early_stopper.best_model, train_losses, val_losses, shared_losses, branch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of standard $k$-fold cross-validation, configurated on training a MBNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_means_unequal_lists(lists):\n",
    "    \"\"\"\n",
    "    Calculates the means of lists with unequal lengths. This is useful\n",
    "    for calculating the average train and validation losses per epoch,\n",
    "    which may have \"early stopped\" at different epoch moments.\n",
    "    \"\"\"\n",
    "    max_len = max([len(list) for list in lists])\n",
    "\n",
    "    for list in lists:                  # First, get lists to equal length with padding,\n",
    "        if len(list) < max_len:         # then, calculate the mean over the lists\n",
    "            list.extend([np.nan] * (max_len - len(list)))\n",
    "    return np.nanmean(lists, axis = 0)\n",
    "\n",
    "\n",
    "def get_idx_KFoldXV_expanding_window(fold, n_folds, dataset_len):\n",
    "    \"\"\"Calculates the indices of expanding window k-fold cross validation\"\"\"\n",
    "    fold_size = dataset_len // n_folds # integer division\n",
    "                                       # determine ending indices of:\n",
    "    if fold == n_folds - 1:            # last fold\n",
    "        train_end_idx = fold_size * fold\n",
    "        val_end_idx = dataset_len\n",
    "    else:                              # all other folds\n",
    "        train_end_idx = fold_size * (fold + 1)\n",
    "        val_end_idx = fold_size * (fold + 2)\n",
    "\n",
    "    train_indices = list(range(0, train_end_idx))\n",
    "    val_indices = list(range(train_end_idx, val_end_idx))\n",
    "    return train_indices, val_indices\n",
    "\n",
    "\n",
    "def get_idx_KFoldXV_sliding_window(fold, n_folds, dataset_len):\n",
    "    \"\"\"Calculates the indices sliding window k-fold cross validation\"\"\"\n",
    "    fold_size = dataset_len // (n_folds + 1)\n",
    "    remainder = dataset_len % (n_folds + 1)\n",
    "\n",
    "    train_start_idx = fold_size * fold + min(fold, remainder)\n",
    "    train_end_idx = fold_size * (fold + 1) + min(fold, remainder)\n",
    "    val_start_idx = train_end_idx\n",
    "    val_end_idx = val_start_idx + fold_size\n",
    "\n",
    "    train_indices = list(range(train_start_idx, train_end_idx))\n",
    "    val_indices = list(range(val_start_idx, val_end_idx))\n",
    "    return train_indices, val_indices\n",
    "    \n",
    "\n",
    "def KFoldXV_MB(hp, train_dataset, verbose = True): \n",
    "    \"\"\"\n",
    "    Performs k-fold cross validation training on a given model:\n",
    "    1. For each fold:\n",
    "        a. Get the indices for the current fold\n",
    "        b. Initialize the train and validation loaders\n",
    "        c. Train the model on the current fold\n",
    "        d. Save the train and validation losses\n",
    "        e. In case it has performed well, save the \"best\" model\n",
    "    2. Calculate the average train and validation losses per epoch\n",
    "    3. Return the best model, and the train and validation losses\n",
    "    \"\"\"\n",
    "    val_losses_kfold = []\n",
    "\n",
    "    for fold in range(hp['k_folds']):\n",
    "        print(f\"\\n\\tFold {fold + 1}/{hp['k_folds']}\") if verbose else None\n",
    "                                        # get indices for the current fold\n",
    "        train_indices, val_indices = get_idx_KFoldXV_sliding_window(\n",
    "            fold, hp['k_folds'], train_dataset.__len__())\n",
    "                                        # create the train and validation loaders,\n",
    "                                        # with random sampling for the train loader\n",
    "        train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], \n",
    "                            sampler = SubsetRandomSampler(train_indices))\n",
    "        val_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], \n",
    "                            sampler = SequentialSampler(val_indices))\n",
    "                                        # Train new model on the current fold\n",
    "        _, _, val_losses, _, _ = train_mb(hp, train_loader, val_loader, verbose)\n",
    "        val_losses_kfold.append(val_losses)\n",
    "\n",
    "    return np.mean([losses[-1] for losses in val_losses_kfold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hyperparameter tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an ordinary grid search through the given hyperparameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dict_by_keys(dict, secondary_dict):\n",
    "    \"\"\"\n",
    "    Filters a dictionary by the keys of another dictionary. For example:\n",
    "    dict = {'a': 1, 'b': 2, 'c': 3} and secondary_dict = {'a': 0, 'c': 0}\n",
    "    returns {'a': 1, 'c': 3}\n",
    "    \"\"\"\n",
    "    return {k: dict[k] for k in secondary_dict if k in dict}\n",
    "\n",
    "\n",
    "def update_dict(dict, secondary_dict):\n",
    "    \"\"\"\n",
    "    Updates a dictionary with the keys of another dictionary. For example:\n",
    "    dict = {'a': 1, 'b': 2, 'c': 3} and secondary_dict = {'a': 0, 'c': 0}\n",
    "    returns {'a': 0, 'b': 2, 'c': 0}\n",
    "    \"\"\"\n",
    "    return {**dict, **secondary_dict}\n",
    "\n",
    "\n",
    "def print_dict_vertically(d):\n",
    "    \"\"\"Prints a dictionary formatted vertically\"\"\"\n",
    "    max_key_len = max(len(key) for key in d.keys())\n",
    "    for key, value in d.items():\n",
    "        print(f\"{key:{max_key_len}}: {value}\")\n",
    "\n",
    "\n",
    "def print_dict_vertically_root(d):\n",
    "    \"\"\"Prints a dictionary formatted vertically\"\"\"\n",
    "    max_key_len = max(len(key) for key in d.keys())\n",
    "    for key, value in d.items():\n",
    "        print(f\"{key:{max_key_len}}: {np.sqrt(value)}\")\n",
    "\n",
    "\n",
    "def ensure_integers(configs):\n",
    "    \"\"\"Ensures the hidden_layers and hidden_units are op type int\"\"\"\n",
    "    for config in configs:\n",
    "        config['hidden_layers'] = int(config['hidden_layers'])\n",
    "        config['hidden_units'] = int(config['hidden_units'])\n",
    "    return configs\n",
    "\n",
    "# https://stackoverflow.com/questions/798854/all-combinations-of-a-list-of-lists\n",
    "def gen_configs(hp_space):\n",
    "    \"\"\"Helper function to initiate the configuration generator\"\"\"\n",
    "    keys = list(hp_space.keys())\n",
    "    values = list(hp_space.values())\n",
    "    configs = [dict(zip(keys, combi))   # Generate all possible combinations, see link^ for source\n",
    "                    for combi in np.array(np.meshgrid(*values)).T.reshape(-1, len(values))]\n",
    "    return ensure_integers(configs)\n",
    "\n",
    "\n",
    "def print_current_config(config):\n",
    "    \"\"\"Prints the current configuration\"\"\"\n",
    "    print(\"CURRENT CONFIGURATION:\")\n",
    "    print_dict_vertically(config)\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_end_of_grid_search(best_hp, best_val_loss):\n",
    "    \"\"\"Prints the results of the grid search\"\"\"\n",
    "    print(f\"##### Best average validation loss so far: {best_val_loss:.6f} #####\")\n",
    "    print(\"With configuration:\")\n",
    "    print_dict_vertically(best_hp)\n",
    "    print()\n",
    "\n",
    "\n",
    "def grid_search(hp, hp_space, train_dataset, verbose = False):\n",
    "    \"\"\"Perform a grid seach through hyperparameter space\"\"\"\n",
    "    best_hp, best_val_loss = {}, np.inf\n",
    "                                        # For each hyperparameter configuration:\n",
    "    for config in gen_configs(hp_space):\n",
    "        config_dict = update_dict(hp, config)\n",
    "        if verbose:\n",
    "            print_current_config(config_dict)\n",
    "                                        # train model with current configuration\n",
    "        current_config_loss = KFoldXV_MB(config_dict, train_dataset, verbose)\n",
    "        if current_config_loss < best_val_loss:\n",
    "            best_hp = config_dict.copy()\n",
    "            best_val_loss = current_config_loss\n",
    "        if verbose:\n",
    "            print(f\"Average final validation loss: {current_config_loss:.6f}\")\n",
    "            print_end_of_grid_search(best_hp, best_val_loss)\n",
    "\n",
    "    if not verbose:                      # print once when not verbose\n",
    "        print_end_of_grid_search(best_hp, best_val_loss)\n",
    "    \n",
    "    return best_hp, best_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters and searchable distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, all (hyper)parameters are defined. The hyperparameters are defined in\n",
    "# a dictionary, which is then passed to the model and the training functions.\n",
    "# The grid search is performed by generating all possible combinations of the\n",
    "# hyperparameters defined in the hp_space dictionary, and then performing k-fold cross\n",
    "# validation on each of these configurations. The best configuration is then returned.\n",
    "# When the search is finished, comment out the hp_space dictionary and save the best found\n",
    "# hyperparameters in the hp dictionary, and train the final model with these.\n",
    "\n",
    "#! TODO voeg nieuwe constructors van Model classes toe aan hp_space en ook de model initialisatie\n",
    "\n",
    "hp = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : HGRU,\n",
    "    'input_units' : train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 64,\n",
    "    'branches' : 4,\n",
    "    'output_units' : train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min', 'factor' : 0.1,\n",
    "                          'patience' : 3, 'cooldown' : 8, 'verbose' : True},\n",
    "    'w_decay' : 1e-7,\n",
    "    'loss_fn' : nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 20,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}                                   # The lr for the branched layer(s) is calculated\n",
    "                                    # based on the \"power ratio\" between the branched\n",
    "                                    # part of the network and the shared layer, which\n",
    "                                    # is *assumed* to be proportional to n_hidden_layers\n",
    "hp['lr_branch'] = hp['lr_shared'] * hp['hidden_layers']\n",
    "\n",
    "# hp_space = {\n",
    "#     'hidden_layers' : [2, 3, 4, 5, 6, 7, 8],\n",
    "#     'hidden_units' : [16, 32, 48, 64, 80, 96, 112, 128],\n",
    "#     'w_decay' : [1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model by performing a grid search and saving the optimally configurated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training now\") if HABROK else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'grid_search_exe_s/exe_of_HGRU_at_{current_time}.txt'\n",
    "# train_dataset_full = ConcatDataset([train_dataset, val_dataset])\n",
    "#                                     If HABROK, print to external file, else print to stdout\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(f\"Grid search execution of HGRU at {current_time}\\n\")\n",
    "#                                     # Train on the full training set\n",
    "#     model, best_hp, val_loss = grid_search(hp, hp_space, train_dataset_full, True)\n",
    "#                                     # Externally save the best model\n",
    "#     tc.save(model.state_dict(), f\"{MODEL_PATH}\\model_HGRU.pth\")\n",
    "\n",
    "#     hp = update_dict(hp, best_hp)   # Update the hp dictionary with the best hyperparameters\n",
    "#     print_dict_vertically(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out model architecture with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing model:\n",
      "HGRU(\n",
      "  (input_layer): GRU(10, 64, batch_first=True)\n",
      "  (shared_layer): GRU(64, 64, batch_first=True)\n",
      "  (branches): ModuleList(\n",
      "    (0-3): 4 x Branch(\n",
      "      (layers): ModuleList(\n",
      "        (0): GRU(64, 16, batch_first=True)\n",
      "        (1): GRU(16, 16, num_layers=3, batch_first=True)\n",
      "        (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    model = HGRU(hp['n_hours_u'],\n",
    "                 hp['n_hours_y'],\n",
    "                 hp['input_units'],\n",
    "                 hp['hidden_layers'],\n",
    "                 hp['hidden_units'], \n",
    "                 hp['branches'],\n",
    "                 hp['output_units'])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on complete training dataset (= train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on full training set...\n",
      "An exception of type <class 'KeyboardInterrupt'> occurred with value \n",
      "Traceback: <traceback object at 0x00000184E38F4C00>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m PrintManager(stdout_location, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, HABROK):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining on full training set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     model_final, train_losses, test_losses, shared_losses, branch_losses \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 9\u001b[0m         \u001b[43mtrain_mb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     tc\u001b[38;5;241m.\u001b[39msave(model_final\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodel_HGRU.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m df_losses \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_train\u001b[39m\u001b[38;5;124m'\u001b[39m: train_losses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL_test\u001b[39m\u001b[38;5;124m'\u001b[39m: test_losses})\n",
      "Cell \u001b[1;32mIn[8], line 161\u001b[0m, in \u001b[0;36mtrain_mb\u001b[1;34m(hp, train_loader, val_loader, verbose)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):   \u001b[38;5;66;03m# For each epoch:\u001b[39;00m\n\u001b[0;32m    158\u001b[0m                                     \u001b[38;5;66;03m# save losses per epoch for both network parts\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     shared_losses\u001b[38;5;241m.\u001b[39mappend(training_epoch_shared_layer(\n\u001b[0;32m    160\u001b[0m         model, main_optimizer, loss_fn, train_loader))\n\u001b[1;32m--> 161\u001b[0m     branch_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtraining_epoch_branches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbranch_optimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    163\u001b[0m                                     \u001b[38;5;66;03m# calculate training and validation loss\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m validation_epoch(model, loss_fn, train_loader)\n",
      "Cell \u001b[1;32mIn[8], line 110\u001b[0m, in \u001b[0;36mtraining_epoch_branches\u001b[1;34m(model, optimizers, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m    107\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m branch_batch_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()       \u001b[38;5;66;03m# Perform the backward pass and update the weights;\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m \u001b[43mbranch_batch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    112\u001b[0m                             \u001b[38;5;66;03m# Finally, unfreeze the shared layer and return.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "                                        # Train the final model on the full training set,\n",
    "                                        # save the final model, and save the losses for plotting\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nTraining on full training set...\")\n",
    "    model_final, train_losses, test_losses, shared_losses, branch_losses = \\\n",
    "        train_mb(hp, train_loader, val_loader, True)\n",
    "    tc.save(model_final.state_dict(), f'{MODEL_PATH}\\model_HGRU.pth')\n",
    "\n",
    "df_losses = pd.DataFrame({'L_train': train_losses, 'L_test': test_losses})\n",
    "df_losses.to_csv(f'{os.path.join(os.getcwd(), \"final_losses\")}\\losses_HGRU_at_{current_time}.csv', \n",
    "                 sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\vwold\\\\Documents\\\\Bachelor\\\\ICML_paper\\\\forecasting_smog_DL\\\\forecasting_smog_DL\\\\src\\\\models\\\\model_MBGRU.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model_final \u001b[38;5;241m=\u001b[39m MBGRU(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_units\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_units\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      2\u001b[0m                      hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbranches\u001b[39m\u001b[38;5;124m'\u001b[39m], hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_units\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m model_final\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel_MBGRU.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_final)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\vwold\\\\Documents\\\\Bachelor\\\\ICML_paper\\\\forecasting_smog_DL\\\\forecasting_smog_DL\\\\src\\\\models\\\\model_MBGRU.pth'"
     ]
    }
   ],
   "source": [
    "model_final = HGRU(hp['input_units'], hp['hidden_layers'], hp['hidden_units'],\n",
    "                     hp['branches'], hp['output_units'])\n",
    "model_final.load_state_dict(tc.load(f\"{MODEL_PATH}\\model_HGRU.pth\"))\n",
    "print(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "test_error = test_hierarchical(model_final, nn.MSELoss(), test_loader)\n",
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_hierarchical(model_final, nn.MSELoss(), train_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), val_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), test_loader))\n",
    "\n",
    "print(\"\\nMSE Training set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Validation set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Test set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRMSE Training set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Validation set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Test set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")\n",
    "np.sqrt(test_hierarchical(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 5\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'NO2', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'O3', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM10', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM25', N_HOURS_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
