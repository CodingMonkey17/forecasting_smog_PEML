{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PEML MLP 2**\n",
    "### Architecture 2 - PINN\n",
    "Multi cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a Optuna hyperparameters optimisation and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n",
    "## **IMPORTANT NOTE**: \n",
    "- do preprocessing from ``preprocess.ipynb`` to obtain data in ``data/data_combined``, before starting this notebook\n",
    "- make sure the notebook is under ``src`` directory before running!\n",
    "- change the global variables defined below for the desired years of data, loss function and NN type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "\n",
    "from modelling.MLP import BasicMLP\n",
    "from modelling import *\n",
    "from modelling.physics import *\n",
    "\n",
    "\n",
    "import optuna\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rachel/forecasting_smog_PEML/src')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/home/rachel/forecasting_smog_PEML/src/config.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MODIFY THESE GLOBAL VARIABLES FOR YOUR MODEL SCENARIO\n",
    "all other variables are defined in config.py\n",
    "\n",
    "LOSS_FUNC: choose from \n",
    "- MSE\n",
    "- LinearShift_MSE\n",
    "- PDE_nmer_const\n",
    "- PDE_nmer_piece\n",
    "- PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this according to the data you want to use\n",
    "YEARS = [2017, 2018, 2020, 2021, 2022, 2023]\n",
    "TRAIN_YEARS = [2017, 2018, 2020, 2021, 2022]\n",
    "VAL_YEARS = [2021, 2022, 2023]\n",
    "TEST_YEARS = [2021, 2022, 2023]\n",
    "\n",
    "LOSS_FUNC = \"PINN\"\n",
    "NN_TYPE = \"MLP\"\n",
    "CITY = 'Multi' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Generation of paths and filenames according to data years, loss func, NN type\n",
    "- will be used throughout the whole notebook\n",
    "- check ``config.py`` for global variables defined outside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years:  allyears\n",
      "idx_dict:  {'NO2_TUINDORP_IDX': 7, 'NO2_BREUKELEN_IDX': 4, 'NO2_OUDEMEER_IDX': 6, 'NO2_ZEGVELD_IDX': 8, 'NO2_KANTERSHOF_IDX': 5, 'WIND_DIR_IDX': 0, 'WIND_SPEED_IDX': 2}\n",
      "station_names:  ['tuindorp', 'breukelen', 'zegveld', 'oudemeer', 'kantershof']\n",
      "main_station:  breukelen\n",
      "RESULTS_PATH:  /home/rachel/forecasting_smog_PEML/src/results/Multi\n",
      "MODEL_PATH:  /home/rachel/forecasting_smog_PEML/src/results/Multi/models\n",
      "MINMAX_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/Multi/all_years/pollutants_minmax_allyears.csv\n",
      "DATASET_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/Multi/all_years\n",
      "Y_PHY_FILENAME:  y_phy_batchsize16_PINN_allyears_Multi\n",
      "MODEL_PATH_NAME:  best_MLP_no2_PINN_allyears_Multi.pth\n",
      "RESULTS_METRICS_FILENAME:  results_MLP_no2_PINN_allyears_Multi.csv\n",
      "BESTPARAMS_FILENAME:  best_params_MLP_no2_PINN_allyears_Multi.txt\n",
      "PLOT_FILENAME:  plot_MLP_no2_PINN_allyears_Multi.png\n"
     ]
    }
   ],
   "source": [
    "years, idx_dict , station_names, main_station, RESULTS_PATH, MODEL_PATH, DATASET_PATH, MINMAX_PATH, Y_PHY_FILENAME,  MODEL_PATH_NAME,RESULTS_METRICS_FILENAME, BESTPARAMS_FILENAME, PLOT_FILENAME  = init_paths(CITY, YEARS, LOSS_FUNC, NN_TYPE)\n",
    "print(\"years: \", years)\n",
    "print(\"idx_dict: \", idx_dict)\n",
    "print(\"station_names: \", station_names)\n",
    "print(\"main_station: \", main_station)\n",
    "print(\"RESULTS_PATH: \", RESULTS_PATH)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "print(\"DATASET_PATH: \", DATASET_PATH)\n",
    "print(\"Y_PHY_FILENAME: \", Y_PHY_FILENAME)\n",
    "print(\"MODEL_PATH_NAME: \", MODEL_PATH_NAME)\n",
    "print(\"RESULTS_METRICS_FILENAME: \", RESULTS_METRICS_FILENAME)\n",
    "print(\"BESTPARAMS_FILENAME: \", BESTPARAMS_FILENAME)\n",
    "print(\"PLOT_FILENAME: \", PLOT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2017_combined_u.csv\n",
      "Imported train_2018_combined_u.csv\n",
      "Imported train_2020_combined_u.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2021_combined_u.csv\n",
      "Imported train_2022_combined_u.csv\n",
      "Warning: train_2023_combined_u.csv does not exist.\n",
      "Imported train_2017_combined_y.csv\n",
      "Imported train_2018_combined_y.csv\n",
      "Imported train_2020_combined_y.csv\n",
      "Imported train_2021_combined_y.csv\n",
      "Imported train_2022_combined_y.csv\n",
      "Warning: train_2023_combined_y.csv does not exist.\n",
      "Warning: val_2017_combined_u.csv does not exist.\n",
      "Warning: val_2018_combined_u.csv does not exist.\n",
      "Warning: val_2020_combined_u.csv does not exist.\n",
      "Imported val_2021_combined_u.csv\n",
      "Imported val_2022_combined_u.csv\n",
      "Imported val_2023_combined_u.csv\n",
      "Warning: val_2017_combined_y.csv does not exist.\n",
      "Warning: val_2018_combined_y.csv does not exist.\n",
      "Warning: val_2020_combined_y.csv does not exist.\n",
      "Imported val_2021_combined_y.csv\n",
      "Imported val_2022_combined_y.csv\n",
      "Imported val_2023_combined_y.csv\n",
      "Warning: test_2017_combined_u.csv does not exist.\n",
      "Warning: test_2018_combined_u.csv does not exist.\n",
      "Warning: test_2020_combined_u.csv does not exist.\n",
      "Imported test_2021_combined_u.csv\n",
      "Imported test_2022_combined_u.csv\n",
      "Imported test_2023_combined_u.csv\n",
      "Warning: test_2017_combined_y.csv does not exist.\n",
      "Warning: test_2018_combined_y.csv does not exist.\n",
      "Warning: test_2020_combined_y.csv does not exist.\n",
      "Imported test_2021_combined_y.csv\n",
      "Imported test_2022_combined_y.csv\n",
      "Imported test_2023_combined_y.csv\n",
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', YEARS, DATASET_PATH)\n",
    "train_output_frames = get_dataframes('train', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', YEARS, DATASET_PATH)\n",
    "val_output_frames = get_dataframes('val', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', YEARS, DATASET_PATH)\n",
    "test_output_frames = get_dataframes('test', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    len(TRAIN_YEARS),                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    len(VAL_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    len(TEST_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmation that the dataset has column indexes the same as those in ``config.py``\n",
    "Indexes are used mainly for the physics calculations, in order to accurately extract the information needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2_TUINDORP_IDX index matches in index: 7\n",
      "NO2_BREUKELEN_IDX index matches in index: 4\n",
      "NO2_OUDEMEER_IDX index matches in index: 6\n",
      "NO2_ZEGVELD_IDX index matches in index: 8\n",
      "NO2_KANTERSHOF_IDX index matches in index: 5\n",
      "WIND_DIR_IDX index matches in index: 0\n",
      "WIND_SPEED_IDX index matches in index: 2\n",
      "All station indexes match.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list(train_dataset.u[0])  # Convert Index to list\n",
    "check_station_indexes(column_names, idx_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning with loss function:  PINN\n",
      "tuning with nn type:  MLP\n"
     ]
    }
   ],
   "source": [
    "print(\"tuning with loss function: \", LOSS_FUNC)\n",
    "print(\"tuning with nn type: \", NN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to search over\n",
    "    set_seed(42)\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 5)\n",
    "    n_hidden_units = trial.suggest_int(\"n_hidden_units\", 32, 256)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-6, 1e-4)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n",
    "    lambda_phy = trial.suggest_loguniform(\"lambda_phy\", 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])  # Match the original hp['batch_sz']\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize MLP model\n",
    "    model = BasicMLP(\n",
    "        N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "        N_HIDDEN_LAYERS=n_hidden_layers,\n",
    "        N_HIDDEN_UNITS=n_hidden_units,\n",
    "        N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "        loss_function=LOSS_FUNC,\n",
    "    )\n",
    "\n",
    "    # Train and return validation loss\n",
    "    val_loss, _ , _, _= model.train_model(train_loader, val_loader, epochs=50, \n",
    "                                    lr=lr, weight_decay=weight_decay, lambda_phy = lambda_phy, device=device, trial = trial, \n",
    "                                    idx_dict= idx_dict, station_names = station_names, main_station = main_station)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\", \n",
    "    study_name=\"mlp_hyperparameter_optimization_PINN_multi\", \n",
    "    storage=\"sqlite:///mlp_hyperparameter_optimization_phy_pde.db\", \n",
    "    load_if_exists=True,\n",
    "    pruner=optuna.pruners.HyperbandPruner(),\n",
    "    )\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Hyperparameters for {NN_TYPE} with {LOSS_FUNC} for {years}:\\n\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the params to a file\n",
    "Parameters saved in ``src/results/best_params/best_params_MLP_no2_PINN_allyears.txt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "best_params_file_path = f\"{RESULTS_PATH}/best_params/{BESTPARAMS_FILENAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters file path:  /home/rachel/forecasting_smog_PEML/src/results/Multi/best_params/best_params_MLP_no2_PINN_allyears_Multi.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters file path: \", best_params_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters saved to /home/rachel/forecasting_smog_PEML/src/results/best_params/best_params_MLP_no2_PINN_allyears.txt\n"
     ]
    }
   ],
   "source": [
    "with open(best_params_file_path, \"w\") as f:\n",
    "    json.dump(best_params, f, indent=4)  # Pretty format for readability\n",
    "\n",
    "print(f\"Best Hyperparameters saved to {best_params_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read params from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Best Parameters: {'n_hidden_layers': 3, 'n_hidden_units': 252, 'lr': 4.500039019535873e-05, 'weight_decay': 4.599161181117004e-07, 'lambda_phy': 0.0007031291303903508, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "with open(best_params_file_path, \"r\") as f:\n",
    "    best_params = json.load(f)  # Automatically converts it to a dictionary\n",
    "\n",
    "print(\"Loaded Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Saving Model\n",
    "Model saved in ``src/results/models/best_MLP_no2_PINN_allyears.pth``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = BasicMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=best_params[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=best_params[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "    loss_function=LOSS_FUNC,\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "_, training_time, train_losses, val_losses = best_model.train_model(train_loader, val_loader, epochs=50, lr=best_params[\"lr\"], \n",
    "                                                                    weight_decay=best_params[\"weight_decay\"], lambda_phy= best_params['lambda_phy'], device=device, \n",
    "                                                                    idx_dict= idx_dict, station_names = station_names, main_station = main_station)\n",
    "print(f\"Training time: {training_time}\")\n",
    "# Save the trained model\n",
    "# torch.save(best_model.state_dict(), f\"{MODEL_PATH}/{MODEL_PATH_NAME}\")\n",
    "# print(f\"Model saved as {MODEL_PATH_NAME} in Model folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Train-Val\n",
    "Plot saved in ``src/results/trainval_plots/trainval_plot_MLP_no2_PINN_allyears.png``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (RMSE)\")\n",
    "plt.title(f\"Training and Validation Loss Over Epochs for {NN_TYPE} with {LOSS_FUNC}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{RESULTS_PATH}/trainval_plots/trainval_{PLOT_FILENAME}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Save Results\n",
    "Results saved in ``src/results/metrics/results_MLP_no2_PINN_allyears.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 44.896909\n",
      "Test RMSE Loss: 6.700516\n",
      "Test SMAPE Loss: 26.399362%\n",
      "Total Inference Time: 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "best_model.load_state_dict(torch.load(f\"{MODEL_PATH}/{MODEL_PATH_NAME}\", map_location=device))\n",
    "best_model.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "mse, rmse, smape, inference_time = best_model.test_model(test_loader, min_value=min_value, max_value=max_value, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Define the CSV file path\n",
    "results_csv_path = f\"{RESULTS_PATH}/metrics/{RESULTS_METRICS_FILENAME}\"\n",
    "\n",
    "# Save metrics in a proper CSV format (header + values in one row)\n",
    "with open(results_csv_path, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow([\"MSE\", \"RMSE\", \"SMAPE\", \"Inference Time\", \"Training Time\"])\n",
    "    \n",
    "    # Write values\n",
    "    writer.writerow([mse, rmse, smape, inference_time, training_time])\n",
    "\n",
    "print(f\"Results saved as {RESULTS_METRICS_FILENAME} in Results/metrics folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Model predictions vs True values\n",
    "Plot saved ``src/results/plots/plot_MLP_no2_PINN_allyears.png``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load min and max values for denormalization\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = torch.tensor(df_minmax[\"min\"].values, dtype=torch.float32)  # shape: (N_OUTPUT_UNITS,)\n",
    "max_value = torch.tensor(df_minmax[\"max\"].values, dtype=torch.float32)  # shape: (N_OUTPUT_UNITS,)\n",
    "\n",
    "# Dynamically detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Ensure model is on the right device and in eval mode\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "# Iterate through the test set and collect predictions & ground truth\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_test, y_true = batch\n",
    "        x_test = x_test.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = best_model(x_test)\n",
    "\n",
    "        # Move to CPU and store\n",
    "        y_preds.append(y_pred.cpu())\n",
    "        y_trues.append(y_true.cpu())\n",
    "\n",
    "# Stack batches\n",
    "y_preds = torch.cat(y_preds, dim=0)  # shape: (batch_size, n_hours_y, n_outputs)\n",
    "y_trues = torch.cat(y_trues, dim=0)\n",
    "\n",
    "# Denormalize\n",
    "min_value = min_value.unsqueeze(0).unsqueeze(0)  # shape: (1, 1, n_outputs)\n",
    "max_value = max_value.unsqueeze(0).unsqueeze(0)\n",
    "y_preds_denorm = y_preds * (max_value - min_value) + min_value\n",
    "y_trues_denorm = y_trues * (max_value - min_value) + min_value\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "y_preds_np = y_preds_denorm.numpy()\n",
    "y_trues_np = y_trues_denorm.numpy()\n",
    "\n",
    "# Plot 1 feature/channel (e.g., station 0)\n",
    "feature_idx = 0\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_trues_np[:, :, feature_idx].flatten(), label=\"Ground Truth (NO₂)\", linestyle=\"-\", color=\"blue\")\n",
    "plt.plot(y_preds_np[:, :, feature_idx].flatten(), label=\"Predictions\", linestyle=\"-\", color=\"black\")\n",
    "\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"NO₂ Level\")\n",
    "plt.title(f\"Predictions vs. Ground Truth (Denormalized) for Utrecht with {NN_TYPE} and {LOSS_FUNC}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# Save the plot\n",
    "plt.savefig(f\"{RESULTS_PATH}/plots/{PLOT_FILENAME}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot 2: First 30 Days (720 Hours) ===\n",
    "time_limit = 30 * 24\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_trues_np[:, :, feature_idx].flatten()[:time_limit], label=\"Ground Truth (NO₂)\", linestyle=\"-\", color=\"blue\")\n",
    "plt.plot(y_preds_np[:, :, feature_idx].flatten()[:time_limit], label=\"Predictions\", linestyle=\"-\", color=\"black\")\n",
    "plt.xlabel(\"Time Step (First 30 Days)\")\n",
    "plt.ylabel(\"NO₂ Level\")\n",
    "plt.title(f\"Predictions vs. Ground Truth (Denormalized) of first 30 days at Breukelen with Multiple Cities input using {NN_TYPE} and {LOSS_FUNC}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{RESULTS_PATH}/plots/plot_{NN_TYPE}_no2_{LOSS_FUNC}_{years}_{CITY}_30days.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
