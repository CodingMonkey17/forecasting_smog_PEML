{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n",
    "For future models, a suggestion is to embed the training/testing functions in a Model class, instead of keeping them loose from each other. (With, optimally, inheritance from a base class, etc etc, such that there is minimal code duplication.) This way, the training procedure can be easily tailored per model. In the current set-up, different functions have to be called for fully-connected networks and hierarchical networks because they handle the data differently. Another way this would be a worth investment, is for implementation of physics-informed models, which require a whole physics injection into the training procedure. In that case, tight coupling is much recommended over the current state of this file. Therefore, I'd first change the code such that it works per model and such that only functionalities independent of model type are actually independent/loosely coupled from the models, therewith facilitating scalable experimentation.\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify potential errors happening on Habrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "- define wd, wind speed and no2 indexes after datasets are defined\n",
    "- make MLP class to parse in a loss function defined in global variable\n",
    "- have the loss function by default is mse \n",
    "- then make the phy loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachel/forecasting_smog_PEML/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "\n",
    "from modelling.MLP import BasicMLP\n",
    "from modelling import *\n",
    "\n",
    "\n",
    "import optuna\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rachel/forecasting_smog_PEML/src')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/rachel/forecasting_smog_PEML\n",
      "MODEL_PATH:  /home/rachel/forecasting_smog_PEML/src/results/models\n",
      "MINMAX_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/pollutants_minmax.csv\n"
     ]
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "\n",
    "BASE_DIR = Path.cwd().parents[0] # set it to the root directory of the project, not src\n",
    "MODEL_PATH = BASE_DIR /\"src\" / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR  / \"data\" / \"data_combined\" / \"pollutants_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 24 * 3               # number of hours to use for input (number of days * 24 hours)\n",
    "N_HOURS_Y = 24                    # number of hours to predict (1 day * 24 hours)\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "\n",
    "# Change this according to the data you want to use\n",
    "YEARS = [2017]\n",
    "TRAIN_YEARS = [2017]\n",
    "VAL_YEARS = [2017]\n",
    "TEST_YEARS = [2017]\n",
    "\n",
    "LOSS_FUNC = \"Physics_MSE\" # choose from \"MSE\" and \"Physics_MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2017_combined_u.csv\n",
      "Imported train_2017_combined_y.csv\n",
      "Imported val_2017_combined_u.csv\n",
      "Imported val_2017_combined_y.csv\n",
      "Imported test_2017_combined_u.csv\n",
      "Imported test_2017_combined_y.csv\n",
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', YEARS)\n",
    "train_output_frames = get_dataframes('train', 'y', YEARS)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', YEARS)\n",
    "val_output_frames = get_dataframes('val', 'y', YEARS)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', YEARS)\n",
    "test_output_frames = get_dataframes('test', 'y', YEARS) \n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    len(TRAIN_YEARS),                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    len(VAL_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    len(TEST_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                           DD   FF        FH        FX       NO2         P  \\\n",
       " DateTime                                                                     \n",
       " 2017-08-01 00:00:00  0.166667  0.1  0.111111  0.000000  0.242115  0.562982   \n",
       " 2017-08-01 01:00:00  0.000000  0.0  0.111111  0.052632  0.223158  0.570694   \n",
       " 2017-08-01 02:00:00  0.000000  0.0  0.000000  0.000000  0.165911  0.560411   \n",
       " 2017-08-01 03:00:00  0.277778  0.1  0.000000  0.000000  0.142363  0.555270   \n",
       " 2017-08-01 04:00:00  0.805556  0.2  0.111111  0.105263  0.156297  0.555270   \n",
       " ...                       ...  ...       ...       ...       ...       ...   \n",
       " 2017-11-16 19:00:00  0.750000  0.2  0.333333  0.210526  0.523871  0.789203   \n",
       " 2017-11-16 20:00:00  0.972222  0.3  0.333333  0.421053  0.512314  0.814910   \n",
       " 2017-11-16 21:00:00  0.888889  0.1  0.222222  0.263158  0.232880  0.827763   \n",
       " 2017-11-16 22:00:00  0.944444  0.2  0.111111  0.105263  0.108123  0.832905   \n",
       " 2017-11-16 23:00:00  0.861111  0.1  0.222222  0.105263  0.205120  0.845758   \n",
       " \n",
       "                       SQ         T        TD  \n",
       " DateTime                                      \n",
       " 2017-08-01 00:00:00  0.0  0.536667  0.726852  \n",
       " 2017-08-01 01:00:00  0.0  0.546667  0.740741  \n",
       " 2017-08-01 02:00:00  0.0  0.506667  0.689815  \n",
       " 2017-08-01 03:00:00  0.0  0.463333  0.634259  \n",
       " 2017-08-01 04:00:00  0.0  0.493333  0.662037  \n",
       " ...                  ...       ...       ...  \n",
       " 2017-11-16 19:00:00  0.0  0.390000  0.513889  \n",
       " 2017-11-16 20:00:00  0.0  0.353333  0.462963  \n",
       " 2017-11-16 21:00:00  0.0  0.330000  0.435185  \n",
       " 2017-11-16 22:00:00  0.0  0.306667  0.407407  \n",
       " 2017-11-16 23:00:00  0.0  0.250000  0.319444  \n",
       " \n",
       " [2592 rows x 9 columns]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                          NO2\n",
       " DateTime                     \n",
       " 2017-08-01 00:00:00  0.223698\n",
       " 2017-08-01 01:00:00  0.145496\n",
       " 2017-08-01 02:00:00  0.275978\n",
       " 2017-08-01 03:00:00  0.423742\n",
       " 2017-08-01 04:00:00  0.478721\n",
       " ...                       ...\n",
       " 2017-11-16 19:00:00  0.606502\n",
       " 2017-11-16 20:00:00  0.456470\n",
       " 2017-11-16 21:00:00  0.483258\n",
       " 2017-11-16 22:00:00  0.468784\n",
       " 2017-11-16 23:00:00  0.473428\n",
       " \n",
       " [2592 rows x 1 columns]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.pairs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1667, 0.1000, 0.1111, 0.0000, 0.2421, 0.5630, 0.0000, 0.5367, 0.7269],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.2232, 0.5707, 0.0000, 0.5467, 0.7407],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1659, 0.5604, 0.0000, 0.5067, 0.6898],\n",
       "        [0.2778, 0.1000, 0.0000, 0.0000, 0.1424, 0.5553, 0.0000, 0.4633, 0.6343],\n",
       "        [0.8056, 0.2000, 0.1111, 0.1053, 0.1563, 0.5553, 0.0000, 0.4933, 0.6620],\n",
       "        [0.0000, 0.0000, 0.1111, 0.1053, 0.3135, 0.5681, 0.3000, 0.6200, 0.7593],\n",
       "        [0.7222, 0.1000, 0.1111, 0.0526, 0.5326, 0.5913, 0.0000, 0.6433, 0.7269],\n",
       "        [0.7500, 0.1000, 0.1111, 0.1053, 0.5367, 0.5938, 0.0000, 0.6500, 0.7037],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1053, 0.5172, 0.5964, 0.0000, 0.6733, 0.6574],\n",
       "        [0.7500, 0.2000, 0.2222, 0.2105, 0.4459, 0.5990, 0.3000, 0.7133, 0.6157],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3129, 0.6041, 0.0000, 0.7167, 0.6019],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.3478, 0.6067, 0.0000, 0.7133, 0.5926],\n",
       "        [0.6528, 0.1000, 0.2222, 0.1053, 0.3649, 0.6041, 0.2000, 0.7800, 0.6435],\n",
       "        [0.6944, 0.2000, 0.2222, 0.1053, 0.3019, 0.6015, 0.2000, 0.7867, 0.4861],\n",
       "        [0.7222, 0.2000, 0.2222, 0.2105, 0.2268, 0.5990, 0.4000, 0.7733, 0.5046],\n",
       "        [0.6111, 0.2000, 0.2222, 0.1579, 0.2246, 0.6015, 0.1000, 0.7633, 0.5972],\n",
       "        [0.6944, 0.2000, 0.2222, 0.2105, 0.2855, 0.6041, 0.7000, 0.7700, 0.5880],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.2469, 0.6015, 0.2000, 0.7267, 0.6435],\n",
       "        [0.7222, 0.2000, 0.2222, 0.1579, 0.2171, 0.6144, 0.2000, 0.6967, 0.6944],\n",
       "        [0.6944, 0.1000, 0.1111, 0.1053, 0.2834, 0.6298, 0.0000, 0.5933, 0.7130],\n",
       "        [0.4167, 0.1000, 0.1111, 0.0526, 0.3918, 0.6298, 0.0000, 0.5267, 0.6944],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0000, 0.4752, 0.6375, 0.0000, 0.5200, 0.6991],\n",
       "        [0.4722, 0.1000, 0.1111, 0.0526, 0.5745, 0.6298, 0.0000, 0.5067, 0.6852],\n",
       "        [0.5000, 0.1000, 0.1111, 0.1053, 0.5891, 0.6247, 0.0000, 0.5033, 0.6713],\n",
       "        [0.0000, 0.0000, 0.1111, 0.0526, 0.5491, 0.6298, 0.0000, 0.5000, 0.6806],\n",
       "        [0.4444, 0.1000, 0.1111, 0.0526, 0.5092, 0.6221, 0.0000, 0.5300, 0.6944],\n",
       "        [0.4167, 0.2000, 0.1111, 0.0526, 0.3212, 0.6144, 0.0000, 0.5267, 0.6898],\n",
       "        [0.3889, 0.1000, 0.2222, 0.1053, 0.2835, 0.6118, 0.0000, 0.5467, 0.6898],\n",
       "        [0.5000, 0.1000, 0.1111, 0.0526, 0.4099, 0.6144, 0.0000, 0.5533, 0.7083],\n",
       "        [0.5833, 0.2000, 0.2222, 0.1579, 0.4797, 0.6272, 0.0000, 0.6067, 0.7083],\n",
       "        [0.6389, 0.2000, 0.2222, 0.2105, 0.5086, 0.6272, 0.0000, 0.6333, 0.6991],\n",
       "        [0.6667, 0.4000, 0.3333, 0.2632, 0.4155, 0.6375, 0.0000, 0.6567, 0.6574],\n",
       "        [0.6667, 0.4000, 0.4444, 0.3158, 0.3375, 0.6478, 0.1000, 0.6800, 0.6574],\n",
       "        [0.6389, 0.3000, 0.3333, 0.3684, 0.2610, 0.6478, 0.1000, 0.7067, 0.6713],\n",
       "        [0.5556, 0.4000, 0.4444, 0.2632, 0.2420, 0.6555, 0.3000, 0.7300, 0.6296],\n",
       "        [0.5833, 0.4000, 0.4444, 0.3158, 0.2146, 0.6427, 0.2000, 0.7433, 0.5741],\n",
       "        [0.5556, 0.3000, 0.3333, 0.2632, 0.1782, 0.6221, 0.1000, 0.7600, 0.5370],\n",
       "        [0.5000, 0.4000, 0.4444, 0.2632, 0.1985, 0.6195, 0.0000, 0.7533, 0.5556],\n",
       "        [0.5000, 0.5000, 0.4444, 0.3684, 0.2416, 0.5964, 0.0000, 0.7600, 0.6204],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4737, 0.2883, 0.5938, 0.0000, 0.7267, 0.5833],\n",
       "        [0.5833, 0.4000, 0.7778, 0.5789, 0.2718, 0.6093, 0.0000, 0.5933, 0.6806],\n",
       "        [0.5556, 0.2000, 0.3333, 0.3158, 0.1936, 0.5964, 0.0000, 0.5600, 0.7083],\n",
       "        [0.4861, 0.1000, 0.1111, 0.1579, 0.2238, 0.5835, 0.0000, 0.5533, 0.7361],\n",
       "        [0.4167, 0.2000, 0.2222, 0.1579, 0.2430, 0.5656, 0.0000, 0.5533, 0.7269],\n",
       "        [0.3889, 0.2000, 0.2222, 0.1579, 0.3154, 0.5681, 0.0000, 0.5567, 0.7315],\n",
       "        [0.4444, 0.3000, 0.2222, 0.1579, 0.2860, 0.5553, 0.0000, 0.5800, 0.7639],\n",
       "        [0.5000, 0.3000, 0.3333, 0.2632, 0.2077, 0.5373, 0.0000, 0.5967, 0.7778],\n",
       "        [0.5000, 0.2000, 0.2222, 0.2105, 0.1640, 0.5167, 0.0000, 0.6000, 0.7778],\n",
       "        [0.4444, 0.3000, 0.3333, 0.2632, 0.1525, 0.4961, 0.0000, 0.5967, 0.7824],\n",
       "        [0.4722, 0.4000, 0.3333, 0.3158, 0.1328, 0.4730, 0.0000, 0.6000, 0.7824],\n",
       "        [0.5000, 0.4000, 0.4444, 0.3158, 0.1252, 0.4422, 0.0000, 0.6100, 0.7639],\n",
       "        [0.4722, 0.2000, 0.3333, 0.2632, 0.1161, 0.4293, 0.0000, 0.6000, 0.7639],\n",
       "        [0.4722, 0.2000, 0.2222, 0.1579, 0.1766, 0.4165, 0.0000, 0.5967, 0.7731],\n",
       "        [0.5556, 0.4000, 0.3333, 0.3158, 0.2840, 0.4139, 0.0000, 0.6533, 0.8056],\n",
       "        [0.5833, 0.5000, 0.4444, 0.4211, 0.3435, 0.4010, 0.5000, 0.6967, 0.8009],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.3057, 0.3985, 0.0000, 0.6867, 0.8194],\n",
       "        [0.5833, 0.6000, 0.5556, 0.4737, 0.2615, 0.4036, 0.0000, 0.6633, 0.8519],\n",
       "        [0.6389, 0.5000, 0.6667, 0.5789, 0.2453, 0.4190, 0.0000, 0.6133, 0.7454],\n",
       "        [0.6389, 0.7000, 0.7778, 0.6316, 0.1434, 0.4216, 0.6000, 0.7067, 0.6806],\n",
       "        [0.6667, 0.9000, 0.8889, 0.7368, 0.1046, 0.4267, 0.7000, 0.7467, 0.6435],\n",
       "        [0.6667, 0.8000, 0.7778, 0.7368, 0.0607, 0.4319, 0.8000, 0.7667, 0.6019],\n",
       "        [0.6667, 0.9000, 1.0000, 0.8947, 0.0700, 0.4319, 1.0000, 0.7867, 0.6065],\n",
       "        [0.6944, 0.8000, 0.8889, 0.7895, 0.0594, 0.4422, 0.4000, 0.7400, 0.6389],\n",
       "        [0.6667, 0.8000, 0.7778, 0.6842, 0.0823, 0.4370, 0.5000, 0.7633, 0.6389],\n",
       "        [0.6667, 0.7000, 0.8889, 0.7368, 0.1016, 0.4473, 1.0000, 0.7467, 0.6944],\n",
       "        [0.6389, 0.9000, 0.8889, 0.8421, 0.1005, 0.4550, 0.5000, 0.7067, 0.5278],\n",
       "        [0.6667, 0.7000, 1.0000, 0.8947, 0.0701, 0.4627, 0.3000, 0.6900, 0.5602],\n",
       "        [0.6389, 1.0000, 0.8889, 0.9474, 0.0572, 0.4653, 0.0000, 0.6800, 0.5648],\n",
       "        [0.6667, 0.9000, 0.8889, 0.8421, 0.0533, 0.4627, 0.0000, 0.6567, 0.5741],\n",
       "        [0.6667, 0.7000, 0.7778, 0.7368, 0.0475, 0.4627, 0.0000, 0.6300, 0.6019],\n",
       "        [0.6389, 0.4000, 0.5556, 0.5789, 0.0376, 0.4576, 0.0000, 0.6100, 0.6204],\n",
       "        [0.6111, 0.5000, 0.4444, 0.4211, 0.0373, 0.4499, 0.0000, 0.5933, 0.6296]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1965],\n",
       "        [0.1501],\n",
       "        [0.1518],\n",
       "        [0.2622],\n",
       "        [0.5524],\n",
       "        [0.4840],\n",
       "        [0.3544],\n",
       "        [0.2754],\n",
       "        [0.1948],\n",
       "        [0.1734],\n",
       "        [0.1505],\n",
       "        [0.1352],\n",
       "        [0.0778],\n",
       "        [0.1184],\n",
       "        [0.1293],\n",
       "        [0.1238],\n",
       "        [0.1043],\n",
       "        [0.0997],\n",
       "        [0.0812],\n",
       "        [0.0823],\n",
       "        [0.1155],\n",
       "        [0.0837],\n",
       "        [0.0570],\n",
       "        [0.1006]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.pairs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2 index:  4\n",
      "DD index (wind direction):  0\n",
      "FH index (Hourly wind speed):  2\n"
     ]
    }
   ],
   "source": [
    "# Assuming train_dataset.u[0] is a pandas Index object with column names\n",
    "column_names = list(train_dataset.u[0])  # Convert Index to list\n",
    "\n",
    "# Now, find the indices of the columns 'NO2', 'DD', 'FH'\n",
    "no2_idx = column_names.index('NO2')\n",
    "dd_idx = column_names.index('DD')\n",
    "fh_idx = column_names.index('FH')\n",
    "\n",
    "print(\"NO2 index: \", no2_idx)\n",
    "print(\"DD index (wind direction): \", dd_idx)\n",
    "print(\"FH index (Hourly wind speed): \", fh_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.242115\n",
       "2017-08-01 01:00:00    0.223158\n",
       "2017-08-01 02:00:00    0.165911\n",
       "2017-08-01 03:00:00    0.142363\n",
       "2017-08-01 04:00:00    0.156297\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.523871\n",
       "2017-11-16 20:00:00    0.512314\n",
       "2017-11-16 21:00:00    0.232880\n",
       "2017-11-16 22:00:00    0.108123\n",
       "2017-11-16 23:00:00    0.205120\n",
       "Name: NO2, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,no2_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.166667\n",
       "2017-08-01 01:00:00    0.000000\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.277778\n",
       "2017-08-01 04:00:00    0.805556\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.750000\n",
       "2017-11-16 20:00:00    0.972222\n",
       "2017-11-16 21:00:00    0.888889\n",
       "2017-11-16 22:00:00    0.944444\n",
       "2017-11-16 23:00:00    0.861111\n",
       "Name: DD, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,dd_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime\n",
       "2017-08-01 00:00:00    0.111111\n",
       "2017-08-01 01:00:00    0.111111\n",
       "2017-08-01 02:00:00    0.000000\n",
       "2017-08-01 03:00:00    0.000000\n",
       "2017-08-01 04:00:00    0.111111\n",
       "                         ...   \n",
       "2017-11-16 19:00:00    0.333333\n",
       "2017-11-16 20:00:00    0.333333\n",
       "2017-11-16 21:00:00    0.222222\n",
       "2017-11-16 22:00:00    0.111111\n",
       "2017-11-16 23:00:00    0.222222\n",
       "Name: FH, Length: 2592, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.u[0].iloc[:,fh_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start hyperparameter searching with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:13:01,461] A new study created in RDB with name: mlp_hyperparameter_optimization_phy12\n",
      "/tmp/ipykernel_10196/3654659993.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
      "/tmp/ipykernel_10196/3654659993.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.124608 - Val Loss: 0.220441\n",
      "Epoch 2/50 - Train Loss: 0.103983 - Val Loss: 0.193476\n",
      "Epoch 3/50 - Train Loss: 0.082671 - Val Loss: 0.170309\n",
      "Epoch 4/50 - Train Loss: 0.074284 - Val Loss: 0.150742\n",
      "Epoch 5/50 - Train Loss: 0.064969 - Val Loss: 0.134066\n",
      "Epoch 6/50 - Train Loss: 0.056640 - Val Loss: 0.120293\n",
      "Epoch 7/50 - Train Loss: 0.053409 - Val Loss: 0.108908\n",
      "Epoch 8/50 - Train Loss: 0.050627 - Val Loss: 0.100186\n",
      "Epoch 9/50 - Train Loss: 0.046628 - Val Loss: 0.092645\n",
      "Epoch 10/50 - Train Loss: 0.040817 - Val Loss: 0.087346\n",
      "Epoch 11/50 - Train Loss: 0.041083 - Val Loss: 0.083334\n",
      "Epoch 12/50 - Train Loss: 0.042433 - Val Loss: 0.080606\n",
      "Epoch 13/50 - Train Loss: 0.040406 - Val Loss: 0.077999\n",
      "Epoch 14/50 - Train Loss: 0.037505 - Val Loss: 0.076397\n",
      "Epoch 15/50 - Train Loss: 0.042029 - Val Loss: 0.075398\n",
      "Epoch 16/50 - Train Loss: 0.041021 - Val Loss: 0.074423\n",
      "Epoch 17/50 - Train Loss: 0.042700 - Val Loss: 0.073493\n",
      "Epoch 18/50 - Train Loss: 0.039964 - Val Loss: 0.072723\n",
      "Epoch 19/50 - Train Loss: 0.045038 - Val Loss: 0.072334\n",
      "Epoch 20/50 - Train Loss: 0.038353 - Val Loss: 0.071182\n",
      "Epoch 21/50 - Train Loss: 0.035243 - Val Loss: 0.070181\n",
      "Epoch 22/50 - Train Loss: 0.037009 - Val Loss: 0.069492\n",
      "Epoch 23/50 - Train Loss: 0.036568 - Val Loss: 0.069051\n",
      "Epoch 24/50 - Train Loss: 0.037004 - Val Loss: 0.068388\n",
      "Epoch 25/50 - Train Loss: 0.037013 - Val Loss: 0.068008\n",
      "Epoch 26/50 - Train Loss: 0.035962 - Val Loss: 0.067738\n",
      "Epoch 27/50 - Train Loss: 0.036373 - Val Loss: 0.067318\n",
      "Epoch 28/50 - Train Loss: 0.032452 - Val Loss: 0.066963\n",
      "Epoch 29/50 - Train Loss: 0.034794 - Val Loss: 0.066092\n",
      "Epoch 30/50 - Train Loss: 0.035181 - Val Loss: 0.065428\n",
      "Epoch 31/50 - Train Loss: 0.035381 - Val Loss: 0.064601\n",
      "Epoch 32/50 - Train Loss: 0.041049 - Val Loss: 0.064149\n",
      "Epoch 33/50 - Train Loss: 0.036788 - Val Loss: 0.063704\n",
      "Epoch 34/50 - Train Loss: 0.037928 - Val Loss: 0.063665\n",
      "Epoch 35/50 - Train Loss: 0.032497 - Val Loss: 0.063132\n",
      "Epoch 36/50 - Train Loss: 0.036171 - Val Loss: 0.062601\n",
      "Epoch 37/50 - Train Loss: 0.037622 - Val Loss: 0.062037\n",
      "Epoch 38/50 - Train Loss: 0.035499 - Val Loss: 0.061374\n",
      "Epoch 39/50 - Train Loss: 0.033682 - Val Loss: 0.060852\n",
      "Epoch 40/50 - Train Loss: 0.031582 - Val Loss: 0.060863\n",
      "Epoch 41/50 - Train Loss: 0.034505 - Val Loss: 0.060820\n",
      "Epoch 42/50 - Train Loss: 0.035957 - Val Loss: 0.060386\n",
      "Epoch 43/50 - Train Loss: 0.032773 - Val Loss: 0.059941\n",
      "Epoch 44/50 - Train Loss: 0.033605 - Val Loss: 0.059451\n",
      "Epoch 45/50 - Train Loss: 0.034006 - Val Loss: 0.059108\n",
      "Epoch 46/50 - Train Loss: 0.033150 - Val Loss: 0.058443\n",
      "Epoch 47/50 - Train Loss: 0.033276 - Val Loss: 0.058402\n",
      "Epoch 48/50 - Train Loss: 0.035103 - Val Loss: 0.058121\n",
      "Epoch 49/50 - Train Loss: 0.033843 - Val Loss: 0.057890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:13:16,975] Trial 0 finished with value: 0.057217858731746674 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 195, 'lr': 3.0462883081823548e-05, 'weight_decay': 1.1371901009505448e-06, 'batch_size': 16}. Best is trial 0 with value: 0.057217858731746674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031409 - Val Loss: 0.057218\n",
      "Epoch 1/50 - Train Loss: 0.205347 - Val Loss: 0.310594\n",
      "Epoch 2/50 - Train Loss: 0.152031 - Val Loss: 0.201963\n",
      "Epoch 3/50 - Train Loss: 0.069586 - Val Loss: 0.099181\n",
      "Epoch 4/50 - Train Loss: 0.053875 - Val Loss: 0.073723\n",
      "Epoch 5/50 - Train Loss: 0.075701 - Val Loss: 0.077920\n",
      "Epoch 6/50 - Train Loss: 0.045937 - Val Loss: 0.103797\n",
      "Epoch 7/50 - Train Loss: 0.044478 - Val Loss: 0.123705\n",
      "Epoch 8/50 - Train Loss: 0.057484 - Val Loss: 0.123431\n",
      "Epoch 9/50 - Train Loss: 0.056736 - Val Loss: 0.106982\n",
      "Epoch 10/50 - Train Loss: 0.050186 - Val Loss: 0.084066\n",
      "Epoch 11/50 - Train Loss: 0.038696 - Val Loss: 0.067850\n",
      "Epoch 12/50 - Train Loss: 0.040645 - Val Loss: 0.062933\n",
      "Epoch 13/50 - Train Loss: 0.040499 - Val Loss: 0.064320\n",
      "Epoch 14/50 - Train Loss: 0.044903 - Val Loss: 0.072469\n",
      "Epoch 15/50 - Train Loss: 0.035661 - Val Loss: 0.079524\n",
      "Epoch 16/50 - Train Loss: 0.037127 - Val Loss: 0.078963\n",
      "Epoch 17/50 - Train Loss: 0.044112 - Val Loss: 0.069203\n",
      "Epoch 18/50 - Train Loss: 0.040768 - Val Loss: 0.057014\n",
      "Epoch 19/50 - Train Loss: 0.041650 - Val Loss: 0.052003\n",
      "Epoch 20/50 - Train Loss: 0.034505 - Val Loss: 0.052780\n",
      "Epoch 21/50 - Train Loss: 0.032778 - Val Loss: 0.056690\n",
      "Epoch 22/50 - Train Loss: 0.031722 - Val Loss: 0.057108\n",
      "Epoch 23/50 - Train Loss: 0.038069 - Val Loss: 0.053341\n",
      "Epoch 24/50 - Train Loss: 0.039200 - Val Loss: 0.049156\n",
      "Epoch 25/50 - Train Loss: 0.038395 - Val Loss: 0.047222\n",
      "Epoch 26/50 - Train Loss: 0.030237 - Val Loss: 0.048060\n",
      "Epoch 27/50 - Train Loss: 0.029805 - Val Loss: 0.049023\n",
      "Epoch 28/50 - Train Loss: 0.030426 - Val Loss: 0.047308\n",
      "Epoch 29/50 - Train Loss: 0.036387 - Val Loss: 0.045971\n",
      "Epoch 30/50 - Train Loss: 0.029837 - Val Loss: 0.045740\n",
      "Epoch 31/50 - Train Loss: 0.036289 - Val Loss: 0.046245\n",
      "Epoch 32/50 - Train Loss: 0.037186 - Val Loss: 0.046992\n",
      "Epoch 33/50 - Train Loss: 0.028986 - Val Loss: 0.045113\n",
      "Epoch 34/50 - Train Loss: 0.029013 - Val Loss: 0.043893\n",
      "Epoch 35/50 - Train Loss: 0.036677 - Val Loss: 0.045349\n",
      "Epoch 36/50 - Train Loss: 0.037092 - Val Loss: 0.047484\n",
      "Epoch 37/50 - Train Loss: 0.028859 - Val Loss: 0.044852\n",
      "Epoch 38/50 - Train Loss: 0.035434 - Val Loss: 0.044500\n",
      "Epoch 39/50 - Train Loss: 0.034608 - Val Loss: 0.047166\n",
      "Epoch 40/50 - Train Loss: 0.028376 - Val Loss: 0.048637\n",
      "Epoch 41/50 - Train Loss: 0.028409 - Val Loss: 0.045092\n",
      "Epoch 42/50 - Train Loss: 0.035252 - Val Loss: 0.044376\n",
      "Epoch 43/50 - Train Loss: 0.027722 - Val Loss: 0.046996\n",
      "Epoch 44/50 - Train Loss: 0.034820 - Val Loss: 0.047191\n",
      "Epoch 45/50 - Train Loss: 0.034717 - Val Loss: 0.045260\n",
      "Epoch 46/50 - Train Loss: 0.027686 - Val Loss: 0.045292\n",
      "Epoch 47/50 - Train Loss: 0.027748 - Val Loss: 0.047322\n",
      "Epoch 48/50 - Train Loss: 0.028382 - Val Loss: 0.045746\n",
      "Epoch 49/50 - Train Loss: 0.034358 - Val Loss: 0.044704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:13:34,697] Trial 1 finished with value: 0.04389261454343796 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 249, 'lr': 0.0007185007887608011, 'weight_decay': 1.3246580882625853e-06, 'batch_size': 64}. Best is trial 1 with value: 0.04389261454343796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.034782 - Val Loss: 0.045859\n",
      "Epoch 1/50 - Train Loss: 0.121124 - Val Loss: 0.164159\n",
      "Epoch 2/50 - Train Loss: 0.069913 - Val Loss: 0.107111\n",
      "Epoch 3/50 - Train Loss: 0.049967 - Val Loss: 0.066332\n",
      "Epoch 4/50 - Train Loss: 0.041209 - Val Loss: 0.090117\n",
      "Epoch 5/50 - Train Loss: 0.041159 - Val Loss: 0.059292\n",
      "Epoch 6/50 - Train Loss: 0.041062 - Val Loss: 0.059259\n",
      "Epoch 7/50 - Train Loss: 0.034304 - Val Loss: 0.058684\n",
      "Epoch 8/50 - Train Loss: 0.033034 - Val Loss: 0.046601\n",
      "Epoch 9/50 - Train Loss: 0.033486 - Val Loss: 0.054049\n",
      "Epoch 10/50 - Train Loss: 0.034306 - Val Loss: 0.044286\n",
      "Epoch 11/50 - Train Loss: 0.030784 - Val Loss: 0.054680\n",
      "Epoch 12/50 - Train Loss: 0.032192 - Val Loss: 0.043858\n",
      "Epoch 13/50 - Train Loss: 0.034559 - Val Loss: 0.051083\n",
      "Epoch 14/50 - Train Loss: 0.035294 - Val Loss: 0.044842\n",
      "Epoch 15/50 - Train Loss: 0.032686 - Val Loss: 0.044431\n",
      "Epoch 16/50 - Train Loss: 0.033239 - Val Loss: 0.047208\n",
      "Epoch 17/50 - Train Loss: 0.032792 - Val Loss: 0.044106\n",
      "Epoch 18/50 - Train Loss: 0.029121 - Val Loss: 0.052438\n",
      "Epoch 19/50 - Train Loss: 0.029195 - Val Loss: 0.046961\n",
      "Epoch 20/50 - Train Loss: 0.036127 - Val Loss: 0.049671\n",
      "Epoch 21/50 - Train Loss: 0.031447 - Val Loss: 0.043729\n",
      "Epoch 22/50 - Train Loss: 0.028690 - Val Loss: 0.050866\n",
      "Epoch 23/50 - Train Loss: 0.028936 - Val Loss: 0.043811\n",
      "Epoch 24/50 - Train Loss: 0.031116 - Val Loss: 0.047166\n",
      "Epoch 25/50 - Train Loss: 0.033669 - Val Loss: 0.044550\n",
      "Epoch 26/50 - Train Loss: 0.030362 - Val Loss: 0.046215\n",
      "Epoch 27/50 - Train Loss: 0.029467 - Val Loss: 0.045435\n",
      "Epoch 28/50 - Train Loss: 0.036059 - Val Loss: 0.045474\n",
      "Epoch 29/50 - Train Loss: 0.029441 - Val Loss: 0.047031\n",
      "Epoch 30/50 - Train Loss: 0.027784 - Val Loss: 0.044380\n",
      "Epoch 31/50 - Train Loss: 0.031971 - Val Loss: 0.046221\n",
      "Epoch 32/50 - Train Loss: 0.031047 - Val Loss: 0.044976\n",
      "Epoch 33/50 - Train Loss: 0.027470 - Val Loss: 0.046936\n",
      "Epoch 34/50 - Train Loss: 0.030720 - Val Loss: 0.045500\n",
      "Epoch 35/50 - Train Loss: 0.026864 - Val Loss: 0.044034\n",
      "Epoch 36/50 - Train Loss: 0.030350 - Val Loss: 0.049088\n",
      "Epoch 37/50 - Train Loss: 0.031232 - Val Loss: 0.045580\n",
      "Epoch 38/50 - Train Loss: 0.034273 - Val Loss: 0.044940\n",
      "Epoch 39/50 - Train Loss: 0.032694 - Val Loss: 0.044962\n",
      "Epoch 40/50 - Train Loss: 0.029188 - Val Loss: 0.046165\n",
      "Epoch 41/50 - Train Loss: 0.027052 - Val Loss: 0.045308\n",
      "Epoch 42/50 - Train Loss: 0.027113 - Val Loss: 0.044175\n",
      "Epoch 43/50 - Train Loss: 0.032397 - Val Loss: 0.048468\n",
      "Epoch 44/50 - Train Loss: 0.031044 - Val Loss: 0.045048\n",
      "Epoch 45/50 - Train Loss: 0.027213 - Val Loss: 0.046497\n",
      "Epoch 46/50 - Train Loss: 0.032222 - Val Loss: 0.045870\n",
      "Epoch 47/50 - Train Loss: 0.026584 - Val Loss: 0.046561\n",
      "Epoch 48/50 - Train Loss: 0.025947 - Val Loss: 0.046326\n",
      "Epoch 49/50 - Train Loss: 0.031866 - Val Loss: 0.045013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:13:55,257] Trial 2 finished with value: 0.043729208409786224 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 202, 'lr': 0.004085478849534725, 'weight_decay': 6.799311720673112e-08, 'batch_size': 32}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026247 - Val Loss: 0.059374\n",
      "Epoch 1/50 - Train Loss: 0.116920 - Val Loss: 0.231880\n",
      "Epoch 2/50 - Train Loss: 0.108921 - Val Loss: 0.225285\n",
      "Epoch 3/50 - Train Loss: 0.107158 - Val Loss: 0.218885\n",
      "Epoch 4/50 - Train Loss: 0.101906 - Val Loss: 0.212680\n",
      "Epoch 5/50 - Train Loss: 0.098821 - Val Loss: 0.206629\n",
      "Epoch 6/50 - Train Loss: 0.094171 - Val Loss: 0.200914\n",
      "Epoch 7/50 - Train Loss: 0.092588 - Val Loss: 0.195266\n",
      "Epoch 8/50 - Train Loss: 0.086343 - Val Loss: 0.189772\n",
      "Epoch 9/50 - Train Loss: 0.085898 - Val Loss: 0.184578\n",
      "Epoch 10/50 - Train Loss: 0.083681 - Val Loss: 0.179352\n",
      "Epoch 11/50 - Train Loss: 0.079347 - Val Loss: 0.174198\n",
      "Epoch 12/50 - Train Loss: 0.078006 - Val Loss: 0.169055\n",
      "Epoch 13/50 - Train Loss: 0.073409 - Val Loss: 0.164141\n",
      "Epoch 14/50 - Train Loss: 0.068165 - Val Loss: 0.159640\n",
      "Epoch 15/50 - Train Loss: 0.064209 - Val Loss: 0.155459\n",
      "Epoch 16/50 - Train Loss: 0.069662 - Val Loss: 0.151191\n",
      "Epoch 17/50 - Train Loss: 0.064188 - Val Loss: 0.147056\n",
      "Epoch 18/50 - Train Loss: 0.062879 - Val Loss: 0.143207\n",
      "Epoch 19/50 - Train Loss: 0.059509 - Val Loss: 0.139522\n",
      "Epoch 20/50 - Train Loss: 0.060420 - Val Loss: 0.135976\n",
      "Epoch 21/50 - Train Loss: 0.057086 - Val Loss: 0.132485\n",
      "Epoch 22/50 - Train Loss: 0.056077 - Val Loss: 0.129276\n",
      "Epoch 23/50 - Train Loss: 0.055315 - Val Loss: 0.126498\n",
      "Epoch 24/50 - Train Loss: 0.056269 - Val Loss: 0.123443\n",
      "Epoch 25/50 - Train Loss: 0.053409 - Val Loss: 0.120734\n",
      "Epoch 26/50 - Train Loss: 0.051892 - Val Loss: 0.118156\n",
      "Epoch 27/50 - Train Loss: 0.052262 - Val Loss: 0.115796\n",
      "Epoch 28/50 - Train Loss: 0.050718 - Val Loss: 0.113403\n",
      "Epoch 29/50 - Train Loss: 0.050750 - Val Loss: 0.111340\n",
      "Epoch 30/50 - Train Loss: 0.050214 - Val Loss: 0.109457\n",
      "Epoch 31/50 - Train Loss: 0.049481 - Val Loss: 0.107765\n",
      "Epoch 32/50 - Train Loss: 0.048280 - Val Loss: 0.105960\n",
      "Epoch 33/50 - Train Loss: 0.046734 - Val Loss: 0.104517\n",
      "Epoch 34/50 - Train Loss: 0.044707 - Val Loss: 0.103148\n",
      "Epoch 35/50 - Train Loss: 0.045831 - Val Loss: 0.101823\n",
      "Epoch 36/50 - Train Loss: 0.045882 - Val Loss: 0.100727\n",
      "Epoch 37/50 - Train Loss: 0.047525 - Val Loss: 0.099519\n",
      "Epoch 38/50 - Train Loss: 0.046866 - Val Loss: 0.098473\n",
      "Epoch 39/50 - Train Loss: 0.046095 - Val Loss: 0.097433\n",
      "Epoch 40/50 - Train Loss: 0.046051 - Val Loss: 0.096451\n",
      "Epoch 41/50 - Train Loss: 0.043528 - Val Loss: 0.095491\n",
      "Epoch 42/50 - Train Loss: 0.043362 - Val Loss: 0.094793\n",
      "Epoch 43/50 - Train Loss: 0.045454 - Val Loss: 0.094043\n",
      "Epoch 44/50 - Train Loss: 0.042409 - Val Loss: 0.093342\n",
      "Epoch 45/50 - Train Loss: 0.044521 - Val Loss: 0.092627\n",
      "Epoch 46/50 - Train Loss: 0.045046 - Val Loss: 0.091969\n",
      "Epoch 47/50 - Train Loss: 0.043213 - Val Loss: 0.091445\n",
      "Epoch 48/50 - Train Loss: 0.043928 - Val Loss: 0.090803\n",
      "Epoch 49/50 - Train Loss: 0.044525 - Val Loss: 0.090194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:14:13,354] Trial 3 finished with value: 0.08962612599134445 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 38, 'lr': 3.4121219939997605e-05, 'weight_decay': 1.8324658768271532e-06, 'batch_size': 8}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.044157 - Val Loss: 0.089626\n",
      "Epoch 1/50 - Train Loss: 0.134438 - Val Loss: 0.220906\n",
      "Epoch 2/50 - Train Loss: 0.089736 - Val Loss: 0.161467\n",
      "Epoch 3/50 - Train Loss: 0.062643 - Val Loss: 0.105863\n",
      "Epoch 4/50 - Train Loss: 0.049896 - Val Loss: 0.072380\n",
      "Epoch 5/50 - Train Loss: 0.047573 - Val Loss: 0.070468\n",
      "Epoch 6/50 - Train Loss: 0.047865 - Val Loss: 0.084451\n",
      "Epoch 7/50 - Train Loss: 0.040714 - Val Loss: 0.084707\n",
      "Epoch 8/50 - Train Loss: 0.042536 - Val Loss: 0.078422\n",
      "Epoch 9/50 - Train Loss: 0.042501 - Val Loss: 0.072953\n",
      "Epoch 10/50 - Train Loss: 0.041465 - Val Loss: 0.073946\n",
      "Epoch 11/50 - Train Loss: 0.042068 - Val Loss: 0.072673\n",
      "Epoch 12/50 - Train Loss: 0.042020 - Val Loss: 0.063524\n",
      "Epoch 13/50 - Train Loss: 0.038594 - Val Loss: 0.061148\n",
      "Epoch 14/50 - Train Loss: 0.035348 - Val Loss: 0.060087\n",
      "Epoch 15/50 - Train Loss: 0.037337 - Val Loss: 0.052503\n",
      "Epoch 16/50 - Train Loss: 0.034093 - Val Loss: 0.051937\n",
      "Epoch 17/50 - Train Loss: 0.032850 - Val Loss: 0.048491\n",
      "Epoch 18/50 - Train Loss: 0.035259 - Val Loss: 0.049565\n",
      "Epoch 19/50 - Train Loss: 0.034551 - Val Loss: 0.047813\n",
      "Epoch 20/50 - Train Loss: 0.034556 - Val Loss: 0.050022\n",
      "Epoch 21/50 - Train Loss: 0.032397 - Val Loss: 0.048931\n",
      "Epoch 22/50 - Train Loss: 0.032764 - Val Loss: 0.046448\n",
      "Epoch 23/50 - Train Loss: 0.033768 - Val Loss: 0.046882\n",
      "Epoch 24/50 - Train Loss: 0.029933 - Val Loss: 0.045760\n",
      "Epoch 25/50 - Train Loss: 0.029046 - Val Loss: 0.046473\n",
      "Epoch 26/50 - Train Loss: 0.028109 - Val Loss: 0.045229\n",
      "Epoch 27/50 - Train Loss: 0.027298 - Val Loss: 0.045041\n",
      "Epoch 28/50 - Train Loss: 0.030215 - Val Loss: 0.045283\n",
      "Epoch 29/50 - Train Loss: 0.032665 - Val Loss: 0.045483\n",
      "Epoch 30/50 - Train Loss: 0.031937 - Val Loss: 0.045292\n",
      "Epoch 31/50 - Train Loss: 0.029721 - Val Loss: 0.044946\n",
      "Epoch 32/50 - Train Loss: 0.029733 - Val Loss: 0.045619\n",
      "Epoch 33/50 - Train Loss: 0.030385 - Val Loss: 0.045078\n",
      "Epoch 34/50 - Train Loss: 0.029815 - Val Loss: 0.045567\n",
      "Epoch 35/50 - Train Loss: 0.031700 - Val Loss: 0.045297\n",
      "Epoch 36/50 - Train Loss: 0.028735 - Val Loss: 0.045576\n",
      "Epoch 37/50 - Train Loss: 0.027570 - Val Loss: 0.045857\n",
      "Epoch 38/50 - Train Loss: 0.030973 - Val Loss: 0.044512\n",
      "Epoch 39/50 - Train Loss: 0.029118 - Val Loss: 0.045730\n",
      "Epoch 40/50 - Train Loss: 0.029613 - Val Loss: 0.046469\n",
      "Epoch 41/50 - Train Loss: 0.031135 - Val Loss: 0.044546\n",
      "Epoch 42/50 - Train Loss: 0.029964 - Val Loss: 0.046820\n",
      "Epoch 43/50 - Train Loss: 0.029066 - Val Loss: 0.045547\n",
      "Epoch 44/50 - Train Loss: 0.031788 - Val Loss: 0.045825\n",
      "Epoch 45/50 - Train Loss: 0.028072 - Val Loss: 0.045188\n",
      "Epoch 46/50 - Train Loss: 0.029768 - Val Loss: 0.045611\n",
      "Epoch 47/50 - Train Loss: 0.029281 - Val Loss: 0.044511\n",
      "Epoch 48/50 - Train Loss: 0.032683 - Val Loss: 0.045691\n",
      "Epoch 49/50 - Train Loss: 0.031118 - Val Loss: 0.046996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:14:30,959] Trial 4 finished with value: 0.04451085813343525 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 43, 'lr': 0.0008538905075914349, 'weight_decay': 1.4532946814411318e-05, 'batch_size': 16}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033428 - Val Loss: 0.045074\n",
      "Epoch 1/50 - Train Loss: 0.219298 - Val Loss: 0.414347\n",
      "Epoch 2/50 - Train Loss: 0.213483 - Val Loss: 0.396902\n",
      "Epoch 3/50 - Train Loss: 0.195017 - Val Loss: 0.379123\n",
      "Epoch 4/50 - Train Loss: 0.178823 - Val Loss: 0.361005\n",
      "Epoch 5/50 - Train Loss: 0.175361 - Val Loss: 0.342022\n",
      "Epoch 6/50 - Train Loss: 0.160806 - Val Loss: 0.319720\n",
      "Epoch 7/50 - Train Loss: 0.140911 - Val Loss: 0.294014\n",
      "Epoch 8/50 - Train Loss: 0.129833 - Val Loss: 0.263468\n",
      "Epoch 9/50 - Train Loss: 0.112972 - Val Loss: 0.227536\n",
      "Epoch 10/50 - Train Loss: 0.095563 - Val Loss: 0.187856\n",
      "Epoch 11/50 - Train Loss: 0.077812 - Val Loss: 0.145935\n",
      "Epoch 12/50 - Train Loss: 0.053402 - Val Loss: 0.113257\n",
      "Epoch 13/50 - Train Loss: 0.049718 - Val Loss: 0.091985\n",
      "Epoch 14/50 - Train Loss: 0.046672 - Val Loss: 0.086627\n",
      "Epoch 15/50 - Train Loss: 0.044761 - Val Loss: 0.086897\n",
      "Epoch 16/50 - Train Loss: 0.045567 - Val Loss: 0.088345\n",
      "Epoch 17/50 - Train Loss: 0.045435 - Val Loss: 0.089523\n",
      "Epoch 18/50 - Train Loss: 0.044088 - Val Loss: 0.086665\n",
      "Epoch 19/50 - Train Loss: 0.044220 - Val Loss: 0.085741\n",
      "Epoch 20/50 - Train Loss: 0.041794 - Val Loss: 0.083216\n",
      "Epoch 21/50 - Train Loss: 0.043178 - Val Loss: 0.082331\n",
      "Epoch 22/50 - Train Loss: 0.039150 - Val Loss: 0.081110\n",
      "Epoch 23/50 - Train Loss: 0.040397 - Val Loss: 0.078999\n",
      "Epoch 24/50 - Train Loss: 0.042424 - Val Loss: 0.079225\n",
      "Epoch 25/50 - Train Loss: 0.039036 - Val Loss: 0.078907\n",
      "Epoch 26/50 - Train Loss: 0.040719 - Val Loss: 0.076803\n",
      "Epoch 27/50 - Train Loss: 0.038393 - Val Loss: 0.075129\n",
      "Epoch 28/50 - Train Loss: 0.038911 - Val Loss: 0.072726\n",
      "Epoch 29/50 - Train Loss: 0.037861 - Val Loss: 0.072241\n",
      "Epoch 30/50 - Train Loss: 0.037167 - Val Loss: 0.071445\n",
      "Epoch 31/50 - Train Loss: 0.036438 - Val Loss: 0.067849\n",
      "Epoch 32/50 - Train Loss: 0.035859 - Val Loss: 0.068276\n",
      "Epoch 33/50 - Train Loss: 0.034901 - Val Loss: 0.067057\n",
      "Epoch 34/50 - Train Loss: 0.036950 - Val Loss: 0.065919\n",
      "Epoch 35/50 - Train Loss: 0.034799 - Val Loss: 0.064089\n",
      "Epoch 36/50 - Train Loss: 0.036231 - Val Loss: 0.062489\n",
      "Epoch 37/50 - Train Loss: 0.035189 - Val Loss: 0.061901\n",
      "Epoch 38/50 - Train Loss: 0.034746 - Val Loss: 0.058853\n",
      "Epoch 39/50 - Train Loss: 0.033400 - Val Loss: 0.059489\n",
      "Epoch 40/50 - Train Loss: 0.035198 - Val Loss: 0.058661\n",
      "Epoch 41/50 - Train Loss: 0.033776 - Val Loss: 0.055428\n",
      "Epoch 42/50 - Train Loss: 0.033263 - Val Loss: 0.056250\n",
      "Epoch 43/50 - Train Loss: 0.031869 - Val Loss: 0.055044\n",
      "Epoch 44/50 - Train Loss: 0.030999 - Val Loss: 0.053966\n",
      "Epoch 45/50 - Train Loss: 0.031714 - Val Loss: 0.052827\n",
      "Epoch 46/50 - Train Loss: 0.032855 - Val Loss: 0.053438\n",
      "Epoch 47/50 - Train Loss: 0.033024 - Val Loss: 0.052149\n",
      "Epoch 48/50 - Train Loss: 0.031566 - Val Loss: 0.052336\n",
      "Epoch 49/50 - Train Loss: 0.029928 - Val Loss: 0.050456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:15:06,702] Trial 5 finished with value: 0.050455822298924126 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 94, 'lr': 6.026132016313157e-05, 'weight_decay': 3.22086948041954e-07, 'batch_size': 8}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.034446 - Val Loss: 0.051817\n",
      "Epoch 1/50 - Train Loss: 0.113811 - Val Loss: 0.222578\n",
      "Epoch 2/50 - Train Loss: 0.092107 - Val Loss: 0.201186\n",
      "Epoch 3/50 - Train Loss: 0.087170 - Val Loss: 0.181578\n",
      "Epoch 4/50 - Train Loss: 0.068337 - Val Loss: 0.164148\n",
      "Epoch 5/50 - Train Loss: 0.066857 - Val Loss: 0.148594\n",
      "Epoch 6/50 - Train Loss: 0.061005 - Val Loss: 0.134504\n",
      "Epoch 7/50 - Train Loss: 0.053340 - Val Loss: 0.122629\n",
      "Epoch 8/50 - Train Loss: 0.048335 - Val Loss: 0.112361\n",
      "Epoch 9/50 - Train Loss: 0.043324 - Val Loss: 0.104384\n",
      "Epoch 10/50 - Train Loss: 0.048150 - Val Loss: 0.098093\n",
      "Epoch 11/50 - Train Loss: 0.041598 - Val Loss: 0.093018\n",
      "Epoch 12/50 - Train Loss: 0.041390 - Val Loss: 0.089179\n",
      "Epoch 13/50 - Train Loss: 0.043318 - Val Loss: 0.086049\n",
      "Epoch 14/50 - Train Loss: 0.042477 - Val Loss: 0.083915\n",
      "Epoch 15/50 - Train Loss: 0.040435 - Val Loss: 0.082658\n",
      "Epoch 16/50 - Train Loss: 0.039840 - Val Loss: 0.081782\n",
      "Epoch 17/50 - Train Loss: 0.041106 - Val Loss: 0.081389\n",
      "Epoch 18/50 - Train Loss: 0.038863 - Val Loss: 0.080541\n",
      "Epoch 19/50 - Train Loss: 0.041510 - Val Loss: 0.080045\n",
      "Epoch 20/50 - Train Loss: 0.043885 - Val Loss: 0.080074\n",
      "Epoch 21/50 - Train Loss: 0.038186 - Val Loss: 0.079914\n",
      "Epoch 22/50 - Train Loss: 0.040169 - Val Loss: 0.079014\n",
      "Epoch 23/50 - Train Loss: 0.038003 - Val Loss: 0.078023\n",
      "Epoch 24/50 - Train Loss: 0.041789 - Val Loss: 0.077063\n",
      "Epoch 25/50 - Train Loss: 0.035128 - Val Loss: 0.076959\n",
      "Epoch 26/50 - Train Loss: 0.036295 - Val Loss: 0.075952\n",
      "Epoch 27/50 - Train Loss: 0.036938 - Val Loss: 0.075406\n",
      "Epoch 28/50 - Train Loss: 0.037198 - Val Loss: 0.074303\n",
      "Epoch 29/50 - Train Loss: 0.036281 - Val Loss: 0.073656\n",
      "Epoch 30/50 - Train Loss: 0.037064 - Val Loss: 0.072652\n",
      "Epoch 31/50 - Train Loss: 0.037366 - Val Loss: 0.071649\n",
      "Epoch 32/50 - Train Loss: 0.035108 - Val Loss: 0.071170\n",
      "Epoch 33/50 - Train Loss: 0.037532 - Val Loss: 0.070275\n",
      "Epoch 34/50 - Train Loss: 0.039649 - Val Loss: 0.070294\n",
      "Epoch 35/50 - Train Loss: 0.034494 - Val Loss: 0.069561\n",
      "Epoch 36/50 - Train Loss: 0.033853 - Val Loss: 0.069258\n",
      "Epoch 37/50 - Train Loss: 0.035211 - Val Loss: 0.068457\n",
      "Epoch 38/50 - Train Loss: 0.031322 - Val Loss: 0.067761\n",
      "Epoch 39/50 - Train Loss: 0.034874 - Val Loss: 0.066750\n",
      "Epoch 40/50 - Train Loss: 0.036244 - Val Loss: 0.065910\n",
      "Epoch 41/50 - Train Loss: 0.035623 - Val Loss: 0.065152\n",
      "Epoch 42/50 - Train Loss: 0.036335 - Val Loss: 0.064322\n",
      "Epoch 43/50 - Train Loss: 0.033147 - Val Loss: 0.064340\n",
      "Epoch 44/50 - Train Loss: 0.033348 - Val Loss: 0.063905\n",
      "Epoch 45/50 - Train Loss: 0.033284 - Val Loss: 0.062893\n",
      "Epoch 46/50 - Train Loss: 0.030429 - Val Loss: 0.062201\n",
      "Epoch 47/50 - Train Loss: 0.032094 - Val Loss: 0.061669\n",
      "Epoch 48/50 - Train Loss: 0.032045 - Val Loss: 0.061213\n",
      "Epoch 49/50 - Train Loss: 0.034914 - Val Loss: 0.060595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:15:18,987] Trial 6 finished with value: 0.06019306927919388 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 91, 'lr': 6.694563242510557e-05, 'weight_decay': 0.0001240594870093528, 'batch_size': 16}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032782 - Val Loss: 0.060193\n",
      "Epoch 1/50 - Train Loss: 0.081425 - Val Loss: 0.171930\n",
      "Epoch 2/50 - Train Loss: 0.072114 - Val Loss: 0.151543\n",
      "Epoch 3/50 - Train Loss: 0.058830 - Val Loss: 0.133357\n",
      "Epoch 4/50 - Train Loss: 0.057006 - Val Loss: 0.119838\n",
      "Epoch 5/50 - Train Loss: 0.053860 - Val Loss: 0.109183\n",
      "Epoch 6/50 - Train Loss: 0.047886 - Val Loss: 0.101740\n",
      "Epoch 7/50 - Train Loss: 0.046261 - Val Loss: 0.095735\n",
      "Epoch 8/50 - Train Loss: 0.045678 - Val Loss: 0.091667\n",
      "Epoch 9/50 - Train Loss: 0.042958 - Val Loss: 0.088689\n",
      "Epoch 10/50 - Train Loss: 0.044035 - Val Loss: 0.086777\n",
      "Epoch 11/50 - Train Loss: 0.042604 - Val Loss: 0.085151\n",
      "Epoch 12/50 - Train Loss: 0.042089 - Val Loss: 0.083889\n",
      "Epoch 13/50 - Train Loss: 0.044319 - Val Loss: 0.082591\n",
      "Epoch 14/50 - Train Loss: 0.043456 - Val Loss: 0.082128\n",
      "Epoch 15/50 - Train Loss: 0.040579 - Val Loss: 0.081392\n",
      "Epoch 16/50 - Train Loss: 0.042794 - Val Loss: 0.080201\n",
      "Epoch 17/50 - Train Loss: 0.041622 - Val Loss: 0.079025\n",
      "Epoch 18/50 - Train Loss: 0.043891 - Val Loss: 0.077448\n",
      "Epoch 19/50 - Train Loss: 0.039449 - Val Loss: 0.076281\n",
      "Epoch 20/50 - Train Loss: 0.038328 - Val Loss: 0.074124\n",
      "Epoch 21/50 - Train Loss: 0.037144 - Val Loss: 0.073535\n",
      "Epoch 22/50 - Train Loss: 0.038201 - Val Loss: 0.074179\n",
      "Epoch 23/50 - Train Loss: 0.035702 - Val Loss: 0.073803\n",
      "Epoch 24/50 - Train Loss: 0.036687 - Val Loss: 0.072527\n",
      "Epoch 25/50 - Train Loss: 0.037557 - Val Loss: 0.070769\n",
      "Epoch 26/50 - Train Loss: 0.035637 - Val Loss: 0.069932\n",
      "Epoch 27/50 - Train Loss: 0.035515 - Val Loss: 0.069045\n",
      "Epoch 28/50 - Train Loss: 0.037089 - Val Loss: 0.068762\n",
      "Epoch 29/50 - Train Loss: 0.037014 - Val Loss: 0.068430\n",
      "Epoch 30/50 - Train Loss: 0.037539 - Val Loss: 0.067750\n",
      "Epoch 31/50 - Train Loss: 0.036589 - Val Loss: 0.066937\n",
      "Epoch 32/50 - Train Loss: 0.038336 - Val Loss: 0.067026\n",
      "Epoch 33/50 - Train Loss: 0.035562 - Val Loss: 0.065866\n",
      "Epoch 34/50 - Train Loss: 0.035487 - Val Loss: 0.064994\n",
      "Epoch 35/50 - Train Loss: 0.034132 - Val Loss: 0.063870\n",
      "Epoch 36/50 - Train Loss: 0.032829 - Val Loss: 0.063102\n",
      "Epoch 37/50 - Train Loss: 0.035639 - Val Loss: 0.063079\n",
      "Epoch 38/50 - Train Loss: 0.033067 - Val Loss: 0.063122\n",
      "Epoch 39/50 - Train Loss: 0.034337 - Val Loss: 0.062851\n",
      "Epoch 40/50 - Train Loss: 0.035366 - Val Loss: 0.061706\n",
      "Epoch 41/50 - Train Loss: 0.033056 - Val Loss: 0.061204\n",
      "Epoch 42/50 - Train Loss: 0.033153 - Val Loss: 0.060882\n",
      "Epoch 43/50 - Train Loss: 0.036781 - Val Loss: 0.060755\n",
      "Epoch 44/50 - Train Loss: 0.034411 - Val Loss: 0.060248\n",
      "Epoch 45/50 - Train Loss: 0.036013 - Val Loss: 0.059882\n",
      "Epoch 46/50 - Train Loss: 0.031992 - Val Loss: 0.059456\n",
      "Epoch 47/50 - Train Loss: 0.033318 - Val Loss: 0.058893\n",
      "Epoch 48/50 - Train Loss: 0.034919 - Val Loss: 0.058737\n",
      "Epoch 49/50 - Train Loss: 0.030347 - Val Loss: 0.057869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:15:32,466] Trial 7 finished with value: 0.05786930397152901 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 196, 'lr': 3.640380128866154e-05, 'weight_decay': 0.00021756665934204186, 'batch_size': 16}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033240 - Val Loss: 0.057934\n",
      "Epoch 1/50 - Train Loss: 0.209366 - Val Loss: 0.376589\n",
      "Epoch 2/50 - Train Loss: 0.221504 - Val Loss: 0.372388\n",
      "Epoch 3/50 - Train Loss: 0.194523 - Val Loss: 0.368271\n",
      "Epoch 4/50 - Train Loss: 0.207024 - Val Loss: 0.364157\n",
      "Epoch 5/50 - Train Loss: 0.173068 - Val Loss: 0.360228\n",
      "Epoch 6/50 - Train Loss: 0.186874 - Val Loss: 0.356388\n",
      "Epoch 7/50 - Train Loss: 0.173736 - Val Loss: 0.352609\n",
      "Epoch 8/50 - Train Loss: 0.199189 - Val Loss: 0.348775\n",
      "Epoch 9/50 - Train Loss: 0.182624 - Val Loss: 0.344934\n",
      "Epoch 10/50 - Train Loss: 0.174945 - Val Loss: 0.341201\n",
      "Epoch 11/50 - Train Loss: 0.206130 - Val Loss: 0.337407\n",
      "Epoch 12/50 - Train Loss: 0.153523 - Val Loss: 0.333585\n",
      "Epoch 13/50 - Train Loss: 0.164643 - Val Loss: 0.329887\n",
      "Epoch 14/50 - Train Loss: 0.157771 - Val Loss: 0.326273\n",
      "Epoch 15/50 - Train Loss: 0.159954 - Val Loss: 0.322731\n",
      "Epoch 16/50 - Train Loss: 0.155160 - Val Loss: 0.319243\n",
      "Epoch 17/50 - Train Loss: 0.149929 - Val Loss: 0.315780\n",
      "Epoch 18/50 - Train Loss: 0.167138 - Val Loss: 0.312213\n",
      "Epoch 19/50 - Train Loss: 0.164558 - Val Loss: 0.308647\n",
      "Epoch 20/50 - Train Loss: 0.155752 - Val Loss: 0.305031\n",
      "Epoch 21/50 - Train Loss: 0.144607 - Val Loss: 0.301395\n",
      "Epoch 22/50 - Train Loss: 0.158975 - Val Loss: 0.297818\n",
      "Epoch 23/50 - Train Loss: 0.149911 - Val Loss: 0.294210\n",
      "Epoch 24/50 - Train Loss: 0.137057 - Val Loss: 0.290565\n",
      "Epoch 25/50 - Train Loss: 0.136684 - Val Loss: 0.286994\n",
      "Epoch 26/50 - Train Loss: 0.141932 - Val Loss: 0.283395\n",
      "Epoch 27/50 - Train Loss: 0.131753 - Val Loss: 0.279727\n",
      "Epoch 28/50 - Train Loss: 0.133199 - Val Loss: 0.276104\n",
      "Epoch 29/50 - Train Loss: 0.135051 - Val Loss: 0.272460\n",
      "Epoch 30/50 - Train Loss: 0.151829 - Val Loss: 0.268720\n",
      "Epoch 31/50 - Train Loss: 0.140367 - Val Loss: 0.264935\n",
      "Epoch 32/50 - Train Loss: 0.136042 - Val Loss: 0.261099\n",
      "Epoch 33/50 - Train Loss: 0.135362 - Val Loss: 0.257183\n",
      "Epoch 34/50 - Train Loss: 0.115592 - Val Loss: 0.253331\n",
      "Epoch 35/50 - Train Loss: 0.115796 - Val Loss: 0.249640\n",
      "Epoch 36/50 - Train Loss: 0.104754 - Val Loss: 0.246033\n",
      "Epoch 37/50 - Train Loss: 0.105990 - Val Loss: 0.242490\n",
      "Epoch 38/50 - Train Loss: 0.109093 - Val Loss: 0.238969\n",
      "Epoch 39/50 - Train Loss: 0.101207 - Val Loss: 0.235462\n",
      "Epoch 40/50 - Train Loss: 0.124396 - Val Loss: 0.231963\n",
      "Epoch 41/50 - Train Loss: 0.120089 - Val Loss: 0.228322\n",
      "Epoch 42/50 - Train Loss: 0.108271 - Val Loss: 0.224587\n",
      "Epoch 43/50 - Train Loss: 0.099880 - Val Loss: 0.220960\n",
      "Epoch 44/50 - Train Loss: 0.100540 - Val Loss: 0.217377\n",
      "Epoch 45/50 - Train Loss: 0.096018 - Val Loss: 0.213842\n",
      "Epoch 46/50 - Train Loss: 0.091601 - Val Loss: 0.210342\n",
      "Epoch 47/50 - Train Loss: 0.084576 - Val Loss: 0.206997\n",
      "Epoch 48/50 - Train Loss: 0.085793 - Val Loss: 0.203710\n",
      "Epoch 49/50 - Train Loss: 0.092721 - Val Loss: 0.200422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:15:43,405] Trial 8 finished with value: 0.19716474413871765 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 112, 'lr': 1.5805038892865957e-05, 'weight_decay': 1.6148271012407063e-07, 'batch_size': 32}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.089618 - Val Loss: 0.197165\n",
      "Epoch 1/50 - Train Loss: 0.158633 - Val Loss: 0.198122\n",
      "Epoch 2/50 - Train Loss: 0.057135 - Val Loss: 0.069713\n",
      "Epoch 3/50 - Train Loss: 0.044549 - Val Loss: 0.090775\n",
      "Epoch 4/50 - Train Loss: 0.041695 - Val Loss: 0.067843\n",
      "Epoch 5/50 - Train Loss: 0.037858 - Val Loss: 0.060485\n",
      "Epoch 6/50 - Train Loss: 0.032599 - Val Loss: 0.055884\n",
      "Epoch 7/50 - Train Loss: 0.030312 - Val Loss: 0.047113\n",
      "Epoch 8/50 - Train Loss: 0.030828 - Val Loss: 0.046100\n",
      "Epoch 9/50 - Train Loss: 0.032457 - Val Loss: 0.048336\n",
      "Epoch 10/50 - Train Loss: 0.027778 - Val Loss: 0.047774\n",
      "Epoch 11/50 - Train Loss: 0.028662 - Val Loss: 0.048060\n",
      "Epoch 12/50 - Train Loss: 0.032255 - Val Loss: 0.050986\n",
      "Epoch 13/50 - Train Loss: 0.030719 - Val Loss: 0.046839\n",
      "Epoch 14/50 - Train Loss: 0.031358 - Val Loss: 0.046727\n",
      "Epoch 15/50 - Train Loss: 0.028440 - Val Loss: 0.046285\n",
      "Epoch 16/50 - Train Loss: 0.030134 - Val Loss: 0.046416\n",
      "Epoch 17/50 - Train Loss: 0.029480 - Val Loss: 0.047739\n",
      "Epoch 18/50 - Train Loss: 0.030703 - Val Loss: 0.046140\n",
      "Epoch 19/50 - Train Loss: 0.028728 - Val Loss: 0.045585\n",
      "Epoch 20/50 - Train Loss: 0.028177 - Val Loss: 0.045430\n",
      "Epoch 21/50 - Train Loss: 0.029127 - Val Loss: 0.045790\n",
      "Epoch 22/50 - Train Loss: 0.030192 - Val Loss: 0.044737\n",
      "Epoch 23/50 - Train Loss: 0.028729 - Val Loss: 0.047955\n",
      "Epoch 24/50 - Train Loss: 0.028877 - Val Loss: 0.047217\n",
      "Epoch 25/50 - Train Loss: 0.029741 - Val Loss: 0.046759\n",
      "Epoch 26/50 - Train Loss: 0.027972 - Val Loss: 0.051147\n",
      "Epoch 27/50 - Train Loss: 0.032199 - Val Loss: 0.052520\n",
      "Epoch 28/50 - Train Loss: 0.030068 - Val Loss: 0.048594\n",
      "Epoch 29/50 - Train Loss: 0.027721 - Val Loss: 0.043977\n",
      "Epoch 30/50 - Train Loss: 0.030533 - Val Loss: 0.047055\n",
      "Epoch 31/50 - Train Loss: 0.030195 - Val Loss: 0.045672\n",
      "Epoch 32/50 - Train Loss: 0.027995 - Val Loss: 0.044483\n",
      "Epoch 33/50 - Train Loss: 0.031160 - Val Loss: 0.044707\n",
      "Epoch 34/50 - Train Loss: 0.030186 - Val Loss: 0.045355\n",
      "Epoch 35/50 - Train Loss: 0.027635 - Val Loss: 0.046045\n",
      "Epoch 36/50 - Train Loss: 0.029117 - Val Loss: 0.044878\n",
      "Epoch 37/50 - Train Loss: 0.028803 - Val Loss: 0.045494\n",
      "Epoch 38/50 - Train Loss: 0.028540 - Val Loss: 0.044315\n",
      "Epoch 39/50 - Train Loss: 0.029801 - Val Loss: 0.046808\n",
      "Epoch 40/50 - Train Loss: 0.030820 - Val Loss: 0.047201\n",
      "Epoch 41/50 - Train Loss: 0.028269 - Val Loss: 0.043869\n",
      "Epoch 42/50 - Train Loss: 0.028758 - Val Loss: 0.044007\n",
      "Epoch 43/50 - Train Loss: 0.029411 - Val Loss: 0.045111\n",
      "Epoch 44/50 - Train Loss: 0.026926 - Val Loss: 0.047462\n",
      "Epoch 45/50 - Train Loss: 0.027851 - Val Loss: 0.043815\n",
      "Epoch 46/50 - Train Loss: 0.028622 - Val Loss: 0.043912\n",
      "Epoch 47/50 - Train Loss: 0.028578 - Val Loss: 0.044502\n",
      "Epoch 48/50 - Train Loss: 0.027923 - Val Loss: 0.049894\n",
      "Epoch 49/50 - Train Loss: 0.028930 - Val Loss: 0.045101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:16:16,519] Trial 9 finished with value: 0.0438151645163695 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 155, 'lr': 0.0003496453714626646, 'weight_decay': 1.2324007886642055e-06, 'batch_size': 8}. Best is trial 2 with value: 0.043729208409786224.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029221 - Val Loss: 0.044095\n",
      "Epoch 1/50 - Train Loss: 0.684396 - Val Loss: 0.385745\n",
      "Epoch 2/50 - Train Loss: 0.189355 - Val Loss: 0.257456\n",
      "Epoch 3/50 - Train Loss: 0.081496 - Val Loss: 0.112827\n",
      "Epoch 4/50 - Train Loss: 0.066141 - Val Loss: 0.086357\n",
      "Epoch 5/50 - Train Loss: 0.047868 - Val Loss: 0.081567\n",
      "Epoch 6/50 - Train Loss: 0.044523 - Val Loss: 0.091599\n",
      "Epoch 7/50 - Train Loss: 0.042865 - Val Loss: 0.062375\n",
      "Epoch 8/50 - Train Loss: 0.043287 - Val Loss: 0.070529\n",
      "Epoch 9/50 - Train Loss: 0.033339 - Val Loss: 0.054672\n",
      "Epoch 10/50 - Train Loss: 0.043018 - Val Loss: 0.066628\n",
      "Epoch 11/50 - Train Loss: 0.032050 - Val Loss: 0.048101\n",
      "Epoch 12/50 - Train Loss: 0.032013 - Val Loss: 0.051721\n",
      "Epoch 13/50 - Train Loss: 0.032479 - Val Loss: 0.051590\n",
      "Epoch 14/50 - Train Loss: 0.036457 - Val Loss: 0.045422\n",
      "Epoch 15/50 - Train Loss: 0.034380 - Val Loss: 0.064531\n",
      "Epoch 16/50 - Train Loss: 0.040415 - Val Loss: 0.044524\n",
      "Epoch 17/50 - Train Loss: 0.032029 - Val Loss: 0.056777\n",
      "Epoch 18/50 - Train Loss: 0.032855 - Val Loss: 0.043968\n",
      "Epoch 19/50 - Train Loss: 0.033229 - Val Loss: 0.048195\n",
      "Epoch 20/50 - Train Loss: 0.029667 - Val Loss: 0.044408\n",
      "Epoch 21/50 - Train Loss: 0.035892 - Val Loss: 0.047191\n",
      "Epoch 22/50 - Train Loss: 0.033326 - Val Loss: 0.045328\n",
      "Epoch 23/50 - Train Loss: 0.033061 - Val Loss: 0.049074\n",
      "Epoch 24/50 - Train Loss: 0.027309 - Val Loss: 0.045652\n",
      "Epoch 25/50 - Train Loss: 0.031442 - Val Loss: 0.047978\n",
      "Epoch 26/50 - Train Loss: 0.028529 - Val Loss: 0.043660\n",
      "Epoch 27/50 - Train Loss: 0.030891 - Val Loss: 0.045855\n",
      "Epoch 28/50 - Train Loss: 0.032345 - Val Loss: 0.045494\n",
      "Epoch 29/50 - Train Loss: 0.037655 - Val Loss: 0.044247\n",
      "Epoch 30/50 - Train Loss: 0.035688 - Val Loss: 0.065052\n",
      "Epoch 31/50 - Train Loss: 0.037013 - Val Loss: 0.045438\n",
      "Epoch 32/50 - Train Loss: 0.033062 - Val Loss: 0.057511\n",
      "Epoch 33/50 - Train Loss: 0.038054 - Val Loss: 0.044704\n",
      "Epoch 34/50 - Train Loss: 0.028686 - Val Loss: 0.048291\n",
      "Epoch 35/50 - Train Loss: 0.031605 - Val Loss: 0.044680\n",
      "Epoch 36/50 - Train Loss: 0.033938 - Val Loss: 0.047694\n",
      "Epoch 37/50 - Train Loss: 0.031379 - Val Loss: 0.046778\n",
      "Epoch 38/50 - Train Loss: 0.035372 - Val Loss: 0.046224\n",
      "Epoch 39/50 - Train Loss: 0.027796 - Val Loss: 0.058558\n",
      "Epoch 40/50 - Train Loss: 0.033100 - Val Loss: 0.043734\n",
      "Epoch 41/50 - Train Loss: 0.029465 - Val Loss: 0.047197\n",
      "Epoch 42/50 - Train Loss: 0.033097 - Val Loss: 0.043664\n",
      "Epoch 43/50 - Train Loss: 0.032681 - Val Loss: 0.048635\n",
      "Epoch 44/50 - Train Loss: 0.031782 - Val Loss: 0.044264\n",
      "Epoch 45/50 - Train Loss: 0.031946 - Val Loss: 0.046637\n",
      "Epoch 46/50 - Train Loss: 0.026856 - Val Loss: 0.043693\n",
      "Epoch 47/50 - Train Loss: 0.029875 - Val Loss: 0.046821\n",
      "Epoch 48/50 - Train Loss: 0.030661 - Val Loss: 0.044708\n",
      "Epoch 49/50 - Train Loss: 0.031454 - Val Loss: 0.047814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:16:41,308] Trial 10 finished with value: 0.04365973919630051 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 251, 'lr': 0.008978358680449066, 'weight_decay': 1.1270729419850272e-08, 'batch_size': 32}. Best is trial 10 with value: 0.04365973919630051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030156 - Val Loss: 0.044560\n",
      "Epoch 1/50 - Train Loss: 0.250765 - Val Loss: 0.253979\n",
      "Epoch 2/50 - Train Loss: 0.113021 - Val Loss: 0.144612\n",
      "Epoch 3/50 - Train Loss: 0.062185 - Val Loss: 0.120901\n",
      "Epoch 4/50 - Train Loss: 0.051886 - Val Loss: 0.113871\n",
      "Epoch 5/50 - Train Loss: 0.045435 - Val Loss: 0.071148\n",
      "Epoch 6/50 - Train Loss: 0.046640 - Val Loss: 0.074352\n",
      "Epoch 7/50 - Train Loss: 0.043864 - Val Loss: 0.103723\n",
      "Epoch 8/50 - Train Loss: 0.045774 - Val Loss: 0.103213\n",
      "Epoch 9/50 - Train Loss: 0.050360 - Val Loss: 0.082404\n",
      "Epoch 10/50 - Train Loss: 0.048139 - Val Loss: 0.071721\n",
      "Epoch 11/50 - Train Loss: 0.038491 - Val Loss: 0.076348\n",
      "Epoch 12/50 - Train Loss: 0.035706 - Val Loss: 0.055424\n",
      "Epoch 13/50 - Train Loss: 0.037107 - Val Loss: 0.054782\n",
      "Epoch 14/50 - Train Loss: 0.038869 - Val Loss: 0.055316\n",
      "Epoch 15/50 - Train Loss: 0.038491 - Val Loss: 0.047447\n",
      "Epoch 16/50 - Train Loss: 0.036720 - Val Loss: 0.046816\n",
      "Epoch 17/50 - Train Loss: 0.033203 - Val Loss: 0.046399\n",
      "Epoch 18/50 - Train Loss: 0.034731 - Val Loss: 0.045813\n",
      "Epoch 19/50 - Train Loss: 0.037038 - Val Loss: 0.047120\n",
      "Epoch 20/50 - Train Loss: 0.028165 - Val Loss: 0.044374\n",
      "Epoch 21/50 - Train Loss: 0.035958 - Val Loss: 0.044186\n",
      "Epoch 22/50 - Train Loss: 0.028767 - Val Loss: 0.043354\n",
      "Epoch 23/50 - Train Loss: 0.029257 - Val Loss: 0.046827\n",
      "Epoch 24/50 - Train Loss: 0.031753 - Val Loss: 0.048847\n",
      "Epoch 25/50 - Train Loss: 0.031348 - Val Loss: 0.043626\n",
      "Epoch 26/50 - Train Loss: 0.031905 - Val Loss: 0.045042\n",
      "Epoch 27/50 - Train Loss: 0.032261 - Val Loss: 0.044491\n",
      "Epoch 28/50 - Train Loss: 0.027895 - Val Loss: 0.047413\n",
      "Epoch 29/50 - Train Loss: 0.029248 - Val Loss: 0.044552\n",
      "Epoch 30/50 - Train Loss: 0.034576 - Val Loss: 0.044073\n",
      "Epoch 31/50 - Train Loss: 0.030830 - Val Loss: 0.047758\n",
      "Epoch 32/50 - Train Loss: 0.034369 - Val Loss: 0.044185\n",
      "Epoch 33/50 - Train Loss: 0.026647 - Val Loss: 0.043581\n",
      "Epoch 34/50 - Train Loss: 0.027433 - Val Loss: 0.045806\n",
      "Epoch 35/50 - Train Loss: 0.028387 - Val Loss: 0.044255\n",
      "Epoch 36/50 - Train Loss: 0.024060 - Val Loss: 0.044607\n",
      "Epoch 37/50 - Train Loss: 0.030499 - Val Loss: 0.043888\n",
      "Epoch 38/50 - Train Loss: 0.028555 - Val Loss: 0.046555\n",
      "Epoch 39/50 - Train Loss: 0.024417 - Val Loss: 0.044126\n",
      "Epoch 40/50 - Train Loss: 0.028170 - Val Loss: 0.044039\n",
      "Epoch 41/50 - Train Loss: 0.027974 - Val Loss: 0.043676\n",
      "Epoch 42/50 - Train Loss: 0.024171 - Val Loss: 0.044191\n",
      "Epoch 43/50 - Train Loss: 0.031030 - Val Loss: 0.043689\n",
      "Epoch 44/50 - Train Loss: 0.030204 - Val Loss: 0.045464\n",
      "Epoch 45/50 - Train Loss: 0.030674 - Val Loss: 0.044706\n",
      "Epoch 46/50 - Train Loss: 0.030117 - Val Loss: 0.044228\n",
      "Epoch 47/50 - Train Loss: 0.029272 - Val Loss: 0.044217\n",
      "Epoch 48/50 - Train Loss: 0.026314 - Val Loss: 0.048512\n",
      "Epoch 49/50 - Train Loss: 0.028975 - Val Loss: 0.044615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:17:06,970] Trial 11 finished with value: 0.04335350915789604 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 255, 'lr': 0.00797205895000156, 'weight_decay': 2.1286589160678562e-08, 'batch_size': 32}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030581 - Val Loss: 0.044133\n",
      "Epoch 1/50 - Train Loss: 0.708530 - Val Loss: 0.297664\n",
      "Epoch 2/50 - Train Loss: 0.130055 - Val Loss: 0.161486\n",
      "Epoch 3/50 - Train Loss: 0.066843 - Val Loss: 0.127118\n",
      "Epoch 4/50 - Train Loss: 0.063638 - Val Loss: 0.158407\n",
      "Epoch 5/50 - Train Loss: 0.063724 - Val Loss: 0.085305\n",
      "Epoch 6/50 - Train Loss: 0.051619 - Val Loss: 0.070104\n",
      "Epoch 7/50 - Train Loss: 0.048850 - Val Loss: 0.096500\n",
      "Epoch 8/50 - Train Loss: 0.039197 - Val Loss: 0.094402\n",
      "Epoch 9/50 - Train Loss: 0.040886 - Val Loss: 0.075455\n",
      "Epoch 10/50 - Train Loss: 0.045423 - Val Loss: 0.066791\n",
      "Epoch 11/50 - Train Loss: 0.038442 - Val Loss: 0.053131\n",
      "Epoch 12/50 - Train Loss: 0.036631 - Val Loss: 0.050284\n",
      "Epoch 13/50 - Train Loss: 0.033585 - Val Loss: 0.055530\n",
      "Epoch 14/50 - Train Loss: 0.030121 - Val Loss: 0.049864\n",
      "Epoch 15/50 - Train Loss: 0.029099 - Val Loss: 0.045635\n",
      "Epoch 16/50 - Train Loss: 0.028647 - Val Loss: 0.047958\n",
      "Epoch 17/50 - Train Loss: 0.034814 - Val Loss: 0.047435\n",
      "Epoch 18/50 - Train Loss: 0.034835 - Val Loss: 0.057356\n",
      "Epoch 19/50 - Train Loss: 0.027873 - Val Loss: 0.046331\n",
      "Epoch 20/50 - Train Loss: 0.031170 - Val Loss: 0.048410\n",
      "Epoch 21/50 - Train Loss: 0.031669 - Val Loss: 0.047029\n",
      "Epoch 22/50 - Train Loss: 0.034308 - Val Loss: 0.054432\n",
      "Epoch 23/50 - Train Loss: 0.028216 - Val Loss: 0.050614\n",
      "Epoch 24/50 - Train Loss: 0.036459 - Val Loss: 0.046895\n",
      "Epoch 25/50 - Train Loss: 0.030062 - Val Loss: 0.047433\n",
      "Epoch 26/50 - Train Loss: 0.035364 - Val Loss: 0.045357\n",
      "Epoch 27/50 - Train Loss: 0.028800 - Val Loss: 0.050277\n",
      "Epoch 28/50 - Train Loss: 0.030667 - Val Loss: 0.046839\n",
      "Epoch 29/50 - Train Loss: 0.031096 - Val Loss: 0.046039\n",
      "Epoch 30/50 - Train Loss: 0.027747 - Val Loss: 0.045287\n",
      "Epoch 31/50 - Train Loss: 0.032559 - Val Loss: 0.045488\n",
      "Epoch 32/50 - Train Loss: 0.029885 - Val Loss: 0.049375\n",
      "Epoch 33/50 - Train Loss: 0.026816 - Val Loss: 0.044016\n",
      "Epoch 34/50 - Train Loss: 0.036172 - Val Loss: 0.046651\n",
      "Epoch 35/50 - Train Loss: 0.032467 - Val Loss: 0.043940\n",
      "Epoch 36/50 - Train Loss: 0.029961 - Val Loss: 0.048763\n",
      "Epoch 37/50 - Train Loss: 0.032510 - Val Loss: 0.044284\n",
      "Epoch 38/50 - Train Loss: 0.026263 - Val Loss: 0.047143\n",
      "Epoch 39/50 - Train Loss: 0.032250 - Val Loss: 0.045228\n",
      "Epoch 40/50 - Train Loss: 0.035712 - Val Loss: 0.051249\n",
      "Epoch 41/50 - Train Loss: 0.028250 - Val Loss: 0.046180\n",
      "Epoch 42/50 - Train Loss: 0.031028 - Val Loss: 0.045090\n",
      "Epoch 43/50 - Train Loss: 0.035389 - Val Loss: 0.047661\n",
      "Epoch 44/50 - Train Loss: 0.025864 - Val Loss: 0.047584\n",
      "Epoch 45/50 - Train Loss: 0.032566 - Val Loss: 0.046633\n",
      "Epoch 46/50 - Train Loss: 0.028225 - Val Loss: 0.044055\n",
      "Epoch 47/50 - Train Loss: 0.030332 - Val Loss: 0.047379\n",
      "Epoch 48/50 - Train Loss: 0.029022 - Val Loss: 0.047092\n",
      "Epoch 49/50 - Train Loss: 0.035861 - Val Loss: 0.057151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:17:32,082] Trial 12 finished with value: 0.04393951594829559 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 254, 'lr': 0.009570407735683349, 'weight_decay': 1.0398581606916029e-08, 'batch_size': 32}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033842 - Val Loss: 0.046930\n",
      "Epoch 1/50 - Train Loss: 0.080446 - Val Loss: 0.110189\n",
      "Epoch 2/50 - Train Loss: 0.050643 - Val Loss: 0.106649\n",
      "Epoch 3/50 - Train Loss: 0.044321 - Val Loss: 0.073985\n",
      "Epoch 4/50 - Train Loss: 0.044284 - Val Loss: 0.084608\n",
      "Epoch 5/50 - Train Loss: 0.040587 - Val Loss: 0.095624\n",
      "Epoch 6/50 - Train Loss: 0.040287 - Val Loss: 0.074194\n",
      "Epoch 7/50 - Train Loss: 0.041357 - Val Loss: 0.054712\n",
      "Epoch 8/50 - Train Loss: 0.032210 - Val Loss: 0.047817\n",
      "Epoch 9/50 - Train Loss: 0.024965 - Val Loss: 0.045909\n",
      "Epoch 10/50 - Train Loss: 0.030491 - Val Loss: 0.047678\n",
      "Epoch 11/50 - Train Loss: 0.027958 - Val Loss: 0.053628\n",
      "Epoch 12/50 - Train Loss: 0.032107 - Val Loss: 0.050530\n",
      "Epoch 13/50 - Train Loss: 0.030190 - Val Loss: 0.046244\n",
      "Epoch 14/50 - Train Loss: 0.030420 - Val Loss: 0.045281\n",
      "Epoch 15/50 - Train Loss: 0.026657 - Val Loss: 0.057850\n",
      "Epoch 16/50 - Train Loss: 0.031885 - Val Loss: 0.044785\n",
      "Epoch 17/50 - Train Loss: 0.033243 - Val Loss: 0.046320\n",
      "Epoch 18/50 - Train Loss: 0.035697 - Val Loss: 0.044953\n",
      "Epoch 19/50 - Train Loss: 0.029215 - Val Loss: 0.047952\n",
      "Epoch 20/50 - Train Loss: 0.033594 - Val Loss: 0.046948\n",
      "Epoch 21/50 - Train Loss: 0.031831 - Val Loss: 0.046880\n",
      "Epoch 22/50 - Train Loss: 0.027563 - Val Loss: 0.046737\n",
      "Epoch 23/50 - Train Loss: 0.031980 - Val Loss: 0.045050\n",
      "Epoch 24/50 - Train Loss: 0.030085 - Val Loss: 0.051477\n",
      "Epoch 25/50 - Train Loss: 0.032833 - Val Loss: 0.045322\n",
      "Epoch 26/50 - Train Loss: 0.030242 - Val Loss: 0.046867\n",
      "Epoch 27/50 - Train Loss: 0.029835 - Val Loss: 0.044356\n",
      "Epoch 28/50 - Train Loss: 0.029163 - Val Loss: 0.049867\n",
      "Epoch 29/50 - Train Loss: 0.028838 - Val Loss: 0.044897\n",
      "Epoch 30/50 - Train Loss: 0.023491 - Val Loss: 0.044882\n",
      "Epoch 31/50 - Train Loss: 0.026539 - Val Loss: 0.044615\n",
      "Epoch 32/50 - Train Loss: 0.021891 - Val Loss: 0.046460\n",
      "Epoch 33/50 - Train Loss: 0.032023 - Val Loss: 0.044697\n",
      "Epoch 34/50 - Train Loss: 0.025486 - Val Loss: 0.055091\n",
      "Epoch 35/50 - Train Loss: 0.031329 - Val Loss: 0.045361\n",
      "Epoch 36/50 - Train Loss: 0.032127 - Val Loss: 0.054988\n",
      "Epoch 37/50 - Train Loss: 0.031807 - Val Loss: 0.045579\n",
      "Epoch 38/50 - Train Loss: 0.031925 - Val Loss: 0.051498\n",
      "Epoch 39/50 - Train Loss: 0.027714 - Val Loss: 0.046253\n",
      "Epoch 40/50 - Train Loss: 0.028150 - Val Loss: 0.047076\n",
      "Epoch 41/50 - Train Loss: 0.029892 - Val Loss: 0.045585\n",
      "Epoch 42/50 - Train Loss: 0.026681 - Val Loss: 0.046556\n",
      "Epoch 43/50 - Train Loss: 0.025999 - Val Loss: 0.044848\n",
      "Epoch 44/50 - Train Loss: 0.030031 - Val Loss: 0.045797\n",
      "Epoch 45/50 - Train Loss: 0.030179 - Val Loss: 0.047038\n",
      "Epoch 46/50 - Train Loss: 0.025433 - Val Loss: 0.046343\n",
      "Epoch 47/50 - Train Loss: 0.034013 - Val Loss: 0.051918\n",
      "Epoch 48/50 - Train Loss: 0.034340 - Val Loss: 0.046438\n",
      "Epoch 49/50 - Train Loss: 0.027280 - Val Loss: 0.051373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:17:54,810] Trial 13 finished with value: 0.044356461614370346 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 222, 'lr': 0.0029612922000826587, 'weight_decay': 2.687518283406429e-08, 'batch_size': 32}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033429 - Val Loss: 0.044425\n",
      "Epoch 1/50 - Train Loss: 0.106645 - Val Loss: 0.063439\n",
      "Epoch 2/50 - Train Loss: 0.056236 - Val Loss: 0.084421\n",
      "Epoch 3/50 - Train Loss: 0.042260 - Val Loss: 0.074411\n",
      "Epoch 4/50 - Train Loss: 0.047096 - Val Loss: 0.060434\n",
      "Epoch 5/50 - Train Loss: 0.043240 - Val Loss: 0.056740\n",
      "Epoch 6/50 - Train Loss: 0.041153 - Val Loss: 0.068288\n",
      "Epoch 7/50 - Train Loss: 0.039672 - Val Loss: 0.048045\n",
      "Epoch 8/50 - Train Loss: 0.032809 - Val Loss: 0.064893\n",
      "Epoch 9/50 - Train Loss: 0.038323 - Val Loss: 0.048601\n",
      "Epoch 10/50 - Train Loss: 0.034694 - Val Loss: 0.046305\n",
      "Epoch 11/50 - Train Loss: 0.037024 - Val Loss: 0.061064\n",
      "Epoch 12/50 - Train Loss: 0.030671 - Val Loss: 0.046799\n",
      "Epoch 13/50 - Train Loss: 0.034164 - Val Loss: 0.051585\n",
      "Epoch 14/50 - Train Loss: 0.033397 - Val Loss: 0.044647\n",
      "Epoch 15/50 - Train Loss: 0.030461 - Val Loss: 0.052410\n",
      "Epoch 16/50 - Train Loss: 0.027846 - Val Loss: 0.045178\n",
      "Epoch 17/50 - Train Loss: 0.030939 - Val Loss: 0.053222\n",
      "Epoch 18/50 - Train Loss: 0.034535 - Val Loss: 0.045069\n",
      "Epoch 19/50 - Train Loss: 0.028553 - Val Loss: 0.049026\n",
      "Epoch 20/50 - Train Loss: 0.037624 - Val Loss: 0.044465\n",
      "Epoch 21/50 - Train Loss: 0.032177 - Val Loss: 0.053884\n",
      "Epoch 22/50 - Train Loss: 0.033608 - Val Loss: 0.044629\n",
      "Epoch 23/50 - Train Loss: 0.034020 - Val Loss: 0.051872\n",
      "Epoch 24/50 - Train Loss: 0.030766 - Val Loss: 0.050800\n",
      "Epoch 25/50 - Train Loss: 0.034858 - Val Loss: 0.044373\n",
      "Epoch 26/50 - Train Loss: 0.029593 - Val Loss: 0.052854\n",
      "Epoch 27/50 - Train Loss: 0.028042 - Val Loss: 0.044056\n",
      "Epoch 28/50 - Train Loss: 0.031687 - Val Loss: 0.046724\n",
      "Epoch 29/50 - Train Loss: 0.029614 - Val Loss: 0.047650\n",
      "Epoch 30/50 - Train Loss: 0.030710 - Val Loss: 0.045769\n",
      "Epoch 31/50 - Train Loss: 0.031140 - Val Loss: 0.049486\n",
      "Epoch 32/50 - Train Loss: 0.033963 - Val Loss: 0.044481\n",
      "Epoch 33/50 - Train Loss: 0.030825 - Val Loss: 0.055289\n",
      "Epoch 34/50 - Train Loss: 0.030240 - Val Loss: 0.044467\n",
      "Epoch 35/50 - Train Loss: 0.032263 - Val Loss: 0.048202\n",
      "Epoch 36/50 - Train Loss: 0.027240 - Val Loss: 0.044609\n",
      "Epoch 37/50 - Train Loss: 0.027754 - Val Loss: 0.043977\n",
      "Epoch 38/50 - Train Loss: 0.027661 - Val Loss: 0.047812\n",
      "Epoch 39/50 - Train Loss: 0.032760 - Val Loss: 0.044709\n",
      "Epoch 40/50 - Train Loss: 0.031241 - Val Loss: 0.052245\n",
      "Epoch 41/50 - Train Loss: 0.032338 - Val Loss: 0.044859\n",
      "Epoch 42/50 - Train Loss: 0.034592 - Val Loss: 0.048401\n",
      "Epoch 43/50 - Train Loss: 0.033488 - Val Loss: 0.047823\n",
      "Epoch 44/50 - Train Loss: 0.031504 - Val Loss: 0.048493\n",
      "Epoch 45/50 - Train Loss: 0.031613 - Val Loss: 0.044530\n",
      "Epoch 46/50 - Train Loss: 0.031193 - Val Loss: 0.048225\n",
      "Epoch 47/50 - Train Loss: 0.033123 - Val Loss: 0.047018\n",
      "Epoch 48/50 - Train Loss: 0.028076 - Val Loss: 0.044542\n",
      "Epoch 49/50 - Train Loss: 0.032384 - Val Loss: 0.051911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:18:09,518] Trial 14 finished with value: 0.04397721216082573 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 156, 'lr': 0.00963509105517008, 'weight_decay': 1.2742245997771509e-08, 'batch_size': 32}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028135 - Val Loss: 0.045509\n",
      "Epoch 1/50 - Train Loss: 0.160861 - Val Loss: 0.113569\n",
      "Epoch 2/50 - Train Loss: 0.110169 - Val Loss: 0.076483\n",
      "Epoch 3/50 - Train Loss: 0.046201 - Val Loss: 0.141027\n",
      "Epoch 4/50 - Train Loss: 0.058960 - Val Loss: 0.159285\n",
      "Epoch 5/50 - Train Loss: 0.063969 - Val Loss: 0.150420\n",
      "Epoch 6/50 - Train Loss: 0.059640 - Val Loss: 0.129852\n",
      "Epoch 7/50 - Train Loss: 0.050573 - Val Loss: 0.104243\n",
      "Epoch 8/50 - Train Loss: 0.049844 - Val Loss: 0.080724\n",
      "Epoch 9/50 - Train Loss: 0.044303 - Val Loss: 0.070102\n",
      "Epoch 10/50 - Train Loss: 0.055112 - Val Loss: 0.070963\n",
      "Epoch 11/50 - Train Loss: 0.051976 - Val Loss: 0.077540\n",
      "Epoch 12/50 - Train Loss: 0.049698 - Val Loss: 0.086930\n",
      "Epoch 13/50 - Train Loss: 0.051076 - Val Loss: 0.094868\n",
      "Epoch 14/50 - Train Loss: 0.048940 - Val Loss: 0.096696\n",
      "Epoch 15/50 - Train Loss: 0.050525 - Val Loss: 0.093835\n",
      "Epoch 16/50 - Train Loss: 0.042976 - Val Loss: 0.087040\n",
      "Epoch 17/50 - Train Loss: 0.039578 - Val Loss: 0.077976\n",
      "Epoch 18/50 - Train Loss: 0.038056 - Val Loss: 0.069941\n",
      "Epoch 19/50 - Train Loss: 0.037715 - Val Loss: 0.064541\n",
      "Epoch 20/50 - Train Loss: 0.044911 - Val Loss: 0.063574\n",
      "Epoch 21/50 - Train Loss: 0.042407 - Val Loss: 0.061299\n",
      "Epoch 22/50 - Train Loss: 0.038941 - Val Loss: 0.052839\n",
      "Epoch 23/50 - Train Loss: 0.030568 - Val Loss: 0.048164\n",
      "Epoch 24/50 - Train Loss: 0.039201 - Val Loss: 0.053876\n",
      "Epoch 25/50 - Train Loss: 0.030761 - Val Loss: 0.050942\n",
      "Epoch 26/50 - Train Loss: 0.036329 - Val Loss: 0.048384\n",
      "Epoch 27/50 - Train Loss: 0.029137 - Val Loss: 0.053386\n",
      "Epoch 28/50 - Train Loss: 0.035512 - Val Loss: 0.054411\n",
      "Epoch 29/50 - Train Loss: 0.028854 - Val Loss: 0.049842\n",
      "Epoch 30/50 - Train Loss: 0.035992 - Val Loss: 0.049716\n",
      "Epoch 31/50 - Train Loss: 0.036089 - Val Loss: 0.048850\n",
      "Epoch 32/50 - Train Loss: 0.035324 - Val Loss: 0.047248\n",
      "Epoch 33/50 - Train Loss: 0.028470 - Val Loss: 0.047637\n",
      "Epoch 34/50 - Train Loss: 0.035084 - Val Loss: 0.047996\n",
      "Epoch 35/50 - Train Loss: 0.027377 - Val Loss: 0.046853\n",
      "Epoch 36/50 - Train Loss: 0.034810 - Val Loss: 0.046415\n",
      "Epoch 37/50 - Train Loss: 0.034182 - Val Loss: 0.046800\n",
      "Epoch 38/50 - Train Loss: 0.027785 - Val Loss: 0.046598\n",
      "Epoch 39/50 - Train Loss: 0.035016 - Val Loss: 0.046956\n",
      "Epoch 40/50 - Train Loss: 0.034699 - Val Loss: 0.046335\n",
      "Epoch 41/50 - Train Loss: 0.027870 - Val Loss: 0.045630\n",
      "Epoch 42/50 - Train Loss: 0.035568 - Val Loss: 0.046650\n",
      "Epoch 43/50 - Train Loss: 0.027514 - Val Loss: 0.045199\n",
      "Epoch 44/50 - Train Loss: 0.027562 - Val Loss: 0.045227\n",
      "Epoch 45/50 - Train Loss: 0.027991 - Val Loss: 0.045724\n",
      "Epoch 46/50 - Train Loss: 0.027436 - Val Loss: 0.045615\n",
      "Epoch 47/50 - Train Loss: 0.026995 - Val Loss: 0.046885\n",
      "Epoch 48/50 - Train Loss: 0.034427 - Val Loss: 0.045899\n",
      "Epoch 49/50 - Train Loss: 0.028231 - Val Loss: 0.046589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:18:26,953] Trial 15 finished with value: 0.045199133455753326 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 231, 'lr': 0.0024168138978373803, 'weight_decay': 8.29660031586514e-08, 'batch_size': 64}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028928 - Val Loss: 0.045885\n",
      "Epoch 1/50 - Train Loss: 0.114474 - Val Loss: 0.081658\n",
      "Epoch 2/50 - Train Loss: 0.066418 - Val Loss: 0.096370\n",
      "Epoch 3/50 - Train Loss: 0.057471 - Val Loss: 0.119976\n",
      "Epoch 4/50 - Train Loss: 0.052343 - Val Loss: 0.089722\n",
      "Epoch 5/50 - Train Loss: 0.040172 - Val Loss: 0.077607\n",
      "Epoch 6/50 - Train Loss: 0.047553 - Val Loss: 0.089251\n",
      "Epoch 7/50 - Train Loss: 0.036596 - Val Loss: 0.090432\n",
      "Epoch 8/50 - Train Loss: 0.041868 - Val Loss: 0.081489\n",
      "Epoch 9/50 - Train Loss: 0.041465 - Val Loss: 0.078548\n",
      "Epoch 10/50 - Train Loss: 0.035249 - Val Loss: 0.069740\n",
      "Epoch 11/50 - Train Loss: 0.034213 - Val Loss: 0.065906\n",
      "Epoch 12/50 - Train Loss: 0.036017 - Val Loss: 0.057955\n",
      "Epoch 13/50 - Train Loss: 0.036981 - Val Loss: 0.057599\n",
      "Epoch 14/50 - Train Loss: 0.033782 - Val Loss: 0.050293\n",
      "Epoch 15/50 - Train Loss: 0.038714 - Val Loss: 0.055233\n",
      "Epoch 16/50 - Train Loss: 0.032964 - Val Loss: 0.050176\n",
      "Epoch 17/50 - Train Loss: 0.031842 - Val Loss: 0.049195\n",
      "Epoch 18/50 - Train Loss: 0.029770 - Val Loss: 0.049698\n",
      "Epoch 19/50 - Train Loss: 0.028714 - Val Loss: 0.047623\n",
      "Epoch 20/50 - Train Loss: 0.028363 - Val Loss: 0.048546\n",
      "Epoch 21/50 - Train Loss: 0.027766 - Val Loss: 0.046680\n",
      "Epoch 22/50 - Train Loss: 0.030028 - Val Loss: 0.052024\n",
      "Epoch 23/50 - Train Loss: 0.037390 - Val Loss: 0.050292\n",
      "Epoch 24/50 - Train Loss: 0.032074 - Val Loss: 0.048472\n",
      "Epoch 25/50 - Train Loss: 0.027813 - Val Loss: 0.053435\n",
      "Epoch 26/50 - Train Loss: 0.033198 - Val Loss: 0.047825\n",
      "Epoch 27/50 - Train Loss: 0.032299 - Val Loss: 0.051983\n",
      "Epoch 28/50 - Train Loss: 0.033652 - Val Loss: 0.049871\n",
      "Epoch 29/50 - Train Loss: 0.030867 - Val Loss: 0.048596\n",
      "Epoch 30/50 - Train Loss: 0.029696 - Val Loss: 0.049970\n",
      "Epoch 31/50 - Train Loss: 0.029165 - Val Loss: 0.050713\n",
      "Epoch 32/50 - Train Loss: 0.032827 - Val Loss: 0.050995\n",
      "Epoch 33/50 - Train Loss: 0.029908 - Val Loss: 0.052499\n",
      "Epoch 34/50 - Train Loss: 0.029636 - Val Loss: 0.048395\n",
      "Epoch 35/50 - Train Loss: 0.030552 - Val Loss: 0.051697\n",
      "Epoch 36/50 - Train Loss: 0.028026 - Val Loss: 0.046197\n",
      "Epoch 37/50 - Train Loss: 0.032943 - Val Loss: 0.052707\n",
      "Epoch 38/50 - Train Loss: 0.028415 - Val Loss: 0.046895\n",
      "Epoch 39/50 - Train Loss: 0.032543 - Val Loss: 0.052498\n",
      "Epoch 40/50 - Train Loss: 0.031498 - Val Loss: 0.047751\n",
      "Epoch 41/50 - Train Loss: 0.032998 - Val Loss: 0.049601\n",
      "Epoch 42/50 - Train Loss: 0.029874 - Val Loss: 0.047062\n",
      "Epoch 43/50 - Train Loss: 0.029947 - Val Loss: 0.050579\n",
      "Epoch 44/50 - Train Loss: 0.029203 - Val Loss: 0.046939\n",
      "Epoch 45/50 - Train Loss: 0.029375 - Val Loss: 0.051625\n",
      "Epoch 46/50 - Train Loss: 0.034168 - Val Loss: 0.047065\n",
      "Epoch 47/50 - Train Loss: 0.030988 - Val Loss: 0.049600\n",
      "Epoch 48/50 - Train Loss: 0.027187 - Val Loss: 0.049999\n",
      "Epoch 49/50 - Train Loss: 0.032510 - Val Loss: 0.051198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:18:44,687] Trial 16 finished with value: 0.04619685560464859 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 181, 'lr': 0.001514958873550394, 'weight_decay': 0.0008232013637216427, 'batch_size': 32}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030596 - Val Loss: 0.051722\n",
      "Epoch 1/50 - Train Loss: 0.273337 - Val Loss: 0.411913\n",
      "Epoch 2/50 - Train Loss: 0.217600 - Val Loss: 0.369602\n",
      "Epoch 3/50 - Train Loss: 0.184984 - Val Loss: 0.324725\n",
      "Epoch 4/50 - Train Loss: 0.142828 - Val Loss: 0.270122\n",
      "Epoch 5/50 - Train Loss: 0.112015 - Val Loss: 0.205156\n",
      "Epoch 6/50 - Train Loss: 0.072619 - Val Loss: 0.133883\n",
      "Epoch 7/50 - Train Loss: 0.048215 - Val Loss: 0.083261\n",
      "Epoch 8/50 - Train Loss: 0.061593 - Val Loss: 0.078065\n",
      "Epoch 9/50 - Train Loss: 0.046793 - Val Loss: 0.089476\n",
      "Epoch 10/50 - Train Loss: 0.052874 - Val Loss: 0.105186\n",
      "Epoch 11/50 - Train Loss: 0.045829 - Val Loss: 0.103844\n",
      "Epoch 12/50 - Train Loss: 0.046078 - Val Loss: 0.093865\n",
      "Epoch 13/50 - Train Loss: 0.043739 - Val Loss: 0.084530\n",
      "Epoch 14/50 - Train Loss: 0.042902 - Val Loss: 0.078150\n",
      "Epoch 15/50 - Train Loss: 0.043797 - Val Loss: 0.081075\n",
      "Epoch 16/50 - Train Loss: 0.043132 - Val Loss: 0.088599\n",
      "Epoch 17/50 - Train Loss: 0.032001 - Val Loss: 0.088020\n",
      "Epoch 18/50 - Train Loss: 0.038899 - Val Loss: 0.081464\n",
      "Epoch 19/50 - Train Loss: 0.038894 - Val Loss: 0.076279\n",
      "Epoch 20/50 - Train Loss: 0.039631 - Val Loss: 0.074481\n",
      "Epoch 21/50 - Train Loss: 0.035179 - Val Loss: 0.072191\n",
      "Epoch 22/50 - Train Loss: 0.042593 - Val Loss: 0.069607\n",
      "Epoch 23/50 - Train Loss: 0.035284 - Val Loss: 0.064936\n",
      "Epoch 24/50 - Train Loss: 0.037203 - Val Loss: 0.066586\n",
      "Epoch 25/50 - Train Loss: 0.037273 - Val Loss: 0.059642\n",
      "Epoch 26/50 - Train Loss: 0.037591 - Val Loss: 0.055029\n",
      "Epoch 27/50 - Train Loss: 0.034513 - Val Loss: 0.059212\n",
      "Epoch 28/50 - Train Loss: 0.032481 - Val Loss: 0.060422\n",
      "Epoch 29/50 - Train Loss: 0.033921 - Val Loss: 0.053915\n",
      "Epoch 30/50 - Train Loss: 0.032732 - Val Loss: 0.051184\n",
      "Epoch 31/50 - Train Loss: 0.033086 - Val Loss: 0.054720\n",
      "Epoch 32/50 - Train Loss: 0.033813 - Val Loss: 0.052210\n",
      "Epoch 33/50 - Train Loss: 0.033091 - Val Loss: 0.047756\n",
      "Epoch 34/50 - Train Loss: 0.032235 - Val Loss: 0.050548\n",
      "Epoch 35/50 - Train Loss: 0.028484 - Val Loss: 0.052060\n",
      "Epoch 36/50 - Train Loss: 0.027089 - Val Loss: 0.047669\n",
      "Epoch 37/50 - Train Loss: 0.034346 - Val Loss: 0.046427\n",
      "Epoch 38/50 - Train Loss: 0.028418 - Val Loss: 0.047504\n",
      "Epoch 39/50 - Train Loss: 0.032274 - Val Loss: 0.047486\n",
      "Epoch 40/50 - Train Loss: 0.037004 - Val Loss: 0.047053\n",
      "Epoch 41/50 - Train Loss: 0.031411 - Val Loss: 0.047559\n",
      "Epoch 42/50 - Train Loss: 0.028386 - Val Loss: 0.046214\n",
      "Epoch 43/50 - Train Loss: 0.027846 - Val Loss: 0.046257\n",
      "Epoch 44/50 - Train Loss: 0.033875 - Val Loss: 0.046483\n",
      "Epoch 45/50 - Train Loss: 0.032002 - Val Loss: 0.046692\n",
      "Epoch 46/50 - Train Loss: 0.036233 - Val Loss: 0.046151\n",
      "Epoch 47/50 - Train Loss: 0.030839 - Val Loss: 0.046218\n",
      "Epoch 48/50 - Train Loss: 0.029164 - Val Loss: 0.046695\n",
      "Epoch 49/50 - Train Loss: 0.028071 - Val Loss: 0.045756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:19:07,369] Trial 17 finished with value: 0.04494789242744446 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 218, 'lr': 0.00020874747591586099, 'weight_decay': 7.931650957189686e-06, 'batch_size': 32}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028450 - Val Loss: 0.044948\n",
      "Epoch 1/50 - Train Loss: 0.232878 - Val Loss: 0.246446\n",
      "Epoch 2/50 - Train Loss: 0.107008 - Val Loss: 0.136555\n",
      "Epoch 3/50 - Train Loss: 0.042338 - Val Loss: 0.073951\n",
      "Epoch 4/50 - Train Loss: 0.051597 - Val Loss: 0.125748\n",
      "Epoch 5/50 - Train Loss: 0.050944 - Val Loss: 0.084593\n",
      "Epoch 6/50 - Train Loss: 0.046208 - Val Loss: 0.056341\n",
      "Epoch 7/50 - Train Loss: 0.035594 - Val Loss: 0.069379\n",
      "Epoch 8/50 - Train Loss: 0.033191 - Val Loss: 0.048709\n",
      "Epoch 9/50 - Train Loss: 0.031609 - Val Loss: 0.053768\n",
      "Epoch 10/50 - Train Loss: 0.040271 - Val Loss: 0.053510\n",
      "Epoch 11/50 - Train Loss: 0.037352 - Val Loss: 0.046889\n",
      "Epoch 12/50 - Train Loss: 0.032639 - Val Loss: 0.052629\n",
      "Epoch 13/50 - Train Loss: 0.031809 - Val Loss: 0.045349\n",
      "Epoch 14/50 - Train Loss: 0.032409 - Val Loss: 0.050298\n",
      "Epoch 15/50 - Train Loss: 0.031257 - Val Loss: 0.046541\n",
      "Epoch 16/50 - Train Loss: 0.029380 - Val Loss: 0.046083\n",
      "Epoch 17/50 - Train Loss: 0.038563 - Val Loss: 0.051694\n",
      "Epoch 18/50 - Train Loss: 0.032336 - Val Loss: 0.045362\n",
      "Epoch 19/50 - Train Loss: 0.028136 - Val Loss: 0.051285\n",
      "Epoch 20/50 - Train Loss: 0.032123 - Val Loss: 0.047973\n",
      "Epoch 21/50 - Train Loss: 0.027075 - Val Loss: 0.045153\n",
      "Epoch 22/50 - Train Loss: 0.030620 - Val Loss: 0.050048\n",
      "Epoch 23/50 - Train Loss: 0.031199 - Val Loss: 0.045471\n",
      "Epoch 24/50 - Train Loss: 0.030669 - Val Loss: 0.049376\n",
      "Epoch 25/50 - Train Loss: 0.026864 - Val Loss: 0.045992\n",
      "Epoch 26/50 - Train Loss: 0.026194 - Val Loss: 0.045338\n",
      "Epoch 27/50 - Train Loss: 0.029407 - Val Loss: 0.046749\n",
      "Epoch 28/50 - Train Loss: 0.040564 - Val Loss: 0.049060\n",
      "Epoch 29/50 - Train Loss: 0.032609 - Val Loss: 0.047205\n",
      "Epoch 30/50 - Train Loss: 0.025974 - Val Loss: 0.054811\n",
      "Epoch 31/50 - Train Loss: 0.034062 - Val Loss: 0.045291\n",
      "Epoch 32/50 - Train Loss: 0.028257 - Val Loss: 0.048316\n",
      "Epoch 33/50 - Train Loss: 0.029648 - Val Loss: 0.046846\n",
      "Epoch 34/50 - Train Loss: 0.033794 - Val Loss: 0.047112\n",
      "Epoch 35/50 - Train Loss: 0.032197 - Val Loss: 0.050628\n",
      "Epoch 36/50 - Train Loss: 0.028022 - Val Loss: 0.045860\n",
      "Epoch 37/50 - Train Loss: 0.027000 - Val Loss: 0.048449\n",
      "Epoch 38/50 - Train Loss: 0.033804 - Val Loss: 0.045547\n",
      "Epoch 39/50 - Train Loss: 0.026851 - Val Loss: 0.046393\n",
      "Epoch 40/50 - Train Loss: 0.034267 - Val Loss: 0.047049\n",
      "Epoch 41/50 - Train Loss: 0.030416 - Val Loss: 0.046298\n",
      "Epoch 42/50 - Train Loss: 0.031895 - Val Loss: 0.049721\n",
      "Epoch 43/50 - Train Loss: 0.029931 - Val Loss: 0.047493\n",
      "Epoch 44/50 - Train Loss: 0.031508 - Val Loss: 0.051624\n",
      "Epoch 45/50 - Train Loss: 0.027400 - Val Loss: 0.046755\n",
      "Epoch 46/50 - Train Loss: 0.033400 - Val Loss: 0.049613\n",
      "Epoch 47/50 - Train Loss: 0.031763 - Val Loss: 0.047265\n",
      "Epoch 48/50 - Train Loss: 0.026178 - Val Loss: 0.046496\n",
      "Epoch 49/50 - Train Loss: 0.032333 - Val Loss: 0.046891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:19:21,385] Trial 18 finished with value: 0.045153312385082245 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 240, 'lr': 0.006306901480195777, 'weight_decay': 2.95497377626835e-07, 'batch_size': 32}. Best is trial 11 with value: 0.04335350915789604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.024997 - Val Loss: 0.046162\n",
      "Epoch 1/50 - Train Loss: 0.123827 - Val Loss: 0.157196\n",
      "Epoch 2/50 - Train Loss: 0.063282 - Val Loss: 0.078545\n",
      "Epoch 3/50 - Train Loss: 0.063629 - Val Loss: 0.073890\n",
      "Epoch 4/50 - Train Loss: 0.045832 - Val Loss: 0.086685\n",
      "Epoch 5/50 - Train Loss: 0.050063 - Val Loss: 0.101847\n",
      "Epoch 6/50 - Train Loss: 0.043739 - Val Loss: 0.102495\n",
      "Epoch 7/50 - Train Loss: 0.050462 - Val Loss: 0.090751\n",
      "Epoch 8/50 - Train Loss: 0.047317 - Val Loss: 0.076040\n",
      "Epoch 9/50 - Train Loss: 0.039644 - Val Loss: 0.065429\n",
      "Epoch 10/50 - Train Loss: 0.040723 - Val Loss: 0.064082\n",
      "Epoch 11/50 - Train Loss: 0.038696 - Val Loss: 0.070911\n",
      "Epoch 12/50 - Train Loss: 0.044421 - Val Loss: 0.077144\n",
      "Epoch 13/50 - Train Loss: 0.036883 - Val Loss: 0.071511\n",
      "Epoch 14/50 - Train Loss: 0.034249 - Val Loss: 0.060316\n",
      "Epoch 15/50 - Train Loss: 0.041965 - Val Loss: 0.053269\n",
      "Epoch 16/50 - Train Loss: 0.040033 - Val Loss: 0.052332\n",
      "Epoch 17/50 - Train Loss: 0.032030 - Val Loss: 0.051733\n",
      "Epoch 18/50 - Train Loss: 0.031164 - Val Loss: 0.048388\n",
      "Epoch 19/50 - Train Loss: 0.037978 - Val Loss: 0.047618\n",
      "Epoch 20/50 - Train Loss: 0.030577 - Val Loss: 0.047718\n",
      "Epoch 21/50 - Train Loss: 0.037679 - Val Loss: 0.046884\n",
      "Epoch 22/50 - Train Loss: 0.037917 - Val Loss: 0.047309\n",
      "Epoch 23/50 - Train Loss: 0.036315 - Val Loss: 0.048370\n",
      "Epoch 24/50 - Train Loss: 0.036279 - Val Loss: 0.049162\n",
      "Epoch 25/50 - Train Loss: 0.035959 - Val Loss: 0.048440\n",
      "Epoch 26/50 - Train Loss: 0.035322 - Val Loss: 0.049027\n",
      "Epoch 27/50 - Train Loss: 0.028180 - Val Loss: 0.046639\n",
      "Epoch 28/50 - Train Loss: 0.028398 - Val Loss: 0.044281\n",
      "Epoch 29/50 - Train Loss: 0.028551 - Val Loss: 0.045863\n",
      "Epoch 30/50 - Train Loss: 0.036919 - Val Loss: 0.044931\n",
      "Epoch 31/50 - Train Loss: 0.028011 - Val Loss: 0.044603\n",
      "Epoch 32/50 - Train Loss: 0.036071 - Val Loss: 0.046976\n",
      "Epoch 33/50 - Train Loss: 0.028232 - Val Loss: 0.046538\n",
      "Epoch 34/50 - Train Loss: 0.034709 - Val Loss: 0.045275\n",
      "Epoch 35/50 - Train Loss: 0.035950 - Val Loss: 0.046291\n",
      "Epoch 36/50 - Train Loss: 0.036120 - Val Loss: 0.044904\n",
      "Epoch 37/50 - Train Loss: 0.034750 - Val Loss: 0.043850\n",
      "Epoch 38/50 - Train Loss: 0.028306 - Val Loss: 0.044999\n",
      "Epoch 39/50 - Train Loss: 0.035185 - Val Loss: 0.044908\n",
      "Epoch 40/50 - Train Loss: 0.034469 - Val Loss: 0.044302\n",
      "Epoch 41/50 - Train Loss: 0.035464 - Val Loss: 0.045753\n",
      "Epoch 42/50 - Train Loss: 0.035160 - Val Loss: 0.044900\n",
      "Epoch 43/50 - Train Loss: 0.034752 - Val Loss: 0.044774\n",
      "Epoch 44/50 - Train Loss: 0.034088 - Val Loss: 0.045402\n",
      "Epoch 45/50 - Train Loss: 0.027659 - Val Loss: 0.043589\n",
      "Epoch 46/50 - Train Loss: 0.027873 - Val Loss: 0.043235\n",
      "Epoch 47/50 - Train Loss: 0.034657 - Val Loss: 0.044008\n",
      "Epoch 48/50 - Train Loss: 0.034207 - Val Loss: 0.043683\n",
      "Epoch 49/50 - Train Loss: 0.035387 - Val Loss: 0.045253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:19:34,791] Trial 19 finished with value: 0.043235134333372116 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 175, 'lr': 0.001396297753768469, 'weight_decay': 3.19683036812383e-08, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.034249 - Val Loss: 0.046667\n",
      "Epoch 1/50 - Train Loss: 0.160566 - Val Loss: 0.249786\n",
      "Epoch 2/50 - Train Loss: 0.116265 - Val Loss: 0.148080\n",
      "Epoch 3/50 - Train Loss: 0.060889 - Val Loss: 0.076406\n",
      "Epoch 4/50 - Train Loss: 0.056019 - Val Loss: 0.071909\n",
      "Epoch 5/50 - Train Loss: 0.066895 - Val Loss: 0.079529\n",
      "Epoch 6/50 - Train Loss: 0.049100 - Val Loss: 0.100803\n",
      "Epoch 7/50 - Train Loss: 0.052254 - Val Loss: 0.112658\n",
      "Epoch 8/50 - Train Loss: 0.055468 - Val Loss: 0.108463\n",
      "Epoch 9/50 - Train Loss: 0.043930 - Val Loss: 0.094293\n",
      "Epoch 10/50 - Train Loss: 0.041523 - Val Loss: 0.078599\n",
      "Epoch 11/50 - Train Loss: 0.046816 - Val Loss: 0.067960\n",
      "Epoch 12/50 - Train Loss: 0.041411 - Val Loss: 0.064447\n",
      "Epoch 13/50 - Train Loss: 0.047359 - Val Loss: 0.068106\n",
      "Epoch 14/50 - Train Loss: 0.045488 - Val Loss: 0.074740\n",
      "Epoch 15/50 - Train Loss: 0.044680 - Val Loss: 0.077466\n",
      "Epoch 16/50 - Train Loss: 0.037311 - Val Loss: 0.073942\n",
      "Epoch 17/50 - Train Loss: 0.043728 - Val Loss: 0.064308\n",
      "Epoch 18/50 - Train Loss: 0.034098 - Val Loss: 0.054103\n",
      "Epoch 19/50 - Train Loss: 0.041134 - Val Loss: 0.050354\n",
      "Epoch 20/50 - Train Loss: 0.039706 - Val Loss: 0.052729\n",
      "Epoch 21/50 - Train Loss: 0.039844 - Val Loss: 0.056834\n",
      "Epoch 22/50 - Train Loss: 0.039882 - Val Loss: 0.053563\n",
      "Epoch 23/50 - Train Loss: 0.038436 - Val Loss: 0.048253\n",
      "Epoch 24/50 - Train Loss: 0.038041 - Val Loss: 0.046633\n",
      "Epoch 25/50 - Train Loss: 0.038860 - Val Loss: 0.047317\n",
      "Epoch 26/50 - Train Loss: 0.037680 - Val Loss: 0.048638\n",
      "Epoch 27/50 - Train Loss: 0.029919 - Val Loss: 0.046380\n",
      "Epoch 28/50 - Train Loss: 0.037151 - Val Loss: 0.046036\n",
      "Epoch 29/50 - Train Loss: 0.029178 - Val Loss: 0.046713\n",
      "Epoch 30/50 - Train Loss: 0.028852 - Val Loss: 0.046365\n",
      "Epoch 31/50 - Train Loss: 0.027980 - Val Loss: 0.045576\n",
      "Epoch 32/50 - Train Loss: 0.035962 - Val Loss: 0.045902\n",
      "Epoch 33/50 - Train Loss: 0.034747 - Val Loss: 0.046611\n",
      "Epoch 34/50 - Train Loss: 0.036206 - Val Loss: 0.046514\n",
      "Epoch 35/50 - Train Loss: 0.034635 - Val Loss: 0.046913\n",
      "Epoch 36/50 - Train Loss: 0.036111 - Val Loss: 0.048116\n",
      "Epoch 37/50 - Train Loss: 0.035072 - Val Loss: 0.047523\n",
      "Epoch 38/50 - Train Loss: 0.028160 - Val Loss: 0.046753\n",
      "Epoch 39/50 - Train Loss: 0.027808 - Val Loss: 0.046397\n",
      "Epoch 40/50 - Train Loss: 0.035790 - Val Loss: 0.046561\n",
      "Epoch 41/50 - Train Loss: 0.035745 - Val Loss: 0.047307\n",
      "Epoch 42/50 - Train Loss: 0.027661 - Val Loss: 0.048151\n",
      "Epoch 43/50 - Train Loss: 0.035902 - Val Loss: 0.047511\n",
      "Epoch 44/50 - Train Loss: 0.035098 - Val Loss: 0.047222\n",
      "Epoch 45/50 - Train Loss: 0.029263 - Val Loss: 0.047193\n",
      "Epoch 46/50 - Train Loss: 0.035038 - Val Loss: 0.046691\n",
      "Epoch 47/50 - Train Loss: 0.035136 - Val Loss: 0.046574\n",
      "Epoch 48/50 - Train Loss: 0.028143 - Val Loss: 0.045715\n",
      "Epoch 49/50 - Train Loss: 0.027741 - Val Loss: 0.045711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:19:46,811] Trial 20 finished with value: 0.04557643085718155 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 131, 'lr': 0.001541925800580751, 'weight_decay': 4.9897101199617875e-08, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.035100 - Val Loss: 0.047347\n",
      "Epoch 1/50 - Train Loss: 0.180174 - Val Loss: 0.081296\n",
      "Epoch 2/50 - Train Loss: 0.065099 - Val Loss: 0.135123\n",
      "Epoch 3/50 - Train Loss: 0.053916 - Val Loss: 0.124266\n",
      "Epoch 4/50 - Train Loss: 0.055673 - Val Loss: 0.082335\n",
      "Epoch 5/50 - Train Loss: 0.041926 - Val Loss: 0.065716\n",
      "Epoch 6/50 - Train Loss: 0.055314 - Val Loss: 0.078377\n",
      "Epoch 7/50 - Train Loss: 0.048444 - Val Loss: 0.096441\n",
      "Epoch 8/50 - Train Loss: 0.048706 - Val Loss: 0.094661\n",
      "Epoch 9/50 - Train Loss: 0.049458 - Val Loss: 0.082837\n",
      "Epoch 10/50 - Train Loss: 0.037654 - Val Loss: 0.069395\n",
      "Epoch 11/50 - Train Loss: 0.046316 - Val Loss: 0.063155\n",
      "Epoch 12/50 - Train Loss: 0.035719 - Val Loss: 0.065633\n",
      "Epoch 13/50 - Train Loss: 0.042273 - Val Loss: 0.063307\n",
      "Epoch 14/50 - Train Loss: 0.033379 - Val Loss: 0.052166\n",
      "Epoch 15/50 - Train Loss: 0.032664 - Val Loss: 0.054417\n",
      "Epoch 16/50 - Train Loss: 0.031380 - Val Loss: 0.054335\n",
      "Epoch 17/50 - Train Loss: 0.031072 - Val Loss: 0.050067\n",
      "Epoch 18/50 - Train Loss: 0.030510 - Val Loss: 0.053356\n",
      "Epoch 19/50 - Train Loss: 0.029640 - Val Loss: 0.050601\n",
      "Epoch 20/50 - Train Loss: 0.029092 - Val Loss: 0.049510\n",
      "Epoch 21/50 - Train Loss: 0.028976 - Val Loss: 0.048631\n",
      "Epoch 22/50 - Train Loss: 0.035800 - Val Loss: 0.048014\n",
      "Epoch 23/50 - Train Loss: 0.036422 - Val Loss: 0.048218\n",
      "Epoch 24/50 - Train Loss: 0.035381 - Val Loss: 0.047977\n",
      "Epoch 25/50 - Train Loss: 0.028141 - Val Loss: 0.048456\n",
      "Epoch 26/50 - Train Loss: 0.035226 - Val Loss: 0.049257\n",
      "Epoch 27/50 - Train Loss: 0.028636 - Val Loss: 0.046056\n",
      "Epoch 28/50 - Train Loss: 0.035197 - Val Loss: 0.048394\n",
      "Epoch 29/50 - Train Loss: 0.028782 - Val Loss: 0.047608\n",
      "Epoch 30/50 - Train Loss: 0.035565 - Val Loss: 0.046715\n",
      "Epoch 31/50 - Train Loss: 0.035074 - Val Loss: 0.049954\n",
      "Epoch 32/50 - Train Loss: 0.035989 - Val Loss: 0.047111\n",
      "Epoch 33/50 - Train Loss: 0.035059 - Val Loss: 0.046339\n",
      "Epoch 34/50 - Train Loss: 0.035700 - Val Loss: 0.046243\n",
      "Epoch 35/50 - Train Loss: 0.027929 - Val Loss: 0.045204\n",
      "Epoch 36/50 - Train Loss: 0.027745 - Val Loss: 0.044953\n",
      "Epoch 37/50 - Train Loss: 0.034526 - Val Loss: 0.048394\n",
      "Epoch 38/50 - Train Loss: 0.028735 - Val Loss: 0.045087\n",
      "Epoch 39/50 - Train Loss: 0.035099 - Val Loss: 0.045404\n",
      "Epoch 40/50 - Train Loss: 0.027392 - Val Loss: 0.046863\n",
      "Epoch 41/50 - Train Loss: 0.026983 - Val Loss: 0.044784\n",
      "Epoch 42/50 - Train Loss: 0.035528 - Val Loss: 0.046267\n",
      "Epoch 43/50 - Train Loss: 0.027562 - Val Loss: 0.046442\n",
      "Epoch 44/50 - Train Loss: 0.028087 - Val Loss: 0.045864\n",
      "Epoch 45/50 - Train Loss: 0.034249 - Val Loss: 0.047941\n",
      "Epoch 46/50 - Train Loss: 0.028968 - Val Loss: 0.045189\n",
      "Epoch 47/50 - Train Loss: 0.034814 - Val Loss: 0.047432\n",
      "Epoch 48/50 - Train Loss: 0.034833 - Val Loss: 0.046720\n",
      "Epoch 49/50 - Train Loss: 0.033724 - Val Loss: 0.045869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:20:01,610] Trial 21 finished with value: 0.04478438198566437 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 175, 'lr': 0.004360256738912261, 'weight_decay': 2.374929493452936e-08, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027879 - Val Loss: 0.047590\n",
      "Epoch 1/50 - Train Loss: 0.149889 - Val Loss: 0.139997\n",
      "Epoch 2/50 - Train Loss: 0.048699 - Val Loss: 0.068375\n",
      "Epoch 3/50 - Train Loss: 0.064458 - Val Loss: 0.088941\n",
      "Epoch 4/50 - Train Loss: 0.042014 - Val Loss: 0.111433\n",
      "Epoch 5/50 - Train Loss: 0.046685 - Val Loss: 0.104722\n",
      "Epoch 6/50 - Train Loss: 0.050744 - Val Loss: 0.080024\n",
      "Epoch 7/50 - Train Loss: 0.046616 - Val Loss: 0.063647\n",
      "Epoch 8/50 - Train Loss: 0.047652 - Val Loss: 0.064979\n",
      "Epoch 9/50 - Train Loss: 0.037310 - Val Loss: 0.070180\n",
      "Epoch 10/50 - Train Loss: 0.041375 - Val Loss: 0.068753\n",
      "Epoch 11/50 - Train Loss: 0.041996 - Val Loss: 0.060418\n",
      "Epoch 12/50 - Train Loss: 0.032672 - Val Loss: 0.051085\n",
      "Epoch 13/50 - Train Loss: 0.032543 - Val Loss: 0.049834\n",
      "Epoch 14/50 - Train Loss: 0.038847 - Val Loss: 0.053250\n",
      "Epoch 15/50 - Train Loss: 0.029931 - Val Loss: 0.048065\n",
      "Epoch 16/50 - Train Loss: 0.038763 - Val Loss: 0.046352\n",
      "Epoch 17/50 - Train Loss: 0.037086 - Val Loss: 0.046793\n",
      "Epoch 18/50 - Train Loss: 0.030353 - Val Loss: 0.045713\n",
      "Epoch 19/50 - Train Loss: 0.036871 - Val Loss: 0.047045\n",
      "Epoch 20/50 - Train Loss: 0.028931 - Val Loss: 0.046833\n",
      "Epoch 21/50 - Train Loss: 0.036177 - Val Loss: 0.046343\n",
      "Epoch 22/50 - Train Loss: 0.028493 - Val Loss: 0.046085\n",
      "Epoch 23/50 - Train Loss: 0.036307 - Val Loss: 0.046666\n",
      "Epoch 24/50 - Train Loss: 0.028532 - Val Loss: 0.045114\n",
      "Epoch 25/50 - Train Loss: 0.028721 - Val Loss: 0.046676\n",
      "Epoch 26/50 - Train Loss: 0.036369 - Val Loss: 0.047855\n",
      "Epoch 27/50 - Train Loss: 0.036920 - Val Loss: 0.046847\n",
      "Epoch 28/50 - Train Loss: 0.029310 - Val Loss: 0.048706\n",
      "Epoch 29/50 - Train Loss: 0.028370 - Val Loss: 0.047864\n",
      "Epoch 30/50 - Train Loss: 0.028242 - Val Loss: 0.048016\n",
      "Epoch 31/50 - Train Loss: 0.027900 - Val Loss: 0.044370\n",
      "Epoch 32/50 - Train Loss: 0.028763 - Val Loss: 0.046114\n",
      "Epoch 33/50 - Train Loss: 0.029627 - Val Loss: 0.046653\n",
      "Epoch 34/50 - Train Loss: 0.027830 - Val Loss: 0.044470\n",
      "Epoch 35/50 - Train Loss: 0.028352 - Val Loss: 0.046237\n",
      "Epoch 36/50 - Train Loss: 0.034359 - Val Loss: 0.051748\n",
      "Epoch 37/50 - Train Loss: 0.028238 - Val Loss: 0.045220\n",
      "Epoch 38/50 - Train Loss: 0.029163 - Val Loss: 0.044523\n",
      "Epoch 39/50 - Train Loss: 0.034879 - Val Loss: 0.052407\n",
      "Epoch 40/50 - Train Loss: 0.036103 - Val Loss: 0.046556\n",
      "Epoch 41/50 - Train Loss: 0.027703 - Val Loss: 0.044477\n",
      "Epoch 42/50 - Train Loss: 0.036875 - Val Loss: 0.048927\n",
      "Epoch 43/50 - Train Loss: 0.029920 - Val Loss: 0.054413\n",
      "Epoch 44/50 - Train Loss: 0.035914 - Val Loss: 0.044960\n",
      "Epoch 45/50 - Train Loss: 0.028340 - Val Loss: 0.044257\n",
      "Epoch 46/50 - Train Loss: 0.034908 - Val Loss: 0.050181\n",
      "Epoch 47/50 - Train Loss: 0.036048 - Val Loss: 0.050560\n",
      "Epoch 48/50 - Train Loss: 0.028025 - Val Loss: 0.044107\n",
      "Epoch 49/50 - Train Loss: 0.028516 - Val Loss: 0.044062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:20:17,624] Trial 22 finished with value: 0.04406175762414932 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 255, 'lr': 0.0015425597820390262, 'weight_decay': 1.024899228485138e-08, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028219 - Val Loss: 0.047881\n",
      "Epoch 1/50 - Train Loss: 0.187037 - Val Loss: 2.360390\n",
      "Epoch 2/50 - Train Loss: 1.518653 - Val Loss: 0.231681\n",
      "Epoch 3/50 - Train Loss: 0.136413 - Val Loss: 0.311998\n",
      "Epoch 4/50 - Train Loss: 0.177701 - Val Loss: 0.321429\n",
      "Epoch 5/50 - Train Loss: 0.154276 - Val Loss: 0.297784\n",
      "Epoch 6/50 - Train Loss: 0.154768 - Val Loss: 0.242993\n",
      "Epoch 7/50 - Train Loss: 0.113493 - Val Loss: 0.141419\n",
      "Epoch 8/50 - Train Loss: 0.051321 - Val Loss: 0.071975\n",
      "Epoch 9/50 - Train Loss: 0.060855 - Val Loss: 0.076874\n",
      "Epoch 10/50 - Train Loss: 0.043015 - Val Loss: 0.120491\n",
      "Epoch 11/50 - Train Loss: 0.048717 - Val Loss: 0.121633\n",
      "Epoch 12/50 - Train Loss: 0.048372 - Val Loss: 0.096110\n",
      "Epoch 13/50 - Train Loss: 0.048676 - Val Loss: 0.073413\n",
      "Epoch 14/50 - Train Loss: 0.051462 - Val Loss: 0.072872\n",
      "Epoch 15/50 - Train Loss: 0.049134 - Val Loss: 0.084054\n",
      "Epoch 16/50 - Train Loss: 0.049138 - Val Loss: 0.085443\n",
      "Epoch 17/50 - Train Loss: 0.046303 - Val Loss: 0.069883\n",
      "Epoch 18/50 - Train Loss: 0.045956 - Val Loss: 0.069094\n",
      "Epoch 19/50 - Train Loss: 0.035293 - Val Loss: 0.060427\n",
      "Epoch 20/50 - Train Loss: 0.033189 - Val Loss: 0.052453\n",
      "Epoch 21/50 - Train Loss: 0.032080 - Val Loss: 0.053002\n",
      "Epoch 22/50 - Train Loss: 0.031780 - Val Loss: 0.048899\n",
      "Epoch 23/50 - Train Loss: 0.039089 - Val Loss: 0.057760\n",
      "Epoch 24/50 - Train Loss: 0.032195 - Val Loss: 0.054519\n",
      "Epoch 25/50 - Train Loss: 0.037360 - Val Loss: 0.048351\n",
      "Epoch 26/50 - Train Loss: 0.037503 - Val Loss: 0.052913\n",
      "Epoch 27/50 - Train Loss: 0.036731 - Val Loss: 0.048994\n",
      "Epoch 28/50 - Train Loss: 0.037017 - Val Loss: 0.047632\n",
      "Epoch 29/50 - Train Loss: 0.035999 - Val Loss: 0.048330\n",
      "Epoch 30/50 - Train Loss: 0.027742 - Val Loss: 0.048063\n",
      "Epoch 31/50 - Train Loss: 0.028960 - Val Loss: 0.047572\n",
      "Epoch 32/50 - Train Loss: 0.027263 - Val Loss: 0.045537\n",
      "Epoch 33/50 - Train Loss: 0.028766 - Val Loss: 0.048502\n",
      "Epoch 34/50 - Train Loss: 0.029782 - Val Loss: 0.047387\n",
      "Epoch 35/50 - Train Loss: 0.028692 - Val Loss: 0.044977\n",
      "Epoch 36/50 - Train Loss: 0.028381 - Val Loss: 0.050962\n",
      "Epoch 37/50 - Train Loss: 0.036526 - Val Loss: 0.047121\n",
      "Epoch 38/50 - Train Loss: 0.028302 - Val Loss: 0.044952\n",
      "Epoch 39/50 - Train Loss: 0.037320 - Val Loss: 0.052118\n",
      "Epoch 40/50 - Train Loss: 0.028562 - Val Loss: 0.047923\n",
      "Epoch 41/50 - Train Loss: 0.035441 - Val Loss: 0.045876\n",
      "Epoch 42/50 - Train Loss: 0.028181 - Val Loss: 0.048162\n",
      "Epoch 43/50 - Train Loss: 0.028554 - Val Loss: 0.046576\n",
      "Epoch 44/50 - Train Loss: 0.035021 - Val Loss: 0.045054\n",
      "Epoch 45/50 - Train Loss: 0.035254 - Val Loss: 0.048921\n",
      "Epoch 46/50 - Train Loss: 0.028613 - Val Loss: 0.047029\n",
      "Epoch 47/50 - Train Loss: 0.035293 - Val Loss: 0.045946\n",
      "Epoch 48/50 - Train Loss: 0.027719 - Val Loss: 0.046680\n",
      "Epoch 49/50 - Train Loss: 0.027562 - Val Loss: 0.044906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:20:35,061] Trial 23 finished with value: 0.04490617662668228 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 233, 'lr': 0.005525686024843109, 'weight_decay': 1.6312983503733066e-07, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.035467 - Val Loss: 0.046681\n",
      "Epoch 1/50 - Train Loss: 0.209163 - Val Loss: 0.341221\n",
      "Epoch 2/50 - Train Loss: 0.152109 - Val Loss: 0.234839\n",
      "Epoch 3/50 - Train Loss: 0.084242 - Val Loss: 0.115292\n",
      "Epoch 4/50 - Train Loss: 0.053299 - Val Loss: 0.075579\n",
      "Epoch 5/50 - Train Loss: 0.064899 - Val Loss: 0.079745\n",
      "Epoch 6/50 - Train Loss: 0.044859 - Val Loss: 0.102448\n",
      "Epoch 7/50 - Train Loss: 0.050212 - Val Loss: 0.106092\n",
      "Epoch 8/50 - Train Loss: 0.044111 - Val Loss: 0.089976\n",
      "Epoch 9/50 - Train Loss: 0.043300 - Val Loss: 0.075247\n",
      "Epoch 10/50 - Train Loss: 0.043865 - Val Loss: 0.074457\n",
      "Epoch 11/50 - Train Loss: 0.040954 - Val Loss: 0.079669\n",
      "Epoch 12/50 - Train Loss: 0.040898 - Val Loss: 0.081872\n",
      "Epoch 13/50 - Train Loss: 0.042411 - Val Loss: 0.069849\n",
      "Epoch 14/50 - Train Loss: 0.036364 - Val Loss: 0.061072\n",
      "Epoch 15/50 - Train Loss: 0.035057 - Val Loss: 0.061116\n",
      "Epoch 16/50 - Train Loss: 0.037469 - Val Loss: 0.069406\n",
      "Epoch 17/50 - Train Loss: 0.037751 - Val Loss: 0.062009\n",
      "Epoch 18/50 - Train Loss: 0.031250 - Val Loss: 0.053261\n",
      "Epoch 19/50 - Train Loss: 0.035297 - Val Loss: 0.053963\n",
      "Epoch 20/50 - Train Loss: 0.033973 - Val Loss: 0.058248\n",
      "Epoch 21/50 - Train Loss: 0.034846 - Val Loss: 0.052313\n",
      "Epoch 22/50 - Train Loss: 0.029040 - Val Loss: 0.047901\n",
      "Epoch 23/50 - Train Loss: 0.033775 - Val Loss: 0.050972\n",
      "Epoch 24/50 - Train Loss: 0.030205 - Val Loss: 0.051116\n",
      "Epoch 25/50 - Train Loss: 0.032775 - Val Loss: 0.045973\n",
      "Epoch 26/50 - Train Loss: 0.038134 - Val Loss: 0.045639\n",
      "Epoch 27/50 - Train Loss: 0.034593 - Val Loss: 0.045904\n",
      "Epoch 28/50 - Train Loss: 0.031589 - Val Loss: 0.047156\n",
      "Epoch 29/50 - Train Loss: 0.028735 - Val Loss: 0.045231\n",
      "Epoch 30/50 - Train Loss: 0.031601 - Val Loss: 0.044324\n",
      "Epoch 31/50 - Train Loss: 0.030614 - Val Loss: 0.048029\n",
      "Epoch 32/50 - Train Loss: 0.028622 - Val Loss: 0.045210\n",
      "Epoch 33/50 - Train Loss: 0.032013 - Val Loss: 0.043960\n",
      "Epoch 34/50 - Train Loss: 0.031957 - Val Loss: 0.045161\n",
      "Epoch 35/50 - Train Loss: 0.034111 - Val Loss: 0.046576\n",
      "Epoch 36/50 - Train Loss: 0.035656 - Val Loss: 0.044086\n",
      "Epoch 37/50 - Train Loss: 0.030489 - Val Loss: 0.045722\n",
      "Epoch 38/50 - Train Loss: 0.035079 - Val Loss: 0.047897\n",
      "Epoch 39/50 - Train Loss: 0.031078 - Val Loss: 0.044943\n",
      "Epoch 40/50 - Train Loss: 0.029234 - Val Loss: 0.045157\n",
      "Epoch 41/50 - Train Loss: 0.030076 - Val Loss: 0.045839\n",
      "Epoch 42/50 - Train Loss: 0.031455 - Val Loss: 0.045788\n",
      "Epoch 43/50 - Train Loss: 0.032648 - Val Loss: 0.045339\n",
      "Epoch 44/50 - Train Loss: 0.032953 - Val Loss: 0.044735\n",
      "Epoch 45/50 - Train Loss: 0.030792 - Val Loss: 0.045728\n",
      "Epoch 46/50 - Train Loss: 0.031926 - Val Loss: 0.045452\n",
      "Epoch 47/50 - Train Loss: 0.029674 - Val Loss: 0.044617\n",
      "Epoch 48/50 - Train Loss: 0.028758 - Val Loss: 0.046267\n",
      "Epoch 49/50 - Train Loss: 0.032255 - Val Loss: 0.044869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:20:53,778] Trial 24 finished with value: 0.04395986720919609 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 208, 'lr': 0.00040444964546919977, 'weight_decay': 2.943311064277932e-08, 'batch_size': 32}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.032628 - Val Loss: 0.044875\n",
      "Epoch 1/50 - Train Loss: 0.154112 - Val Loss: 0.294286\n",
      "Epoch 2/50 - Train Loss: 0.145246 - Val Loss: 0.263267\n",
      "Epoch 3/50 - Train Loss: 0.116230 - Val Loss: 0.233497\n",
      "Epoch 4/50 - Train Loss: 0.112617 - Val Loss: 0.204692\n",
      "Epoch 5/50 - Train Loss: 0.090131 - Val Loss: 0.175402\n",
      "Epoch 6/50 - Train Loss: 0.075328 - Val Loss: 0.145795\n",
      "Epoch 7/50 - Train Loss: 0.059411 - Val Loss: 0.118592\n",
      "Epoch 8/50 - Train Loss: 0.047554 - Val Loss: 0.096782\n",
      "Epoch 9/50 - Train Loss: 0.045682 - Val Loss: 0.083915\n",
      "Epoch 10/50 - Train Loss: 0.048594 - Val Loss: 0.078113\n",
      "Epoch 11/50 - Train Loss: 0.053167 - Val Loss: 0.076114\n",
      "Epoch 12/50 - Train Loss: 0.041946 - Val Loss: 0.077742\n",
      "Epoch 13/50 - Train Loss: 0.046341 - Val Loss: 0.081045\n",
      "Epoch 14/50 - Train Loss: 0.042285 - Val Loss: 0.084227\n",
      "Epoch 15/50 - Train Loss: 0.039613 - Val Loss: 0.084174\n",
      "Epoch 16/50 - Train Loss: 0.039747 - Val Loss: 0.080115\n",
      "Epoch 17/50 - Train Loss: 0.037749 - Val Loss: 0.074464\n",
      "Epoch 18/50 - Train Loss: 0.041101 - Val Loss: 0.071409\n",
      "Epoch 19/50 - Train Loss: 0.041423 - Val Loss: 0.069915\n",
      "Epoch 20/50 - Train Loss: 0.037698 - Val Loss: 0.069948\n",
      "Epoch 21/50 - Train Loss: 0.037705 - Val Loss: 0.070423\n",
      "Epoch 22/50 - Train Loss: 0.041774 - Val Loss: 0.067445\n",
      "Epoch 23/50 - Train Loss: 0.032469 - Val Loss: 0.063228\n",
      "Epoch 24/50 - Train Loss: 0.032869 - Val Loss: 0.059682\n",
      "Epoch 25/50 - Train Loss: 0.036372 - Val Loss: 0.058240\n",
      "Epoch 26/50 - Train Loss: 0.034999 - Val Loss: 0.057162\n",
      "Epoch 27/50 - Train Loss: 0.035694 - Val Loss: 0.054066\n",
      "Epoch 28/50 - Train Loss: 0.029588 - Val Loss: 0.051584\n",
      "Epoch 29/50 - Train Loss: 0.032711 - Val Loss: 0.051033\n",
      "Epoch 30/50 - Train Loss: 0.029368 - Val Loss: 0.050159\n",
      "Epoch 31/50 - Train Loss: 0.030959 - Val Loss: 0.047963\n",
      "Epoch 32/50 - Train Loss: 0.029673 - Val Loss: 0.046981\n",
      "Epoch 33/50 - Train Loss: 0.036136 - Val Loss: 0.047143\n",
      "Epoch 34/50 - Train Loss: 0.028486 - Val Loss: 0.047139\n",
      "Epoch 35/50 - Train Loss: 0.031464 - Val Loss: 0.046770\n",
      "Epoch 36/50 - Train Loss: 0.024376 - Val Loss: 0.045786\n",
      "Epoch 37/50 - Train Loss: 0.027493 - Val Loss: 0.045868\n",
      "Epoch 38/50 - Train Loss: 0.034994 - Val Loss: 0.046607\n",
      "Epoch 39/50 - Train Loss: 0.026774 - Val Loss: 0.045590\n",
      "Epoch 40/50 - Train Loss: 0.028167 - Val Loss: 0.045382\n",
      "Epoch 41/50 - Train Loss: 0.031332 - Val Loss: 0.045676\n",
      "Epoch 42/50 - Train Loss: 0.030346 - Val Loss: 0.047268\n",
      "Epoch 43/50 - Train Loss: 0.031316 - Val Loss: 0.047077\n",
      "Epoch 44/50 - Train Loss: 0.033379 - Val Loss: 0.045762\n",
      "Epoch 45/50 - Train Loss: 0.030986 - Val Loss: 0.046002\n",
      "Epoch 46/50 - Train Loss: 0.032164 - Val Loss: 0.047564\n",
      "Epoch 47/50 - Train Loss: 0.032287 - Val Loss: 0.047141\n",
      "Epoch 48/50 - Train Loss: 0.030355 - Val Loss: 0.046928\n",
      "Epoch 49/50 - Train Loss: 0.030188 - Val Loss: 0.046679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:21:08,494] Trial 25 finished with value: 0.045381780713796616 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 176, 'lr': 0.000148317467663379, 'weight_decay': 4.335308838359912e-07, 'batch_size': 32}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026557 - Val Loss: 0.046535\n",
      "Epoch 1/50 - Train Loss: 0.104337 - Val Loss: 0.139483\n",
      "Epoch 2/50 - Train Loss: 0.058340 - Val Loss: 0.068422\n",
      "Epoch 3/50 - Train Loss: 0.054457 - Val Loss: 0.078301\n",
      "Epoch 4/50 - Train Loss: 0.048305 - Val Loss: 0.098169\n",
      "Epoch 5/50 - Train Loss: 0.051297 - Val Loss: 0.104146\n",
      "Epoch 6/50 - Train Loss: 0.051259 - Val Loss: 0.097426\n",
      "Epoch 7/50 - Train Loss: 0.042471 - Val Loss: 0.087685\n",
      "Epoch 8/50 - Train Loss: 0.048514 - Val Loss: 0.079922\n",
      "Epoch 9/50 - Train Loss: 0.048941 - Val Loss: 0.075331\n",
      "Epoch 10/50 - Train Loss: 0.049400 - Val Loss: 0.075725\n",
      "Epoch 11/50 - Train Loss: 0.039393 - Val Loss: 0.078781\n",
      "Epoch 12/50 - Train Loss: 0.046739 - Val Loss: 0.078416\n",
      "Epoch 13/50 - Train Loss: 0.045700 - Val Loss: 0.070425\n",
      "Epoch 14/50 - Train Loss: 0.036081 - Val Loss: 0.057440\n",
      "Epoch 15/50 - Train Loss: 0.041399 - Val Loss: 0.057295\n",
      "Epoch 16/50 - Train Loss: 0.041536 - Val Loss: 0.057707\n",
      "Epoch 17/50 - Train Loss: 0.038366 - Val Loss: 0.048189\n",
      "Epoch 18/50 - Train Loss: 0.040056 - Val Loss: 0.053100\n",
      "Epoch 19/50 - Train Loss: 0.031470 - Val Loss: 0.052332\n",
      "Epoch 20/50 - Train Loss: 0.029183 - Val Loss: 0.046932\n",
      "Epoch 21/50 - Train Loss: 0.030859 - Val Loss: 0.050070\n",
      "Epoch 22/50 - Train Loss: 0.028714 - Val Loss: 0.050238\n",
      "Epoch 23/50 - Train Loss: 0.029338 - Val Loss: 0.046668\n",
      "Epoch 24/50 - Train Loss: 0.028266 - Val Loss: 0.046082\n",
      "Epoch 25/50 - Train Loss: 0.036064 - Val Loss: 0.047361\n",
      "Epoch 26/50 - Train Loss: 0.027830 - Val Loss: 0.045543\n",
      "Epoch 27/50 - Train Loss: 0.029234 - Val Loss: 0.047819\n",
      "Epoch 28/50 - Train Loss: 0.027602 - Val Loss: 0.046303\n",
      "Epoch 29/50 - Train Loss: 0.034417 - Val Loss: 0.044842\n",
      "Epoch 30/50 - Train Loss: 0.035496 - Val Loss: 0.046010\n",
      "Epoch 31/50 - Train Loss: 0.027893 - Val Loss: 0.045664\n",
      "Epoch 32/50 - Train Loss: 0.035794 - Val Loss: 0.045561\n",
      "Epoch 33/50 - Train Loss: 0.035322 - Val Loss: 0.049300\n",
      "Epoch 34/50 - Train Loss: 0.028800 - Val Loss: 0.045536\n",
      "Epoch 35/50 - Train Loss: 0.034858 - Val Loss: 0.046368\n",
      "Epoch 36/50 - Train Loss: 0.033952 - Val Loss: 0.048267\n",
      "Epoch 37/50 - Train Loss: 0.028463 - Val Loss: 0.044306\n",
      "Epoch 38/50 - Train Loss: 0.027328 - Val Loss: 0.043629\n",
      "Epoch 39/50 - Train Loss: 0.027923 - Val Loss: 0.044214\n",
      "Epoch 40/50 - Train Loss: 0.027059 - Val Loss: 0.043649\n",
      "Epoch 41/50 - Train Loss: 0.034741 - Val Loss: 0.045291\n",
      "Epoch 42/50 - Train Loss: 0.033994 - Val Loss: 0.047946\n",
      "Epoch 43/50 - Train Loss: 0.035271 - Val Loss: 0.047159\n",
      "Epoch 44/50 - Train Loss: 0.028968 - Val Loss: 0.045875\n",
      "Epoch 45/50 - Train Loss: 0.034062 - Val Loss: 0.046168\n",
      "Epoch 46/50 - Train Loss: 0.034880 - Val Loss: 0.044554\n",
      "Epoch 47/50 - Train Loss: 0.028096 - Val Loss: 0.044703\n",
      "Epoch 48/50 - Train Loss: 0.035189 - Val Loss: 0.045890\n",
      "Epoch 49/50 - Train Loss: 0.027035 - Val Loss: 0.044861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:21:21,178] Trial 26 finished with value: 0.04362882673740387 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 136, 'lr': 0.0025060533402622557, 'weight_decay': 1.1338840530047579e-07, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.033870 - Val Loss: 0.047044\n",
      "Epoch 1/50 - Train Loss: 0.246695 - Val Loss: 0.423567\n",
      "Epoch 2/50 - Train Loss: 0.210059 - Val Loss: 0.371114\n",
      "Epoch 3/50 - Train Loss: 0.203308 - Val Loss: 0.320271\n",
      "Epoch 4/50 - Train Loss: 0.141532 - Val Loss: 0.268350\n",
      "Epoch 5/50 - Train Loss: 0.130696 - Val Loss: 0.213107\n",
      "Epoch 6/50 - Train Loss: 0.093006 - Val Loss: 0.155768\n",
      "Epoch 7/50 - Train Loss: 0.063390 - Val Loss: 0.105085\n",
      "Epoch 8/50 - Train Loss: 0.051514 - Val Loss: 0.078173\n",
      "Epoch 9/50 - Train Loss: 0.068812 - Val Loss: 0.073800\n",
      "Epoch 10/50 - Train Loss: 0.059591 - Val Loss: 0.074853\n",
      "Epoch 11/50 - Train Loss: 0.051655 - Val Loss: 0.082564\n",
      "Epoch 12/50 - Train Loss: 0.043370 - Val Loss: 0.094797\n",
      "Epoch 13/50 - Train Loss: 0.042389 - Val Loss: 0.106153\n",
      "Epoch 14/50 - Train Loss: 0.042403 - Val Loss: 0.113129\n",
      "Epoch 15/50 - Train Loss: 0.053694 - Val Loss: 0.113870\n",
      "Epoch 16/50 - Train Loss: 0.053040 - Val Loss: 0.108408\n",
      "Epoch 17/50 - Train Loss: 0.050548 - Val Loss: 0.098920\n",
      "Epoch 18/50 - Train Loss: 0.048079 - Val Loss: 0.088606\n",
      "Epoch 19/50 - Train Loss: 0.046143 - Val Loss: 0.079726\n",
      "Epoch 20/50 - Train Loss: 0.039182 - Val Loss: 0.074542\n",
      "Epoch 21/50 - Train Loss: 0.039482 - Val Loss: 0.072350\n",
      "Epoch 22/50 - Train Loss: 0.046767 - Val Loss: 0.073012\n",
      "Epoch 23/50 - Train Loss: 0.048099 - Val Loss: 0.076668\n",
      "Epoch 24/50 - Train Loss: 0.036851 - Val Loss: 0.079287\n",
      "Epoch 25/50 - Train Loss: 0.037163 - Val Loss: 0.080344\n",
      "Epoch 26/50 - Train Loss: 0.036961 - Val Loss: 0.079569\n",
      "Epoch 27/50 - Train Loss: 0.036052 - Val Loss: 0.076238\n",
      "Epoch 28/50 - Train Loss: 0.043592 - Val Loss: 0.071839\n",
      "Epoch 29/50 - Train Loss: 0.042525 - Val Loss: 0.067813\n",
      "Epoch 30/50 - Train Loss: 0.041565 - Val Loss: 0.065741\n",
      "Epoch 31/50 - Train Loss: 0.041040 - Val Loss: 0.064760\n",
      "Epoch 32/50 - Train Loss: 0.033435 - Val Loss: 0.064512\n",
      "Epoch 33/50 - Train Loss: 0.033759 - Val Loss: 0.063676\n",
      "Epoch 34/50 - Train Loss: 0.033258 - Val Loss: 0.062015\n",
      "Epoch 35/50 - Train Loss: 0.032475 - Val Loss: 0.058843\n",
      "Epoch 36/50 - Train Loss: 0.039370 - Val Loss: 0.056254\n",
      "Epoch 37/50 - Train Loss: 0.038942 - Val Loss: 0.054923\n",
      "Epoch 38/50 - Train Loss: 0.037915 - Val Loss: 0.054317\n",
      "Epoch 39/50 - Train Loss: 0.030691 - Val Loss: 0.053290\n",
      "Epoch 40/50 - Train Loss: 0.036965 - Val Loss: 0.051705\n",
      "Epoch 41/50 - Train Loss: 0.029601 - Val Loss: 0.050048\n",
      "Epoch 42/50 - Train Loss: 0.029328 - Val Loss: 0.048691\n",
      "Epoch 43/50 - Train Loss: 0.029372 - Val Loss: 0.048570\n",
      "Epoch 44/50 - Train Loss: 0.029357 - Val Loss: 0.048393\n",
      "Epoch 45/50 - Train Loss: 0.035877 - Val Loss: 0.048774\n",
      "Epoch 46/50 - Train Loss: 0.029032 - Val Loss: 0.048323\n",
      "Epoch 47/50 - Train Loss: 0.035585 - Val Loss: 0.047306\n",
      "Epoch 48/50 - Train Loss: 0.035226 - Val Loss: 0.046960\n",
      "Epoch 49/50 - Train Loss: 0.029500 - Val Loss: 0.047171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:21:32,488] Trial 27 finished with value: 0.04696023464202881 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 129, 'lr': 0.0006624611398036666, 'weight_decay': 1.1629691927028444e-07, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.036031 - Val Loss: 0.048115\n",
      "Epoch 1/50 - Train Loss: 0.114345 - Val Loss: 0.177021\n",
      "Epoch 2/50 - Train Loss: 0.062663 - Val Loss: 0.100115\n",
      "Epoch 3/50 - Train Loss: 0.043773 - Val Loss: 0.068576\n",
      "Epoch 4/50 - Train Loss: 0.066704 - Val Loss: 0.072161\n",
      "Epoch 5/50 - Train Loss: 0.043763 - Val Loss: 0.088432\n",
      "Epoch 6/50 - Train Loss: 0.041089 - Val Loss: 0.102314\n",
      "Epoch 7/50 - Train Loss: 0.050902 - Val Loss: 0.105409\n",
      "Epoch 8/50 - Train Loss: 0.051890 - Val Loss: 0.099875\n",
      "Epoch 9/50 - Train Loss: 0.049610 - Val Loss: 0.089488\n",
      "Epoch 10/50 - Train Loss: 0.040810 - Val Loss: 0.079105\n",
      "Epoch 11/50 - Train Loss: 0.047929 - Val Loss: 0.072741\n",
      "Epoch 12/50 - Train Loss: 0.047393 - Val Loss: 0.070864\n",
      "Epoch 13/50 - Train Loss: 0.046094 - Val Loss: 0.072666\n",
      "Epoch 14/50 - Train Loss: 0.046362 - Val Loss: 0.074862\n",
      "Epoch 15/50 - Train Loss: 0.038011 - Val Loss: 0.073653\n",
      "Epoch 16/50 - Train Loss: 0.037077 - Val Loss: 0.065747\n",
      "Epoch 17/50 - Train Loss: 0.042074 - Val Loss: 0.056250\n",
      "Epoch 18/50 - Train Loss: 0.033088 - Val Loss: 0.050839\n",
      "Epoch 19/50 - Train Loss: 0.038413 - Val Loss: 0.056645\n",
      "Epoch 20/50 - Train Loss: 0.032270 - Val Loss: 0.053576\n",
      "Epoch 21/50 - Train Loss: 0.038602 - Val Loss: 0.046677\n",
      "Epoch 22/50 - Train Loss: 0.035090 - Val Loss: 0.047310\n",
      "Epoch 23/50 - Train Loss: 0.039385 - Val Loss: 0.062363\n",
      "Epoch 24/50 - Train Loss: 0.042459 - Val Loss: 0.056625\n",
      "Epoch 25/50 - Train Loss: 0.037284 - Val Loss: 0.046532\n",
      "Epoch 26/50 - Train Loss: 0.030916 - Val Loss: 0.046698\n",
      "Epoch 27/50 - Train Loss: 0.030662 - Val Loss: 0.052299\n",
      "Epoch 28/50 - Train Loss: 0.036768 - Val Loss: 0.056691\n",
      "Epoch 29/50 - Train Loss: 0.038188 - Val Loss: 0.050795\n",
      "Epoch 30/50 - Train Loss: 0.029190 - Val Loss: 0.046237\n",
      "Epoch 31/50 - Train Loss: 0.038713 - Val Loss: 0.046845\n",
      "Epoch 32/50 - Train Loss: 0.027577 - Val Loss: 0.049625\n",
      "Epoch 33/50 - Train Loss: 0.029425 - Val Loss: 0.050346\n",
      "Epoch 34/50 - Train Loss: 0.028976 - Val Loss: 0.046392\n",
      "Epoch 35/50 - Train Loss: 0.035914 - Val Loss: 0.045607\n",
      "Epoch 36/50 - Train Loss: 0.035831 - Val Loss: 0.048334\n",
      "Epoch 37/50 - Train Loss: 0.028475 - Val Loss: 0.051849\n",
      "Epoch 38/50 - Train Loss: 0.029486 - Val Loss: 0.047801\n",
      "Epoch 39/50 - Train Loss: 0.035830 - Val Loss: 0.045675\n",
      "Epoch 40/50 - Train Loss: 0.028767 - Val Loss: 0.046846\n",
      "Epoch 41/50 - Train Loss: 0.034693 - Val Loss: 0.049506\n",
      "Epoch 42/50 - Train Loss: 0.028715 - Val Loss: 0.047778\n",
      "Epoch 43/50 - Train Loss: 0.035300 - Val Loss: 0.046417\n",
      "Epoch 44/50 - Train Loss: 0.026880 - Val Loss: 0.046434\n",
      "Epoch 45/50 - Train Loss: 0.035072 - Val Loss: 0.047839\n",
      "Epoch 46/50 - Train Loss: 0.033971 - Val Loss: 0.048172\n",
      "Epoch 47/50 - Train Loss: 0.034876 - Val Loss: 0.047965\n",
      "Epoch 48/50 - Train Loss: 0.028500 - Val Loss: 0.047622\n",
      "Epoch 49/50 - Train Loss: 0.027936 - Val Loss: 0.046540\n",
      "Epoch 50/50 - Train Loss: 0.034337 - Val Loss: 0.048544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:21:43,503] Trial 28 finished with value: 0.04560676962137222 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 86, 'lr': 0.0024712641068696055, 'weight_decay': 4.47243308635774e-08, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.247555 - Val Loss: 0.431516\n",
      "Epoch 2/50 - Train Loss: 0.253948 - Val Loss: 0.393683\n",
      "Epoch 3/50 - Train Loss: 0.198738 - Val Loss: 0.358066\n",
      "Epoch 4/50 - Train Loss: 0.166157 - Val Loss: 0.323045\n",
      "Epoch 5/50 - Train Loss: 0.175326 - Val Loss: 0.286494\n",
      "Epoch 6/50 - Train Loss: 0.147278 - Val Loss: 0.246336\n",
      "Epoch 7/50 - Train Loss: 0.102032 - Val Loss: 0.204718\n",
      "Epoch 8/50 - Train Loss: 0.090187 - Val Loss: 0.161808\n",
      "Epoch 9/50 - Train Loss: 0.069064 - Val Loss: 0.121202\n",
      "Epoch 10/50 - Train Loss: 0.052790 - Val Loss: 0.088487\n",
      "Epoch 11/50 - Train Loss: 0.051991 - Val Loss: 0.072838\n",
      "Epoch 12/50 - Train Loss: 0.060954 - Val Loss: 0.069559\n",
      "Epoch 13/50 - Train Loss: 0.052432 - Val Loss: 0.070301\n",
      "Epoch 14/50 - Train Loss: 0.048091 - Val Loss: 0.073711\n",
      "Epoch 15/50 - Train Loss: 0.044155 - Val Loss: 0.079444\n",
      "Epoch 16/50 - Train Loss: 0.041196 - Val Loss: 0.086104\n",
      "Epoch 17/50 - Train Loss: 0.040579 - Val Loss: 0.092183\n",
      "Epoch 18/50 - Train Loss: 0.041258 - Val Loss: 0.096077\n",
      "Epoch 19/50 - Train Loss: 0.040559 - Val Loss: 0.097101\n",
      "Epoch 20/50 - Train Loss: 0.048749 - Val Loss: 0.096004\n",
      "Epoch 21/50 - Train Loss: 0.040454 - Val Loss: 0.093117\n",
      "Epoch 22/50 - Train Loss: 0.040622 - Val Loss: 0.088834\n",
      "Epoch 23/50 - Train Loss: 0.038943 - Val Loss: 0.083737\n",
      "Epoch 24/50 - Train Loss: 0.037542 - Val Loss: 0.078785\n",
      "Epoch 25/50 - Train Loss: 0.045648 - Val Loss: 0.074871\n",
      "Epoch 26/50 - Train Loss: 0.038594 - Val Loss: 0.072186\n",
      "Epoch 27/50 - Train Loss: 0.044510 - Val Loss: 0.071516\n",
      "Epoch 28/50 - Train Loss: 0.038616 - Val Loss: 0.071734\n",
      "Epoch 29/50 - Train Loss: 0.043533 - Val Loss: 0.072378\n",
      "Epoch 30/50 - Train Loss: 0.043645 - Val Loss: 0.073424\n",
      "Epoch 31/50 - Train Loss: 0.043159 - Val Loss: 0.073667\n",
      "Epoch 32/50 - Train Loss: 0.036746 - Val Loss: 0.072668\n",
      "Epoch 33/50 - Train Loss: 0.042727 - Val Loss: 0.070538\n",
      "Epoch 34/50 - Train Loss: 0.041274 - Val Loss: 0.067590\n",
      "Epoch 35/50 - Train Loss: 0.041572 - Val Loss: 0.064509\n",
      "Epoch 36/50 - Train Loss: 0.032836 - Val Loss: 0.061215\n",
      "Epoch 37/50 - Train Loss: 0.040965 - Val Loss: 0.059234\n",
      "Epoch 38/50 - Train Loss: 0.032683 - Val Loss: 0.058193\n",
      "Epoch 39/50 - Train Loss: 0.039029 - Val Loss: 0.058067\n",
      "Epoch 40/50 - Train Loss: 0.038514 - Val Loss: 0.058326\n",
      "Epoch 41/50 - Train Loss: 0.037937 - Val Loss: 0.057935\n",
      "Epoch 42/50 - Train Loss: 0.030265 - Val Loss: 0.056446\n",
      "Epoch 43/50 - Train Loss: 0.037978 - Val Loss: 0.054559\n",
      "Epoch 44/50 - Train Loss: 0.029766 - Val Loss: 0.052510\n",
      "Epoch 45/50 - Train Loss: 0.031121 - Val Loss: 0.050579\n",
      "Epoch 46/50 - Train Loss: 0.036885 - Val Loss: 0.051105\n",
      "Epoch 47/50 - Train Loss: 0.029723 - Val Loss: 0.052202\n",
      "Epoch 48/50 - Train Loss: 0.036060 - Val Loss: 0.051862\n",
      "Epoch 49/50 - Train Loss: 0.036490 - Val Loss: 0.051005\n",
      "Epoch 50/50 - Train Loss: 0.035909 - Val Loss: 0.050262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:21:52,909] Trial 29 finished with value: 0.0502624586224556 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 66, 'lr': 0.00101101052430989, 'weight_decay': 5.422505937771717e-07, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.177684 - Val Loss: 0.166470\n",
      "Epoch 2/50 - Train Loss: 0.060662 - Val Loss: 0.081018\n",
      "Epoch 3/50 - Train Loss: 0.078093 - Val Loss: 0.077371\n",
      "Epoch 4/50 - Train Loss: 0.047546 - Val Loss: 0.115116\n",
      "Epoch 5/50 - Train Loss: 0.049518 - Val Loss: 0.118318\n",
      "Epoch 6/50 - Train Loss: 0.056766 - Val Loss: 0.088991\n",
      "Epoch 7/50 - Train Loss: 0.038542 - Val Loss: 0.061359\n",
      "Epoch 8/50 - Train Loss: 0.047035 - Val Loss: 0.056152\n",
      "Epoch 9/50 - Train Loss: 0.038490 - Val Loss: 0.059867\n",
      "Epoch 10/50 - Train Loss: 0.040894 - Val Loss: 0.069059\n",
      "Epoch 11/50 - Train Loss: 0.041172 - Val Loss: 0.068741\n",
      "Epoch 12/50 - Train Loss: 0.034084 - Val Loss: 0.058398\n",
      "Epoch 13/50 - Train Loss: 0.032041 - Val Loss: 0.049663\n",
      "Epoch 14/50 - Train Loss: 0.039550 - Val Loss: 0.049532\n",
      "Epoch 15/50 - Train Loss: 0.038572 - Val Loss: 0.052745\n",
      "Epoch 16/50 - Train Loss: 0.038468 - Val Loss: 0.051662\n",
      "Epoch 17/50 - Train Loss: 0.030561 - Val Loss: 0.047988\n",
      "Epoch 18/50 - Train Loss: 0.037812 - Val Loss: 0.048164\n",
      "Epoch 19/50 - Train Loss: 0.030032 - Val Loss: 0.049243\n",
      "Epoch 20/50 - Train Loss: 0.037075 - Val Loss: 0.048756\n",
      "Epoch 21/50 - Train Loss: 0.028960 - Val Loss: 0.046387\n",
      "Epoch 22/50 - Train Loss: 0.030479 - Val Loss: 0.046221\n",
      "Epoch 23/50 - Train Loss: 0.035584 - Val Loss: 0.050058\n",
      "Epoch 24/50 - Train Loss: 0.035194 - Val Loss: 0.046609\n",
      "Epoch 25/50 - Train Loss: 0.035209 - Val Loss: 0.045237\n",
      "Epoch 26/50 - Train Loss: 0.035544 - Val Loss: 0.046931\n",
      "Epoch 27/50 - Train Loss: 0.028089 - Val Loss: 0.046656\n",
      "Epoch 28/50 - Train Loss: 0.035917 - Val Loss: 0.045592\n",
      "Epoch 29/50 - Train Loss: 0.027301 - Val Loss: 0.044897\n",
      "Epoch 30/50 - Train Loss: 0.036369 - Val Loss: 0.048273\n",
      "Epoch 31/50 - Train Loss: 0.036089 - Val Loss: 0.046383\n",
      "Epoch 32/50 - Train Loss: 0.029441 - Val Loss: 0.044678\n",
      "Epoch 33/50 - Train Loss: 0.028757 - Val Loss: 0.046483\n",
      "Epoch 34/50 - Train Loss: 0.027827 - Val Loss: 0.046986\n",
      "Epoch 35/50 - Train Loss: 0.028627 - Val Loss: 0.044720\n",
      "Epoch 36/50 - Train Loss: 0.028660 - Val Loss: 0.044156\n",
      "Epoch 37/50 - Train Loss: 0.035881 - Val Loss: 0.046968\n",
      "Epoch 38/50 - Train Loss: 0.035616 - Val Loss: 0.045415\n",
      "Epoch 39/50 - Train Loss: 0.026996 - Val Loss: 0.044438\n",
      "Epoch 40/50 - Train Loss: 0.035061 - Val Loss: 0.047321\n",
      "Epoch 41/50 - Train Loss: 0.034472 - Val Loss: 0.049261\n",
      "Epoch 42/50 - Train Loss: 0.028324 - Val Loss: 0.045026\n",
      "Epoch 43/50 - Train Loss: 0.028512 - Val Loss: 0.044278\n",
      "Epoch 44/50 - Train Loss: 0.028194 - Val Loss: 0.045523\n",
      "Epoch 45/50 - Train Loss: 0.035076 - Val Loss: 0.046981\n",
      "Epoch 46/50 - Train Loss: 0.035091 - Val Loss: 0.044419\n",
      "Epoch 47/50 - Train Loss: 0.027787 - Val Loss: 0.044374\n",
      "Epoch 48/50 - Train Loss: 0.027615 - Val Loss: 0.045856\n",
      "Epoch 49/50 - Train Loss: 0.035349 - Val Loss: 0.046459\n",
      "Epoch 50/50 - Train Loss: 0.027397 - Val Loss: 0.044965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:22:02,937] Trial 30 finished with value: 0.044156163930892944 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 148, 'lr': 0.0028952888138644934, 'weight_decay': 1.2333531839773461e-05, 'batch_size': 64}. Best is trial 19 with value: 0.043235134333372116.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.075882 - Val Loss: 0.066396\n",
      "Epoch 2/50 - Train Loss: 0.045143 - Val Loss: 0.066857\n",
      "Epoch 3/50 - Train Loss: 0.041976 - Val Loss: 0.052274\n",
      "Epoch 4/50 - Train Loss: 0.035436 - Val Loss: 0.056984\n",
      "Epoch 5/50 - Train Loss: 0.032220 - Val Loss: 0.045689\n",
      "Epoch 6/50 - Train Loss: 0.034614 - Val Loss: 0.056163\n",
      "Epoch 7/50 - Train Loss: 0.038461 - Val Loss: 0.047033\n",
      "Epoch 8/50 - Train Loss: 0.034614 - Val Loss: 0.057225\n",
      "Epoch 9/50 - Train Loss: 0.032839 - Val Loss: 0.056782\n",
      "Epoch 10/50 - Train Loss: 0.029673 - Val Loss: 0.048000\n",
      "Epoch 11/50 - Train Loss: 0.030004 - Val Loss: 0.048177\n",
      "Epoch 12/50 - Train Loss: 0.027964 - Val Loss: 0.045654\n",
      "Epoch 13/50 - Train Loss: 0.030901 - Val Loss: 0.057831\n",
      "Epoch 14/50 - Train Loss: 0.029224 - Val Loss: 0.051990\n",
      "Epoch 15/50 - Train Loss: 0.029392 - Val Loss: 0.043387\n",
      "Epoch 16/50 - Train Loss: 0.031074 - Val Loss: 0.043699\n",
      "Epoch 17/50 - Train Loss: 0.027879 - Val Loss: 0.052036\n",
      "Epoch 18/50 - Train Loss: 0.030118 - Val Loss: 0.052638\n",
      "Epoch 19/50 - Train Loss: 0.032155 - Val Loss: 0.043004\n",
      "Epoch 20/50 - Train Loss: 0.029216 - Val Loss: 0.045611\n",
      "Epoch 21/50 - Train Loss: 0.029148 - Val Loss: 0.042916\n",
      "Epoch 22/50 - Train Loss: 0.028124 - Val Loss: 0.046017\n",
      "Epoch 23/50 - Train Loss: 0.031068 - Val Loss: 0.046601\n",
      "Epoch 24/50 - Train Loss: 0.031644 - Val Loss: 0.056616\n",
      "Epoch 25/50 - Train Loss: 0.030390 - Val Loss: 0.045309\n",
      "Epoch 26/50 - Train Loss: 0.034867 - Val Loss: 0.048910\n",
      "Epoch 27/50 - Train Loss: 0.032289 - Val Loss: 0.075825\n",
      "Epoch 28/50 - Train Loss: 0.031116 - Val Loss: 0.047018\n",
      "Epoch 29/50 - Train Loss: 0.032613 - Val Loss: 0.043769\n",
      "Epoch 30/50 - Train Loss: 0.029952 - Val Loss: 0.050883\n",
      "Epoch 31/50 - Train Loss: 0.028814 - Val Loss: 0.048865\n",
      "Epoch 32/50 - Train Loss: 0.031262 - Val Loss: 0.044928\n",
      "Epoch 33/50 - Train Loss: 0.030601 - Val Loss: 0.043853\n",
      "Epoch 34/50 - Train Loss: 0.031537 - Val Loss: 0.043406\n",
      "Epoch 35/50 - Train Loss: 0.029472 - Val Loss: 0.047277\n",
      "Epoch 36/50 - Train Loss: 0.028662 - Val Loss: 0.043015\n",
      "Epoch 37/50 - Train Loss: 0.031573 - Val Loss: 0.043877\n",
      "Epoch 38/50 - Train Loss: 0.029811 - Val Loss: 0.046542\n",
      "Epoch 39/50 - Train Loss: 0.028316 - Val Loss: 0.043392\n",
      "Epoch 40/50 - Train Loss: 0.030986 - Val Loss: 0.045665\n",
      "Epoch 41/50 - Train Loss: 0.027467 - Val Loss: 0.045018\n",
      "Epoch 42/50 - Train Loss: 0.030134 - Val Loss: 0.049382\n",
      "Epoch 43/50 - Train Loss: 0.026811 - Val Loss: 0.043883\n",
      "Epoch 44/50 - Train Loss: 0.029559 - Val Loss: 0.043705\n",
      "Epoch 45/50 - Train Loss: 0.031203 - Val Loss: 0.046734\n",
      "Epoch 46/50 - Train Loss: 0.028869 - Val Loss: 0.048366\n",
      "Epoch 47/50 - Train Loss: 0.028612 - Val Loss: 0.045178\n",
      "Epoch 48/50 - Train Loss: 0.024948 - Val Loss: 0.045583\n",
      "Epoch 49/50 - Train Loss: 0.027175 - Val Loss: 0.050417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:22:46,821] Trial 31 finished with value: 0.04291552553574244 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 134, 'lr': 0.007421837276464341, 'weight_decay': 2.9528706794922363e-08, 'batch_size': 8}. Best is trial 31 with value: 0.04291552553574244.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030907 - Val Loss: 0.044492\n",
      "Epoch 1/50 - Train Loss: 0.093667 - Val Loss: 0.085793\n",
      "Epoch 2/50 - Train Loss: 0.043630 - Val Loss: 0.108948\n",
      "Epoch 3/50 - Train Loss: 0.045650 - Val Loss: 0.072185\n",
      "Epoch 4/50 - Train Loss: 0.041867 - Val Loss: 0.064959\n",
      "Epoch 5/50 - Train Loss: 0.038924 - Val Loss: 0.066991\n",
      "Epoch 6/50 - Train Loss: 0.037551 - Val Loss: 0.050221\n",
      "Epoch 7/50 - Train Loss: 0.033645 - Val Loss: 0.066439\n",
      "Epoch 8/50 - Train Loss: 0.031114 - Val Loss: 0.044451\n",
      "Epoch 9/50 - Train Loss: 0.035153 - Val Loss: 0.047789\n",
      "Epoch 10/50 - Train Loss: 0.032204 - Val Loss: 0.045624\n",
      "Epoch 11/50 - Train Loss: 0.034239 - Val Loss: 0.047937\n",
      "Epoch 12/50 - Train Loss: 0.030911 - Val Loss: 0.053396\n",
      "Epoch 13/50 - Train Loss: 0.032279 - Val Loss: 0.043900\n",
      "Epoch 14/50 - Train Loss: 0.029760 - Val Loss: 0.046562\n",
      "Epoch 15/50 - Train Loss: 0.032682 - Val Loss: 0.043905\n",
      "Epoch 16/50 - Train Loss: 0.029498 - Val Loss: 0.047131\n",
      "Epoch 17/50 - Train Loss: 0.032151 - Val Loss: 0.051610\n",
      "Epoch 18/50 - Train Loss: 0.031523 - Val Loss: 0.046671\n",
      "Epoch 19/50 - Train Loss: 0.030856 - Val Loss: 0.044555\n",
      "Epoch 20/50 - Train Loss: 0.028886 - Val Loss: 0.044306\n",
      "Epoch 21/50 - Train Loss: 0.029864 - Val Loss: 0.046108\n",
      "Epoch 22/50 - Train Loss: 0.027680 - Val Loss: 0.046022\n",
      "Epoch 23/50 - Train Loss: 0.029793 - Val Loss: 0.045858\n",
      "Epoch 24/50 - Train Loss: 0.030393 - Val Loss: 0.046666\n",
      "Epoch 25/50 - Train Loss: 0.030486 - Val Loss: 0.044648\n",
      "Epoch 26/50 - Train Loss: 0.028551 - Val Loss: 0.048493\n",
      "Epoch 27/50 - Train Loss: 0.030496 - Val Loss: 0.045189\n",
      "Epoch 28/50 - Train Loss: 0.032669 - Val Loss: 0.052996\n",
      "Epoch 29/50 - Train Loss: 0.031634 - Val Loss: 0.046467\n",
      "Epoch 30/50 - Train Loss: 0.028914 - Val Loss: 0.048295\n",
      "Epoch 31/50 - Train Loss: 0.031039 - Val Loss: 0.046457\n",
      "Epoch 32/50 - Train Loss: 0.028747 - Val Loss: 0.044720\n",
      "Epoch 33/50 - Train Loss: 0.028131 - Val Loss: 0.045320\n",
      "Epoch 34/50 - Train Loss: 0.029687 - Val Loss: 0.052098\n",
      "Epoch 35/50 - Train Loss: 0.029395 - Val Loss: 0.047795\n",
      "Epoch 36/50 - Train Loss: 0.030380 - Val Loss: 0.046861\n",
      "Epoch 37/50 - Train Loss: 0.027512 - Val Loss: 0.046060\n",
      "Epoch 38/50 - Train Loss: 0.028483 - Val Loss: 0.049592\n",
      "Epoch 39/50 - Train Loss: 0.031225 - Val Loss: 0.052818\n",
      "Epoch 40/50 - Train Loss: 0.031116 - Val Loss: 0.045350\n",
      "Epoch 41/50 - Train Loss: 0.029985 - Val Loss: 0.057043\n",
      "Epoch 42/50 - Train Loss: 0.029284 - Val Loss: 0.050005\n",
      "Epoch 43/50 - Train Loss: 0.032031 - Val Loss: 0.047257\n",
      "Epoch 44/50 - Train Loss: 0.030178 - Val Loss: 0.047339\n",
      "Epoch 45/50 - Train Loss: 0.028283 - Val Loss: 0.045948\n",
      "Epoch 46/50 - Train Loss: 0.028032 - Val Loss: 0.046901\n",
      "Epoch 47/50 - Train Loss: 0.028735 - Val Loss: 0.047606\n",
      "Epoch 48/50 - Train Loss: 0.027583 - Val Loss: 0.046782\n",
      "Epoch 49/50 - Train Loss: 0.029256 - Val Loss: 0.047586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:23:21,025] Trial 32 finished with value: 0.043900457521279655 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 120, 'lr': 0.006069071222321913, 'weight_decay': 2.0420720378316904e-08, 'batch_size': 8}. Best is trial 31 with value: 0.04291552553574244.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026139 - Val Loss: 0.046957\n",
      "Epoch 1/50 - Train Loss: 0.057642 - Val Loss: 0.083443\n",
      "Epoch 2/50 - Train Loss: 0.045550 - Val Loss: 0.084279\n",
      "Epoch 3/50 - Train Loss: 0.038747 - Val Loss: 0.066971\n",
      "Epoch 4/50 - Train Loss: 0.035112 - Val Loss: 0.058058\n",
      "Epoch 5/50 - Train Loss: 0.030181 - Val Loss: 0.045610\n",
      "Epoch 6/50 - Train Loss: 0.032902 - Val Loss: 0.045691\n",
      "Epoch 7/50 - Train Loss: 0.034472 - Val Loss: 0.062544\n",
      "Epoch 8/50 - Train Loss: 0.036831 - Val Loss: 0.054129\n",
      "Epoch 9/50 - Train Loss: 0.032788 - Val Loss: 0.068085\n",
      "Epoch 10/50 - Train Loss: 0.030126 - Val Loss: 0.044302\n",
      "Epoch 11/50 - Train Loss: 0.030960 - Val Loss: 0.044154\n",
      "Epoch 12/50 - Train Loss: 0.029371 - Val Loss: 0.043950\n",
      "Epoch 13/50 - Train Loss: 0.029691 - Val Loss: 0.049985\n",
      "Epoch 14/50 - Train Loss: 0.030954 - Val Loss: 0.044432\n",
      "Epoch 15/50 - Train Loss: 0.030338 - Val Loss: 0.043528\n",
      "Epoch 16/50 - Train Loss: 0.029407 - Val Loss: 0.045016\n",
      "Epoch 17/50 - Train Loss: 0.029190 - Val Loss: 0.046114\n",
      "Epoch 18/50 - Train Loss: 0.029611 - Val Loss: 0.044429\n",
      "Epoch 19/50 - Train Loss: 0.029224 - Val Loss: 0.042854\n",
      "Epoch 20/50 - Train Loss: 0.030161 - Val Loss: 0.049610\n",
      "Epoch 21/50 - Train Loss: 0.029333 - Val Loss: 0.044032\n",
      "Epoch 22/50 - Train Loss: 0.028856 - Val Loss: 0.044065\n",
      "Epoch 23/50 - Train Loss: 0.029318 - Val Loss: 0.042725\n",
      "Epoch 24/50 - Train Loss: 0.031012 - Val Loss: 0.042892\n",
      "Epoch 25/50 - Train Loss: 0.029888 - Val Loss: 0.043972\n",
      "Epoch 26/50 - Train Loss: 0.029818 - Val Loss: 0.044240\n",
      "Epoch 27/50 - Train Loss: 0.027216 - Val Loss: 0.043299\n",
      "Epoch 28/50 - Train Loss: 0.030211 - Val Loss: 0.043411\n",
      "Epoch 29/50 - Train Loss: 0.028381 - Val Loss: 0.047314\n",
      "Epoch 30/50 - Train Loss: 0.030609 - Val Loss: 0.043616\n",
      "Epoch 31/50 - Train Loss: 0.027008 - Val Loss: 0.044836\n",
      "Epoch 32/50 - Train Loss: 0.026381 - Val Loss: 0.045710\n",
      "Epoch 33/50 - Train Loss: 0.026137 - Val Loss: 0.046093\n",
      "Epoch 34/50 - Train Loss: 0.030487 - Val Loss: 0.052466\n",
      "Epoch 35/50 - Train Loss: 0.027698 - Val Loss: 0.044230\n",
      "Epoch 36/50 - Train Loss: 0.028398 - Val Loss: 0.043809\n",
      "Epoch 37/50 - Train Loss: 0.029895 - Val Loss: 0.044588\n",
      "Epoch 38/50 - Train Loss: 0.028537 - Val Loss: 0.044165\n",
      "Epoch 39/50 - Train Loss: 0.027604 - Val Loss: 0.046607\n",
      "Epoch 40/50 - Train Loss: 0.027016 - Val Loss: 0.048325\n",
      "Epoch 41/50 - Train Loss: 0.028959 - Val Loss: 0.044776\n",
      "Epoch 42/50 - Train Loss: 0.027520 - Val Loss: 0.044364\n",
      "Epoch 43/50 - Train Loss: 0.026170 - Val Loss: 0.044677\n",
      "Epoch 44/50 - Train Loss: 0.029399 - Val Loss: 0.044962\n",
      "Epoch 45/50 - Train Loss: 0.031194 - Val Loss: 0.052163\n",
      "Epoch 46/50 - Train Loss: 0.034242 - Val Loss: 0.053629\n",
      "Epoch 47/50 - Train Loss: 0.028375 - Val Loss: 0.043397\n",
      "Epoch 48/50 - Train Loss: 0.028460 - Val Loss: 0.046456\n",
      "Epoch 49/50 - Train Loss: 0.030029 - Val Loss: 0.043759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:24:25,540] Trial 33 finished with value: 0.042725492268800735 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 137, 'lr': 0.003803614069521229, 'weight_decay': 5.7452635518622826e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027999 - Val Loss: 0.045105\n",
      "Epoch 1/50 - Train Loss: 0.058626 - Val Loss: 0.060884\n",
      "Epoch 2/50 - Train Loss: 0.035430 - Val Loss: 0.048707\n",
      "Epoch 3/50 - Train Loss: 0.032407 - Val Loss: 0.053028\n",
      "Epoch 4/50 - Train Loss: 0.031825 - Val Loss: 0.047989\n",
      "Epoch 5/50 - Train Loss: 0.029524 - Val Loss: 0.044518\n",
      "Epoch 6/50 - Train Loss: 0.031798 - Val Loss: 0.045134\n",
      "Epoch 7/50 - Train Loss: 0.030636 - Val Loss: 0.053693\n",
      "Epoch 8/50 - Train Loss: 0.030308 - Val Loss: 0.048329\n",
      "Epoch 9/50 - Train Loss: 0.031858 - Val Loss: 0.051563\n",
      "Epoch 10/50 - Train Loss: 0.031945 - Val Loss: 0.045185\n",
      "Epoch 11/50 - Train Loss: 0.031385 - Val Loss: 0.044465\n",
      "Epoch 12/50 - Train Loss: 0.030579 - Val Loss: 0.067281\n",
      "Epoch 13/50 - Train Loss: 0.030738 - Val Loss: 0.045183\n",
      "Epoch 14/50 - Train Loss: 0.030118 - Val Loss: 0.046423\n",
      "Epoch 15/50 - Train Loss: 0.029098 - Val Loss: 0.045455\n",
      "Epoch 16/50 - Train Loss: 0.030676 - Val Loss: 0.046572\n",
      "Epoch 17/50 - Train Loss: 0.029204 - Val Loss: 0.045406\n",
      "Epoch 18/50 - Train Loss: 0.030072 - Val Loss: 0.043966\n",
      "Epoch 19/50 - Train Loss: 0.031040 - Val Loss: 0.045333\n",
      "Epoch 20/50 - Train Loss: 0.027632 - Val Loss: 0.043753\n",
      "Epoch 21/50 - Train Loss: 0.031583 - Val Loss: 0.055686\n",
      "Epoch 22/50 - Train Loss: 0.029823 - Val Loss: 0.045151\n",
      "Epoch 23/50 - Train Loss: 0.029965 - Val Loss: 0.048132\n",
      "Epoch 24/50 - Train Loss: 0.031536 - Val Loss: 0.046852\n",
      "Epoch 25/50 - Train Loss: 0.029864 - Val Loss: 0.049801\n",
      "Epoch 26/50 - Train Loss: 0.030046 - Val Loss: 0.048301\n",
      "Epoch 27/50 - Train Loss: 0.030836 - Val Loss: 0.043704\n",
      "Epoch 28/50 - Train Loss: 0.027950 - Val Loss: 0.045701\n",
      "Epoch 29/50 - Train Loss: 0.028513 - Val Loss: 0.043992\n",
      "Epoch 30/50 - Train Loss: 0.030782 - Val Loss: 0.046442\n",
      "Epoch 31/50 - Train Loss: 0.028118 - Val Loss: 0.044115\n",
      "Epoch 32/50 - Train Loss: 0.028655 - Val Loss: 0.046501\n",
      "Epoch 33/50 - Train Loss: 0.029077 - Val Loss: 0.044535\n",
      "Epoch 34/50 - Train Loss: 0.029184 - Val Loss: 0.050589\n",
      "Epoch 35/50 - Train Loss: 0.029694 - Val Loss: 0.044111\n",
      "Epoch 36/50 - Train Loss: 0.026214 - Val Loss: 0.046305\n",
      "Epoch 37/50 - Train Loss: 0.029160 - Val Loss: 0.046038\n",
      "Epoch 38/50 - Train Loss: 0.028713 - Val Loss: 0.044758\n",
      "Epoch 39/50 - Train Loss: 0.028584 - Val Loss: 0.047267\n",
      "Epoch 40/50 - Train Loss: 0.027155 - Val Loss: 0.046122\n",
      "Epoch 41/50 - Train Loss: 0.028869 - Val Loss: 0.047060\n",
      "Epoch 42/50 - Train Loss: 0.027722 - Val Loss: 0.047394\n",
      "Epoch 43/50 - Train Loss: 0.030601 - Val Loss: 0.060283\n",
      "Epoch 44/50 - Train Loss: 0.030958 - Val Loss: 0.048004\n",
      "Epoch 45/50 - Train Loss: 0.028723 - Val Loss: 0.047506\n",
      "Epoch 46/50 - Train Loss: 0.028254 - Val Loss: 0.046270\n",
      "Epoch 47/50 - Train Loss: 0.027226 - Val Loss: 0.047174\n",
      "Epoch 48/50 - Train Loss: 0.026759 - Val Loss: 0.051214\n",
      "Epoch 49/50 - Train Loss: 0.028312 - Val Loss: 0.047854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:25:08,309] Trial 34 finished with value: 0.043703801929950714 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 164, 'lr': 0.004322212445891291, 'weight_decay': 4.308473836091206e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030107 - Val Loss: 0.049421\n",
      "Epoch 1/50 - Train Loss: 0.061492 - Val Loss: 0.071007\n",
      "Epoch 2/50 - Train Loss: 0.044577 - Val Loss: 0.082348\n",
      "Epoch 3/50 - Train Loss: 0.041569 - Val Loss: 0.078368\n",
      "Epoch 4/50 - Train Loss: 0.038016 - Val Loss: 0.060164\n",
      "Epoch 5/50 - Train Loss: 0.033727 - Val Loss: 0.056737\n",
      "Epoch 6/50 - Train Loss: 0.036289 - Val Loss: 0.051625\n",
      "Epoch 7/50 - Train Loss: 0.032590 - Val Loss: 0.048766\n",
      "Epoch 8/50 - Train Loss: 0.029263 - Val Loss: 0.044888\n",
      "Epoch 9/50 - Train Loss: 0.031534 - Val Loss: 0.047895\n",
      "Epoch 10/50 - Train Loss: 0.031576 - Val Loss: 0.049735\n",
      "Epoch 11/50 - Train Loss: 0.028335 - Val Loss: 0.045132\n",
      "Epoch 12/50 - Train Loss: 0.030541 - Val Loss: 0.048518\n",
      "Epoch 13/50 - Train Loss: 0.028202 - Val Loss: 0.048560\n",
      "Epoch 14/50 - Train Loss: 0.027492 - Val Loss: 0.044569\n",
      "Epoch 15/50 - Train Loss: 0.033447 - Val Loss: 0.044581\n",
      "Epoch 16/50 - Train Loss: 0.031336 - Val Loss: 0.053919\n",
      "Epoch 17/50 - Train Loss: 0.031477 - Val Loss: 0.050605\n",
      "Epoch 18/50 - Train Loss: 0.029900 - Val Loss: 0.048226\n",
      "Epoch 19/50 - Train Loss: 0.028497 - Val Loss: 0.044853\n",
      "Epoch 20/50 - Train Loss: 0.031415 - Val Loss: 0.044146\n",
      "Epoch 21/50 - Train Loss: 0.030506 - Val Loss: 0.045538\n",
      "Epoch 22/50 - Train Loss: 0.030982 - Val Loss: 0.043779\n",
      "Epoch 23/50 - Train Loss: 0.028569 - Val Loss: 0.046088\n",
      "Epoch 24/50 - Train Loss: 0.027496 - Val Loss: 0.045057\n",
      "Epoch 25/50 - Train Loss: 0.030869 - Val Loss: 0.044338\n",
      "Epoch 26/50 - Train Loss: 0.030639 - Val Loss: 0.050517\n",
      "Epoch 27/50 - Train Loss: 0.030691 - Val Loss: 0.043841\n",
      "Epoch 28/50 - Train Loss: 0.028002 - Val Loss: 0.049199\n",
      "Epoch 29/50 - Train Loss: 0.030406 - Val Loss: 0.044633\n",
      "Epoch 30/50 - Train Loss: 0.028001 - Val Loss: 0.046821\n",
      "Epoch 31/50 - Train Loss: 0.033438 - Val Loss: 0.046610\n",
      "Epoch 32/50 - Train Loss: 0.030185 - Val Loss: 0.053652\n",
      "Epoch 33/50 - Train Loss: 0.027635 - Val Loss: 0.044628\n",
      "Epoch 34/50 - Train Loss: 0.028007 - Val Loss: 0.046874\n",
      "Epoch 35/50 - Train Loss: 0.030502 - Val Loss: 0.045657\n",
      "Epoch 36/50 - Train Loss: 0.029860 - Val Loss: 0.050413\n",
      "Epoch 37/50 - Train Loss: 0.031464 - Val Loss: 0.044954\n",
      "Epoch 38/50 - Train Loss: 0.029500 - Val Loss: 0.044708\n",
      "Epoch 39/50 - Train Loss: 0.030830 - Val Loss: 0.047037\n",
      "Epoch 40/50 - Train Loss: 0.028265 - Val Loss: 0.045018\n",
      "Epoch 41/50 - Train Loss: 0.030127 - Val Loss: 0.045643\n",
      "Epoch 42/50 - Train Loss: 0.031676 - Val Loss: 0.044832\n",
      "Epoch 43/50 - Train Loss: 0.026887 - Val Loss: 0.047891\n",
      "Epoch 44/50 - Train Loss: 0.031168 - Val Loss: 0.045240\n",
      "Epoch 45/50 - Train Loss: 0.030520 - Val Loss: 0.045008\n",
      "Epoch 46/50 - Train Loss: 0.030483 - Val Loss: 0.048622\n",
      "Epoch 47/50 - Train Loss: 0.030848 - Val Loss: 0.044938\n",
      "Epoch 48/50 - Train Loss: 0.029374 - Val Loss: 0.046484\n",
      "Epoch 49/50 - Train Loss: 0.027892 - Val Loss: 0.044492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:25:45,051] Trial 35 finished with value: 0.043778604517380394 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 114, 'lr': 0.006614372572634779, 'weight_decay': 6.248813894213538e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029727 - Val Loss: 0.052307\n",
      "Epoch 1/50 - Train Loss: 0.066517 - Val Loss: 0.074383\n",
      "Epoch 2/50 - Train Loss: 0.043184 - Val Loss: 0.055725\n",
      "Epoch 3/50 - Train Loss: 0.034096 - Val Loss: 0.064389\n",
      "Epoch 4/50 - Train Loss: 0.033464 - Val Loss: 0.062102\n",
      "Epoch 5/50 - Train Loss: 0.030917 - Val Loss: 0.044637\n",
      "Epoch 6/50 - Train Loss: 0.030745 - Val Loss: 0.044743\n",
      "Epoch 7/50 - Train Loss: 0.032771 - Val Loss: 0.053204\n",
      "Epoch 8/50 - Train Loss: 0.030168 - Val Loss: 0.043847\n",
      "Epoch 9/50 - Train Loss: 0.031071 - Val Loss: 0.056228\n",
      "Epoch 10/50 - Train Loss: 0.030937 - Val Loss: 0.044259\n",
      "Epoch 11/50 - Train Loss: 0.029254 - Val Loss: 0.044854\n",
      "Epoch 12/50 - Train Loss: 0.030555 - Val Loss: 0.051099\n",
      "Epoch 13/50 - Train Loss: 0.029672 - Val Loss: 0.047156\n",
      "Epoch 14/50 - Train Loss: 0.032573 - Val Loss: 0.045148\n",
      "Epoch 15/50 - Train Loss: 0.032216 - Val Loss: 0.045430\n",
      "Epoch 16/50 - Train Loss: 0.032237 - Val Loss: 0.055842\n",
      "Epoch 17/50 - Train Loss: 0.030416 - Val Loss: 0.045259\n",
      "Epoch 18/50 - Train Loss: 0.029729 - Val Loss: 0.053807\n",
      "Epoch 19/50 - Train Loss: 0.027373 - Val Loss: 0.053189\n",
      "Epoch 20/50 - Train Loss: 0.029816 - Val Loss: 0.045926\n",
      "Epoch 21/50 - Train Loss: 0.029592 - Val Loss: 0.045089\n",
      "Epoch 22/50 - Train Loss: 0.030781 - Val Loss: 0.070178\n",
      "Epoch 23/50 - Train Loss: 0.032857 - Val Loss: 0.050783\n",
      "Epoch 24/50 - Train Loss: 0.030523 - Val Loss: 0.043878\n",
      "Epoch 25/50 - Train Loss: 0.030514 - Val Loss: 0.048548\n",
      "Epoch 26/50 - Train Loss: 0.030744 - Val Loss: 0.044662\n",
      "Epoch 27/50 - Train Loss: 0.032209 - Val Loss: 0.044537\n",
      "Epoch 28/50 - Train Loss: 0.030298 - Val Loss: 0.044416\n",
      "Epoch 29/50 - Train Loss: 0.026892 - Val Loss: 0.045808\n",
      "Epoch 30/50 - Train Loss: 0.033687 - Val Loss: 0.051366\n",
      "Epoch 31/50 - Train Loss: 0.031066 - Val Loss: 0.045122\n",
      "Epoch 32/50 - Train Loss: 0.031961 - Val Loss: 0.044559\n",
      "Epoch 33/50 - Train Loss: 0.029819 - Val Loss: 0.043782\n",
      "Epoch 34/50 - Train Loss: 0.029053 - Val Loss: 0.053591\n",
      "Epoch 35/50 - Train Loss: 0.027595 - Val Loss: 0.045053\n",
      "Epoch 36/50 - Train Loss: 0.029926 - Val Loss: 0.045206\n",
      "Epoch 37/50 - Train Loss: 0.026639 - Val Loss: 0.045809\n",
      "Epoch 38/50 - Train Loss: 0.030226 - Val Loss: 0.044382\n",
      "Epoch 39/50 - Train Loss: 0.031317 - Val Loss: 0.045148\n",
      "Epoch 40/50 - Train Loss: 0.029887 - Val Loss: 0.045415\n",
      "Epoch 41/50 - Train Loss: 0.028201 - Val Loss: 0.045067\n",
      "Epoch 42/50 - Train Loss: 0.028920 - Val Loss: 0.045581\n",
      "Epoch 43/50 - Train Loss: 0.027728 - Val Loss: 0.046067\n",
      "Epoch 44/50 - Train Loss: 0.026651 - Val Loss: 0.044105\n",
      "Epoch 45/50 - Train Loss: 0.030166 - Val Loss: 0.045958\n",
      "Epoch 46/50 - Train Loss: 0.030450 - Val Loss: 0.058946\n",
      "Epoch 47/50 - Train Loss: 0.033635 - Val Loss: 0.045809\n",
      "Epoch 48/50 - Train Loss: 0.027724 - Val Loss: 0.046323\n",
      "Epoch 49/50 - Train Loss: 0.026734 - Val Loss: 0.045612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:26:48,340] Trial 36 finished with value: 0.04378237699468931 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 188, 'lr': 0.004111015443941865, 'weight_decay': 3.2377161148345538e-06, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028090 - Val Loss: 0.045396\n",
      "Epoch 1/50 - Train Loss: 0.082572 - Val Loss: 0.095547\n",
      "Epoch 2/50 - Train Loss: 0.042737 - Val Loss: 0.074410\n",
      "Epoch 3/50 - Train Loss: 0.042369 - Val Loss: 0.089090\n",
      "Epoch 4/50 - Train Loss: 0.041081 - Val Loss: 0.073198\n",
      "Epoch 5/50 - Train Loss: 0.038786 - Val Loss: 0.062890\n",
      "Epoch 6/50 - Train Loss: 0.035587 - Val Loss: 0.056097\n",
      "Epoch 7/50 - Train Loss: 0.029944 - Val Loss: 0.053049\n",
      "Epoch 8/50 - Train Loss: 0.033045 - Val Loss: 0.053593\n",
      "Epoch 9/50 - Train Loss: 0.032041 - Val Loss: 0.053048\n",
      "Epoch 10/50 - Train Loss: 0.031922 - Val Loss: 0.051156\n",
      "Epoch 11/50 - Train Loss: 0.029713 - Val Loss: 0.050851\n",
      "Epoch 12/50 - Train Loss: 0.030157 - Val Loss: 0.047754\n",
      "Epoch 13/50 - Train Loss: 0.027419 - Val Loss: 0.044032\n",
      "Epoch 14/50 - Train Loss: 0.029121 - Val Loss: 0.045064\n",
      "Epoch 15/50 - Train Loss: 0.031283 - Val Loss: 0.049792\n",
      "Epoch 16/50 - Train Loss: 0.029910 - Val Loss: 0.051611\n",
      "Epoch 17/50 - Train Loss: 0.028171 - Val Loss: 0.053306\n",
      "Epoch 18/50 - Train Loss: 0.027775 - Val Loss: 0.049381\n",
      "Epoch 19/50 - Train Loss: 0.028740 - Val Loss: 0.049692\n",
      "Epoch 20/50 - Train Loss: 0.029909 - Val Loss: 0.045358\n",
      "Epoch 21/50 - Train Loss: 0.027355 - Val Loss: 0.044575\n",
      "Epoch 22/50 - Train Loss: 0.030887 - Val Loss: 0.046066\n",
      "Epoch 23/50 - Train Loss: 0.030007 - Val Loss: 0.044816\n",
      "Epoch 24/50 - Train Loss: 0.026429 - Val Loss: 0.044056\n",
      "Epoch 25/50 - Train Loss: 0.029914 - Val Loss: 0.044015\n",
      "Epoch 26/50 - Train Loss: 0.029500 - Val Loss: 0.045025\n",
      "Epoch 27/50 - Train Loss: 0.028444 - Val Loss: 0.048026\n",
      "Epoch 28/50 - Train Loss: 0.029102 - Val Loss: 0.047913\n",
      "Epoch 29/50 - Train Loss: 0.027887 - Val Loss: 0.046024\n",
      "Epoch 30/50 - Train Loss: 0.028386 - Val Loss: 0.045876\n",
      "Epoch 31/50 - Train Loss: 0.027852 - Val Loss: 0.047491\n",
      "Epoch 32/50 - Train Loss: 0.026690 - Val Loss: 0.044543\n",
      "Epoch 33/50 - Train Loss: 0.027547 - Val Loss: 0.044535\n",
      "Epoch 34/50 - Train Loss: 0.029861 - Val Loss: 0.045131\n",
      "Epoch 35/50 - Train Loss: 0.031892 - Val Loss: 0.045478\n",
      "Epoch 36/50 - Train Loss: 0.028715 - Val Loss: 0.046101\n",
      "Epoch 37/50 - Train Loss: 0.027817 - Val Loss: 0.052033\n",
      "Epoch 38/50 - Train Loss: 0.030953 - Val Loss: 0.045136\n",
      "Epoch 39/50 - Train Loss: 0.029691 - Val Loss: 0.043957\n",
      "Epoch 40/50 - Train Loss: 0.030919 - Val Loss: 0.045397\n",
      "Epoch 41/50 - Train Loss: 0.026270 - Val Loss: 0.046872\n",
      "Epoch 42/50 - Train Loss: 0.032499 - Val Loss: 0.050504\n",
      "Epoch 43/50 - Train Loss: 0.030372 - Val Loss: 0.059860\n",
      "Epoch 44/50 - Train Loss: 0.033087 - Val Loss: 0.047519\n",
      "Epoch 45/50 - Train Loss: 0.028108 - Val Loss: 0.045367\n",
      "Epoch 46/50 - Train Loss: 0.029343 - Val Loss: 0.043895\n",
      "Epoch 47/50 - Train Loss: 0.032147 - Val Loss: 0.047809\n",
      "Epoch 48/50 - Train Loss: 0.029505 - Val Loss: 0.048018\n",
      "Epoch 49/50 - Train Loss: 0.028814 - Val Loss: 0.046488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:27:20,116] Trial 37 finished with value: 0.04389521976312002 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 103, 'lr': 0.0006257332803867805, 'weight_decay': 7.166816413825919e-07, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030505 - Val Loss: 0.051151\n",
      "Epoch 1/50 - Train Loss: 0.113723 - Val Loss: 0.105511\n",
      "Epoch 2/50 - Train Loss: 0.044608 - Val Loss: 0.084199\n",
      "Epoch 3/50 - Train Loss: 0.044587 - Val Loss: 0.088825\n",
      "Epoch 4/50 - Train Loss: 0.043245 - Val Loss: 0.079510\n",
      "Epoch 5/50 - Train Loss: 0.039602 - Val Loss: 0.074059\n",
      "Epoch 6/50 - Train Loss: 0.036153 - Val Loss: 0.067045\n",
      "Epoch 7/50 - Train Loss: 0.036643 - Val Loss: 0.049654\n",
      "Epoch 8/50 - Train Loss: 0.031622 - Val Loss: 0.052969\n",
      "Epoch 9/50 - Train Loss: 0.031889 - Val Loss: 0.055615\n",
      "Epoch 10/50 - Train Loss: 0.028835 - Val Loss: 0.045407\n",
      "Epoch 11/50 - Train Loss: 0.030903 - Val Loss: 0.048554\n",
      "Epoch 12/50 - Train Loss: 0.029044 - Val Loss: 0.050849\n",
      "Epoch 13/50 - Train Loss: 0.030741 - Val Loss: 0.047973\n",
      "Epoch 14/50 - Train Loss: 0.030569 - Val Loss: 0.051122\n",
      "Epoch 15/50 - Train Loss: 0.029893 - Val Loss: 0.052595\n",
      "Epoch 16/50 - Train Loss: 0.030119 - Val Loss: 0.050042\n",
      "Epoch 17/50 - Train Loss: 0.032116 - Val Loss: 0.045719\n",
      "Epoch 18/50 - Train Loss: 0.029999 - Val Loss: 0.046112\n",
      "Epoch 19/50 - Train Loss: 0.029555 - Val Loss: 0.044247\n",
      "Epoch 20/50 - Train Loss: 0.026601 - Val Loss: 0.044118\n",
      "Epoch 21/50 - Train Loss: 0.027995 - Val Loss: 0.044096\n",
      "Epoch 22/50 - Train Loss: 0.027392 - Val Loss: 0.046363\n",
      "Epoch 23/50 - Train Loss: 0.031648 - Val Loss: 0.056853\n",
      "Epoch 24/50 - Train Loss: 0.028747 - Val Loss: 0.046360\n",
      "Epoch 25/50 - Train Loss: 0.029321 - Val Loss: 0.044963\n",
      "Epoch 26/50 - Train Loss: 0.030165 - Val Loss: 0.045466\n",
      "Epoch 27/50 - Train Loss: 0.029834 - Val Loss: 0.045252\n",
      "Epoch 28/50 - Train Loss: 0.030009 - Val Loss: 0.045596\n",
      "Epoch 29/50 - Train Loss: 0.028042 - Val Loss: 0.043948\n",
      "Epoch 30/50 - Train Loss: 0.027265 - Val Loss: 0.043476\n",
      "Epoch 31/50 - Train Loss: 0.028014 - Val Loss: 0.046106\n",
      "Epoch 32/50 - Train Loss: 0.029797 - Val Loss: 0.047087\n",
      "Epoch 33/50 - Train Loss: 0.030761 - Val Loss: 0.050106\n",
      "Epoch 34/50 - Train Loss: 0.027121 - Val Loss: 0.043613\n",
      "Epoch 35/50 - Train Loss: 0.030940 - Val Loss: 0.043728\n",
      "Epoch 36/50 - Train Loss: 0.029907 - Val Loss: 0.044558\n",
      "Epoch 37/50 - Train Loss: 0.029375 - Val Loss: 0.043579\n",
      "Epoch 38/50 - Train Loss: 0.028751 - Val Loss: 0.044696\n",
      "Epoch 39/50 - Train Loss: 0.028806 - Val Loss: 0.043434\n",
      "Epoch 40/50 - Train Loss: 0.027966 - Val Loss: 0.044504\n",
      "Epoch 41/50 - Train Loss: 0.026530 - Val Loss: 0.044159\n",
      "Epoch 42/50 - Train Loss: 0.029997 - Val Loss: 0.047615\n",
      "Epoch 43/50 - Train Loss: 0.027403 - Val Loss: 0.043655\n",
      "Epoch 44/50 - Train Loss: 0.028014 - Val Loss: 0.043552\n",
      "Epoch 45/50 - Train Loss: 0.029751 - Val Loss: 0.045060\n",
      "Epoch 46/50 - Train Loss: 0.027788 - Val Loss: 0.044770\n",
      "Epoch 47/50 - Train Loss: 0.027976 - Val Loss: 0.044070\n",
      "Epoch 48/50 - Train Loss: 0.028519 - Val Loss: 0.044366\n",
      "Epoch 49/50 - Train Loss: 0.027279 - Val Loss: 0.043883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:27:51,266] Trial 38 finished with value: 0.04343394686778387 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 65, 'lr': 0.0012354998284631199, 'weight_decay': 2.3684678937911896e-07, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030510 - Val Loss: 0.044011\n",
      "Epoch 1/50 - Train Loss: 0.070576 - Val Loss: 0.088765\n",
      "Epoch 2/50 - Train Loss: 0.045880 - Val Loss: 0.101094\n",
      "Epoch 3/50 - Train Loss: 0.040249 - Val Loss: 0.063055\n",
      "Epoch 4/50 - Train Loss: 0.040263 - Val Loss: 0.089107\n",
      "Epoch 5/50 - Train Loss: 0.038477 - Val Loss: 0.056718\n",
      "Epoch 6/50 - Train Loss: 0.038148 - Val Loss: 0.058676\n",
      "Epoch 7/50 - Train Loss: 0.035873 - Val Loss: 0.052227\n",
      "Epoch 8/50 - Train Loss: 0.033251 - Val Loss: 0.046396\n",
      "Epoch 9/50 - Train Loss: 0.032230 - Val Loss: 0.045685\n",
      "Epoch 10/50 - Train Loss: 0.029425 - Val Loss: 0.043890\n",
      "Epoch 11/50 - Train Loss: 0.031668 - Val Loss: 0.043924\n",
      "Epoch 12/50 - Train Loss: 0.031589 - Val Loss: 0.044670\n",
      "Epoch 13/50 - Train Loss: 0.031236 - Val Loss: 0.056260\n",
      "Epoch 14/50 - Train Loss: 0.033256 - Val Loss: 0.045102\n",
      "Epoch 15/50 - Train Loss: 0.027584 - Val Loss: 0.046305\n",
      "Epoch 16/50 - Train Loss: 0.029518 - Val Loss: 0.049026\n",
      "Epoch 17/50 - Train Loss: 0.031967 - Val Loss: 0.044097\n",
      "Epoch 18/50 - Train Loss: 0.030389 - Val Loss: 0.044792\n",
      "Epoch 19/50 - Train Loss: 0.033428 - Val Loss: 0.045102\n",
      "Epoch 20/50 - Train Loss: 0.029205 - Val Loss: 0.046976\n",
      "Epoch 21/50 - Train Loss: 0.026754 - Val Loss: 0.044144\n",
      "Epoch 22/50 - Train Loss: 0.029494 - Val Loss: 0.044649\n",
      "Epoch 23/50 - Train Loss: 0.033749 - Val Loss: 0.045771\n",
      "Epoch 24/50 - Train Loss: 0.030981 - Val Loss: 0.045323\n",
      "Epoch 25/50 - Train Loss: 0.031518 - Val Loss: 0.044000\n",
      "Epoch 26/50 - Train Loss: 0.028383 - Val Loss: 0.044562\n",
      "Epoch 27/50 - Train Loss: 0.031837 - Val Loss: 0.043938\n",
      "Epoch 28/50 - Train Loss: 0.029076 - Val Loss: 0.047518\n",
      "Epoch 29/50 - Train Loss: 0.030697 - Val Loss: 0.044172\n",
      "Epoch 30/50 - Train Loss: 0.029261 - Val Loss: 0.045417\n",
      "Epoch 31/50 - Train Loss: 0.030207 - Val Loss: 0.046489\n",
      "Epoch 32/50 - Train Loss: 0.031064 - Val Loss: 0.043832\n",
      "Epoch 33/50 - Train Loss: 0.030752 - Val Loss: 0.043581\n",
      "Epoch 34/50 - Train Loss: 0.030336 - Val Loss: 0.044654\n",
      "Epoch 35/50 - Train Loss: 0.029307 - Val Loss: 0.043683\n",
      "Epoch 36/50 - Train Loss: 0.027027 - Val Loss: 0.043708\n",
      "Epoch 37/50 - Train Loss: 0.031218 - Val Loss: 0.045595\n",
      "Epoch 38/50 - Train Loss: 0.029093 - Val Loss: 0.046781\n",
      "Epoch 39/50 - Train Loss: 0.030447 - Val Loss: 0.047035\n",
      "Epoch 40/50 - Train Loss: 0.029046 - Val Loss: 0.045672\n",
      "Epoch 41/50 - Train Loss: 0.033301 - Val Loss: 0.052005\n",
      "Epoch 42/50 - Train Loss: 0.029964 - Val Loss: 0.050650\n",
      "Epoch 43/50 - Train Loss: 0.033813 - Val Loss: 0.043789\n",
      "Epoch 44/50 - Train Loss: 0.026976 - Val Loss: 0.044111\n",
      "Epoch 45/50 - Train Loss: 0.028821 - Val Loss: 0.044533\n",
      "Epoch 46/50 - Train Loss: 0.031323 - Val Loss: 0.044289\n",
      "Epoch 47/50 - Train Loss: 0.029483 - Val Loss: 0.051971\n",
      "Epoch 48/50 - Train Loss: 0.031750 - Val Loss: 0.043938\n",
      "Epoch 49/50 - Train Loss: 0.027965 - Val Loss: 0.044226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:28:14,160] Trial 39 finished with value: 0.043580567464232445 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 147, 'lr': 0.001881606860891966, 'weight_decay': 6.169128318589859e-05, 'batch_size': 16}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028197 - Val Loss: 0.045372\n",
      "Epoch 1/50 - Train Loss: 0.060123 - Val Loss: 0.067146\n",
      "Epoch 2/50 - Train Loss: 0.036937 - Val Loss: 0.051768\n",
      "Epoch 3/50 - Train Loss: 0.032779 - Val Loss: 0.048865\n",
      "Epoch 4/50 - Train Loss: 0.030962 - Val Loss: 0.053443\n",
      "Epoch 5/50 - Train Loss: 0.031972 - Val Loss: 0.061621\n",
      "Epoch 6/50 - Train Loss: 0.030898 - Val Loss: 0.044647\n",
      "Epoch 7/50 - Train Loss: 0.031259 - Val Loss: 0.044716\n",
      "Epoch 8/50 - Train Loss: 0.031049 - Val Loss: 0.049533\n",
      "Epoch 9/50 - Train Loss: 0.032079 - Val Loss: 0.057539\n",
      "Epoch 10/50 - Train Loss: 0.028090 - Val Loss: 0.045280\n",
      "Epoch 11/50 - Train Loss: 0.034092 - Val Loss: 0.045420\n",
      "Epoch 12/50 - Train Loss: 0.031161 - Val Loss: 0.044599\n",
      "Epoch 13/50 - Train Loss: 0.028804 - Val Loss: 0.044355\n",
      "Epoch 14/50 - Train Loss: 0.029804 - Val Loss: 0.045458\n",
      "Epoch 15/50 - Train Loss: 0.028642 - Val Loss: 0.045459\n",
      "Epoch 16/50 - Train Loss: 0.028431 - Val Loss: 0.046747\n",
      "Epoch 17/50 - Train Loss: 0.028568 - Val Loss: 0.044373\n",
      "Epoch 18/50 - Train Loss: 0.029830 - Val Loss: 0.047033\n",
      "Epoch 19/50 - Train Loss: 0.030308 - Val Loss: 0.045677\n",
      "Epoch 20/50 - Train Loss: 0.028199 - Val Loss: 0.044583\n",
      "Epoch 21/50 - Train Loss: 0.027369 - Val Loss: 0.048621\n",
      "Epoch 22/50 - Train Loss: 0.033280 - Val Loss: 0.048831\n",
      "Epoch 23/50 - Train Loss: 0.030176 - Val Loss: 0.044701\n",
      "Epoch 24/50 - Train Loss: 0.027539 - Val Loss: 0.044604\n",
      "Epoch 25/50 - Train Loss: 0.029177 - Val Loss: 0.043789\n",
      "Epoch 26/50 - Train Loss: 0.027566 - Val Loss: 0.046214\n",
      "Epoch 27/50 - Train Loss: 0.030907 - Val Loss: 0.043417\n",
      "Epoch 28/50 - Train Loss: 0.029572 - Val Loss: 0.044229\n",
      "Epoch 29/50 - Train Loss: 0.029564 - Val Loss: 0.044042\n",
      "Epoch 30/50 - Train Loss: 0.029269 - Val Loss: 0.044691\n",
      "Epoch 31/50 - Train Loss: 0.029363 - Val Loss: 0.046257\n",
      "Epoch 32/50 - Train Loss: 0.031227 - Val Loss: 0.044207\n",
      "Epoch 33/50 - Train Loss: 0.028466 - Val Loss: 0.044047\n",
      "Epoch 34/50 - Train Loss: 0.030652 - Val Loss: 0.044836\n",
      "Epoch 35/50 - Train Loss: 0.029647 - Val Loss: 0.046875\n",
      "Epoch 36/50 - Train Loss: 0.026558 - Val Loss: 0.043663\n",
      "Epoch 37/50 - Train Loss: 0.028181 - Val Loss: 0.046153\n",
      "Epoch 38/50 - Train Loss: 0.031691 - Val Loss: 0.051642\n",
      "Epoch 39/50 - Train Loss: 0.029490 - Val Loss: 0.043400\n",
      "Epoch 40/50 - Train Loss: 0.032369 - Val Loss: 0.044147\n",
      "Epoch 41/50 - Train Loss: 0.030878 - Val Loss: 0.046700\n",
      "Epoch 42/50 - Train Loss: 0.030508 - Val Loss: 0.043444\n",
      "Epoch 43/50 - Train Loss: 0.032081 - Val Loss: 0.043699\n",
      "Epoch 44/50 - Train Loss: 0.029311 - Val Loss: 0.043911\n",
      "Epoch 45/50 - Train Loss: 0.027941 - Val Loss: 0.045437\n",
      "Epoch 46/50 - Train Loss: 0.026351 - Val Loss: 0.046066\n",
      "Epoch 47/50 - Train Loss: 0.030053 - Val Loss: 0.047261\n",
      "Epoch 48/50 - Train Loss: 0.030155 - Val Loss: 0.044476\n",
      "Epoch 49/50 - Train Loss: 0.029959 - Val Loss: 0.044257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:28:41,942] Trial 40 finished with value: 0.043400329848130546 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 167, 'lr': 0.00344012784693181, 'weight_decay': 2.2185755411022134e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030966 - Val Loss: 0.046635\n",
      "Epoch 1/50 - Train Loss: 0.059936 - Val Loss: 0.075164\n",
      "Epoch 2/50 - Train Loss: 0.041677 - Val Loss: 0.060992\n",
      "Epoch 3/50 - Train Loss: 0.030667 - Val Loss: 0.064729\n",
      "Epoch 4/50 - Train Loss: 0.031636 - Val Loss: 0.058741\n",
      "Epoch 5/50 - Train Loss: 0.034177 - Val Loss: 0.048861\n",
      "Epoch 6/50 - Train Loss: 0.029956 - Val Loss: 0.044428\n",
      "Epoch 7/50 - Train Loss: 0.031945 - Val Loss: 0.043992\n",
      "Epoch 8/50 - Train Loss: 0.031378 - Val Loss: 0.044704\n",
      "Epoch 9/50 - Train Loss: 0.030219 - Val Loss: 0.044069\n",
      "Epoch 10/50 - Train Loss: 0.028314 - Val Loss: 0.044059\n",
      "Epoch 11/50 - Train Loss: 0.029235 - Val Loss: 0.047165\n",
      "Epoch 12/50 - Train Loss: 0.028011 - Val Loss: 0.047055\n",
      "Epoch 13/50 - Train Loss: 0.029115 - Val Loss: 0.045943\n",
      "Epoch 14/50 - Train Loss: 0.029345 - Val Loss: 0.047350\n",
      "Epoch 15/50 - Train Loss: 0.030158 - Val Loss: 0.044030\n",
      "Epoch 16/50 - Train Loss: 0.029440 - Val Loss: 0.046609\n",
      "Epoch 17/50 - Train Loss: 0.029906 - Val Loss: 0.051830\n",
      "Epoch 18/50 - Train Loss: 0.032131 - Val Loss: 0.045719\n",
      "Epoch 19/50 - Train Loss: 0.028639 - Val Loss: 0.043496\n",
      "Epoch 20/50 - Train Loss: 0.029068 - Val Loss: 0.043591\n",
      "Epoch 21/50 - Train Loss: 0.029270 - Val Loss: 0.043690\n",
      "Epoch 22/50 - Train Loss: 0.030919 - Val Loss: 0.046898\n",
      "Epoch 23/50 - Train Loss: 0.030275 - Val Loss: 0.042990\n",
      "Epoch 24/50 - Train Loss: 0.031364 - Val Loss: 0.042972\n",
      "Epoch 25/50 - Train Loss: 0.030651 - Val Loss: 0.043634\n",
      "Epoch 26/50 - Train Loss: 0.029747 - Val Loss: 0.043199\n",
      "Epoch 27/50 - Train Loss: 0.031800 - Val Loss: 0.043096\n",
      "Epoch 28/50 - Train Loss: 0.026687 - Val Loss: 0.046691\n",
      "Epoch 29/50 - Train Loss: 0.029691 - Val Loss: 0.045632\n",
      "Epoch 30/50 - Train Loss: 0.027535 - Val Loss: 0.042783\n",
      "Epoch 31/50 - Train Loss: 0.028187 - Val Loss: 0.043203\n",
      "Epoch 32/50 - Train Loss: 0.030465 - Val Loss: 0.043134\n",
      "Epoch 33/50 - Train Loss: 0.029340 - Val Loss: 0.047328\n",
      "Epoch 34/50 - Train Loss: 0.029450 - Val Loss: 0.050574\n",
      "Epoch 35/50 - Train Loss: 0.030311 - Val Loss: 0.042925\n",
      "Epoch 36/50 - Train Loss: 0.029685 - Val Loss: 0.045278\n",
      "Epoch 37/50 - Train Loss: 0.027437 - Val Loss: 0.042922\n",
      "Epoch 38/50 - Train Loss: 0.029224 - Val Loss: 0.044943\n",
      "Epoch 39/50 - Train Loss: 0.028043 - Val Loss: 0.046421\n",
      "Epoch 40/50 - Train Loss: 0.029380 - Val Loss: 0.043505\n",
      "Epoch 41/50 - Train Loss: 0.028928 - Val Loss: 0.044351\n",
      "Epoch 42/50 - Train Loss: 0.027141 - Val Loss: 0.043670\n",
      "Epoch 43/50 - Train Loss: 0.027512 - Val Loss: 0.043395\n",
      "Epoch 44/50 - Train Loss: 0.029271 - Val Loss: 0.043455\n",
      "Epoch 45/50 - Train Loss: 0.027227 - Val Loss: 0.043421\n",
      "Epoch 46/50 - Train Loss: 0.029323 - Val Loss: 0.047195\n",
      "Epoch 47/50 - Train Loss: 0.026367 - Val Loss: 0.043821\n",
      "Epoch 48/50 - Train Loss: 0.027838 - Val Loss: 0.043714\n",
      "Epoch 49/50 - Train Loss: 0.026457 - Val Loss: 0.043414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:29:10,552] Trial 41 finished with value: 0.042783395697673164 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 166, 'lr': 0.003331176817773389, 'weight_decay': 1.8668848441917643e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028140 - Val Loss: 0.044936\n",
      "Epoch 1/50 - Train Loss: 0.152646 - Val Loss: 0.066119\n",
      "Epoch 2/50 - Train Loss: 0.039679 - Val Loss: 0.070110\n",
      "Epoch 3/50 - Train Loss: 0.036312 - Val Loss: 0.052712\n",
      "Epoch 4/50 - Train Loss: 0.030174 - Val Loss: 0.043956\n",
      "Epoch 5/50 - Train Loss: 0.029966 - Val Loss: 0.048534\n",
      "Epoch 6/50 - Train Loss: 0.033449 - Val Loss: 0.054705\n",
      "Epoch 7/50 - Train Loss: 0.028934 - Val Loss: 0.046301\n",
      "Epoch 8/50 - Train Loss: 0.030454 - Val Loss: 0.043698\n",
      "Epoch 9/50 - Train Loss: 0.030666 - Val Loss: 0.050821\n",
      "Epoch 10/50 - Train Loss: 0.033082 - Val Loss: 0.046587\n",
      "Epoch 11/50 - Train Loss: 0.032547 - Val Loss: 0.043444\n",
      "Epoch 12/50 - Train Loss: 0.032503 - Val Loss: 0.047720\n",
      "Epoch 13/50 - Train Loss: 0.031319 - Val Loss: 0.052622\n",
      "Epoch 14/50 - Train Loss: 0.031393 - Val Loss: 0.050350\n",
      "Epoch 15/50 - Train Loss: 0.031242 - Val Loss: 0.045749\n",
      "Epoch 16/50 - Train Loss: 0.030369 - Val Loss: 0.044115\n",
      "Epoch 17/50 - Train Loss: 0.030594 - Val Loss: 0.043658\n",
      "Epoch 18/50 - Train Loss: 0.030400 - Val Loss: 0.060655\n",
      "Epoch 19/50 - Train Loss: 0.032134 - Val Loss: 0.047735\n",
      "Epoch 20/50 - Train Loss: 0.028907 - Val Loss: 0.043808\n",
      "Epoch 21/50 - Train Loss: 0.030797 - Val Loss: 0.044328\n",
      "Epoch 22/50 - Train Loss: 0.034701 - Val Loss: 0.049764\n",
      "Epoch 23/50 - Train Loss: 0.028849 - Val Loss: 0.043317\n",
      "Epoch 24/50 - Train Loss: 0.028308 - Val Loss: 0.044098\n",
      "Epoch 25/50 - Train Loss: 0.030396 - Val Loss: 0.044111\n",
      "Epoch 26/50 - Train Loss: 0.029325 - Val Loss: 0.044098\n",
      "Epoch 27/50 - Train Loss: 0.029324 - Val Loss: 0.043629\n",
      "Epoch 28/50 - Train Loss: 0.028160 - Val Loss: 0.044871\n",
      "Epoch 29/50 - Train Loss: 0.029055 - Val Loss: 0.046439\n",
      "Epoch 30/50 - Train Loss: 0.030138 - Val Loss: 0.044019\n",
      "Epoch 31/50 - Train Loss: 0.030468 - Val Loss: 0.044327\n",
      "Epoch 32/50 - Train Loss: 0.029375 - Val Loss: 0.046564\n",
      "Epoch 33/50 - Train Loss: 0.031067 - Val Loss: 0.044398\n",
      "Epoch 34/50 - Train Loss: 0.028376 - Val Loss: 0.050252\n",
      "Epoch 35/50 - Train Loss: 0.029076 - Val Loss: 0.048637\n",
      "Epoch 36/50 - Train Loss: 0.032346 - Val Loss: 0.045990\n",
      "Epoch 37/50 - Train Loss: 0.028680 - Val Loss: 0.045784\n",
      "Epoch 38/50 - Train Loss: 0.030758 - Val Loss: 0.051199\n",
      "Epoch 39/50 - Train Loss: 0.029326 - Val Loss: 0.049315\n",
      "Epoch 40/50 - Train Loss: 0.029053 - Val Loss: 0.044861\n",
      "Epoch 41/50 - Train Loss: 0.029295 - Val Loss: 0.046791\n",
      "Epoch 42/50 - Train Loss: 0.028261 - Val Loss: 0.048090\n",
      "Epoch 43/50 - Train Loss: 0.027662 - Val Loss: 0.049045\n",
      "Epoch 44/50 - Train Loss: 0.028268 - Val Loss: 0.047205\n",
      "Epoch 45/50 - Train Loss: 0.031269 - Val Loss: 0.046245\n",
      "Epoch 46/50 - Train Loss: 0.028340 - Val Loss: 0.045632\n",
      "Epoch 47/50 - Train Loss: 0.030901 - Val Loss: 0.048245\n",
      "Epoch 48/50 - Train Loss: 0.028671 - Val Loss: 0.045392\n",
      "Epoch 49/50 - Train Loss: 0.026555 - Val Loss: 0.048240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:29:39,740] Trial 42 finished with value: 0.043317001312971115 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 204, 'lr': 0.007271132747649227, 'weight_decay': 1.717304207539127e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028381 - Val Loss: 0.046433\n",
      "Epoch 1/50 - Train Loss: 0.060670 - Val Loss: 0.079216\n",
      "Epoch 2/50 - Train Loss: 0.035812 - Val Loss: 0.048866\n",
      "Epoch 3/50 - Train Loss: 0.032263 - Val Loss: 0.046417\n",
      "Epoch 4/50 - Train Loss: 0.028871 - Val Loss: 0.048799\n",
      "Epoch 5/50 - Train Loss: 0.031843 - Val Loss: 0.045304\n",
      "Epoch 6/50 - Train Loss: 0.030567 - Val Loss: 0.044922\n",
      "Epoch 7/50 - Train Loss: 0.030369 - Val Loss: 0.045043\n",
      "Epoch 8/50 - Train Loss: 0.031132 - Val Loss: 0.049438\n",
      "Epoch 9/50 - Train Loss: 0.030209 - Val Loss: 0.046592\n",
      "Epoch 10/50 - Train Loss: 0.031407 - Val Loss: 0.046778\n",
      "Epoch 11/50 - Train Loss: 0.028173 - Val Loss: 0.044995\n",
      "Epoch 12/50 - Train Loss: 0.028783 - Val Loss: 0.053310\n",
      "Epoch 13/50 - Train Loss: 0.030113 - Val Loss: 0.045427\n",
      "Epoch 14/50 - Train Loss: 0.030971 - Val Loss: 0.044634\n",
      "Epoch 15/50 - Train Loss: 0.030322 - Val Loss: 0.050955\n",
      "Epoch 16/50 - Train Loss: 0.029897 - Val Loss: 0.046830\n",
      "Epoch 17/50 - Train Loss: 0.029625 - Val Loss: 0.054232\n",
      "Epoch 18/50 - Train Loss: 0.029979 - Val Loss: 0.045914\n",
      "Epoch 19/50 - Train Loss: 0.029356 - Val Loss: 0.044926\n",
      "Epoch 20/50 - Train Loss: 0.028790 - Val Loss: 0.046457\n",
      "Epoch 21/50 - Train Loss: 0.031502 - Val Loss: 0.044154\n",
      "Epoch 22/50 - Train Loss: 0.029095 - Val Loss: 0.046843\n",
      "Epoch 23/50 - Train Loss: 0.028664 - Val Loss: 0.052788\n",
      "Epoch 24/50 - Train Loss: 0.028826 - Val Loss: 0.046651\n",
      "Epoch 25/50 - Train Loss: 0.028078 - Val Loss: 0.044859\n",
      "Epoch 26/50 - Train Loss: 0.031702 - Val Loss: 0.045959\n",
      "Epoch 27/50 - Train Loss: 0.030272 - Val Loss: 0.046228\n",
      "Epoch 28/50 - Train Loss: 0.030797 - Val Loss: 0.055391\n",
      "Epoch 29/50 - Train Loss: 0.025348 - Val Loss: 0.047806\n",
      "Epoch 30/50 - Train Loss: 0.030842 - Val Loss: 0.045510\n",
      "Epoch 31/50 - Train Loss: 0.029969 - Val Loss: 0.048449\n",
      "Epoch 32/50 - Train Loss: 0.028354 - Val Loss: 0.046418\n",
      "Epoch 33/50 - Train Loss: 0.031882 - Val Loss: 0.044805\n",
      "Epoch 34/50 - Train Loss: 0.029752 - Val Loss: 0.044767\n",
      "Epoch 35/50 - Train Loss: 0.029253 - Val Loss: 0.043969\n",
      "Epoch 36/50 - Train Loss: 0.028127 - Val Loss: 0.044791\n",
      "Epoch 37/50 - Train Loss: 0.028725 - Val Loss: 0.045834\n",
      "Epoch 38/50 - Train Loss: 0.029200 - Val Loss: 0.043513\n",
      "Epoch 39/50 - Train Loss: 0.029838 - Val Loss: 0.044431\n",
      "Epoch 40/50 - Train Loss: 0.028483 - Val Loss: 0.044209\n",
      "Epoch 41/50 - Train Loss: 0.030479 - Val Loss: 0.048350\n",
      "Epoch 42/50 - Train Loss: 0.028118 - Val Loss: 0.044706\n",
      "Epoch 43/50 - Train Loss: 0.027710 - Val Loss: 0.046393\n",
      "Epoch 44/50 - Train Loss: 0.029828 - Val Loss: 0.044883\n",
      "Epoch 45/50 - Train Loss: 0.030899 - Val Loss: 0.045537\n",
      "Epoch 46/50 - Train Loss: 0.026748 - Val Loss: 0.051551\n",
      "Epoch 47/50 - Train Loss: 0.028652 - Val Loss: 0.049882\n",
      "Epoch 48/50 - Train Loss: 0.025882 - Val Loss: 0.044910\n",
      "Epoch 49/50 - Train Loss: 0.028984 - Val Loss: 0.052019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:30:09,694] Trial 43 finished with value: 0.04351317510008812 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 198, 'lr': 0.005082063032064106, 'weight_decay': 7.600496050337618e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030090 - Val Loss: 0.044383\n",
      "Epoch 1/50 - Train Loss: 0.062218 - Val Loss: 0.071696\n",
      "Epoch 2/50 - Train Loss: 0.036097 - Val Loss: 0.056123\n",
      "Epoch 3/50 - Train Loss: 0.032825 - Val Loss: 0.047044\n",
      "Epoch 4/50 - Train Loss: 0.031368 - Val Loss: 0.053293\n",
      "Epoch 5/50 - Train Loss: 0.031315 - Val Loss: 0.053227\n",
      "Epoch 6/50 - Train Loss: 0.028921 - Val Loss: 0.048448\n",
      "Epoch 7/50 - Train Loss: 0.029779 - Val Loss: 0.044379\n",
      "Epoch 8/50 - Train Loss: 0.030783 - Val Loss: 0.044269\n",
      "Epoch 9/50 - Train Loss: 0.029629 - Val Loss: 0.044447\n",
      "Epoch 10/50 - Train Loss: 0.028537 - Val Loss: 0.046047\n",
      "Epoch 11/50 - Train Loss: 0.032194 - Val Loss: 0.045914\n",
      "Epoch 12/50 - Train Loss: 0.027312 - Val Loss: 0.043937\n",
      "Epoch 13/50 - Train Loss: 0.029141 - Val Loss: 0.043527\n",
      "Epoch 14/50 - Train Loss: 0.029256 - Val Loss: 0.046269\n",
      "Epoch 15/50 - Train Loss: 0.029735 - Val Loss: 0.043802\n",
      "Epoch 16/50 - Train Loss: 0.029095 - Val Loss: 0.046246\n",
      "Epoch 17/50 - Train Loss: 0.029423 - Val Loss: 0.043926\n",
      "Epoch 18/50 - Train Loss: 0.029402 - Val Loss: 0.051217\n",
      "Epoch 19/50 - Train Loss: 0.031283 - Val Loss: 0.047029\n",
      "Epoch 20/50 - Train Loss: 0.030366 - Val Loss: 0.043099\n",
      "Epoch 21/50 - Train Loss: 0.028079 - Val Loss: 0.046119\n",
      "Epoch 22/50 - Train Loss: 0.029770 - Val Loss: 0.043626\n",
      "Epoch 23/50 - Train Loss: 0.030576 - Val Loss: 0.043343\n",
      "Epoch 24/50 - Train Loss: 0.027492 - Val Loss: 0.044056\n",
      "Epoch 25/50 - Train Loss: 0.029316 - Val Loss: 0.047486\n",
      "Epoch 26/50 - Train Loss: 0.028644 - Val Loss: 0.043471\n",
      "Epoch 27/50 - Train Loss: 0.031188 - Val Loss: 0.043521\n",
      "Epoch 28/50 - Train Loss: 0.029045 - Val Loss: 0.044759\n",
      "Epoch 29/50 - Train Loss: 0.028493 - Val Loss: 0.043734\n",
      "Epoch 30/50 - Train Loss: 0.027438 - Val Loss: 0.043563\n",
      "Epoch 31/50 - Train Loss: 0.028950 - Val Loss: 0.043728\n",
      "Epoch 32/50 - Train Loss: 0.025546 - Val Loss: 0.043159\n",
      "Epoch 33/50 - Train Loss: 0.029369 - Val Loss: 0.045611\n",
      "Epoch 34/50 - Train Loss: 0.029162 - Val Loss: 0.043891\n",
      "Epoch 35/50 - Train Loss: 0.029756 - Val Loss: 0.046172\n",
      "Epoch 36/50 - Train Loss: 0.028017 - Val Loss: 0.043615\n",
      "Epoch 37/50 - Train Loss: 0.027786 - Val Loss: 0.043674\n",
      "Epoch 38/50 - Train Loss: 0.027001 - Val Loss: 0.046196\n",
      "Epoch 39/50 - Train Loss: 0.028271 - Val Loss: 0.047569\n",
      "Epoch 40/50 - Train Loss: 0.027677 - Val Loss: 0.044095\n",
      "Epoch 41/50 - Train Loss: 0.029686 - Val Loss: 0.043801\n",
      "Epoch 42/50 - Train Loss: 0.029847 - Val Loss: 0.045218\n",
      "Epoch 43/50 - Train Loss: 0.028827 - Val Loss: 0.043944\n",
      "Epoch 44/50 - Train Loss: 0.031244 - Val Loss: 0.044159\n",
      "Epoch 45/50 - Train Loss: 0.025608 - Val Loss: 0.045901\n",
      "Epoch 46/50 - Train Loss: 0.027357 - Val Loss: 0.049388\n",
      "Epoch 47/50 - Train Loss: 0.028956 - Val Loss: 0.044064\n",
      "Epoch 48/50 - Train Loss: 0.029554 - Val Loss: 0.044427\n",
      "Epoch 49/50 - Train Loss: 0.027326 - Val Loss: 0.043686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:30:32,470] Trial 44 finished with value: 0.043099322666724525 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 189, 'lr': 0.0020848700152803195, 'weight_decay': 3.442273800307092e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027115 - Val Loss: 0.044630\n",
      "Epoch 1/50 - Train Loss: 0.100957 - Val Loss: 0.070703\n",
      "Epoch 2/50 - Train Loss: 0.043880 - Val Loss: 0.088682\n",
      "Epoch 3/50 - Train Loss: 0.038587 - Val Loss: 0.062981\n",
      "Epoch 4/50 - Train Loss: 0.035421 - Val Loss: 0.059526\n",
      "Epoch 5/50 - Train Loss: 0.033925 - Val Loss: 0.060398\n",
      "Epoch 6/50 - Train Loss: 0.031073 - Val Loss: 0.048489\n",
      "Epoch 7/50 - Train Loss: 0.029037 - Val Loss: 0.046696\n",
      "Epoch 8/50 - Train Loss: 0.030995 - Val Loss: 0.046715\n",
      "Epoch 9/50 - Train Loss: 0.031478 - Val Loss: 0.045463\n",
      "Epoch 10/50 - Train Loss: 0.029104 - Val Loss: 0.046305\n",
      "Epoch 11/50 - Train Loss: 0.031672 - Val Loss: 0.048293\n",
      "Epoch 12/50 - Train Loss: 0.029065 - Val Loss: 0.047404\n",
      "Epoch 13/50 - Train Loss: 0.029842 - Val Loss: 0.046237\n",
      "Epoch 14/50 - Train Loss: 0.028594 - Val Loss: 0.046065\n",
      "Epoch 15/50 - Train Loss: 0.030019 - Val Loss: 0.053923\n",
      "Epoch 16/50 - Train Loss: 0.030021 - Val Loss: 0.051321\n",
      "Epoch 17/50 - Train Loss: 0.029719 - Val Loss: 0.046867\n",
      "Epoch 18/50 - Train Loss: 0.030221 - Val Loss: 0.046205\n",
      "Epoch 19/50 - Train Loss: 0.028323 - Val Loss: 0.044015\n",
      "Epoch 20/50 - Train Loss: 0.030535 - Val Loss: 0.045671\n",
      "Epoch 21/50 - Train Loss: 0.028887 - Val Loss: 0.045605\n",
      "Epoch 22/50 - Train Loss: 0.027943 - Val Loss: 0.044891\n",
      "Epoch 23/50 - Train Loss: 0.028853 - Val Loss: 0.045481\n",
      "Epoch 24/50 - Train Loss: 0.029990 - Val Loss: 0.044290\n",
      "Epoch 25/50 - Train Loss: 0.028396 - Val Loss: 0.044394\n",
      "Epoch 26/50 - Train Loss: 0.030995 - Val Loss: 0.044425\n",
      "Epoch 27/50 - Train Loss: 0.029685 - Val Loss: 0.045084\n",
      "Epoch 28/50 - Train Loss: 0.029576 - Val Loss: 0.044535\n",
      "Epoch 29/50 - Train Loss: 0.026752 - Val Loss: 0.045975\n",
      "Epoch 30/50 - Train Loss: 0.028324 - Val Loss: 0.046137\n",
      "Epoch 31/50 - Train Loss: 0.030568 - Val Loss: 0.047207\n",
      "Epoch 32/50 - Train Loss: 0.027987 - Val Loss: 0.045566\n",
      "Epoch 33/50 - Train Loss: 0.026673 - Val Loss: 0.049085\n",
      "Epoch 34/50 - Train Loss: 0.029943 - Val Loss: 0.048465\n",
      "Epoch 35/50 - Train Loss: 0.029361 - Val Loss: 0.048596\n",
      "Epoch 36/50 - Train Loss: 0.029086 - Val Loss: 0.044262\n",
      "Epoch 37/50 - Train Loss: 0.028717 - Val Loss: 0.044581\n",
      "Epoch 38/50 - Train Loss: 0.030120 - Val Loss: 0.044420\n",
      "Epoch 39/50 - Train Loss: 0.029746 - Val Loss: 0.048276\n",
      "Epoch 40/50 - Train Loss: 0.029768 - Val Loss: 0.046128\n",
      "Epoch 41/50 - Train Loss: 0.028714 - Val Loss: 0.045234\n",
      "Epoch 42/50 - Train Loss: 0.028780 - Val Loss: 0.044475\n",
      "Epoch 43/50 - Train Loss: 0.029971 - Val Loss: 0.044211\n",
      "Epoch 44/50 - Train Loss: 0.027478 - Val Loss: 0.046645\n",
      "Epoch 45/50 - Train Loss: 0.031595 - Val Loss: 0.044880\n",
      "Epoch 46/50 - Train Loss: 0.027091 - Val Loss: 0.045848\n",
      "Epoch 47/50 - Train Loss: 0.027858 - Val Loss: 0.044653\n",
      "Epoch 48/50 - Train Loss: 0.028869 - Val Loss: 0.044750\n",
      "Epoch 49/50 - Train Loss: 0.030413 - Val Loss: 0.044746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:30:55,497] Trial 45 finished with value: 0.044014559437831245 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 187, 'lr': 0.000852154030683347, 'weight_decay': 3.753175450090966e-08, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027627 - Val Loss: 0.044908\n",
      "Epoch 1/50 - Train Loss: 0.067031 - Val Loss: 0.094026\n",
      "Epoch 2/50 - Train Loss: 0.035146 - Val Loss: 0.047956\n",
      "Epoch 3/50 - Train Loss: 0.034133 - Val Loss: 0.045943\n",
      "Epoch 4/50 - Train Loss: 0.031472 - Val Loss: 0.052204\n",
      "Epoch 5/50 - Train Loss: 0.030277 - Val Loss: 0.045069\n",
      "Epoch 6/50 - Train Loss: 0.028340 - Val Loss: 0.046219\n",
      "Epoch 7/50 - Train Loss: 0.030835 - Val Loss: 0.046595\n",
      "Epoch 8/50 - Train Loss: 0.030779 - Val Loss: 0.049350\n",
      "Epoch 9/50 - Train Loss: 0.029956 - Val Loss: 0.045953\n",
      "Epoch 10/50 - Train Loss: 0.027195 - Val Loss: 0.051058\n",
      "Epoch 11/50 - Train Loss: 0.032513 - Val Loss: 0.049874\n",
      "Epoch 12/50 - Train Loss: 0.030164 - Val Loss: 0.044573\n",
      "Epoch 13/50 - Train Loss: 0.031410 - Val Loss: 0.044364\n",
      "Epoch 14/50 - Train Loss: 0.029137 - Val Loss: 0.046039\n",
      "Epoch 15/50 - Train Loss: 0.030972 - Val Loss: 0.046958\n",
      "Epoch 16/50 - Train Loss: 0.030323 - Val Loss: 0.045175\n",
      "Epoch 17/50 - Train Loss: 0.029103 - Val Loss: 0.047623\n",
      "Epoch 18/50 - Train Loss: 0.029429 - Val Loss: 0.044689\n",
      "Epoch 19/50 - Train Loss: 0.030001 - Val Loss: 0.047638\n",
      "Epoch 20/50 - Train Loss: 0.028081 - Val Loss: 0.049309\n",
      "Epoch 21/50 - Train Loss: 0.029459 - Val Loss: 0.045428\n",
      "Epoch 22/50 - Train Loss: 0.028097 - Val Loss: 0.044924\n",
      "Epoch 23/50 - Train Loss: 0.030221 - Val Loss: 0.045078\n",
      "Epoch 24/50 - Train Loss: 0.027180 - Val Loss: 0.045039\n",
      "Epoch 25/50 - Train Loss: 0.030117 - Val Loss: 0.047546\n",
      "Epoch 26/50 - Train Loss: 0.029610 - Val Loss: 0.044614\n",
      "Epoch 27/50 - Train Loss: 0.027860 - Val Loss: 0.045880\n",
      "Epoch 28/50 - Train Loss: 0.030051 - Val Loss: 0.048059\n",
      "Epoch 29/50 - Train Loss: 0.031710 - Val Loss: 0.046585\n",
      "Epoch 30/50 - Train Loss: 0.029679 - Val Loss: 0.045350\n",
      "Epoch 31/50 - Train Loss: 0.027739 - Val Loss: 0.046188\n",
      "Epoch 32/50 - Train Loss: 0.027333 - Val Loss: 0.045305\n",
      "Epoch 33/50 - Train Loss: 0.029828 - Val Loss: 0.047947\n",
      "Epoch 34/50 - Train Loss: 0.027648 - Val Loss: 0.046791\n",
      "Epoch 35/50 - Train Loss: 0.028523 - Val Loss: 0.046112\n",
      "Epoch 36/50 - Train Loss: 0.029461 - Val Loss: 0.049461\n",
      "Epoch 37/50 - Train Loss: 0.032392 - Val Loss: 0.048534\n",
      "Epoch 38/50 - Train Loss: 0.029297 - Val Loss: 0.046882\n",
      "Epoch 39/50 - Train Loss: 0.027822 - Val Loss: 0.046550\n",
      "Epoch 40/50 - Train Loss: 0.029880 - Val Loss: 0.047786\n",
      "Epoch 41/50 - Train Loss: 0.029475 - Val Loss: 0.053731\n",
      "Epoch 42/50 - Train Loss: 0.027971 - Val Loss: 0.047267\n",
      "Epoch 43/50 - Train Loss: 0.028481 - Val Loss: 0.049902\n",
      "Epoch 44/50 - Train Loss: 0.030318 - Val Loss: 0.048094\n",
      "Epoch 45/50 - Train Loss: 0.028525 - Val Loss: 0.046478\n",
      "Epoch 46/50 - Train Loss: 0.029315 - Val Loss: 0.048172\n",
      "Epoch 47/50 - Train Loss: 0.026025 - Val Loss: 0.045836\n",
      "Epoch 48/50 - Train Loss: 0.026794 - Val Loss: 0.046785\n",
      "Epoch 49/50 - Train Loss: 0.029693 - Val Loss: 0.047803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:31:17,765] Trial 46 finished with value: 0.044364153097073235 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 163, 'lr': 0.0018892662927759622, 'weight_decay': 1.570757357724141e-07, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028043 - Val Loss: 0.047517\n",
      "Epoch 1/50 - Train Loss: 0.070117 - Val Loss: 0.114395\n",
      "Epoch 2/50 - Train Loss: 0.042246 - Val Loss: 0.071110\n",
      "Epoch 3/50 - Train Loss: 0.034193 - Val Loss: 0.054278\n",
      "Epoch 4/50 - Train Loss: 0.029659 - Val Loss: 0.047919\n",
      "Epoch 5/50 - Train Loss: 0.033794 - Val Loss: 0.047265\n",
      "Epoch 6/50 - Train Loss: 0.030351 - Val Loss: 0.046497\n",
      "Epoch 7/50 - Train Loss: 0.026937 - Val Loss: 0.045717\n",
      "Epoch 8/50 - Train Loss: 0.029602 - Val Loss: 0.046376\n",
      "Epoch 9/50 - Train Loss: 0.032671 - Val Loss: 0.047303\n",
      "Epoch 10/50 - Train Loss: 0.030363 - Val Loss: 0.050614\n",
      "Epoch 11/50 - Train Loss: 0.029402 - Val Loss: 0.051575\n",
      "Epoch 12/50 - Train Loss: 0.030199 - Val Loss: 0.046856\n",
      "Epoch 13/50 - Train Loss: 0.028162 - Val Loss: 0.044507\n",
      "Epoch 14/50 - Train Loss: 0.030834 - Val Loss: 0.044868\n",
      "Epoch 15/50 - Train Loss: 0.030128 - Val Loss: 0.044888\n",
      "Epoch 16/50 - Train Loss: 0.028218 - Val Loss: 0.045423\n",
      "Epoch 17/50 - Train Loss: 0.029640 - Val Loss: 0.046539\n",
      "Epoch 18/50 - Train Loss: 0.028764 - Val Loss: 0.044800\n",
      "Epoch 19/50 - Train Loss: 0.028220 - Val Loss: 0.044478\n",
      "Epoch 20/50 - Train Loss: 0.028612 - Val Loss: 0.048209\n",
      "Epoch 21/50 - Train Loss: 0.029762 - Val Loss: 0.048985\n",
      "Epoch 22/50 - Train Loss: 0.030900 - Val Loss: 0.048169\n",
      "Epoch 23/50 - Train Loss: 0.030110 - Val Loss: 0.044385\n",
      "Epoch 24/50 - Train Loss: 0.029898 - Val Loss: 0.044640\n",
      "Epoch 25/50 - Train Loss: 0.028656 - Val Loss: 0.044108\n",
      "Epoch 26/50 - Train Loss: 0.029670 - Val Loss: 0.044667\n",
      "Epoch 27/50 - Train Loss: 0.027430 - Val Loss: 0.044904\n",
      "Epoch 28/50 - Train Loss: 0.029279 - Val Loss: 0.046179\n",
      "Epoch 29/50 - Train Loss: 0.029031 - Val Loss: 0.045349\n",
      "Epoch 30/50 - Train Loss: 0.027730 - Val Loss: 0.044912\n",
      "Epoch 31/50 - Train Loss: 0.028269 - Val Loss: 0.053389\n",
      "Epoch 32/50 - Train Loss: 0.030081 - Val Loss: 0.049397\n",
      "Epoch 33/50 - Train Loss: 0.029721 - Val Loss: 0.047269\n",
      "Epoch 34/50 - Train Loss: 0.031637 - Val Loss: 0.048459\n",
      "Epoch 35/50 - Train Loss: 0.026710 - Val Loss: 0.046001\n",
      "Epoch 36/50 - Train Loss: 0.031665 - Val Loss: 0.044447\n",
      "Epoch 37/50 - Train Loss: 0.028803 - Val Loss: 0.044401\n",
      "Epoch 38/50 - Train Loss: 0.027488 - Val Loss: 0.046960\n",
      "Epoch 39/50 - Train Loss: 0.029129 - Val Loss: 0.044882\n",
      "Epoch 40/50 - Train Loss: 0.029027 - Val Loss: 0.047202\n",
      "Epoch 41/50 - Train Loss: 0.026254 - Val Loss: 0.045332\n",
      "Epoch 42/50 - Train Loss: 0.028000 - Val Loss: 0.045529\n",
      "Epoch 43/50 - Train Loss: 0.027598 - Val Loss: 0.046257\n",
      "Epoch 44/50 - Train Loss: 0.029296 - Val Loss: 0.046786\n",
      "Epoch 45/50 - Train Loss: 0.029914 - Val Loss: 0.048710\n",
      "Epoch 46/50 - Train Loss: 0.026087 - Val Loss: 0.045763\n",
      "Epoch 47/50 - Train Loss: 0.031553 - Val Loss: 0.048139\n",
      "Epoch 48/50 - Train Loss: 0.027781 - Val Loss: 0.047586\n",
      "Epoch 49/50 - Train Loss: 0.028649 - Val Loss: 0.046793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:31:39,776] Trial 47 finished with value: 0.04410833244522413 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 140, 'lr': 0.002098952145878124, 'weight_decay': 2.5018306280429086e-06, 'batch_size': 8}. Best is trial 33 with value: 0.042725492268800735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028913 - Val Loss: 0.048222\n",
      "Epoch 1/50 - Train Loss: 0.063262 - Val Loss: 0.075716\n",
      "Epoch 2/50 - Train Loss: 0.038624 - Val Loss: 0.088438\n",
      "Epoch 3/50 - Train Loss: 0.038595 - Val Loss: 0.067202\n",
      "Epoch 4/50 - Train Loss: 0.034436 - Val Loss: 0.051216\n",
      "Epoch 5/50 - Train Loss: 0.035526 - Val Loss: 0.045810\n",
      "Epoch 6/50 - Train Loss: 0.030328 - Val Loss: 0.044656\n",
      "Epoch 7/50 - Train Loss: 0.033669 - Val Loss: 0.045560\n",
      "Epoch 8/50 - Train Loss: 0.031655 - Val Loss: 0.045308\n",
      "Epoch 9/50 - Train Loss: 0.027350 - Val Loss: 0.045326\n",
      "Epoch 10/50 - Train Loss: 0.029480 - Val Loss: 0.045933\n",
      "Epoch 11/50 - Train Loss: 0.033850 - Val Loss: 0.043335\n",
      "Epoch 12/50 - Train Loss: 0.033718 - Val Loss: 0.047457\n",
      "Epoch 13/50 - Train Loss: 0.030203 - Val Loss: 0.043325\n",
      "Epoch 14/50 - Train Loss: 0.028169 - Val Loss: 0.042893\n",
      "Epoch 15/50 - Train Loss: 0.031537 - Val Loss: 0.042987\n",
      "Epoch 16/50 - Train Loss: 0.029044 - Val Loss: 0.042981\n",
      "Epoch 17/50 - Train Loss: 0.030128 - Val Loss: 0.043629\n",
      "Epoch 18/50 - Train Loss: 0.031282 - Val Loss: 0.053098\n",
      "Epoch 19/50 - Train Loss: 0.030323 - Val Loss: 0.046866\n",
      "Epoch 20/50 - Train Loss: 0.032034 - Val Loss: 0.044153\n",
      "Epoch 21/50 - Train Loss: 0.029153 - Val Loss: 0.046085\n",
      "Epoch 22/50 - Train Loss: 0.032456 - Val Loss: 0.044228\n",
      "Epoch 23/50 - Train Loss: 0.030187 - Val Loss: 0.043318\n",
      "Epoch 24/50 - Train Loss: 0.028627 - Val Loss: 0.043286\n",
      "Epoch 25/50 - Train Loss: 0.029989 - Val Loss: 0.046474\n",
      "Epoch 26/50 - Train Loss: 0.028563 - Val Loss: 0.043135\n",
      "Epoch 27/50 - Train Loss: 0.027447 - Val Loss: 0.042914\n",
      "Epoch 28/50 - Train Loss: 0.029265 - Val Loss: 0.043927\n",
      "Epoch 29/50 - Train Loss: 0.030670 - Val Loss: 0.044471\n",
      "Epoch 30/50 - Train Loss: 0.029772 - Val Loss: 0.043082\n",
      "Epoch 31/50 - Train Loss: 0.027407 - Val Loss: 0.048066\n",
      "Epoch 32/50 - Train Loss: 0.029327 - Val Loss: 0.045684\n",
      "Epoch 33/50 - Train Loss: 0.030799 - Val Loss: 0.043365\n",
      "Epoch 34/50 - Train Loss: 0.027787 - Val Loss: 0.042818\n",
      "Epoch 35/50 - Train Loss: 0.028879 - Val Loss: 0.043424\n",
      "Epoch 36/50 - Train Loss: 0.029140 - Val Loss: 0.043006\n",
      "Epoch 37/50 - Train Loss: 0.032877 - Val Loss: 0.048168\n",
      "Epoch 38/50 - Train Loss: 0.029397 - Val Loss: 0.042644\n",
      "Epoch 39/50 - Train Loss: 0.027938 - Val Loss: 0.042691\n",
      "Epoch 40/50 - Train Loss: 0.032915 - Val Loss: 0.042974\n",
      "Epoch 41/50 - Train Loss: 0.027857 - Val Loss: 0.043552\n",
      "Epoch 42/50 - Train Loss: 0.032442 - Val Loss: 0.046705\n",
      "Epoch 43/50 - Train Loss: 0.029626 - Val Loss: 0.043641\n",
      "Epoch 44/50 - Train Loss: 0.026090 - Val Loss: 0.048258\n",
      "Epoch 45/50 - Train Loss: 0.031390 - Val Loss: 0.044742\n",
      "Epoch 46/50 - Train Loss: 0.028085 - Val Loss: 0.043627\n",
      "Epoch 47/50 - Train Loss: 0.027450 - Val Loss: 0.044692\n",
      "Epoch 48/50 - Train Loss: 0.027503 - Val Loss: 0.043586\n",
      "Epoch 49/50 - Train Loss: 0.030865 - Val Loss: 0.044038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:32:01,460] Trial 48 finished with value: 0.042643917724490166 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 159, 'lr': 0.0037355082619079895, 'weight_decay': 7.623484019497244e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030947 - Val Loss: 0.044257\n",
      "Epoch 1/50 - Train Loss: 0.111459 - Val Loss: 0.103699\n",
      "Epoch 2/50 - Train Loss: 0.046928 - Val Loss: 0.060443\n",
      "Epoch 3/50 - Train Loss: 0.036901 - Val Loss: 0.056281\n",
      "Epoch 4/50 - Train Loss: 0.034679 - Val Loss: 0.046416\n",
      "Epoch 5/50 - Train Loss: 0.031345 - Val Loss: 0.045233\n",
      "Epoch 6/50 - Train Loss: 0.034113 - Val Loss: 0.044080\n",
      "Epoch 7/50 - Train Loss: 0.032350 - Val Loss: 0.047634\n",
      "Epoch 8/50 - Train Loss: 0.031366 - Val Loss: 0.042877\n",
      "Epoch 9/50 - Train Loss: 0.028966 - Val Loss: 0.047950\n",
      "Epoch 10/50 - Train Loss: 0.029426 - Val Loss: 0.043087\n",
      "Epoch 11/50 - Train Loss: 0.030674 - Val Loss: 0.046280\n",
      "Epoch 12/50 - Train Loss: 0.031948 - Val Loss: 0.044954\n",
      "Epoch 13/50 - Train Loss: 0.028789 - Val Loss: 0.044309\n",
      "Epoch 14/50 - Train Loss: 0.029906 - Val Loss: 0.046787\n",
      "Epoch 15/50 - Train Loss: 0.029281 - Val Loss: 0.042786\n",
      "Epoch 16/50 - Train Loss: 0.031243 - Val Loss: 0.044864\n",
      "Epoch 17/50 - Train Loss: 0.033079 - Val Loss: 0.046690\n",
      "Epoch 18/50 - Train Loss: 0.032190 - Val Loss: 0.045803\n",
      "Epoch 19/50 - Train Loss: 0.030578 - Val Loss: 0.047989\n",
      "Epoch 20/50 - Train Loss: 0.030215 - Val Loss: 0.044396\n",
      "Epoch 21/50 - Train Loss: 0.026287 - Val Loss: 0.043173\n",
      "Epoch 22/50 - Train Loss: 0.031019 - Val Loss: 0.042778\n",
      "Epoch 23/50 - Train Loss: 0.031911 - Val Loss: 0.045665\n",
      "Epoch 24/50 - Train Loss: 0.031082 - Val Loss: 0.052067\n",
      "Epoch 25/50 - Train Loss: 0.029246 - Val Loss: 0.042871\n",
      "Epoch 26/50 - Train Loss: 0.030846 - Val Loss: 0.045320\n",
      "Epoch 27/50 - Train Loss: 0.031234 - Val Loss: 0.046918\n",
      "Epoch 28/50 - Train Loss: 0.026156 - Val Loss: 0.043183\n",
      "Epoch 29/50 - Train Loss: 0.027093 - Val Loss: 0.044129\n",
      "Epoch 30/50 - Train Loss: 0.027942 - Val Loss: 0.043449\n",
      "Epoch 31/50 - Train Loss: 0.031186 - Val Loss: 0.048868\n",
      "Epoch 32/50 - Train Loss: 0.028910 - Val Loss: 0.043066\n",
      "Epoch 33/50 - Train Loss: 0.031538 - Val Loss: 0.043287\n",
      "Epoch 34/50 - Train Loss: 0.029985 - Val Loss: 0.046911\n",
      "Epoch 35/50 - Train Loss: 0.029860 - Val Loss: 0.045382\n",
      "Epoch 36/50 - Train Loss: 0.027960 - Val Loss: 0.042766\n",
      "Epoch 37/50 - Train Loss: 0.030827 - Val Loss: 0.042738\n",
      "Epoch 38/50 - Train Loss: 0.031099 - Val Loss: 0.046830\n",
      "Epoch 39/50 - Train Loss: 0.031372 - Val Loss: 0.043847\n",
      "Epoch 40/50 - Train Loss: 0.030635 - Val Loss: 0.044247\n",
      "Epoch 41/50 - Train Loss: 0.028391 - Val Loss: 0.043143\n",
      "Epoch 42/50 - Train Loss: 0.029977 - Val Loss: 0.049714\n",
      "Epoch 43/50 - Train Loss: 0.029374 - Val Loss: 0.048423\n",
      "Epoch 44/50 - Train Loss: 0.028986 - Val Loss: 0.042798\n",
      "Epoch 45/50 - Train Loss: 0.029511 - Val Loss: 0.042826\n",
      "Epoch 46/50 - Train Loss: 0.027407 - Val Loss: 0.043166\n",
      "Epoch 47/50 - Train Loss: 0.028667 - Val Loss: 0.043035\n",
      "Epoch 48/50 - Train Loss: 0.028522 - Val Loss: 0.042903\n",
      "Epoch 49/50 - Train Loss: 0.029417 - Val Loss: 0.043418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:32:15,852] Trial 49 finished with value: 0.04273765720427036 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 153, 'lr': 0.003390741781379448, 'weight_decay': 8.778621167839585e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029673 - Val Loss: 0.043304\n",
      "Epoch 1/50 - Train Loss: 0.180976 - Val Loss: 0.339425\n",
      "Epoch 2/50 - Train Loss: 0.162973 - Val Loss: 0.334058\n",
      "Epoch 3/50 - Train Loss: 0.169228 - Val Loss: 0.328858\n",
      "Epoch 4/50 - Train Loss: 0.171062 - Val Loss: 0.323802\n",
      "Epoch 5/50 - Train Loss: 0.163821 - Val Loss: 0.318723\n",
      "Epoch 6/50 - Train Loss: 0.149788 - Val Loss: 0.313777\n",
      "Epoch 7/50 - Train Loss: 0.159974 - Val Loss: 0.308935\n",
      "Epoch 8/50 - Train Loss: 0.147699 - Val Loss: 0.304083\n",
      "Epoch 9/50 - Train Loss: 0.138459 - Val Loss: 0.299375\n",
      "Epoch 10/50 - Train Loss: 0.147213 - Val Loss: 0.294675\n",
      "Epoch 11/50 - Train Loss: 0.139062 - Val Loss: 0.289789\n",
      "Epoch 12/50 - Train Loss: 0.137567 - Val Loss: 0.284620\n",
      "Epoch 13/50 - Train Loss: 0.135262 - Val Loss: 0.279599\n",
      "Epoch 14/50 - Train Loss: 0.132554 - Val Loss: 0.274437\n",
      "Epoch 15/50 - Train Loss: 0.126618 - Val Loss: 0.269295\n",
      "Epoch 16/50 - Train Loss: 0.120091 - Val Loss: 0.263980\n",
      "Epoch 17/50 - Train Loss: 0.127446 - Val Loss: 0.258760\n",
      "Epoch 18/50 - Train Loss: 0.128319 - Val Loss: 0.253644\n",
      "Epoch 19/50 - Train Loss: 0.124287 - Val Loss: 0.248269\n",
      "Epoch 20/50 - Train Loss: 0.118499 - Val Loss: 0.243139\n",
      "Epoch 21/50 - Train Loss: 0.119037 - Val Loss: 0.238104\n",
      "Epoch 22/50 - Train Loss: 0.105764 - Val Loss: 0.232984\n",
      "Epoch 23/50 - Train Loss: 0.107441 - Val Loss: 0.228067\n",
      "Epoch 24/50 - Train Loss: 0.101339 - Val Loss: 0.223115\n",
      "Epoch 25/50 - Train Loss: 0.092574 - Val Loss: 0.218388\n",
      "Epoch 26/50 - Train Loss: 0.100902 - Val Loss: 0.213638\n",
      "Epoch 27/50 - Train Loss: 0.100967 - Val Loss: 0.208708\n",
      "Epoch 28/50 - Train Loss: 0.094936 - Val Loss: 0.203695\n",
      "Epoch 29/50 - Train Loss: 0.095100 - Val Loss: 0.198766\n",
      "Epoch 30/50 - Train Loss: 0.090565 - Val Loss: 0.193880\n",
      "Epoch 31/50 - Train Loss: 0.087270 - Val Loss: 0.189015\n",
      "Epoch 32/50 - Train Loss: 0.088868 - Val Loss: 0.184141\n",
      "Epoch 33/50 - Train Loss: 0.084069 - Val Loss: 0.179259\n",
      "Epoch 34/50 - Train Loss: 0.077728 - Val Loss: 0.174435\n",
      "Epoch 35/50 - Train Loss: 0.078576 - Val Loss: 0.169555\n",
      "Epoch 36/50 - Train Loss: 0.071459 - Val Loss: 0.164709\n",
      "Epoch 37/50 - Train Loss: 0.073484 - Val Loss: 0.160117\n",
      "Epoch 38/50 - Train Loss: 0.067491 - Val Loss: 0.155696\n",
      "Epoch 39/50 - Train Loss: 0.063824 - Val Loss: 0.151248\n",
      "Epoch 40/50 - Train Loss: 0.065674 - Val Loss: 0.147109\n",
      "Epoch 41/50 - Train Loss: 0.064778 - Val Loss: 0.143031\n",
      "Epoch 42/50 - Train Loss: 0.061951 - Val Loss: 0.138871\n",
      "Epoch 43/50 - Train Loss: 0.056918 - Val Loss: 0.135236\n",
      "Epoch 44/50 - Train Loss: 0.056136 - Val Loss: 0.131800\n",
      "Epoch 45/50 - Train Loss: 0.054206 - Val Loss: 0.128435\n",
      "Epoch 46/50 - Train Loss: 0.052103 - Val Loss: 0.125346\n",
      "Epoch 47/50 - Train Loss: 0.053623 - Val Loss: 0.122426\n",
      "Epoch 48/50 - Train Loss: 0.051018 - Val Loss: 0.119522\n",
      "Epoch 49/50 - Train Loss: 0.056303 - Val Loss: 0.116903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:32:35,738] Trial 50 finished with value: 0.11418497934937477 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 123, 'lr': 1.3345533531235566e-05, 'weight_decay': 9.243128960715951e-07, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.050219 - Val Loss: 0.114185\n",
      "Epoch 1/50 - Train Loss: 0.095770 - Val Loss: 0.085246\n",
      "Epoch 2/50 - Train Loss: 0.049683 - Val Loss: 0.075708\n",
      "Epoch 3/50 - Train Loss: 0.035325 - Val Loss: 0.048521\n",
      "Epoch 4/50 - Train Loss: 0.036350 - Val Loss: 0.059510\n",
      "Epoch 5/50 - Train Loss: 0.034226 - Val Loss: 0.046904\n",
      "Epoch 6/50 - Train Loss: 0.032457 - Val Loss: 0.050607\n",
      "Epoch 7/50 - Train Loss: 0.029074 - Val Loss: 0.045046\n",
      "Epoch 8/50 - Train Loss: 0.030121 - Val Loss: 0.046079\n",
      "Epoch 9/50 - Train Loss: 0.032517 - Val Loss: 0.044038\n",
      "Epoch 10/50 - Train Loss: 0.028342 - Val Loss: 0.045900\n",
      "Epoch 11/50 - Train Loss: 0.030876 - Val Loss: 0.045696\n",
      "Epoch 12/50 - Train Loss: 0.030722 - Val Loss: 0.044200\n",
      "Epoch 13/50 - Train Loss: 0.030294 - Val Loss: 0.045616\n",
      "Epoch 14/50 - Train Loss: 0.029393 - Val Loss: 0.043801\n",
      "Epoch 15/50 - Train Loss: 0.030529 - Val Loss: 0.043208\n",
      "Epoch 16/50 - Train Loss: 0.029924 - Val Loss: 0.045680\n",
      "Epoch 17/50 - Train Loss: 0.028914 - Val Loss: 0.047047\n",
      "Epoch 18/50 - Train Loss: 0.029525 - Val Loss: 0.043343\n",
      "Epoch 19/50 - Train Loss: 0.032123 - Val Loss: 0.045212\n",
      "Epoch 20/50 - Train Loss: 0.029237 - Val Loss: 0.043358\n",
      "Epoch 21/50 - Train Loss: 0.028528 - Val Loss: 0.044096\n",
      "Epoch 22/50 - Train Loss: 0.029109 - Val Loss: 0.044023\n",
      "Epoch 23/50 - Train Loss: 0.030167 - Val Loss: 0.043994\n",
      "Epoch 24/50 - Train Loss: 0.030092 - Val Loss: 0.044171\n",
      "Epoch 25/50 - Train Loss: 0.030756 - Val Loss: 0.044952\n",
      "Epoch 26/50 - Train Loss: 0.028238 - Val Loss: 0.043475\n",
      "Epoch 27/50 - Train Loss: 0.028003 - Val Loss: 0.043768\n",
      "Epoch 28/50 - Train Loss: 0.028016 - Val Loss: 0.043871\n",
      "Epoch 29/50 - Train Loss: 0.026146 - Val Loss: 0.043191\n",
      "Epoch 30/50 - Train Loss: 0.027784 - Val Loss: 0.043442\n",
      "Epoch 31/50 - Train Loss: 0.031386 - Val Loss: 0.043832\n",
      "Epoch 32/50 - Train Loss: 0.030583 - Val Loss: 0.044044\n",
      "Epoch 33/50 - Train Loss: 0.028057 - Val Loss: 0.047230\n",
      "Epoch 34/50 - Train Loss: 0.029885 - Val Loss: 0.044583\n",
      "Epoch 35/50 - Train Loss: 0.029170 - Val Loss: 0.049826\n",
      "Epoch 36/50 - Train Loss: 0.034877 - Val Loss: 0.044104\n",
      "Epoch 37/50 - Train Loss: 0.029694 - Val Loss: 0.044929\n",
      "Epoch 38/50 - Train Loss: 0.029885 - Val Loss: 0.044084\n",
      "Epoch 39/50 - Train Loss: 0.028083 - Val Loss: 0.053488\n",
      "Epoch 40/50 - Train Loss: 0.033234 - Val Loss: 0.053892\n",
      "Epoch 41/50 - Train Loss: 0.034304 - Val Loss: 0.049130\n",
      "Epoch 42/50 - Train Loss: 0.030649 - Val Loss: 0.045864\n",
      "Epoch 43/50 - Train Loss: 0.034096 - Val Loss: 0.044955\n",
      "Epoch 44/50 - Train Loss: 0.030316 - Val Loss: 0.045054\n",
      "Epoch 45/50 - Train Loss: 0.032383 - Val Loss: 0.043113\n",
      "Epoch 46/50 - Train Loss: 0.031073 - Val Loss: 0.043356\n",
      "Epoch 47/50 - Train Loss: 0.027367 - Val Loss: 0.044731\n",
      "Epoch 48/50 - Train Loss: 0.029627 - Val Loss: 0.043189\n",
      "Epoch 49/50 - Train Loss: 0.026148 - Val Loss: 0.043933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:32:50,211] Trial 51 finished with value: 0.043112555518746376 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 158, 'lr': 0.003442902056986079, 'weight_decay': 8.357276335012507e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.025601 - Val Loss: 0.044099\n",
      "Epoch 1/50 - Train Loss: 0.087286 - Val Loss: 0.137457\n",
      "Epoch 2/50 - Train Loss: 0.049278 - Val Loss: 0.054894\n",
      "Epoch 3/50 - Train Loss: 0.037167 - Val Loss: 0.060585\n",
      "Epoch 4/50 - Train Loss: 0.031553 - Val Loss: 0.050103\n",
      "Epoch 5/50 - Train Loss: 0.031352 - Val Loss: 0.051238\n",
      "Epoch 6/50 - Train Loss: 0.031297 - Val Loss: 0.047584\n",
      "Epoch 7/50 - Train Loss: 0.032054 - Val Loss: 0.055306\n",
      "Epoch 8/50 - Train Loss: 0.033938 - Val Loss: 0.046409\n",
      "Epoch 9/50 - Train Loss: 0.029738 - Val Loss: 0.045877\n",
      "Epoch 10/50 - Train Loss: 0.030258 - Val Loss: 0.047622\n",
      "Epoch 11/50 - Train Loss: 0.031111 - Val Loss: 0.051257\n",
      "Epoch 12/50 - Train Loss: 0.027497 - Val Loss: 0.047751\n",
      "Epoch 13/50 - Train Loss: 0.032166 - Val Loss: 0.044699\n",
      "Epoch 14/50 - Train Loss: 0.032773 - Val Loss: 0.049748\n",
      "Epoch 15/50 - Train Loss: 0.031825 - Val Loss: 0.052081\n",
      "Epoch 16/50 - Train Loss: 0.034469 - Val Loss: 0.045317\n",
      "Epoch 17/50 - Train Loss: 0.029564 - Val Loss: 0.047166\n",
      "Epoch 18/50 - Train Loss: 0.031310 - Val Loss: 0.047560\n",
      "Epoch 19/50 - Train Loss: 0.027671 - Val Loss: 0.045619\n",
      "Epoch 20/50 - Train Loss: 0.030625 - Val Loss: 0.044097\n",
      "Epoch 21/50 - Train Loss: 0.033180 - Val Loss: 0.046945\n",
      "Epoch 22/50 - Train Loss: 0.027656 - Val Loss: 0.054004\n",
      "Epoch 23/50 - Train Loss: 0.032298 - Val Loss: 0.044056\n",
      "Epoch 24/50 - Train Loss: 0.030810 - Val Loss: 0.047807\n",
      "Epoch 25/50 - Train Loss: 0.028338 - Val Loss: 0.047678\n",
      "Epoch 26/50 - Train Loss: 0.027425 - Val Loss: 0.045401\n",
      "Epoch 27/50 - Train Loss: 0.028638 - Val Loss: 0.043795\n",
      "Epoch 28/50 - Train Loss: 0.029852 - Val Loss: 0.043968\n",
      "Epoch 29/50 - Train Loss: 0.031731 - Val Loss: 0.052543\n",
      "Epoch 30/50 - Train Loss: 0.030083 - Val Loss: 0.044480\n",
      "Epoch 31/50 - Train Loss: 0.032704 - Val Loss: 0.043610\n",
      "Epoch 32/50 - Train Loss: 0.030500 - Val Loss: 0.045256\n",
      "Epoch 33/50 - Train Loss: 0.026855 - Val Loss: 0.044138\n",
      "Epoch 34/50 - Train Loss: 0.028443 - Val Loss: 0.045508\n",
      "Epoch 35/50 - Train Loss: 0.030760 - Val Loss: 0.047406\n",
      "Epoch 36/50 - Train Loss: 0.029499 - Val Loss: 0.043714\n",
      "Epoch 37/50 - Train Loss: 0.031267 - Val Loss: 0.045614\n",
      "Epoch 38/50 - Train Loss: 0.028969 - Val Loss: 0.047036\n",
      "Epoch 39/50 - Train Loss: 0.028771 - Val Loss: 0.046236\n",
      "Epoch 40/50 - Train Loss: 0.032666 - Val Loss: 0.044941\n",
      "Epoch 41/50 - Train Loss: 0.027594 - Val Loss: 0.044759\n",
      "Epoch 42/50 - Train Loss: 0.030747 - Val Loss: 0.043945\n",
      "Epoch 43/50 - Train Loss: 0.029645 - Val Loss: 0.044020\n",
      "Epoch 44/50 - Train Loss: 0.031190 - Val Loss: 0.044048\n",
      "Epoch 45/50 - Train Loss: 0.026326 - Val Loss: 0.045532\n",
      "Epoch 46/50 - Train Loss: 0.027371 - Val Loss: 0.048109\n",
      "Epoch 47/50 - Train Loss: 0.032001 - Val Loss: 0.046283\n",
      "Epoch 48/50 - Train Loss: 0.031631 - Val Loss: 0.044372\n",
      "Epoch 49/50 - Train Loss: 0.029551 - Val Loss: 0.047422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:33:04,440] Trial 52 finished with value: 0.04361017048358917 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 149, 'lr': 0.0037323648895740795, 'weight_decay': 1.436758106088507e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029351 - Val Loss: 0.044996\n",
      "Epoch 1/50 - Train Loss: 0.092867 - Val Loss: 0.063957\n",
      "Epoch 2/50 - Train Loss: 0.035673 - Val Loss: 0.056459\n",
      "Epoch 3/50 - Train Loss: 0.032797 - Val Loss: 0.056135\n",
      "Epoch 4/50 - Train Loss: 0.028555 - Val Loss: 0.046244\n",
      "Epoch 5/50 - Train Loss: 0.032407 - Val Loss: 0.044608\n",
      "Epoch 6/50 - Train Loss: 0.029361 - Val Loss: 0.046269\n",
      "Epoch 7/50 - Train Loss: 0.029987 - Val Loss: 0.050137\n",
      "Epoch 8/50 - Train Loss: 0.031426 - Val Loss: 0.049701\n",
      "Epoch 9/50 - Train Loss: 0.031069 - Val Loss: 0.049269\n",
      "Epoch 10/50 - Train Loss: 0.031222 - Val Loss: 0.045492\n",
      "Epoch 11/50 - Train Loss: 0.032933 - Val Loss: 0.058833\n",
      "Epoch 12/50 - Train Loss: 0.030608 - Val Loss: 0.048064\n",
      "Epoch 13/50 - Train Loss: 0.029931 - Val Loss: 0.053654\n",
      "Epoch 14/50 - Train Loss: 0.034166 - Val Loss: 0.051348\n",
      "Epoch 15/50 - Train Loss: 0.032661 - Val Loss: 0.061704\n",
      "Epoch 16/50 - Train Loss: 0.035283 - Val Loss: 0.055883\n",
      "Epoch 17/50 - Train Loss: 0.036762 - Val Loss: 0.050933\n",
      "Epoch 18/50 - Train Loss: 0.032448 - Val Loss: 0.045173\n",
      "Epoch 19/50 - Train Loss: 0.031050 - Val Loss: 0.043927\n",
      "Epoch 20/50 - Train Loss: 0.029063 - Val Loss: 0.046296\n",
      "Epoch 21/50 - Train Loss: 0.030318 - Val Loss: 0.044826\n",
      "Epoch 22/50 - Train Loss: 0.028048 - Val Loss: 0.043979\n",
      "Epoch 23/50 - Train Loss: 0.032523 - Val Loss: 0.043258\n",
      "Epoch 24/50 - Train Loss: 0.030801 - Val Loss: 0.045032\n",
      "Epoch 25/50 - Train Loss: 0.031099 - Val Loss: 0.047262\n",
      "Epoch 26/50 - Train Loss: 0.029988 - Val Loss: 0.045702\n",
      "Epoch 27/50 - Train Loss: 0.030656 - Val Loss: 0.043513\n",
      "Epoch 28/50 - Train Loss: 0.030966 - Val Loss: 0.043301\n",
      "Epoch 29/50 - Train Loss: 0.031220 - Val Loss: 0.043767\n",
      "Epoch 30/50 - Train Loss: 0.029167 - Val Loss: 0.048053\n",
      "Epoch 31/50 - Train Loss: 0.029366 - Val Loss: 0.043578\n",
      "Epoch 32/50 - Train Loss: 0.026187 - Val Loss: 0.043885\n",
      "Epoch 33/50 - Train Loss: 0.028185 - Val Loss: 0.043414\n",
      "Epoch 34/50 - Train Loss: 0.030125 - Val Loss: 0.043312\n",
      "Epoch 35/50 - Train Loss: 0.032026 - Val Loss: 0.044074\n",
      "Epoch 36/50 - Train Loss: 0.027502 - Val Loss: 0.043645\n",
      "Epoch 37/50 - Train Loss: 0.028443 - Val Loss: 0.044105\n",
      "Epoch 38/50 - Train Loss: 0.031987 - Val Loss: 0.044076\n",
      "Epoch 39/50 - Train Loss: 0.028491 - Val Loss: 0.054274\n",
      "Epoch 40/50 - Train Loss: 0.033211 - Val Loss: 0.046763\n",
      "Epoch 41/50 - Train Loss: 0.027744 - Val Loss: 0.043370\n",
      "Epoch 42/50 - Train Loss: 0.029154 - Val Loss: 0.043726\n",
      "Epoch 43/50 - Train Loss: 0.032622 - Val Loss: 0.044395\n",
      "Epoch 44/50 - Train Loss: 0.031918 - Val Loss: 0.051017\n",
      "Epoch 45/50 - Train Loss: 0.029737 - Val Loss: 0.046689\n",
      "Epoch 46/50 - Train Loss: 0.027352 - Val Loss: 0.043650\n",
      "Epoch 47/50 - Train Loss: 0.029881 - Val Loss: 0.043672\n",
      "Epoch 48/50 - Train Loss: 0.029367 - Val Loss: 0.048027\n",
      "Epoch 49/50 - Train Loss: 0.028294 - Val Loss: 0.044077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:33:21,901] Trial 53 finished with value: 0.04325789026916027 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 143, 'lr': 0.008233396371634605, 'weight_decay': 2.0556163389406224e-07, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029425 - Val Loss: 0.045176\n",
      "Epoch 1/50 - Train Loss: 0.113351 - Val Loss: 0.060854\n",
      "Epoch 2/50 - Train Loss: 0.043855 - Val Loss: 0.074405\n",
      "Epoch 3/50 - Train Loss: 0.039795 - Val Loss: 0.046934\n",
      "Epoch 4/50 - Train Loss: 0.035250 - Val Loss: 0.045381\n",
      "Epoch 5/50 - Train Loss: 0.035239 - Val Loss: 0.055556\n",
      "Epoch 6/50 - Train Loss: 0.032524 - Val Loss: 0.048780\n",
      "Epoch 7/50 - Train Loss: 0.031722 - Val Loss: 0.044181\n",
      "Epoch 8/50 - Train Loss: 0.030074 - Val Loss: 0.045417\n",
      "Epoch 9/50 - Train Loss: 0.030105 - Val Loss: 0.046687\n",
      "Epoch 10/50 - Train Loss: 0.034906 - Val Loss: 0.049484\n",
      "Epoch 11/50 - Train Loss: 0.033212 - Val Loss: 0.045209\n",
      "Epoch 12/50 - Train Loss: 0.031665 - Val Loss: 0.050597\n",
      "Epoch 13/50 - Train Loss: 0.031906 - Val Loss: 0.054640\n",
      "Epoch 14/50 - Train Loss: 0.031378 - Val Loss: 0.045609\n",
      "Epoch 15/50 - Train Loss: 0.032287 - Val Loss: 0.043119\n",
      "Epoch 16/50 - Train Loss: 0.029771 - Val Loss: 0.047654\n",
      "Epoch 17/50 - Train Loss: 0.027268 - Val Loss: 0.047176\n",
      "Epoch 18/50 - Train Loss: 0.029022 - Val Loss: 0.047255\n",
      "Epoch 19/50 - Train Loss: 0.031323 - Val Loss: 0.045523\n",
      "Epoch 20/50 - Train Loss: 0.029877 - Val Loss: 0.049887\n",
      "Epoch 21/50 - Train Loss: 0.031942 - Val Loss: 0.045299\n",
      "Epoch 22/50 - Train Loss: 0.028812 - Val Loss: 0.043799\n",
      "Epoch 23/50 - Train Loss: 0.028345 - Val Loss: 0.044813\n",
      "Epoch 24/50 - Train Loss: 0.027616 - Val Loss: 0.045345\n",
      "Epoch 25/50 - Train Loss: 0.029487 - Val Loss: 0.047961\n",
      "Epoch 26/50 - Train Loss: 0.028808 - Val Loss: 0.051480\n",
      "Epoch 27/50 - Train Loss: 0.028731 - Val Loss: 0.053328\n",
      "Epoch 28/50 - Train Loss: 0.031033 - Val Loss: 0.048418\n",
      "Epoch 29/50 - Train Loss: 0.027240 - Val Loss: 0.044541\n",
      "Epoch 30/50 - Train Loss: 0.028780 - Val Loss: 0.044477\n",
      "Epoch 31/50 - Train Loss: 0.028820 - Val Loss: 0.043589\n",
      "Epoch 32/50 - Train Loss: 0.030979 - Val Loss: 0.043632\n",
      "Epoch 33/50 - Train Loss: 0.030209 - Val Loss: 0.044439\n",
      "Epoch 34/50 - Train Loss: 0.026826 - Val Loss: 0.043325\n",
      "Epoch 35/50 - Train Loss: 0.033218 - Val Loss: 0.043150\n",
      "Epoch 36/50 - Train Loss: 0.027299 - Val Loss: 0.048186\n",
      "Epoch 37/50 - Train Loss: 0.026787 - Val Loss: 0.044803\n",
      "Epoch 38/50 - Train Loss: 0.029040 - Val Loss: 0.044073\n",
      "Epoch 39/50 - Train Loss: 0.028148 - Val Loss: 0.048609\n",
      "Epoch 40/50 - Train Loss: 0.027459 - Val Loss: 0.045298\n",
      "Epoch 41/50 - Train Loss: 0.030862 - Val Loss: 0.055629\n",
      "Epoch 42/50 - Train Loss: 0.032494 - Val Loss: 0.064255\n",
      "Epoch 43/50 - Train Loss: 0.032143 - Val Loss: 0.047564\n",
      "Epoch 44/50 - Train Loss: 0.031015 - Val Loss: 0.051921\n",
      "Epoch 45/50 - Train Loss: 0.031692 - Val Loss: 0.057379\n",
      "Epoch 46/50 - Train Loss: 0.032988 - Val Loss: 0.050274\n",
      "Epoch 47/50 - Train Loss: 0.027850 - Val Loss: 0.044893\n",
      "Epoch 48/50 - Train Loss: 0.030577 - Val Loss: 0.044135\n",
      "Epoch 49/50 - Train Loss: 0.029172 - Val Loss: 0.044611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:33:39,653] Trial 54 finished with value: 0.04311862774193287 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 154, 'lr': 0.009991491895548859, 'weight_decay': 6.084666233134877e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026376 - Val Loss: 0.044816\n",
      "Epoch 1/50 - Train Loss: 0.095017 - Val Loss: 0.103132\n",
      "Epoch 2/50 - Train Loss: 0.039764 - Val Loss: 0.047615\n",
      "Epoch 3/50 - Train Loss: 0.035003 - Val Loss: 0.052568\n",
      "Epoch 4/50 - Train Loss: 0.032628 - Val Loss: 0.047057\n",
      "Epoch 5/50 - Train Loss: 0.034710 - Val Loss: 0.044842\n",
      "Epoch 6/50 - Train Loss: 0.030765 - Val Loss: 0.043765\n",
      "Epoch 7/50 - Train Loss: 0.030155 - Val Loss: 0.046375\n",
      "Epoch 8/50 - Train Loss: 0.032096 - Val Loss: 0.045049\n",
      "Epoch 9/50 - Train Loss: 0.027201 - Val Loss: 0.044277\n",
      "Epoch 10/50 - Train Loss: 0.029983 - Val Loss: 0.044002\n",
      "Epoch 11/50 - Train Loss: 0.033300 - Val Loss: 0.045937\n",
      "Epoch 12/50 - Train Loss: 0.031359 - Val Loss: 0.044113\n",
      "Epoch 13/50 - Train Loss: 0.028898 - Val Loss: 0.044907\n",
      "Epoch 14/50 - Train Loss: 0.031070 - Val Loss: 0.047542\n",
      "Epoch 15/50 - Train Loss: 0.031001 - Val Loss: 0.043556\n",
      "Epoch 16/50 - Train Loss: 0.028658 - Val Loss: 0.045970\n",
      "Epoch 17/50 - Train Loss: 0.030405 - Val Loss: 0.045160\n",
      "Epoch 18/50 - Train Loss: 0.031536 - Val Loss: 0.044001\n",
      "Epoch 19/50 - Train Loss: 0.030459 - Val Loss: 0.050020\n",
      "Epoch 20/50 - Train Loss: 0.028758 - Val Loss: 0.050677\n",
      "Epoch 21/50 - Train Loss: 0.033990 - Val Loss: 0.043339\n",
      "Epoch 22/50 - Train Loss: 0.028265 - Val Loss: 0.044109\n",
      "Epoch 23/50 - Train Loss: 0.029849 - Val Loss: 0.043266\n",
      "Epoch 24/50 - Train Loss: 0.028620 - Val Loss: 0.043565\n",
      "Epoch 25/50 - Train Loss: 0.029760 - Val Loss: 0.043364\n",
      "Epoch 26/50 - Train Loss: 0.030358 - Val Loss: 0.046511\n",
      "Epoch 27/50 - Train Loss: 0.029958 - Val Loss: 0.044392\n",
      "Epoch 28/50 - Train Loss: 0.025807 - Val Loss: 0.044169\n",
      "Epoch 29/50 - Train Loss: 0.029634 - Val Loss: 0.043995\n",
      "Epoch 30/50 - Train Loss: 0.030257 - Val Loss: 0.049476\n",
      "Epoch 31/50 - Train Loss: 0.028081 - Val Loss: 0.044236\n",
      "Epoch 32/50 - Train Loss: 0.030713 - Val Loss: 0.043612\n",
      "Epoch 33/50 - Train Loss: 0.029137 - Val Loss: 0.043170\n",
      "Epoch 34/50 - Train Loss: 0.028538 - Val Loss: 0.047818\n",
      "Epoch 35/50 - Train Loss: 0.032477 - Val Loss: 0.045930\n",
      "Epoch 36/50 - Train Loss: 0.030756 - Val Loss: 0.043496\n",
      "Epoch 37/50 - Train Loss: 0.028400 - Val Loss: 0.045006\n",
      "Epoch 38/50 - Train Loss: 0.024411 - Val Loss: 0.048031\n",
      "Epoch 39/50 - Train Loss: 0.031188 - Val Loss: 0.044775\n",
      "Epoch 40/50 - Train Loss: 0.030896 - Val Loss: 0.044136\n",
      "Epoch 41/50 - Train Loss: 0.027850 - Val Loss: 0.044498\n",
      "Epoch 42/50 - Train Loss: 0.029343 - Val Loss: 0.044263\n",
      "Epoch 43/50 - Train Loss: 0.028878 - Val Loss: 0.045936\n",
      "Epoch 44/50 - Train Loss: 0.029734 - Val Loss: 0.047621\n",
      "Epoch 45/50 - Train Loss: 0.028813 - Val Loss: 0.044113\n",
      "Epoch 46/50 - Train Loss: 0.031596 - Val Loss: 0.044734\n",
      "Epoch 47/50 - Train Loss: 0.028495 - Val Loss: 0.044851\n",
      "Epoch 48/50 - Train Loss: 0.028518 - Val Loss: 0.044705\n",
      "Epoch 49/50 - Train Loss: 0.027603 - Val Loss: 0.044094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:33:54,036] Trial 55 finished with value: 0.043170059099793434 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 169, 'lr': 0.005625830314999145, 'weight_decay': 1.0290244227383217e-07, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026805 - Val Loss: 0.044440\n",
      "Epoch 1/50 - Train Loss: 0.064995 - Val Loss: 0.053482\n",
      "Epoch 2/50 - Train Loss: 0.034484 - Val Loss: 0.050325\n",
      "Epoch 3/50 - Train Loss: 0.035158 - Val Loss: 0.054410\n",
      "Epoch 4/50 - Train Loss: 0.031881 - Val Loss: 0.049141\n",
      "Epoch 5/50 - Train Loss: 0.030508 - Val Loss: 0.048558\n",
      "Epoch 6/50 - Train Loss: 0.029756 - Val Loss: 0.044397\n",
      "Epoch 7/50 - Train Loss: 0.029327 - Val Loss: 0.050364\n",
      "Epoch 8/50 - Train Loss: 0.031788 - Val Loss: 0.046393\n",
      "Epoch 9/50 - Train Loss: 0.034042 - Val Loss: 0.044814\n",
      "Epoch 10/50 - Train Loss: 0.030066 - Val Loss: 0.044942\n",
      "Epoch 11/50 - Train Loss: 0.029342 - Val Loss: 0.045096\n",
      "Epoch 12/50 - Train Loss: 0.027760 - Val Loss: 0.048426\n",
      "Epoch 13/50 - Train Loss: 0.029181 - Val Loss: 0.047711\n",
      "Epoch 14/50 - Train Loss: 0.030394 - Val Loss: 0.044881\n",
      "Epoch 15/50 - Train Loss: 0.028878 - Val Loss: 0.043744\n",
      "Epoch 16/50 - Train Loss: 0.032342 - Val Loss: 0.049179\n",
      "Epoch 17/50 - Train Loss: 0.030440 - Val Loss: 0.055672\n",
      "Epoch 18/50 - Train Loss: 0.032851 - Val Loss: 0.045124\n",
      "Epoch 19/50 - Train Loss: 0.030408 - Val Loss: 0.043565\n",
      "Epoch 20/50 - Train Loss: 0.031010 - Val Loss: 0.048388\n",
      "Epoch 21/50 - Train Loss: 0.030804 - Val Loss: 0.043135\n",
      "Epoch 22/50 - Train Loss: 0.032480 - Val Loss: 0.050652\n",
      "Epoch 23/50 - Train Loss: 0.029094 - Val Loss: 0.045304\n",
      "Epoch 24/50 - Train Loss: 0.032421 - Val Loss: 0.044066\n",
      "Epoch 25/50 - Train Loss: 0.030582 - Val Loss: 0.046704\n",
      "Epoch 26/50 - Train Loss: 0.030560 - Val Loss: 0.044007\n",
      "Epoch 27/50 - Train Loss: 0.028843 - Val Loss: 0.048873\n",
      "Epoch 28/50 - Train Loss: 0.030550 - Val Loss: 0.044034\n",
      "Epoch 29/50 - Train Loss: 0.027637 - Val Loss: 0.045997\n",
      "Epoch 30/50 - Train Loss: 0.032814 - Val Loss: 0.051890\n",
      "Epoch 31/50 - Train Loss: 0.029391 - Val Loss: 0.043750\n",
      "Epoch 32/50 - Train Loss: 0.028906 - Val Loss: 0.046895\n",
      "Epoch 33/50 - Train Loss: 0.031332 - Val Loss: 0.045744\n",
      "Epoch 34/50 - Train Loss: 0.032453 - Val Loss: 0.044442\n",
      "Epoch 35/50 - Train Loss: 0.030880 - Val Loss: 0.045293\n",
      "Epoch 36/50 - Train Loss: 0.030344 - Val Loss: 0.043383\n",
      "Epoch 37/50 - Train Loss: 0.029279 - Val Loss: 0.044635\n",
      "Epoch 38/50 - Train Loss: 0.029358 - Val Loss: 0.045648\n",
      "Epoch 39/50 - Train Loss: 0.028808 - Val Loss: 0.043253\n",
      "Epoch 40/50 - Train Loss: 0.028777 - Val Loss: 0.042972\n",
      "Epoch 41/50 - Train Loss: 0.026651 - Val Loss: 0.044460\n",
      "Epoch 42/50 - Train Loss: 0.025850 - Val Loss: 0.044382\n",
      "Epoch 43/50 - Train Loss: 0.028850 - Val Loss: 0.043621\n",
      "Epoch 44/50 - Train Loss: 0.027190 - Val Loss: 0.044883\n",
      "Epoch 45/50 - Train Loss: 0.030278 - Val Loss: 0.045300\n",
      "Epoch 46/50 - Train Loss: 0.025577 - Val Loss: 0.046324\n",
      "Epoch 47/50 - Train Loss: 0.030906 - Val Loss: 0.044812\n",
      "Epoch 48/50 - Train Loss: 0.026782 - Val Loss: 0.049134\n",
      "Epoch 49/50 - Train Loss: 0.028347 - Val Loss: 0.044925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:34:23,375] Trial 56 finished with value: 0.04297167559464773 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 187, 'lr': 0.002937312718068467, 'weight_decay': 3.291014905919004e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029050 - Val Loss: 0.045897\n",
      "Epoch 1/50 - Train Loss: 0.056468 - Val Loss: 0.104496\n",
      "Epoch 2/50 - Train Loss: 0.043020 - Val Loss: 0.061583\n",
      "Epoch 3/50 - Train Loss: 0.036382 - Val Loss: 0.062307\n",
      "Epoch 4/50 - Train Loss: 0.033856 - Val Loss: 0.068894\n",
      "Epoch 5/50 - Train Loss: 0.035430 - Val Loss: 0.052087\n",
      "Epoch 6/50 - Train Loss: 0.034684 - Val Loss: 0.049198\n",
      "Epoch 7/50 - Train Loss: 0.028172 - Val Loss: 0.045957\n",
      "Epoch 8/50 - Train Loss: 0.032000 - Val Loss: 0.045991\n",
      "Epoch 9/50 - Train Loss: 0.030349 - Val Loss: 0.047908\n",
      "Epoch 10/50 - Train Loss: 0.031708 - Val Loss: 0.046472\n",
      "Epoch 11/50 - Train Loss: 0.030713 - Val Loss: 0.048650\n",
      "Epoch 12/50 - Train Loss: 0.029258 - Val Loss: 0.048995\n",
      "Epoch 13/50 - Train Loss: 0.029853 - Val Loss: 0.053329\n",
      "Epoch 14/50 - Train Loss: 0.027176 - Val Loss: 0.044770\n",
      "Epoch 15/50 - Train Loss: 0.030980 - Val Loss: 0.045962\n",
      "Epoch 16/50 - Train Loss: 0.030872 - Val Loss: 0.056711\n",
      "Epoch 17/50 - Train Loss: 0.031869 - Val Loss: 0.051719\n",
      "Epoch 18/50 - Train Loss: 0.030452 - Val Loss: 0.044115\n",
      "Epoch 19/50 - Train Loss: 0.031280 - Val Loss: 0.046617\n",
      "Epoch 20/50 - Train Loss: 0.027846 - Val Loss: 0.050816\n",
      "Epoch 21/50 - Train Loss: 0.033870 - Val Loss: 0.051529\n",
      "Epoch 22/50 - Train Loss: 0.032185 - Val Loss: 0.047142\n",
      "Epoch 23/50 - Train Loss: 0.029741 - Val Loss: 0.043689\n",
      "Epoch 24/50 - Train Loss: 0.029262 - Val Loss: 0.044619\n",
      "Epoch 25/50 - Train Loss: 0.029940 - Val Loss: 0.057232\n",
      "Epoch 26/50 - Train Loss: 0.032989 - Val Loss: 0.045233\n",
      "Epoch 27/50 - Train Loss: 0.028496 - Val Loss: 0.043843\n",
      "Epoch 28/50 - Train Loss: 0.030239 - Val Loss: 0.045820\n",
      "Epoch 29/50 - Train Loss: 0.030714 - Val Loss: 0.052269\n",
      "Epoch 30/50 - Train Loss: 0.030769 - Val Loss: 0.043960\n",
      "Epoch 31/50 - Train Loss: 0.030997 - Val Loss: 0.044074\n",
      "Epoch 32/50 - Train Loss: 0.029522 - Val Loss: 0.046545\n",
      "Epoch 33/50 - Train Loss: 0.029138 - Val Loss: 0.045726\n",
      "Epoch 34/50 - Train Loss: 0.027263 - Val Loss: 0.044490\n",
      "Epoch 35/50 - Train Loss: 0.029264 - Val Loss: 0.044213\n",
      "Epoch 36/50 - Train Loss: 0.030793 - Val Loss: 0.051582\n",
      "Epoch 37/50 - Train Loss: 0.027798 - Val Loss: 0.044532\n",
      "Epoch 38/50 - Train Loss: 0.027758 - Val Loss: 0.044612\n",
      "Epoch 39/50 - Train Loss: 0.031658 - Val Loss: 0.046003\n",
      "Epoch 40/50 - Train Loss: 0.030840 - Val Loss: 0.048594\n",
      "Epoch 41/50 - Train Loss: 0.027958 - Val Loss: 0.044307\n",
      "Epoch 42/50 - Train Loss: 0.030536 - Val Loss: 0.044555\n",
      "Epoch 43/50 - Train Loss: 0.027652 - Val Loss: 0.047873\n",
      "Epoch 44/50 - Train Loss: 0.030721 - Val Loss: 0.050904\n",
      "Epoch 45/50 - Train Loss: 0.027638 - Val Loss: 0.044056\n",
      "Epoch 46/50 - Train Loss: 0.027080 - Val Loss: 0.044248\n",
      "Epoch 47/50 - Train Loss: 0.029006 - Val Loss: 0.044860\n",
      "Epoch 48/50 - Train Loss: 0.027454 - Val Loss: 0.049165\n",
      "Epoch 49/50 - Train Loss: 0.029301 - Val Loss: 0.044338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:34:44,361] Trial 57 finished with value: 0.04368922859430313 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 131, 'lr': 0.0034002156382645825, 'weight_decay': 3.790980517738143e-07, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028580 - Val Loss: 0.044662\n",
      "Epoch 1/50 - Train Loss: 0.115817 - Val Loss: 0.241781\n",
      "Epoch 2/50 - Train Loss: 0.099177 - Val Loss: 0.230617\n",
      "Epoch 3/50 - Train Loss: 0.096822 - Val Loss: 0.219080\n",
      "Epoch 4/50 - Train Loss: 0.088237 - Val Loss: 0.207831\n",
      "Epoch 5/50 - Train Loss: 0.081385 - Val Loss: 0.196624\n",
      "Epoch 6/50 - Train Loss: 0.081411 - Val Loss: 0.186394\n",
      "Epoch 7/50 - Train Loss: 0.077818 - Val Loss: 0.175813\n",
      "Epoch 8/50 - Train Loss: 0.070332 - Val Loss: 0.165665\n",
      "Epoch 9/50 - Train Loss: 0.064013 - Val Loss: 0.156834\n",
      "Epoch 10/50 - Train Loss: 0.061932 - Val Loss: 0.149117\n",
      "Epoch 11/50 - Train Loss: 0.058824 - Val Loss: 0.140953\n",
      "Epoch 12/50 - Train Loss: 0.056532 - Val Loss: 0.133582\n",
      "Epoch 13/50 - Train Loss: 0.054017 - Val Loss: 0.127029\n",
      "Epoch 14/50 - Train Loss: 0.053400 - Val Loss: 0.121770\n",
      "Epoch 15/50 - Train Loss: 0.050435 - Val Loss: 0.116710\n",
      "Epoch 16/50 - Train Loss: 0.048845 - Val Loss: 0.112982\n",
      "Epoch 17/50 - Train Loss: 0.049675 - Val Loss: 0.108639\n",
      "Epoch 18/50 - Train Loss: 0.049362 - Val Loss: 0.105120\n",
      "Epoch 19/50 - Train Loss: 0.047838 - Val Loss: 0.102585\n",
      "Epoch 20/50 - Train Loss: 0.048567 - Val Loss: 0.100613\n",
      "Epoch 21/50 - Train Loss: 0.046036 - Val Loss: 0.098938\n",
      "Epoch 22/50 - Train Loss: 0.046421 - Val Loss: 0.096968\n",
      "Epoch 23/50 - Train Loss: 0.045142 - Val Loss: 0.095898\n",
      "Epoch 24/50 - Train Loss: 0.043312 - Val Loss: 0.094262\n",
      "Epoch 25/50 - Train Loss: 0.046526 - Val Loss: 0.093662\n",
      "Epoch 26/50 - Train Loss: 0.041679 - Val Loss: 0.092524\n",
      "Epoch 27/50 - Train Loss: 0.043051 - Val Loss: 0.091107\n",
      "Epoch 28/50 - Train Loss: 0.045616 - Val Loss: 0.089815\n",
      "Epoch 29/50 - Train Loss: 0.045044 - Val Loss: 0.088405\n",
      "Epoch 30/50 - Train Loss: 0.042835 - Val Loss: 0.086998\n",
      "Epoch 31/50 - Train Loss: 0.042475 - Val Loss: 0.085710\n",
      "Epoch 32/50 - Train Loss: 0.042188 - Val Loss: 0.084400\n",
      "Epoch 33/50 - Train Loss: 0.042257 - Val Loss: 0.084099\n",
      "Epoch 34/50 - Train Loss: 0.040625 - Val Loss: 0.083017\n",
      "Epoch 35/50 - Train Loss: 0.040031 - Val Loss: 0.081905\n",
      "Epoch 36/50 - Train Loss: 0.040201 - Val Loss: 0.080601\n",
      "Epoch 37/50 - Train Loss: 0.039872 - Val Loss: 0.078549\n",
      "Epoch 38/50 - Train Loss: 0.042439 - Val Loss: 0.078932\n",
      "Epoch 39/50 - Train Loss: 0.040485 - Val Loss: 0.077841\n",
      "Epoch 40/50 - Train Loss: 0.039072 - Val Loss: 0.076944\n",
      "Epoch 41/50 - Train Loss: 0.041774 - Val Loss: 0.076147\n",
      "Epoch 42/50 - Train Loss: 0.035724 - Val Loss: 0.074910\n",
      "Epoch 43/50 - Train Loss: 0.036823 - Val Loss: 0.074076\n",
      "Epoch 44/50 - Train Loss: 0.037091 - Val Loss: 0.073559\n",
      "Epoch 45/50 - Train Loss: 0.038260 - Val Loss: 0.072453\n",
      "Epoch 46/50 - Train Loss: 0.038340 - Val Loss: 0.071017\n",
      "Epoch 47/50 - Train Loss: 0.036821 - Val Loss: 0.070137\n",
      "Epoch 48/50 - Train Loss: 0.038255 - Val Loss: 0.069531\n",
      "Epoch 49/50 - Train Loss: 0.035908 - Val Loss: 0.069107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:35:06,454] Trial 58 finished with value: 0.06813971449931462 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 109, 'lr': 2.3782923424263016e-05, 'weight_decay': 1.5281306151788458e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.037677 - Val Loss: 0.068140\n",
      "Epoch 1/50 - Train Loss: 0.106304 - Val Loss: 0.101771\n",
      "Epoch 2/50 - Train Loss: 0.052512 - Val Loss: 0.119025\n",
      "Epoch 3/50 - Train Loss: 0.042504 - Val Loss: 0.055399\n",
      "Epoch 4/50 - Train Loss: 0.034631 - Val Loss: 0.056741\n",
      "Epoch 5/50 - Train Loss: 0.033727 - Val Loss: 0.048242\n",
      "Epoch 6/50 - Train Loss: 0.033414 - Val Loss: 0.048312\n",
      "Epoch 7/50 - Train Loss: 0.027944 - Val Loss: 0.044784\n",
      "Epoch 8/50 - Train Loss: 0.033949 - Val Loss: 0.044285\n",
      "Epoch 9/50 - Train Loss: 0.033102 - Val Loss: 0.047313\n",
      "Epoch 10/50 - Train Loss: 0.031067 - Val Loss: 0.045150\n",
      "Epoch 11/50 - Train Loss: 0.029339 - Val Loss: 0.063806\n",
      "Epoch 12/50 - Train Loss: 0.033622 - Val Loss: 0.044166\n",
      "Epoch 13/50 - Train Loss: 0.034215 - Val Loss: 0.043479\n",
      "Epoch 14/50 - Train Loss: 0.032823 - Val Loss: 0.048979\n",
      "Epoch 15/50 - Train Loss: 0.032180 - Val Loss: 0.047458\n",
      "Epoch 16/50 - Train Loss: 0.028911 - Val Loss: 0.046147\n",
      "Epoch 17/50 - Train Loss: 0.032259 - Val Loss: 0.043109\n",
      "Epoch 18/50 - Train Loss: 0.031931 - Val Loss: 0.048065\n",
      "Epoch 19/50 - Train Loss: 0.028772 - Val Loss: 0.060062\n",
      "Epoch 20/50 - Train Loss: 0.031589 - Val Loss: 0.044412\n",
      "Epoch 21/50 - Train Loss: 0.032170 - Val Loss: 0.048221\n",
      "Epoch 22/50 - Train Loss: 0.027910 - Val Loss: 0.050049\n",
      "Epoch 23/50 - Train Loss: 0.031524 - Val Loss: 0.045508\n",
      "Epoch 24/50 - Train Loss: 0.030652 - Val Loss: 0.044257\n",
      "Epoch 25/50 - Train Loss: 0.029438 - Val Loss: 0.043541\n",
      "Epoch 26/50 - Train Loss: 0.031486 - Val Loss: 0.044178\n",
      "Epoch 27/50 - Train Loss: 0.031486 - Val Loss: 0.047371\n",
      "Epoch 28/50 - Train Loss: 0.029554 - Val Loss: 0.054252\n",
      "Epoch 29/50 - Train Loss: 0.028941 - Val Loss: 0.048375\n",
      "Epoch 30/50 - Train Loss: 0.031861 - Val Loss: 0.043393\n",
      "Epoch 31/50 - Train Loss: 0.029177 - Val Loss: 0.043377\n",
      "Epoch 32/50 - Train Loss: 0.025730 - Val Loss: 0.048240\n",
      "Epoch 33/50 - Train Loss: 0.028269 - Val Loss: 0.046153\n",
      "Epoch 34/50 - Train Loss: 0.031397 - Val Loss: 0.043717\n",
      "Epoch 35/50 - Train Loss: 0.033230 - Val Loss: 0.052578\n",
      "Epoch 36/50 - Train Loss: 0.027085 - Val Loss: 0.065325\n",
      "Epoch 37/50 - Train Loss: 0.034132 - Val Loss: 0.044111\n",
      "Epoch 38/50 - Train Loss: 0.028551 - Val Loss: 0.051897\n",
      "Epoch 39/50 - Train Loss: 0.027884 - Val Loss: 0.046890\n",
      "Epoch 40/50 - Train Loss: 0.025524 - Val Loss: 0.043467\n",
      "Epoch 41/50 - Train Loss: 0.026250 - Val Loss: 0.043966\n",
      "Epoch 42/50 - Train Loss: 0.028580 - Val Loss: 0.045458\n",
      "Epoch 43/50 - Train Loss: 0.028925 - Val Loss: 0.049739\n",
      "Epoch 44/50 - Train Loss: 0.026835 - Val Loss: 0.043891\n",
      "Epoch 45/50 - Train Loss: 0.030162 - Val Loss: 0.047254\n",
      "Epoch 46/50 - Train Loss: 0.029399 - Val Loss: 0.044907\n",
      "Epoch 47/50 - Train Loss: 0.029293 - Val Loss: 0.044480\n",
      "Epoch 48/50 - Train Loss: 0.030443 - Val Loss: 0.044294\n",
      "Epoch 49/50 - Train Loss: 0.029349 - Val Loss: 0.049542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:35:29,077] Trial 59 finished with value: 0.0431094765663147 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 212, 'lr': 0.005061513508585429, 'weight_decay': 5.457625061351898e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026982 - Val Loss: 0.051693\n",
      "Epoch 1/50 - Train Loss: 0.061250 - Val Loss: 0.070261\n",
      "Epoch 2/50 - Train Loss: 0.037314 - Val Loss: 0.071333\n",
      "Epoch 3/50 - Train Loss: 0.032072 - Val Loss: 0.060073\n",
      "Epoch 4/50 - Train Loss: 0.032140 - Val Loss: 0.055878\n",
      "Epoch 5/50 - Train Loss: 0.031827 - Val Loss: 0.045987\n",
      "Epoch 6/50 - Train Loss: 0.032322 - Val Loss: 0.044124\n",
      "Epoch 7/50 - Train Loss: 0.032825 - Val Loss: 0.046559\n",
      "Epoch 8/50 - Train Loss: 0.029777 - Val Loss: 0.043368\n",
      "Epoch 9/50 - Train Loss: 0.029270 - Val Loss: 0.048744\n",
      "Epoch 10/50 - Train Loss: 0.027785 - Val Loss: 0.045766\n",
      "Epoch 11/50 - Train Loss: 0.029759 - Val Loss: 0.044024\n",
      "Epoch 12/50 - Train Loss: 0.029544 - Val Loss: 0.048419\n",
      "Epoch 13/50 - Train Loss: 0.031176 - Val Loss: 0.052193\n",
      "Epoch 14/50 - Train Loss: 0.028842 - Val Loss: 0.046356\n",
      "Epoch 15/50 - Train Loss: 0.031518 - Val Loss: 0.048160\n",
      "Epoch 16/50 - Train Loss: 0.031039 - Val Loss: 0.049624\n",
      "Epoch 17/50 - Train Loss: 0.031362 - Val Loss: 0.047182\n",
      "Epoch 18/50 - Train Loss: 0.031334 - Val Loss: 0.046943\n",
      "Epoch 19/50 - Train Loss: 0.031308 - Val Loss: 0.043984\n",
      "Epoch 20/50 - Train Loss: 0.030841 - Val Loss: 0.043581\n",
      "Epoch 21/50 - Train Loss: 0.027180 - Val Loss: 0.043740\n",
      "Epoch 22/50 - Train Loss: 0.031757 - Val Loss: 0.053659\n",
      "Epoch 23/50 - Train Loss: 0.027083 - Val Loss: 0.054663\n",
      "Epoch 24/50 - Train Loss: 0.029394 - Val Loss: 0.051332\n",
      "Epoch 25/50 - Train Loss: 0.030881 - Val Loss: 0.053588\n",
      "Epoch 26/50 - Train Loss: 0.028933 - Val Loss: 0.044689\n",
      "Epoch 27/50 - Train Loss: 0.028447 - Val Loss: 0.046426\n",
      "Epoch 28/50 - Train Loss: 0.027228 - Val Loss: 0.048102\n",
      "Epoch 29/50 - Train Loss: 0.026811 - Val Loss: 0.044925\n",
      "Epoch 30/50 - Train Loss: 0.027391 - Val Loss: 0.044051\n",
      "Epoch 31/50 - Train Loss: 0.030479 - Val Loss: 0.051122\n",
      "Epoch 32/50 - Train Loss: 0.030109 - Val Loss: 0.049153\n",
      "Epoch 33/50 - Train Loss: 0.028714 - Val Loss: 0.047777\n",
      "Epoch 34/50 - Train Loss: 0.029709 - Val Loss: 0.049912\n",
      "Epoch 35/50 - Train Loss: 0.028931 - Val Loss: 0.044753\n",
      "Epoch 36/50 - Train Loss: 0.025794 - Val Loss: 0.044745\n",
      "Epoch 37/50 - Train Loss: 0.025995 - Val Loss: 0.045392\n",
      "Epoch 38/50 - Train Loss: 0.028194 - Val Loss: 0.047126\n",
      "Epoch 39/50 - Train Loss: 0.026397 - Val Loss: 0.048305\n",
      "Epoch 40/50 - Train Loss: 0.029812 - Val Loss: 0.048443\n",
      "Epoch 41/50 - Train Loss: 0.028176 - Val Loss: 0.046421\n",
      "Epoch 42/50 - Train Loss: 0.027455 - Val Loss: 0.046098\n",
      "Epoch 43/50 - Train Loss: 0.026708 - Val Loss: 0.046918\n",
      "Epoch 44/50 - Train Loss: 0.029186 - Val Loss: 0.046639\n",
      "Epoch 45/50 - Train Loss: 0.028851 - Val Loss: 0.049086\n",
      "Epoch 46/50 - Train Loss: 0.026643 - Val Loss: 0.045440\n",
      "Epoch 47/50 - Train Loss: 0.027591 - Val Loss: 0.045257\n",
      "Epoch 48/50 - Train Loss: 0.027804 - Val Loss: 0.046925\n",
      "Epoch 49/50 - Train Loss: 0.026884 - Val Loss: 0.046702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:35:58,358] Trial 60 finished with value: 0.04336848855018616 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 182, 'lr': 0.0028067306863438046, 'weight_decay': 1.194345816877662e-07, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.024593 - Val Loss: 0.047624\n",
      "Epoch 1/50 - Train Loss: 0.070895 - Val Loss: 0.070396\n",
      "Epoch 2/50 - Train Loss: 0.037356 - Val Loss: 0.072005\n",
      "Epoch 3/50 - Train Loss: 0.035479 - Val Loss: 0.048760\n",
      "Epoch 4/50 - Train Loss: 0.034244 - Val Loss: 0.045929\n",
      "Epoch 5/50 - Train Loss: 0.031838 - Val Loss: 0.044080\n",
      "Epoch 6/50 - Train Loss: 0.029784 - Val Loss: 0.048801\n",
      "Epoch 7/50 - Train Loss: 0.030940 - Val Loss: 0.045592\n",
      "Epoch 8/50 - Train Loss: 0.031209 - Val Loss: 0.049189\n",
      "Epoch 9/50 - Train Loss: 0.031133 - Val Loss: 0.044456\n",
      "Epoch 10/50 - Train Loss: 0.030211 - Val Loss: 0.046169\n",
      "Epoch 11/50 - Train Loss: 0.028340 - Val Loss: 0.044345\n",
      "Epoch 12/50 - Train Loss: 0.030246 - Val Loss: 0.044452\n",
      "Epoch 13/50 - Train Loss: 0.030803 - Val Loss: 0.045231\n",
      "Epoch 14/50 - Train Loss: 0.030706 - Val Loss: 0.050274\n",
      "Epoch 15/50 - Train Loss: 0.033250 - Val Loss: 0.050684\n",
      "Epoch 16/50 - Train Loss: 0.029907 - Val Loss: 0.051808\n",
      "Epoch 17/50 - Train Loss: 0.029643 - Val Loss: 0.060976\n",
      "Epoch 18/50 - Train Loss: 0.030938 - Val Loss: 0.050347\n",
      "Epoch 19/50 - Train Loss: 0.028991 - Val Loss: 0.046133\n",
      "Epoch 20/50 - Train Loss: 0.028895 - Val Loss: 0.045379\n",
      "Epoch 21/50 - Train Loss: 0.030067 - Val Loss: 0.044350\n",
      "Epoch 22/50 - Train Loss: 0.029150 - Val Loss: 0.045693\n",
      "Epoch 23/50 - Train Loss: 0.030390 - Val Loss: 0.045636\n",
      "Epoch 24/50 - Train Loss: 0.029923 - Val Loss: 0.045987\n",
      "Epoch 25/50 - Train Loss: 0.030984 - Val Loss: 0.046155\n",
      "Epoch 26/50 - Train Loss: 0.030633 - Val Loss: 0.045434\n",
      "Epoch 27/50 - Train Loss: 0.030339 - Val Loss: 0.044645\n",
      "Epoch 28/50 - Train Loss: 0.029657 - Val Loss: 0.044463\n",
      "Epoch 29/50 - Train Loss: 0.029089 - Val Loss: 0.045968\n",
      "Epoch 30/50 - Train Loss: 0.029131 - Val Loss: 0.045279\n",
      "Epoch 31/50 - Train Loss: 0.027069 - Val Loss: 0.045062\n",
      "Epoch 32/50 - Train Loss: 0.029073 - Val Loss: 0.049912\n",
      "Epoch 33/50 - Train Loss: 0.028835 - Val Loss: 0.045704\n",
      "Epoch 34/50 - Train Loss: 0.027745 - Val Loss: 0.045670\n",
      "Epoch 35/50 - Train Loss: 0.029854 - Val Loss: 0.045921\n",
      "Epoch 36/50 - Train Loss: 0.027056 - Val Loss: 0.047068\n",
      "Epoch 37/50 - Train Loss: 0.026086 - Val Loss: 0.046631\n",
      "Epoch 38/50 - Train Loss: 0.030280 - Val Loss: 0.044385\n",
      "Epoch 39/50 - Train Loss: 0.028742 - Val Loss: 0.045969\n",
      "Epoch 40/50 - Train Loss: 0.028320 - Val Loss: 0.045074\n",
      "Epoch 41/50 - Train Loss: 0.030360 - Val Loss: 0.048362\n",
      "Epoch 42/50 - Train Loss: 0.028597 - Val Loss: 0.047004\n",
      "Epoch 43/50 - Train Loss: 0.028059 - Val Loss: 0.044872\n",
      "Epoch 44/50 - Train Loss: 0.030872 - Val Loss: 0.044887\n",
      "Epoch 45/50 - Train Loss: 0.028593 - Val Loss: 0.047712\n",
      "Epoch 46/50 - Train Loss: 0.030328 - Val Loss: 0.046021\n",
      "Epoch 47/50 - Train Loss: 0.028764 - Val Loss: 0.045822\n",
      "Epoch 48/50 - Train Loss: 0.028213 - Val Loss: 0.045862\n",
      "Epoch 49/50 - Train Loss: 0.028113 - Val Loss: 0.045842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:36:21,372] Trial 61 finished with value: 0.04408017173409462 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 195, 'lr': 0.0020075769291212748, 'weight_decay': 3.435821052569086e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030408 - Val Loss: 0.047184\n",
      "Epoch 1/50 - Train Loss: 0.134886 - Val Loss: 0.227756\n",
      "Epoch 2/50 - Train Loss: 0.079509 - Val Loss: 0.151019\n",
      "Epoch 3/50 - Train Loss: 0.053672 - Val Loss: 0.111018\n",
      "Epoch 4/50 - Train Loss: 0.043661 - Val Loss: 0.094283\n",
      "Epoch 5/50 - Train Loss: 0.043686 - Val Loss: 0.088495\n",
      "Epoch 6/50 - Train Loss: 0.042785 - Val Loss: 0.086290\n",
      "Epoch 7/50 - Train Loss: 0.042479 - Val Loss: 0.085126\n",
      "Epoch 8/50 - Train Loss: 0.040809 - Val Loss: 0.082828\n",
      "Epoch 9/50 - Train Loss: 0.039979 - Val Loss: 0.077637\n",
      "Epoch 10/50 - Train Loss: 0.036221 - Val Loss: 0.076091\n",
      "Epoch 11/50 - Train Loss: 0.036962 - Val Loss: 0.074122\n",
      "Epoch 12/50 - Train Loss: 0.033826 - Val Loss: 0.070480\n",
      "Epoch 13/50 - Train Loss: 0.035415 - Val Loss: 0.069753\n",
      "Epoch 14/50 - Train Loss: 0.034302 - Val Loss: 0.066463\n",
      "Epoch 15/50 - Train Loss: 0.034891 - Val Loss: 0.066742\n",
      "Epoch 16/50 - Train Loss: 0.033097 - Val Loss: 0.062709\n",
      "Epoch 17/50 - Train Loss: 0.032814 - Val Loss: 0.062433\n",
      "Epoch 18/50 - Train Loss: 0.032970 - Val Loss: 0.060938\n",
      "Epoch 19/50 - Train Loss: 0.033010 - Val Loss: 0.059919\n",
      "Epoch 20/50 - Train Loss: 0.032549 - Val Loss: 0.057945\n",
      "Epoch 21/50 - Train Loss: 0.034125 - Val Loss: 0.058123\n",
      "Epoch 22/50 - Train Loss: 0.031957 - Val Loss: 0.055200\n",
      "Epoch 23/50 - Train Loss: 0.030627 - Val Loss: 0.055797\n",
      "Epoch 24/50 - Train Loss: 0.030121 - Val Loss: 0.054317\n",
      "Epoch 25/50 - Train Loss: 0.030420 - Val Loss: 0.053154\n",
      "Epoch 26/50 - Train Loss: 0.030115 - Val Loss: 0.052823\n",
      "Epoch 27/50 - Train Loss: 0.029875 - Val Loss: 0.052111\n",
      "Epoch 28/50 - Train Loss: 0.030467 - Val Loss: 0.052113\n",
      "Epoch 29/50 - Train Loss: 0.030227 - Val Loss: 0.051638\n",
      "Epoch 30/50 - Train Loss: 0.029766 - Val Loss: 0.050724\n",
      "Epoch 31/50 - Train Loss: 0.032846 - Val Loss: 0.050475\n",
      "Epoch 32/50 - Train Loss: 0.029402 - Val Loss: 0.051323\n",
      "Epoch 33/50 - Train Loss: 0.030142 - Val Loss: 0.048899\n",
      "Epoch 34/50 - Train Loss: 0.031342 - Val Loss: 0.049793\n",
      "Epoch 35/50 - Train Loss: 0.030049 - Val Loss: 0.048665\n",
      "Epoch 36/50 - Train Loss: 0.028357 - Val Loss: 0.049138\n",
      "Epoch 37/50 - Train Loss: 0.030319 - Val Loss: 0.048875\n",
      "Epoch 38/50 - Train Loss: 0.029607 - Val Loss: 0.047927\n",
      "Epoch 39/50 - Train Loss: 0.030707 - Val Loss: 0.048080\n",
      "Epoch 40/50 - Train Loss: 0.031733 - Val Loss: 0.048188\n",
      "Epoch 41/50 - Train Loss: 0.028010 - Val Loss: 0.047563\n",
      "Epoch 42/50 - Train Loss: 0.030365 - Val Loss: 0.047261\n",
      "Epoch 43/50 - Train Loss: 0.027205 - Val Loss: 0.047069\n",
      "Epoch 44/50 - Train Loss: 0.030314 - Val Loss: 0.047022\n",
      "Epoch 45/50 - Train Loss: 0.029111 - Val Loss: 0.047269\n",
      "Epoch 46/50 - Train Loss: 0.028857 - Val Loss: 0.046962\n",
      "Epoch 47/50 - Train Loss: 0.031816 - Val Loss: 0.047038\n",
      "Epoch 48/50 - Train Loss: 0.029649 - Val Loss: 0.047055\n",
      "Epoch 49/50 - Train Loss: 0.028322 - Val Loss: 0.047357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:36:43,903] Trial 62 finished with value: 0.046844061464071274 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 155, 'lr': 0.00010273167447452212, 'weight_decay': 1.0337588853073355e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030809 - Val Loss: 0.046844\n",
      "Epoch 1/50 - Train Loss: 0.056665 - Val Loss: 0.057952\n",
      "Epoch 2/50 - Train Loss: 0.035588 - Val Loss: 0.045383\n",
      "Epoch 3/50 - Train Loss: 0.033280 - Val Loss: 0.056098\n",
      "Epoch 4/50 - Train Loss: 0.033482 - Val Loss: 0.048166\n",
      "Epoch 5/50 - Train Loss: 0.032596 - Val Loss: 0.046100\n",
      "Epoch 6/50 - Train Loss: 0.033960 - Val Loss: 0.043738\n",
      "Epoch 7/50 - Train Loss: 0.030968 - Val Loss: 0.050348\n",
      "Epoch 8/50 - Train Loss: 0.030755 - Val Loss: 0.044076\n",
      "Epoch 9/50 - Train Loss: 0.030318 - Val Loss: 0.045556\n",
      "Epoch 10/50 - Train Loss: 0.028048 - Val Loss: 0.046228\n",
      "Epoch 11/50 - Train Loss: 0.031943 - Val Loss: 0.043511\n",
      "Epoch 12/50 - Train Loss: 0.031219 - Val Loss: 0.054364\n",
      "Epoch 13/50 - Train Loss: 0.031619 - Val Loss: 0.047127\n",
      "Epoch 14/50 - Train Loss: 0.029543 - Val Loss: 0.050788\n",
      "Epoch 15/50 - Train Loss: 0.029048 - Val Loss: 0.069464\n",
      "Epoch 16/50 - Train Loss: 0.032658 - Val Loss: 0.047732\n",
      "Epoch 17/50 - Train Loss: 0.029888 - Val Loss: 0.045905\n",
      "Epoch 18/50 - Train Loss: 0.030260 - Val Loss: 0.043660\n",
      "Epoch 19/50 - Train Loss: 0.029443 - Val Loss: 0.051988\n",
      "Epoch 20/50 - Train Loss: 0.030586 - Val Loss: 0.043397\n",
      "Epoch 21/50 - Train Loss: 0.032143 - Val Loss: 0.045456\n",
      "Epoch 22/50 - Train Loss: 0.029683 - Val Loss: 0.044968\n",
      "Epoch 23/50 - Train Loss: 0.028164 - Val Loss: 0.043481\n",
      "Epoch 24/50 - Train Loss: 0.029604 - Val Loss: 0.045301\n",
      "Epoch 25/50 - Train Loss: 0.029325 - Val Loss: 0.046693\n",
      "Epoch 26/50 - Train Loss: 0.029913 - Val Loss: 0.043500\n",
      "Epoch 27/50 - Train Loss: 0.027488 - Val Loss: 0.047983\n",
      "Epoch 28/50 - Train Loss: 0.029831 - Val Loss: 0.043479\n",
      "Epoch 29/50 - Train Loss: 0.029529 - Val Loss: 0.044270\n",
      "Epoch 30/50 - Train Loss: 0.029127 - Val Loss: 0.045970\n",
      "Epoch 31/50 - Train Loss: 0.029531 - Val Loss: 0.045203\n",
      "Epoch 32/50 - Train Loss: 0.028687 - Val Loss: 0.061212\n",
      "Epoch 33/50 - Train Loss: 0.033545 - Val Loss: 0.043896\n",
      "Epoch 34/50 - Train Loss: 0.030557 - Val Loss: 0.045853\n",
      "Epoch 35/50 - Train Loss: 0.029487 - Val Loss: 0.048909\n",
      "Epoch 36/50 - Train Loss: 0.030348 - Val Loss: 0.062829\n",
      "Epoch 37/50 - Train Loss: 0.030512 - Val Loss: 0.044369\n",
      "Epoch 38/50 - Train Loss: 0.028300 - Val Loss: 0.044815\n",
      "Epoch 39/50 - Train Loss: 0.029895 - Val Loss: 0.044627\n",
      "Epoch 40/50 - Train Loss: 0.029216 - Val Loss: 0.044458\n",
      "Epoch 41/50 - Train Loss: 0.029534 - Val Loss: 0.046011\n",
      "Epoch 42/50 - Train Loss: 0.032450 - Val Loss: 0.047667\n",
      "Epoch 43/50 - Train Loss: 0.028835 - Val Loss: 0.044269\n",
      "Epoch 44/50 - Train Loss: 0.026783 - Val Loss: 0.048147\n",
      "Epoch 45/50 - Train Loss: 0.027759 - Val Loss: 0.049663\n",
      "Epoch 46/50 - Train Loss: 0.029319 - Val Loss: 0.053462\n",
      "Epoch 47/50 - Train Loss: 0.026912 - Val Loss: 0.044982\n",
      "Epoch 48/50 - Train Loss: 0.028484 - Val Loss: 0.045968\n",
      "Epoch 49/50 - Train Loss: 0.028808 - Val Loss: 0.045811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:37:19,541] Trial 63 finished with value: 0.04339703296621641 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 190, 'lr': 0.004558302543685476, 'weight_decay': 3.012974202508986e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026648 - Val Loss: 0.046021\n",
      "Epoch 1/50 - Train Loss: 0.061996 - Val Loss: 0.097729\n",
      "Epoch 2/50 - Train Loss: 0.039573 - Val Loss: 0.050920\n",
      "Epoch 3/50 - Train Loss: 0.030777 - Val Loss: 0.046589\n",
      "Epoch 4/50 - Train Loss: 0.032998 - Val Loss: 0.045231\n",
      "Epoch 5/50 - Train Loss: 0.031389 - Val Loss: 0.045319\n",
      "Epoch 6/50 - Train Loss: 0.031396 - Val Loss: 0.048782\n",
      "Epoch 7/50 - Train Loss: 0.030865 - Val Loss: 0.046276\n",
      "Epoch 8/50 - Train Loss: 0.029563 - Val Loss: 0.044487\n",
      "Epoch 9/50 - Train Loss: 0.026463 - Val Loss: 0.042978\n",
      "Epoch 10/50 - Train Loss: 0.030457 - Val Loss: 0.047991\n",
      "Epoch 11/50 - Train Loss: 0.027981 - Val Loss: 0.048072\n",
      "Epoch 12/50 - Train Loss: 0.032651 - Val Loss: 0.048736\n",
      "Epoch 13/50 - Train Loss: 0.027769 - Val Loss: 0.045395\n",
      "Epoch 14/50 - Train Loss: 0.030744 - Val Loss: 0.047101\n",
      "Epoch 15/50 - Train Loss: 0.030033 - Val Loss: 0.044077\n",
      "Epoch 16/50 - Train Loss: 0.028773 - Val Loss: 0.043779\n",
      "Epoch 17/50 - Train Loss: 0.029819 - Val Loss: 0.043471\n",
      "Epoch 18/50 - Train Loss: 0.030618 - Val Loss: 0.045347\n",
      "Epoch 19/50 - Train Loss: 0.030299 - Val Loss: 0.044075\n",
      "Epoch 20/50 - Train Loss: 0.028243 - Val Loss: 0.054115\n",
      "Epoch 21/50 - Train Loss: 0.029406 - Val Loss: 0.045894\n",
      "Epoch 22/50 - Train Loss: 0.027212 - Val Loss: 0.043671\n",
      "Epoch 23/50 - Train Loss: 0.027827 - Val Loss: 0.043804\n",
      "Epoch 24/50 - Train Loss: 0.029266 - Val Loss: 0.043787\n",
      "Epoch 25/50 - Train Loss: 0.028403 - Val Loss: 0.045233\n",
      "Epoch 26/50 - Train Loss: 0.027679 - Val Loss: 0.043927\n",
      "Epoch 27/50 - Train Loss: 0.028156 - Val Loss: 0.044159\n",
      "Epoch 28/50 - Train Loss: 0.026775 - Val Loss: 0.043915\n",
      "Epoch 29/50 - Train Loss: 0.029707 - Val Loss: 0.044627\n",
      "Epoch 30/50 - Train Loss: 0.029577 - Val Loss: 0.046899\n",
      "Epoch 31/50 - Train Loss: 0.028407 - Val Loss: 0.044465\n",
      "Epoch 32/50 - Train Loss: 0.028077 - Val Loss: 0.044112\n",
      "Epoch 33/50 - Train Loss: 0.030169 - Val Loss: 0.046847\n",
      "Epoch 34/50 - Train Loss: 0.029183 - Val Loss: 0.050105\n",
      "Epoch 35/50 - Train Loss: 0.027884 - Val Loss: 0.053405\n",
      "Epoch 36/50 - Train Loss: 0.027999 - Val Loss: 0.048894\n",
      "Epoch 37/50 - Train Loss: 0.031345 - Val Loss: 0.046558\n",
      "Epoch 38/50 - Train Loss: 0.027169 - Val Loss: 0.043807\n",
      "Epoch 39/50 - Train Loss: 0.027404 - Val Loss: 0.044413\n",
      "Epoch 40/50 - Train Loss: 0.030254 - Val Loss: 0.044373\n",
      "Epoch 41/50 - Train Loss: 0.028436 - Val Loss: 0.044499\n",
      "Epoch 42/50 - Train Loss: 0.030185 - Val Loss: 0.045269\n",
      "Epoch 43/50 - Train Loss: 0.028508 - Val Loss: 0.049263\n",
      "Epoch 44/50 - Train Loss: 0.029196 - Val Loss: 0.046687\n",
      "Epoch 45/50 - Train Loss: 0.029488 - Val Loss: 0.044391\n",
      "Epoch 46/50 - Train Loss: 0.027202 - Val Loss: 0.044647\n",
      "Epoch 47/50 - Train Loss: 0.028407 - Val Loss: 0.044880\n",
      "Epoch 48/50 - Train Loss: 0.028669 - Val Loss: 0.044639\n",
      "Epoch 49/50 - Train Loss: 0.029457 - Val Loss: 0.045384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:37:42,845] Trial 64 finished with value: 0.04297782356540362 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 173, 'lr': 0.0012533085503918604, 'weight_decay': 2.1160494833151203e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027407 - Val Loss: 0.045979\n",
      "Epoch 1/50 - Train Loss: 0.079083 - Val Loss: 0.067652\n",
      "Epoch 2/50 - Train Loss: 0.045960 - Val Loss: 0.095792\n",
      "Epoch 3/50 - Train Loss: 0.039005 - Val Loss: 0.058159\n",
      "Epoch 4/50 - Train Loss: 0.036690 - Val Loss: 0.070145\n",
      "Epoch 5/50 - Train Loss: 0.034196 - Val Loss: 0.053324\n",
      "Epoch 6/50 - Train Loss: 0.035524 - Val Loss: 0.050324\n",
      "Epoch 7/50 - Train Loss: 0.031014 - Val Loss: 0.047256\n",
      "Epoch 8/50 - Train Loss: 0.032059 - Val Loss: 0.047255\n",
      "Epoch 9/50 - Train Loss: 0.030936 - Val Loss: 0.051254\n",
      "Epoch 10/50 - Train Loss: 0.031439 - Val Loss: 0.045848\n",
      "Epoch 11/50 - Train Loss: 0.027289 - Val Loss: 0.044756\n",
      "Epoch 12/50 - Train Loss: 0.030816 - Val Loss: 0.045086\n",
      "Epoch 13/50 - Train Loss: 0.028624 - Val Loss: 0.045977\n",
      "Epoch 14/50 - Train Loss: 0.030428 - Val Loss: 0.043709\n",
      "Epoch 15/50 - Train Loss: 0.030342 - Val Loss: 0.044585\n",
      "Epoch 16/50 - Train Loss: 0.030654 - Val Loss: 0.045050\n",
      "Epoch 17/50 - Train Loss: 0.029549 - Val Loss: 0.045645\n",
      "Epoch 18/50 - Train Loss: 0.026505 - Val Loss: 0.043931\n",
      "Epoch 19/50 - Train Loss: 0.028615 - Val Loss: 0.044231\n",
      "Epoch 20/50 - Train Loss: 0.028614 - Val Loss: 0.045187\n",
      "Epoch 21/50 - Train Loss: 0.028311 - Val Loss: 0.047429\n",
      "Epoch 22/50 - Train Loss: 0.031048 - Val Loss: 0.046698\n",
      "Epoch 23/50 - Train Loss: 0.026923 - Val Loss: 0.044316\n",
      "Epoch 24/50 - Train Loss: 0.029619 - Val Loss: 0.047149\n",
      "Epoch 25/50 - Train Loss: 0.027409 - Val Loss: 0.043482\n",
      "Epoch 26/50 - Train Loss: 0.030326 - Val Loss: 0.043685\n",
      "Epoch 27/50 - Train Loss: 0.027208 - Val Loss: 0.043715\n",
      "Epoch 28/50 - Train Loss: 0.030430 - Val Loss: 0.044703\n",
      "Epoch 29/50 - Train Loss: 0.030470 - Val Loss: 0.046038\n",
      "Epoch 30/50 - Train Loss: 0.029154 - Val Loss: 0.044754\n",
      "Epoch 31/50 - Train Loss: 0.027997 - Val Loss: 0.044209\n",
      "Epoch 32/50 - Train Loss: 0.027901 - Val Loss: 0.044222\n",
      "Epoch 33/50 - Train Loss: 0.028036 - Val Loss: 0.046026\n",
      "Epoch 34/50 - Train Loss: 0.027911 - Val Loss: 0.044881\n",
      "Epoch 35/50 - Train Loss: 0.027569 - Val Loss: 0.043738\n",
      "Epoch 36/50 - Train Loss: 0.029161 - Val Loss: 0.043938\n",
      "Epoch 37/50 - Train Loss: 0.029046 - Val Loss: 0.044627\n",
      "Epoch 38/50 - Train Loss: 0.029766 - Val Loss: 0.043746\n",
      "Epoch 39/50 - Train Loss: 0.027962 - Val Loss: 0.044417\n",
      "Epoch 40/50 - Train Loss: 0.031150 - Val Loss: 0.044289\n",
      "Epoch 41/50 - Train Loss: 0.028855 - Val Loss: 0.044135\n",
      "Epoch 42/50 - Train Loss: 0.030036 - Val Loss: 0.045110\n",
      "Epoch 43/50 - Train Loss: 0.027803 - Val Loss: 0.045352\n",
      "Epoch 44/50 - Train Loss: 0.028766 - Val Loss: 0.046231\n",
      "Epoch 45/50 - Train Loss: 0.030153 - Val Loss: 0.045258\n",
      "Epoch 46/50 - Train Loss: 0.029089 - Val Loss: 0.043784\n",
      "Epoch 47/50 - Train Loss: 0.029072 - Val Loss: 0.043562\n",
      "Epoch 48/50 - Train Loss: 0.027814 - Val Loss: 0.043590\n",
      "Epoch 49/50 - Train Loss: 0.029154 - Val Loss: 0.044275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:38:12,028] Trial 65 finished with value: 0.04348217944304148 and parameters: {'n_hidden_layers': 2, 'n_hidden_units': 172, 'lr': 0.0004886118699717594, 'weight_decay': 2.3518874985202497e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.025324 - Val Loss: 0.043739\n",
      "Epoch 1/50 - Train Loss: 0.077365 - Val Loss: 0.062819\n",
      "Epoch 2/50 - Train Loss: 0.052570 - Val Loss: 0.091857\n",
      "Epoch 3/50 - Train Loss: 0.040196 - Val Loss: 0.071986\n",
      "Epoch 4/50 - Train Loss: 0.035415 - Val Loss: 0.051436\n",
      "Epoch 5/50 - Train Loss: 0.034601 - Val Loss: 0.060378\n",
      "Epoch 6/50 - Train Loss: 0.032422 - Val Loss: 0.048507\n",
      "Epoch 7/50 - Train Loss: 0.036344 - Val Loss: 0.052258\n",
      "Epoch 8/50 - Train Loss: 0.032581 - Val Loss: 0.047938\n",
      "Epoch 9/50 - Train Loss: 0.032617 - Val Loss: 0.045043\n",
      "Epoch 10/50 - Train Loss: 0.027953 - Val Loss: 0.046882\n",
      "Epoch 11/50 - Train Loss: 0.032590 - Val Loss: 0.043806\n",
      "Epoch 12/50 - Train Loss: 0.031157 - Val Loss: 0.046574\n",
      "Epoch 13/50 - Train Loss: 0.029267 - Val Loss: 0.043457\n",
      "Epoch 14/50 - Train Loss: 0.029383 - Val Loss: 0.047056\n",
      "Epoch 15/50 - Train Loss: 0.029453 - Val Loss: 0.044039\n",
      "Epoch 16/50 - Train Loss: 0.028968 - Val Loss: 0.045951\n",
      "Epoch 17/50 - Train Loss: 0.031267 - Val Loss: 0.045369\n",
      "Epoch 18/50 - Train Loss: 0.028723 - Val Loss: 0.046430\n",
      "Epoch 19/50 - Train Loss: 0.028628 - Val Loss: 0.044481\n",
      "Epoch 20/50 - Train Loss: 0.028558 - Val Loss: 0.046471\n",
      "Epoch 21/50 - Train Loss: 0.028772 - Val Loss: 0.044219\n",
      "Epoch 22/50 - Train Loss: 0.029558 - Val Loss: 0.045782\n",
      "Epoch 23/50 - Train Loss: 0.030504 - Val Loss: 0.044143\n",
      "Epoch 24/50 - Train Loss: 0.029287 - Val Loss: 0.044304\n",
      "Epoch 25/50 - Train Loss: 0.030333 - Val Loss: 0.046451\n",
      "Epoch 26/50 - Train Loss: 0.028973 - Val Loss: 0.044199\n",
      "Epoch 27/50 - Train Loss: 0.029109 - Val Loss: 0.047657\n",
      "Epoch 28/50 - Train Loss: 0.029521 - Val Loss: 0.044095\n",
      "Epoch 29/50 - Train Loss: 0.032003 - Val Loss: 0.045487\n",
      "Epoch 30/50 - Train Loss: 0.029558 - Val Loss: 0.045340\n",
      "Epoch 31/50 - Train Loss: 0.028697 - Val Loss: 0.044290\n",
      "Epoch 32/50 - Train Loss: 0.030330 - Val Loss: 0.044159\n",
      "Epoch 33/50 - Train Loss: 0.029669 - Val Loss: 0.048829\n",
      "Epoch 34/50 - Train Loss: 0.027295 - Val Loss: 0.045118\n",
      "Epoch 35/50 - Train Loss: 0.033451 - Val Loss: 0.048477\n",
      "Epoch 36/50 - Train Loss: 0.029123 - Val Loss: 0.044475\n",
      "Epoch 37/50 - Train Loss: 0.030794 - Val Loss: 0.045972\n",
      "Epoch 38/50 - Train Loss: 0.029564 - Val Loss: 0.045071\n",
      "Epoch 39/50 - Train Loss: 0.028193 - Val Loss: 0.044133\n",
      "Epoch 40/50 - Train Loss: 0.030608 - Val Loss: 0.044493\n",
      "Epoch 41/50 - Train Loss: 0.027980 - Val Loss: 0.045593\n",
      "Epoch 42/50 - Train Loss: 0.024885 - Val Loss: 0.044297\n",
      "Epoch 43/50 - Train Loss: 0.025553 - Val Loss: 0.045707\n",
      "Epoch 44/50 - Train Loss: 0.028386 - Val Loss: 0.044736\n",
      "Epoch 45/50 - Train Loss: 0.031804 - Val Loss: 0.045104\n",
      "Epoch 46/50 - Train Loss: 0.029463 - Val Loss: 0.044304\n",
      "Epoch 47/50 - Train Loss: 0.028149 - Val Loss: 0.045846\n",
      "Epoch 48/50 - Train Loss: 0.029861 - Val Loss: 0.044206\n",
      "Epoch 49/50 - Train Loss: 0.028191 - Val Loss: 0.044904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:38:26,620] Trial 66 finished with value: 0.043456535786390305 and parameters: {'n_hidden_layers': 1, 'n_hidden_units': 140, 'lr': 0.0013010208973249315, 'weight_decay': 1.6323000672268206e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027808 - Val Loss: 0.044232\n",
      "Epoch 1/50 - Train Loss: 0.069428 - Val Loss: 0.080455\n",
      "Epoch 2/50 - Train Loss: 0.034224 - Val Loss: 0.048360\n",
      "Epoch 3/50 - Train Loss: 0.033057 - Val Loss: 0.052772\n",
      "Epoch 4/50 - Train Loss: 0.030981 - Val Loss: 0.045496\n",
      "Epoch 5/50 - Train Loss: 0.032170 - Val Loss: 0.071484\n",
      "Epoch 6/50 - Train Loss: 0.032408 - Val Loss: 0.045996\n",
      "Epoch 7/50 - Train Loss: 0.030258 - Val Loss: 0.047509\n",
      "Epoch 8/50 - Train Loss: 0.031409 - Val Loss: 0.044381\n",
      "Epoch 9/50 - Train Loss: 0.030066 - Val Loss: 0.053144\n",
      "Epoch 10/50 - Train Loss: 0.032332 - Val Loss: 0.043863\n",
      "Epoch 11/50 - Train Loss: 0.030416 - Val Loss: 0.055140\n",
      "Epoch 12/50 - Train Loss: 0.030171 - Val Loss: 0.047180\n",
      "Epoch 13/50 - Train Loss: 0.029226 - Val Loss: 0.046111\n",
      "Epoch 14/50 - Train Loss: 0.032142 - Val Loss: 0.044907\n",
      "Epoch 15/50 - Train Loss: 0.028161 - Val Loss: 0.043408\n",
      "Epoch 16/50 - Train Loss: 0.028757 - Val Loss: 0.043719\n",
      "Epoch 17/50 - Train Loss: 0.031280 - Val Loss: 0.044381\n",
      "Epoch 18/50 - Train Loss: 0.028212 - Val Loss: 0.044366\n",
      "Epoch 19/50 - Train Loss: 0.027600 - Val Loss: 0.044385\n",
      "Epoch 20/50 - Train Loss: 0.030067 - Val Loss: 0.043837\n",
      "Epoch 21/50 - Train Loss: 0.031421 - Val Loss: 0.052085\n",
      "Epoch 22/50 - Train Loss: 0.032306 - Val Loss: 0.046151\n",
      "Epoch 23/50 - Train Loss: 0.030694 - Val Loss: 0.043891\n",
      "Epoch 24/50 - Train Loss: 0.031092 - Val Loss: 0.044462\n",
      "Epoch 25/50 - Train Loss: 0.030350 - Val Loss: 0.043259\n",
      "Epoch 26/50 - Train Loss: 0.028797 - Val Loss: 0.043989\n",
      "Epoch 27/50 - Train Loss: 0.029396 - Val Loss: 0.043258\n",
      "Epoch 28/50 - Train Loss: 0.028025 - Val Loss: 0.045618\n",
      "Epoch 29/50 - Train Loss: 0.030945 - Val Loss: 0.043362\n",
      "Epoch 30/50 - Train Loss: 0.027634 - Val Loss: 0.054791\n",
      "Epoch 31/50 - Train Loss: 0.032118 - Val Loss: 0.048621\n",
      "Epoch 32/50 - Train Loss: 0.032741 - Val Loss: 0.048552\n",
      "Epoch 33/50 - Train Loss: 0.030834 - Val Loss: 0.043609\n",
      "Epoch 34/50 - Train Loss: 0.029197 - Val Loss: 0.042948\n",
      "Epoch 35/50 - Train Loss: 0.027500 - Val Loss: 0.043216\n",
      "Epoch 36/50 - Train Loss: 0.027711 - Val Loss: 0.045104\n",
      "Epoch 37/50 - Train Loss: 0.030172 - Val Loss: 0.043560\n",
      "Epoch 38/50 - Train Loss: 0.028412 - Val Loss: 0.043138\n",
      "Epoch 39/50 - Train Loss: 0.028264 - Val Loss: 0.049803\n",
      "Epoch 40/50 - Train Loss: 0.029862 - Val Loss: 0.044720\n",
      "Epoch 41/50 - Train Loss: 0.029922 - Val Loss: 0.045893\n",
      "Epoch 42/50 - Train Loss: 0.026655 - Val Loss: 0.047368\n",
      "Epoch 43/50 - Train Loss: 0.029470 - Val Loss: 0.044976\n",
      "Epoch 44/50 - Train Loss: 0.033241 - Val Loss: 0.051904\n",
      "Epoch 45/50 - Train Loss: 0.028708 - Val Loss: 0.045278\n",
      "Epoch 46/50 - Train Loss: 0.030012 - Val Loss: 0.046357\n",
      "Epoch 47/50 - Train Loss: 0.031745 - Val Loss: 0.043973\n",
      "Epoch 48/50 - Train Loss: 0.028312 - Val Loss: 0.055737\n",
      "Epoch 49/50 - Train Loss: 0.027511 - Val Loss: 0.044865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:39:02,121] Trial 67 finished with value: 0.04294796163837115 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 180, 'lr': 0.002927946062339878, 'weight_decay': 5.121007529044049e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028908 - Val Loss: 0.045670\n",
      "Epoch 1/50 - Train Loss: 0.221598 - Val Loss: 0.288729\n",
      "Epoch 2/50 - Train Loss: 0.087641 - Val Loss: 0.104077\n",
      "Epoch 3/50 - Train Loss: 0.049552 - Val Loss: 0.069383\n",
      "Epoch 4/50 - Train Loss: 0.044970 - Val Loss: 0.085097\n",
      "Epoch 5/50 - Train Loss: 0.038845 - Val Loss: 0.071554\n",
      "Epoch 6/50 - Train Loss: 0.039029 - Val Loss: 0.068336\n",
      "Epoch 7/50 - Train Loss: 0.037844 - Val Loss: 0.064082\n",
      "Epoch 8/50 - Train Loss: 0.034885 - Val Loss: 0.059485\n",
      "Epoch 9/50 - Train Loss: 0.034600 - Val Loss: 0.055865\n",
      "Epoch 10/50 - Train Loss: 0.031024 - Val Loss: 0.053835\n",
      "Epoch 11/50 - Train Loss: 0.032204 - Val Loss: 0.050765\n",
      "Epoch 12/50 - Train Loss: 0.034228 - Val Loss: 0.049076\n",
      "Epoch 13/50 - Train Loss: 0.030990 - Val Loss: 0.054103\n",
      "Epoch 14/50 - Train Loss: 0.032768 - Val Loss: 0.048082\n",
      "Epoch 15/50 - Train Loss: 0.030525 - Val Loss: 0.047664\n",
      "Epoch 16/50 - Train Loss: 0.030932 - Val Loss: 0.050789\n",
      "Epoch 17/50 - Train Loss: 0.030170 - Val Loss: 0.047260\n",
      "Epoch 18/50 - Train Loss: 0.031583 - Val Loss: 0.046114\n",
      "Epoch 19/50 - Train Loss: 0.031498 - Val Loss: 0.048758\n",
      "Epoch 20/50 - Train Loss: 0.031898 - Val Loss: 0.047192\n",
      "Epoch 21/50 - Train Loss: 0.031675 - Val Loss: 0.046427\n",
      "Epoch 22/50 - Train Loss: 0.031750 - Val Loss: 0.055371\n",
      "Epoch 23/50 - Train Loss: 0.029578 - Val Loss: 0.045524\n",
      "Epoch 24/50 - Train Loss: 0.030685 - Val Loss: 0.045887\n",
      "Epoch 25/50 - Train Loss: 0.029827 - Val Loss: 0.048317\n",
      "Epoch 26/50 - Train Loss: 0.030364 - Val Loss: 0.046968\n",
      "Epoch 27/50 - Train Loss: 0.028917 - Val Loss: 0.046102\n",
      "Epoch 28/50 - Train Loss: 0.028690 - Val Loss: 0.049509\n",
      "Epoch 29/50 - Train Loss: 0.031095 - Val Loss: 0.046404\n",
      "Epoch 30/50 - Train Loss: 0.029033 - Val Loss: 0.045463\n",
      "Epoch 31/50 - Train Loss: 0.029312 - Val Loss: 0.046180\n",
      "Epoch 32/50 - Train Loss: 0.030256 - Val Loss: 0.046168\n",
      "Epoch 33/50 - Train Loss: 0.031698 - Val Loss: 0.047858\n",
      "Epoch 34/50 - Train Loss: 0.029080 - Val Loss: 0.045607\n",
      "Epoch 35/50 - Train Loss: 0.029353 - Val Loss: 0.045194\n",
      "Epoch 36/50 - Train Loss: 0.030305 - Val Loss: 0.045817\n",
      "Epoch 37/50 - Train Loss: 0.028582 - Val Loss: 0.048395\n",
      "Epoch 38/50 - Train Loss: 0.030066 - Val Loss: 0.045012\n",
      "Epoch 39/50 - Train Loss: 0.030308 - Val Loss: 0.047279\n",
      "Epoch 40/50 - Train Loss: 0.029471 - Val Loss: 0.045469\n",
      "Epoch 41/50 - Train Loss: 0.030953 - Val Loss: 0.046474\n",
      "Epoch 42/50 - Train Loss: 0.027299 - Val Loss: 0.044962\n",
      "Epoch 43/50 - Train Loss: 0.030142 - Val Loss: 0.045770\n",
      "Epoch 44/50 - Train Loss: 0.028851 - Val Loss: 0.046038\n",
      "Epoch 45/50 - Train Loss: 0.028538 - Val Loss: 0.045261\n",
      "Epoch 46/50 - Train Loss: 0.028643 - Val Loss: 0.048386\n",
      "Epoch 47/50 - Train Loss: 0.028676 - Val Loss: 0.044967\n",
      "Epoch 48/50 - Train Loss: 0.031396 - Val Loss: 0.046278\n",
      "Epoch 49/50 - Train Loss: 0.030504 - Val Loss: 0.047560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:39:36,450] Trial 68 finished with value: 0.044962347795565925 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 160, 'lr': 0.0002598425480501358, 'weight_decay': 1.607621382606942e-07, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030470 - Val Loss: 0.046567\n",
      "Epoch 1/50 - Train Loss: 0.094313 - Val Loss: 0.107037\n",
      "Epoch 2/50 - Train Loss: 0.040655 - Val Loss: 0.069516\n",
      "Epoch 3/50 - Train Loss: 0.034738 - Val Loss: 0.052560\n",
      "Epoch 4/50 - Train Loss: 0.030520 - Val Loss: 0.048542\n",
      "Epoch 5/50 - Train Loss: 0.032657 - Val Loss: 0.050407\n",
      "Epoch 6/50 - Train Loss: 0.031337 - Val Loss: 0.050821\n",
      "Epoch 7/50 - Train Loss: 0.031303 - Val Loss: 0.045828\n",
      "Epoch 8/50 - Train Loss: 0.030607 - Val Loss: 0.048965\n",
      "Epoch 9/50 - Train Loss: 0.029966 - Val Loss: 0.048755\n",
      "Epoch 10/50 - Train Loss: 0.031242 - Val Loss: 0.045474\n",
      "Epoch 11/50 - Train Loss: 0.032209 - Val Loss: 0.050399\n",
      "Epoch 12/50 - Train Loss: 0.033667 - Val Loss: 0.065188\n",
      "Epoch 13/50 - Train Loss: 0.031114 - Val Loss: 0.046560\n",
      "Epoch 14/50 - Train Loss: 0.028014 - Val Loss: 0.048112\n",
      "Epoch 15/50 - Train Loss: 0.027744 - Val Loss: 0.045287\n",
      "Epoch 16/50 - Train Loss: 0.029129 - Val Loss: 0.048515\n",
      "Epoch 17/50 - Train Loss: 0.026905 - Val Loss: 0.045110\n",
      "Epoch 18/50 - Train Loss: 0.028795 - Val Loss: 0.044961\n",
      "Epoch 19/50 - Train Loss: 0.033151 - Val Loss: 0.060942\n",
      "Epoch 20/50 - Train Loss: 0.032898 - Val Loss: 0.052715\n",
      "Epoch 21/50 - Train Loss: 0.031682 - Val Loss: 0.044313\n",
      "Epoch 22/50 - Train Loss: 0.029517 - Val Loss: 0.044314\n",
      "Epoch 23/50 - Train Loss: 0.027443 - Val Loss: 0.044891\n",
      "Epoch 24/50 - Train Loss: 0.029618 - Val Loss: 0.046951\n",
      "Epoch 25/50 - Train Loss: 0.028729 - Val Loss: 0.045497\n",
      "Epoch 26/50 - Train Loss: 0.029671 - Val Loss: 0.047560\n",
      "Epoch 27/50 - Train Loss: 0.031215 - Val Loss: 0.046101\n",
      "Epoch 28/50 - Train Loss: 0.029498 - Val Loss: 0.044223\n",
      "Epoch 29/50 - Train Loss: 0.028361 - Val Loss: 0.044461\n",
      "Epoch 30/50 - Train Loss: 0.030057 - Val Loss: 0.046508\n",
      "Epoch 31/50 - Train Loss: 0.031524 - Val Loss: 0.050846\n",
      "Epoch 32/50 - Train Loss: 0.031681 - Val Loss: 0.045213\n",
      "Epoch 33/50 - Train Loss: 0.027785 - Val Loss: 0.045335\n",
      "Epoch 34/50 - Train Loss: 0.029212 - Val Loss: 0.048121\n",
      "Epoch 35/50 - Train Loss: 0.029262 - Val Loss: 0.044883\n",
      "Epoch 36/50 - Train Loss: 0.031485 - Val Loss: 0.046556\n",
      "Epoch 37/50 - Train Loss: 0.034426 - Val Loss: 0.058806\n",
      "Epoch 38/50 - Train Loss: 0.030355 - Val Loss: 0.054384\n",
      "Epoch 39/50 - Train Loss: 0.030454 - Val Loss: 0.045362\n",
      "Epoch 40/50 - Train Loss: 0.029929 - Val Loss: 0.044902\n",
      "Epoch 41/50 - Train Loss: 0.029682 - Val Loss: 0.046966\n",
      "Epoch 42/50 - Train Loss: 0.030375 - Val Loss: 0.045936\n",
      "Epoch 43/50 - Train Loss: 0.029309 - Val Loss: 0.044990\n",
      "Epoch 44/50 - Train Loss: 0.027467 - Val Loss: 0.045817\n",
      "Epoch 45/50 - Train Loss: 0.029955 - Val Loss: 0.044864\n",
      "Epoch 46/50 - Train Loss: 0.026365 - Val Loss: 0.046142\n",
      "Epoch 47/50 - Train Loss: 0.028046 - Val Loss: 0.044686\n",
      "Epoch 48/50 - Train Loss: 0.026835 - Val Loss: 0.045833\n",
      "Epoch 49/50 - Train Loss: 0.028307 - Val Loss: 0.045398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:40:03,150] Trial 69 finished with value: 0.044222764670848846 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 124, 'lr': 0.0028964661704781796, 'weight_decay': 4.8072980992921915e-08, 'batch_size': 8}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030839 - Val Loss: 0.047214\n",
      "Epoch 1/50 - Train Loss: 0.075274 - Val Loss: 0.057161\n",
      "Epoch 2/50 - Train Loss: 0.037366 - Val Loss: 0.064739\n",
      "Epoch 3/50 - Train Loss: 0.039027 - Val Loss: 0.082706\n",
      "Epoch 4/50 - Train Loss: 0.039525 - Val Loss: 0.047057\n",
      "Epoch 5/50 - Train Loss: 0.031696 - Val Loss: 0.045040\n",
      "Epoch 6/50 - Train Loss: 0.031501 - Val Loss: 0.044747\n",
      "Epoch 7/50 - Train Loss: 0.031531 - Val Loss: 0.046266\n",
      "Epoch 8/50 - Train Loss: 0.031327 - Val Loss: 0.043131\n",
      "Epoch 9/50 - Train Loss: 0.030800 - Val Loss: 0.043710\n",
      "Epoch 10/50 - Train Loss: 0.029056 - Val Loss: 0.044032\n",
      "Epoch 11/50 - Train Loss: 0.029403 - Val Loss: 0.045918\n",
      "Epoch 12/50 - Train Loss: 0.029204 - Val Loss: 0.045720\n",
      "Epoch 13/50 - Train Loss: 0.031852 - Val Loss: 0.042896\n",
      "Epoch 14/50 - Train Loss: 0.030205 - Val Loss: 0.047946\n",
      "Epoch 15/50 - Train Loss: 0.030524 - Val Loss: 0.043439\n",
      "Epoch 16/50 - Train Loss: 0.028544 - Val Loss: 0.049855\n",
      "Epoch 17/50 - Train Loss: 0.030423 - Val Loss: 0.043297\n",
      "Epoch 18/50 - Train Loss: 0.028516 - Val Loss: 0.046727\n",
      "Epoch 19/50 - Train Loss: 0.028446 - Val Loss: 0.043603\n",
      "Epoch 20/50 - Train Loss: 0.030155 - Val Loss: 0.045229\n",
      "Epoch 21/50 - Train Loss: 0.028758 - Val Loss: 0.044652\n",
      "Epoch 22/50 - Train Loss: 0.032543 - Val Loss: 0.047135\n",
      "Epoch 23/50 - Train Loss: 0.029612 - Val Loss: 0.043870\n",
      "Epoch 24/50 - Train Loss: 0.030466 - Val Loss: 0.043890\n",
      "Epoch 25/50 - Train Loss: 0.026764 - Val Loss: 0.049391\n",
      "Epoch 26/50 - Train Loss: 0.029523 - Val Loss: 0.045060\n",
      "Epoch 27/50 - Train Loss: 0.027508 - Val Loss: 0.045339\n",
      "Epoch 28/50 - Train Loss: 0.027136 - Val Loss: 0.043752\n",
      "Epoch 29/50 - Train Loss: 0.027662 - Val Loss: 0.044283\n",
      "Epoch 30/50 - Train Loss: 0.029767 - Val Loss: 0.048038\n",
      "Epoch 31/50 - Train Loss: 0.027700 - Val Loss: 0.043677\n",
      "Epoch 32/50 - Train Loss: 0.033016 - Val Loss: 0.051253\n",
      "Epoch 33/50 - Train Loss: 0.030839 - Val Loss: 0.044522\n",
      "Epoch 34/50 - Train Loss: 0.030114 - Val Loss: 0.050218\n",
      "Epoch 35/50 - Train Loss: 0.030875 - Val Loss: 0.044870\n",
      "Epoch 36/50 - Train Loss: 0.027648 - Val Loss: 0.044156\n",
      "Epoch 37/50 - Train Loss: 0.030139 - Val Loss: 0.045938\n",
      "Epoch 38/50 - Train Loss: 0.029694 - Val Loss: 0.044663\n",
      "Epoch 39/50 - Train Loss: 0.029639 - Val Loss: 0.044878\n",
      "Epoch 40/50 - Train Loss: 0.031934 - Val Loss: 0.044636\n",
      "Epoch 41/50 - Train Loss: 0.028258 - Val Loss: 0.047766\n",
      "Epoch 42/50 - Train Loss: 0.030963 - Val Loss: 0.045652\n",
      "Epoch 43/50 - Train Loss: 0.029994 - Val Loss: 0.049105\n",
      "Epoch 44/50 - Train Loss: 0.032103 - Val Loss: 0.050546\n",
      "Epoch 45/50 - Train Loss: 0.034129 - Val Loss: 0.056471\n",
      "Epoch 46/50 - Train Loss: 0.031799 - Val Loss: 0.045145\n",
      "Epoch 47/50 - Train Loss: 0.036068 - Val Loss: 0.044997\n",
      "Epoch 48/50 - Train Loss: 0.032829 - Val Loss: 0.045766\n",
      "Epoch 49/50 - Train Loss: 0.029697 - Val Loss: 0.047513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:40:24,695] Trial 70 finished with value: 0.0428963378071785 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 180, 'lr': 0.007384661103641369, 'weight_decay': 8.190973959313485e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029355 - Val Loss: 0.050057\n",
      "Epoch 1/50 - Train Loss: 0.124808 - Val Loss: 0.103448\n",
      "Epoch 2/50 - Train Loss: 0.055156 - Val Loss: 0.125037\n",
      "Epoch 3/50 - Train Loss: 0.045681 - Val Loss: 0.057810\n",
      "Epoch 4/50 - Train Loss: 0.038875 - Val Loss: 0.076853\n",
      "Epoch 5/50 - Train Loss: 0.037949 - Val Loss: 0.047721\n",
      "Epoch 6/50 - Train Loss: 0.034540 - Val Loss: 0.054367\n",
      "Epoch 7/50 - Train Loss: 0.034115 - Val Loss: 0.046947\n",
      "Epoch 8/50 - Train Loss: 0.034557 - Val Loss: 0.047263\n",
      "Epoch 9/50 - Train Loss: 0.030842 - Val Loss: 0.051777\n",
      "Epoch 10/50 - Train Loss: 0.031919 - Val Loss: 0.044426\n",
      "Epoch 11/50 - Train Loss: 0.031536 - Val Loss: 0.043528\n",
      "Epoch 12/50 - Train Loss: 0.030046 - Val Loss: 0.051686\n",
      "Epoch 13/50 - Train Loss: 0.029791 - Val Loss: 0.050020\n",
      "Epoch 14/50 - Train Loss: 0.034305 - Val Loss: 0.044729\n",
      "Epoch 15/50 - Train Loss: 0.031114 - Val Loss: 0.051302\n",
      "Epoch 16/50 - Train Loss: 0.030263 - Val Loss: 0.046988\n",
      "Epoch 17/50 - Train Loss: 0.027944 - Val Loss: 0.042962\n",
      "Epoch 18/50 - Train Loss: 0.029327 - Val Loss: 0.046337\n",
      "Epoch 19/50 - Train Loss: 0.027505 - Val Loss: 0.042835\n",
      "Epoch 20/50 - Train Loss: 0.028924 - Val Loss: 0.043693\n",
      "Epoch 21/50 - Train Loss: 0.030756 - Val Loss: 0.047947\n",
      "Epoch 22/50 - Train Loss: 0.028988 - Val Loss: 0.043686\n",
      "Epoch 23/50 - Train Loss: 0.032408 - Val Loss: 0.043664\n",
      "Epoch 24/50 - Train Loss: 0.026801 - Val Loss: 0.045905\n",
      "Epoch 25/50 - Train Loss: 0.026818 - Val Loss: 0.050487\n",
      "Epoch 26/50 - Train Loss: 0.030680 - Val Loss: 0.043380\n",
      "Epoch 27/50 - Train Loss: 0.027088 - Val Loss: 0.043080\n",
      "Epoch 28/50 - Train Loss: 0.029322 - Val Loss: 0.043280\n",
      "Epoch 29/50 - Train Loss: 0.029128 - Val Loss: 0.050397\n",
      "Epoch 30/50 - Train Loss: 0.030844 - Val Loss: 0.047001\n",
      "Epoch 31/50 - Train Loss: 0.028631 - Val Loss: 0.042782\n",
      "Epoch 32/50 - Train Loss: 0.025267 - Val Loss: 0.043161\n",
      "Epoch 33/50 - Train Loss: 0.031195 - Val Loss: 0.042841\n",
      "Epoch 34/50 - Train Loss: 0.029492 - Val Loss: 0.043287\n",
      "Epoch 35/50 - Train Loss: 0.030270 - Val Loss: 0.043456\n",
      "Epoch 36/50 - Train Loss: 0.028505 - Val Loss: 0.042900\n",
      "Epoch 37/50 - Train Loss: 0.027607 - Val Loss: 0.043113\n",
      "Epoch 38/50 - Train Loss: 0.029169 - Val Loss: 0.044957\n",
      "Epoch 39/50 - Train Loss: 0.028045 - Val Loss: 0.042646\n",
      "Epoch 40/50 - Train Loss: 0.025696 - Val Loss: 0.042862\n",
      "Epoch 41/50 - Train Loss: 0.030411 - Val Loss: 0.042770\n",
      "Epoch 42/50 - Train Loss: 0.029653 - Val Loss: 0.042857\n",
      "Epoch 43/50 - Train Loss: 0.032246 - Val Loss: 0.047595\n",
      "Epoch 44/50 - Train Loss: 0.031609 - Val Loss: 0.047154\n",
      "Epoch 45/50 - Train Loss: 0.029063 - Val Loss: 0.043916\n",
      "Epoch 46/50 - Train Loss: 0.027182 - Val Loss: 0.049034\n",
      "Epoch 47/50 - Train Loss: 0.030088 - Val Loss: 0.043075\n",
      "Epoch 48/50 - Train Loss: 0.029335 - Val Loss: 0.047888\n",
      "Epoch 49/50 - Train Loss: 0.031290 - Val Loss: 0.044825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:40:46,772] Trial 71 finished with value: 0.04264550842344761 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 180, 'lr': 0.00755902291549825, 'weight_decay': 8.138492526622088e-08, 'batch_size': 16}. Best is trial 48 with value: 0.042643917724490166.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026232 - Val Loss: 0.044434\n",
      "Epoch 1/50 - Train Loss: 0.084181 - Val Loss: 0.072553\n",
      "Epoch 2/50 - Train Loss: 0.037339 - Val Loss: 0.053532\n",
      "Epoch 3/50 - Train Loss: 0.033127 - Val Loss: 0.046341\n",
      "Epoch 4/50 - Train Loss: 0.031644 - Val Loss: 0.044495\n",
      "Epoch 5/50 - Train Loss: 0.031960 - Val Loss: 0.047549\n",
      "Epoch 6/50 - Train Loss: 0.031371 - Val Loss: 0.045353\n",
      "Epoch 7/50 - Train Loss: 0.031432 - Val Loss: 0.043792\n",
      "Epoch 8/50 - Train Loss: 0.031436 - Val Loss: 0.046634\n",
      "Epoch 9/50 - Train Loss: 0.032918 - Val Loss: 0.043436\n",
      "Epoch 10/50 - Train Loss: 0.039609 - Val Loss: 0.043730\n",
      "Epoch 11/50 - Train Loss: 0.034846 - Val Loss: 0.056091\n",
      "Epoch 12/50 - Train Loss: 0.033862 - Val Loss: 0.063375\n",
      "Epoch 13/50 - Train Loss: 0.035315 - Val Loss: 0.044101\n",
      "Epoch 14/50 - Train Loss: 0.030751 - Val Loss: 0.043087\n",
      "Epoch 15/50 - Train Loss: 0.029122 - Val Loss: 0.043451\n",
      "Epoch 16/50 - Train Loss: 0.031743 - Val Loss: 0.045422\n",
      "Epoch 17/50 - Train Loss: 0.030809 - Val Loss: 0.045974\n",
      "Epoch 18/50 - Train Loss: 0.030653 - Val Loss: 0.047787\n",
      "Epoch 19/50 - Train Loss: 0.031243 - Val Loss: 0.059243\n",
      "Epoch 20/50 - Train Loss: 0.032688 - Val Loss: 0.058616\n",
      "Epoch 21/50 - Train Loss: 0.032448 - Val Loss: 0.045682\n",
      "Epoch 22/50 - Train Loss: 0.031225 - Val Loss: 0.043331\n",
      "Epoch 23/50 - Train Loss: 0.031142 - Val Loss: 0.043293\n",
      "Epoch 24/50 - Train Loss: 0.031553 - Val Loss: 0.046722\n",
      "Epoch 25/50 - Train Loss: 0.031664 - Val Loss: 0.060049\n",
      "Epoch 26/50 - Train Loss: 0.030992 - Val Loss: 0.045176\n",
      "Epoch 27/50 - Train Loss: 0.028286 - Val Loss: 0.043371\n",
      "Epoch 28/50 - Train Loss: 0.027985 - Val Loss: 0.042988\n",
      "Epoch 29/50 - Train Loss: 0.029750 - Val Loss: 0.043482\n",
      "Epoch 30/50 - Train Loss: 0.030254 - Val Loss: 0.051074\n",
      "Epoch 31/50 - Train Loss: 0.027550 - Val Loss: 0.049929\n",
      "Epoch 32/50 - Train Loss: 0.028297 - Val Loss: 0.046376\n",
      "Epoch 33/50 - Train Loss: 0.030206 - Val Loss: 0.048469\n",
      "Epoch 34/50 - Train Loss: 0.029352 - Val Loss: 0.046496\n",
      "Epoch 35/50 - Train Loss: 0.031444 - Val Loss: 0.042500\n",
      "Epoch 36/50 - Train Loss: 0.030481 - Val Loss: 0.043601\n",
      "Epoch 37/50 - Train Loss: 0.028050 - Val Loss: 0.046315\n",
      "Epoch 38/50 - Train Loss: 0.031138 - Val Loss: 0.043825\n",
      "Epoch 39/50 - Train Loss: 0.029086 - Val Loss: 0.044271\n",
      "Epoch 40/50 - Train Loss: 0.029653 - Val Loss: 0.044425\n",
      "Epoch 41/50 - Train Loss: 0.029880 - Val Loss: 0.044194\n",
      "Epoch 42/50 - Train Loss: 0.031259 - Val Loss: 0.059454\n",
      "Epoch 43/50 - Train Loss: 0.027756 - Val Loss: 0.047913\n",
      "Epoch 44/50 - Train Loss: 0.031515 - Val Loss: 0.043002\n",
      "Epoch 45/50 - Train Loss: 0.026369 - Val Loss: 0.043239\n",
      "Epoch 46/50 - Train Loss: 0.031254 - Val Loss: 0.042991\n",
      "Epoch 47/50 - Train Loss: 0.030514 - Val Loss: 0.042946\n",
      "Epoch 48/50 - Train Loss: 0.026188 - Val Loss: 0.043491\n",
      "Epoch 49/50 - Train Loss: 0.028225 - Val Loss: 0.042911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:41:08,016] Trial 72 finished with value: 0.042499855160713196 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 152, 'lr': 0.007435155528189941, 'weight_decay': 2.4037008842966983e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028669 - Val Loss: 0.046821\n",
      "Epoch 1/50 - Train Loss: 0.061964 - Val Loss: 0.062782\n",
      "Epoch 2/50 - Train Loss: 0.040176 - Val Loss: 0.053342\n",
      "Epoch 3/50 - Train Loss: 0.038346 - Val Loss: 0.047916\n",
      "Epoch 4/50 - Train Loss: 0.031859 - Val Loss: 0.045793\n",
      "Epoch 5/50 - Train Loss: 0.032401 - Val Loss: 0.046121\n",
      "Epoch 6/50 - Train Loss: 0.029918 - Val Loss: 0.044967\n",
      "Epoch 7/50 - Train Loss: 0.031828 - Val Loss: 0.053790\n",
      "Epoch 8/50 - Train Loss: 0.033609 - Val Loss: 0.057019\n",
      "Epoch 9/50 - Train Loss: 0.032205 - Val Loss: 0.047926\n",
      "Epoch 10/50 - Train Loss: 0.032264 - Val Loss: 0.050009\n",
      "Epoch 11/50 - Train Loss: 0.032516 - Val Loss: 0.043359\n",
      "Epoch 12/50 - Train Loss: 0.029315 - Val Loss: 0.050177\n",
      "Epoch 13/50 - Train Loss: 0.026403 - Val Loss: 0.054049\n",
      "Epoch 14/50 - Train Loss: 0.031711 - Val Loss: 0.054093\n",
      "Epoch 15/50 - Train Loss: 0.029490 - Val Loss: 0.043712\n",
      "Epoch 16/50 - Train Loss: 0.032903 - Val Loss: 0.045162\n",
      "Epoch 17/50 - Train Loss: 0.032518 - Val Loss: 0.045698\n",
      "Epoch 18/50 - Train Loss: 0.031742 - Val Loss: 0.046405\n",
      "Epoch 19/50 - Train Loss: 0.027738 - Val Loss: 0.043271\n",
      "Epoch 20/50 - Train Loss: 0.030854 - Val Loss: 0.053847\n",
      "Epoch 21/50 - Train Loss: 0.030042 - Val Loss: 0.044460\n",
      "Epoch 22/50 - Train Loss: 0.028551 - Val Loss: 0.049067\n",
      "Epoch 23/50 - Train Loss: 0.031145 - Val Loss: 0.043598\n",
      "Epoch 24/50 - Train Loss: 0.031584 - Val Loss: 0.045189\n",
      "Epoch 25/50 - Train Loss: 0.028326 - Val Loss: 0.043584\n",
      "Epoch 26/50 - Train Loss: 0.027342 - Val Loss: 0.043119\n",
      "Epoch 27/50 - Train Loss: 0.024921 - Val Loss: 0.045288\n",
      "Epoch 28/50 - Train Loss: 0.029633 - Val Loss: 0.044380\n",
      "Epoch 29/50 - Train Loss: 0.034733 - Val Loss: 0.044197\n",
      "Epoch 30/50 - Train Loss: 0.030231 - Val Loss: 0.043728\n",
      "Epoch 31/50 - Train Loss: 0.029930 - Val Loss: 0.043795\n",
      "Epoch 32/50 - Train Loss: 0.028291 - Val Loss: 0.045695\n",
      "Epoch 33/50 - Train Loss: 0.028031 - Val Loss: 0.044166\n",
      "Epoch 34/50 - Train Loss: 0.026130 - Val Loss: 0.044922\n",
      "Epoch 35/50 - Train Loss: 0.027624 - Val Loss: 0.044252\n",
      "Epoch 36/50 - Train Loss: 0.027947 - Val Loss: 0.051797\n",
      "Epoch 37/50 - Train Loss: 0.026569 - Val Loss: 0.044084\n",
      "Epoch 38/50 - Train Loss: 0.027466 - Val Loss: 0.043862\n",
      "Epoch 39/50 - Train Loss: 0.029971 - Val Loss: 0.050594\n",
      "Epoch 40/50 - Train Loss: 0.029077 - Val Loss: 0.044276\n",
      "Epoch 41/50 - Train Loss: 0.027335 - Val Loss: 0.045062\n",
      "Epoch 42/50 - Train Loss: 0.025955 - Val Loss: 0.044500\n",
      "Epoch 43/50 - Train Loss: 0.029724 - Val Loss: 0.044679\n",
      "Epoch 44/50 - Train Loss: 0.026330 - Val Loss: 0.045377\n",
      "Epoch 45/50 - Train Loss: 0.027932 - Val Loss: 0.044830\n",
      "Epoch 46/50 - Train Loss: 0.030004 - Val Loss: 0.053151\n",
      "Epoch 47/50 - Train Loss: 0.031421 - Val Loss: 0.045062\n",
      "Epoch 48/50 - Train Loss: 0.028607 - Val Loss: 0.044568\n",
      "Epoch 49/50 - Train Loss: 0.027888 - Val Loss: 0.046504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:41:28,795] Trial 73 finished with value: 0.04311863332986832 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 135, 'lr': 0.007476328324600326, 'weight_decay': 2.2972363436887366e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027865 - Val Loss: 0.047892\n",
      "Epoch 1/50 - Train Loss: 0.100999 - Val Loss: 0.191129\n",
      "Epoch 2/50 - Train Loss: 0.070991 - Val Loss: 0.067368\n",
      "Epoch 3/50 - Train Loss: 0.043418 - Val Loss: 0.101105\n",
      "Epoch 4/50 - Train Loss: 0.041454 - Val Loss: 0.048790\n",
      "Epoch 5/50 - Train Loss: 0.030176 - Val Loss: 0.069131\n",
      "Epoch 6/50 - Train Loss: 0.032018 - Val Loss: 0.046998\n",
      "Epoch 7/50 - Train Loss: 0.034253 - Val Loss: 0.046034\n",
      "Epoch 8/50 - Train Loss: 0.032171 - Val Loss: 0.046481\n",
      "Epoch 9/50 - Train Loss: 0.035903 - Val Loss: 0.051745\n",
      "Epoch 10/50 - Train Loss: 0.030098 - Val Loss: 0.051148\n",
      "Epoch 11/50 - Train Loss: 0.032585 - Val Loss: 0.044741\n",
      "Epoch 12/50 - Train Loss: 0.031802 - Val Loss: 0.044342\n",
      "Epoch 13/50 - Train Loss: 0.033085 - Val Loss: 0.046293\n",
      "Epoch 14/50 - Train Loss: 0.032755 - Val Loss: 0.050052\n",
      "Epoch 15/50 - Train Loss: 0.028732 - Val Loss: 0.046401\n",
      "Epoch 16/50 - Train Loss: 0.029955 - Val Loss: 0.044274\n",
      "Epoch 17/50 - Train Loss: 0.030022 - Val Loss: 0.043673\n",
      "Epoch 18/50 - Train Loss: 0.033499 - Val Loss: 0.044254\n",
      "Epoch 19/50 - Train Loss: 0.035577 - Val Loss: 0.058494\n",
      "Epoch 20/50 - Train Loss: 0.027375 - Val Loss: 0.043733\n",
      "Epoch 21/50 - Train Loss: 0.031251 - Val Loss: 0.043683\n",
      "Epoch 22/50 - Train Loss: 0.036396 - Val Loss: 0.053146\n",
      "Epoch 23/50 - Train Loss: 0.032258 - Val Loss: 0.053907\n",
      "Epoch 24/50 - Train Loss: 0.032976 - Val Loss: 0.043303\n",
      "Epoch 25/50 - Train Loss: 0.028842 - Val Loss: 0.045531\n",
      "Epoch 26/50 - Train Loss: 0.030719 - Val Loss: 0.043463\n",
      "Epoch 27/50 - Train Loss: 0.031062 - Val Loss: 0.044230\n",
      "Epoch 28/50 - Train Loss: 0.030135 - Val Loss: 0.045896\n",
      "Epoch 29/50 - Train Loss: 0.031447 - Val Loss: 0.044056\n",
      "Epoch 30/50 - Train Loss: 0.028969 - Val Loss: 0.043871\n",
      "Epoch 31/50 - Train Loss: 0.025465 - Val Loss: 0.043434\n",
      "Epoch 32/50 - Train Loss: 0.030822 - Val Loss: 0.044830\n",
      "Epoch 33/50 - Train Loss: 0.030000 - Val Loss: 0.043660\n",
      "Epoch 34/50 - Train Loss: 0.028725 - Val Loss: 0.043300\n",
      "Epoch 35/50 - Train Loss: 0.029071 - Val Loss: 0.043372\n",
      "Epoch 36/50 - Train Loss: 0.029889 - Val Loss: 0.044900\n",
      "Epoch 37/50 - Train Loss: 0.028998 - Val Loss: 0.043210\n",
      "Epoch 38/50 - Train Loss: 0.028620 - Val Loss: 0.043456\n",
      "Epoch 39/50 - Train Loss: 0.029703 - Val Loss: 0.057036\n",
      "Epoch 40/50 - Train Loss: 0.032556 - Val Loss: 0.047759\n",
      "Epoch 41/50 - Train Loss: 0.028885 - Val Loss: 0.043728\n",
      "Epoch 42/50 - Train Loss: 0.031142 - Val Loss: 0.044031\n",
      "Epoch 43/50 - Train Loss: 0.031778 - Val Loss: 0.046445\n",
      "Epoch 44/50 - Train Loss: 0.026724 - Val Loss: 0.043715\n",
      "Epoch 45/50 - Train Loss: 0.030649 - Val Loss: 0.043461\n",
      "Epoch 46/50 - Train Loss: 0.030708 - Val Loss: 0.043885\n",
      "Epoch 47/50 - Train Loss: 0.030730 - Val Loss: 0.043554\n",
      "Epoch 48/50 - Train Loss: 0.028657 - Val Loss: 0.046051\n",
      "Epoch 49/50 - Train Loss: 0.029024 - Val Loss: 0.044369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:41:49,780] Trial 74 finished with value: 0.043209709227085114 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 146, 'lr': 0.007004028222428264, 'weight_decay': 9.509264331769837e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028209 - Val Loss: 0.043766\n",
      "Epoch 1/50 - Train Loss: 0.096875 - Val Loss: 0.076890\n",
      "Epoch 2/50 - Train Loss: 0.049993 - Val Loss: 0.093490\n",
      "Epoch 3/50 - Train Loss: 0.039430 - Val Loss: 0.053989\n",
      "Epoch 4/50 - Train Loss: 0.036320 - Val Loss: 0.052901\n",
      "Epoch 5/50 - Train Loss: 0.034661 - Val Loss: 0.046157\n",
      "Epoch 6/50 - Train Loss: 0.030398 - Val Loss: 0.048796\n",
      "Epoch 7/50 - Train Loss: 0.030865 - Val Loss: 0.058089\n",
      "Epoch 8/50 - Train Loss: 0.030652 - Val Loss: 0.047748\n",
      "Epoch 9/50 - Train Loss: 0.034370 - Val Loss: 0.047602\n",
      "Epoch 10/50 - Train Loss: 0.032303 - Val Loss: 0.044202\n",
      "Epoch 11/50 - Train Loss: 0.033898 - Val Loss: 0.045818\n",
      "Epoch 12/50 - Train Loss: 0.030159 - Val Loss: 0.045326\n",
      "Epoch 13/50 - Train Loss: 0.028388 - Val Loss: 0.046539\n",
      "Epoch 14/50 - Train Loss: 0.028355 - Val Loss: 0.045752\n",
      "Epoch 15/50 - Train Loss: 0.028759 - Val Loss: 0.047889\n",
      "Epoch 16/50 - Train Loss: 0.029720 - Val Loss: 0.044594\n",
      "Epoch 17/50 - Train Loss: 0.027042 - Val Loss: 0.046549\n",
      "Epoch 18/50 - Train Loss: 0.029256 - Val Loss: 0.045776\n",
      "Epoch 19/50 - Train Loss: 0.026543 - Val Loss: 0.043349\n",
      "Epoch 20/50 - Train Loss: 0.029121 - Val Loss: 0.046029\n",
      "Epoch 21/50 - Train Loss: 0.028089 - Val Loss: 0.043458\n",
      "Epoch 22/50 - Train Loss: 0.028173 - Val Loss: 0.043407\n",
      "Epoch 23/50 - Train Loss: 0.032170 - Val Loss: 0.045397\n",
      "Epoch 24/50 - Train Loss: 0.030513 - Val Loss: 0.045773\n",
      "Epoch 25/50 - Train Loss: 0.031529 - Val Loss: 0.045560\n",
      "Epoch 26/50 - Train Loss: 0.028112 - Val Loss: 0.048272\n",
      "Epoch 27/50 - Train Loss: 0.029911 - Val Loss: 0.046377\n",
      "Epoch 28/50 - Train Loss: 0.026648 - Val Loss: 0.045272\n",
      "Epoch 29/50 - Train Loss: 0.030348 - Val Loss: 0.045121\n",
      "Epoch 30/50 - Train Loss: 0.030327 - Val Loss: 0.043505\n",
      "Epoch 31/50 - Train Loss: 0.032826 - Val Loss: 0.043497\n",
      "Epoch 32/50 - Train Loss: 0.033187 - Val Loss: 0.046656\n",
      "Epoch 33/50 - Train Loss: 0.030429 - Val Loss: 0.043387\n",
      "Epoch 34/50 - Train Loss: 0.032316 - Val Loss: 0.043474\n",
      "Epoch 35/50 - Train Loss: 0.031583 - Val Loss: 0.048231\n",
      "Epoch 36/50 - Train Loss: 0.028793 - Val Loss: 0.043324\n",
      "Epoch 37/50 - Train Loss: 0.031641 - Val Loss: 0.048174\n",
      "Epoch 38/50 - Train Loss: 0.029640 - Val Loss: 0.052673\n",
      "Epoch 39/50 - Train Loss: 0.029618 - Val Loss: 0.044635\n",
      "Epoch 40/50 - Train Loss: 0.033684 - Val Loss: 0.044523\n",
      "Epoch 41/50 - Train Loss: 0.032165 - Val Loss: 0.043271\n",
      "Epoch 42/50 - Train Loss: 0.030958 - Val Loss: 0.044055\n",
      "Epoch 43/50 - Train Loss: 0.030606 - Val Loss: 0.044092\n",
      "Epoch 44/50 - Train Loss: 0.031695 - Val Loss: 0.043288\n",
      "Epoch 45/50 - Train Loss: 0.028089 - Val Loss: 0.048065\n",
      "Epoch 46/50 - Train Loss: 0.027871 - Val Loss: 0.047047\n",
      "Epoch 47/50 - Train Loss: 0.028357 - Val Loss: 0.046592\n",
      "Epoch 48/50 - Train Loss: 0.028268 - Val Loss: 0.043301\n",
      "Epoch 49/50 - Train Loss: 0.028228 - Val Loss: 0.043413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:42:13,012] Trial 75 finished with value: 0.04327065125107765 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 154, 'lr': 0.008431680204418398, 'weight_decay': 6.996634990849533e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029010 - Val Loss: 0.043634\n",
      "Epoch 1/50 - Train Loss: 0.139233 - Val Loss: 0.279676\n",
      "Epoch 2/50 - Train Loss: 0.091092 - Val Loss: 0.058886\n",
      "Epoch 3/50 - Train Loss: 0.048479 - Val Loss: 0.105458\n",
      "Epoch 4/50 - Train Loss: 0.041979 - Val Loss: 0.054226\n",
      "Epoch 5/50 - Train Loss: 0.038936 - Val Loss: 0.066189\n",
      "Epoch 6/50 - Train Loss: 0.033239 - Val Loss: 0.052603\n",
      "Epoch 7/50 - Train Loss: 0.031180 - Val Loss: 0.052170\n",
      "Epoch 8/50 - Train Loss: 0.031311 - Val Loss: 0.049386\n",
      "Epoch 9/50 - Train Loss: 0.030970 - Val Loss: 0.047794\n",
      "Epoch 10/50 - Train Loss: 0.031936 - Val Loss: 0.045319\n",
      "Epoch 11/50 - Train Loss: 0.032106 - Val Loss: 0.044190\n",
      "Epoch 12/50 - Train Loss: 0.032050 - Val Loss: 0.046819\n",
      "Epoch 13/50 - Train Loss: 0.028074 - Val Loss: 0.044794\n",
      "Epoch 14/50 - Train Loss: 0.031031 - Val Loss: 0.047416\n",
      "Epoch 15/50 - Train Loss: 0.029874 - Val Loss: 0.045262\n",
      "Epoch 16/50 - Train Loss: 0.030946 - Val Loss: 0.044816\n",
      "Epoch 17/50 - Train Loss: 0.028912 - Val Loss: 0.047764\n",
      "Epoch 18/50 - Train Loss: 0.032158 - Val Loss: 0.044390\n",
      "Epoch 19/50 - Train Loss: 0.032956 - Val Loss: 0.045071\n",
      "Epoch 20/50 - Train Loss: 0.031901 - Val Loss: 0.044368\n",
      "Epoch 21/50 - Train Loss: 0.029820 - Val Loss: 0.043862\n",
      "Epoch 22/50 - Train Loss: 0.030432 - Val Loss: 0.050320\n",
      "Epoch 23/50 - Train Loss: 0.031973 - Val Loss: 0.051439\n",
      "Epoch 24/50 - Train Loss: 0.028487 - Val Loss: 0.044373\n",
      "Epoch 25/50 - Train Loss: 0.031828 - Val Loss: 0.044041\n",
      "Epoch 26/50 - Train Loss: 0.029343 - Val Loss: 0.044175\n",
      "Epoch 27/50 - Train Loss: 0.034022 - Val Loss: 0.044568\n",
      "Epoch 28/50 - Train Loss: 0.032447 - Val Loss: 0.047737\n",
      "Epoch 29/50 - Train Loss: 0.032319 - Val Loss: 0.047766\n",
      "Epoch 30/50 - Train Loss: 0.030930 - Val Loss: 0.043591\n",
      "Epoch 31/50 - Train Loss: 0.029092 - Val Loss: 0.044436\n",
      "Epoch 32/50 - Train Loss: 0.030118 - Val Loss: 0.048360\n",
      "Epoch 33/50 - Train Loss: 0.033413 - Val Loss: 0.045139\n",
      "Epoch 34/50 - Train Loss: 0.026557 - Val Loss: 0.044769\n",
      "Epoch 35/50 - Train Loss: 0.029608 - Val Loss: 0.046333\n",
      "Epoch 36/50 - Train Loss: 0.026319 - Val Loss: 0.044273\n",
      "Epoch 37/50 - Train Loss: 0.030197 - Val Loss: 0.043732\n",
      "Epoch 38/50 - Train Loss: 0.032094 - Val Loss: 0.044952\n",
      "Epoch 39/50 - Train Loss: 0.030429 - Val Loss: 0.057247\n",
      "Epoch 40/50 - Train Loss: 0.029500 - Val Loss: 0.044108\n",
      "Epoch 41/50 - Train Loss: 0.032109 - Val Loss: 0.043424\n",
      "Epoch 42/50 - Train Loss: 0.028710 - Val Loss: 0.045262\n",
      "Epoch 43/50 - Train Loss: 0.029903 - Val Loss: 0.047308\n",
      "Epoch 44/50 - Train Loss: 0.025366 - Val Loss: 0.044163\n",
      "Epoch 45/50 - Train Loss: 0.029461 - Val Loss: 0.045388\n",
      "Epoch 46/50 - Train Loss: 0.028675 - Val Loss: 0.045605\n",
      "Epoch 47/50 - Train Loss: 0.029426 - Val Loss: 0.044111\n",
      "Epoch 48/50 - Train Loss: 0.025689 - Val Loss: 0.044059\n",
      "Epoch 49/50 - Train Loss: 0.027888 - Val Loss: 0.044504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:42:37,488] Trial 76 finished with value: 0.043424179777503014 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 162, 'lr': 0.006053807316208851, 'weight_decay': 1.343976386438141e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028599 - Val Loss: 0.048761\n",
      "Epoch 1/50 - Train Loss: 0.067530 - Val Loss: 0.057066\n",
      "Epoch 2/50 - Train Loss: 0.049986 - Val Loss: 0.056672\n",
      "Epoch 3/50 - Train Loss: 0.041545 - Val Loss: 0.080347\n",
      "Epoch 4/50 - Train Loss: 0.039298 - Val Loss: 0.049732\n",
      "Epoch 5/50 - Train Loss: 0.034228 - Val Loss: 0.045369\n",
      "Epoch 6/50 - Train Loss: 0.033306 - Val Loss: 0.047339\n",
      "Epoch 7/50 - Train Loss: 0.033841 - Val Loss: 0.045639\n",
      "Epoch 8/50 - Train Loss: 0.030940 - Val Loss: 0.045825\n",
      "Epoch 9/50 - Train Loss: 0.032206 - Val Loss: 0.044203\n",
      "Epoch 10/50 - Train Loss: 0.031477 - Val Loss: 0.045344\n",
      "Epoch 11/50 - Train Loss: 0.029857 - Val Loss: 0.047862\n",
      "Epoch 12/50 - Train Loss: 0.031903 - Val Loss: 0.049297\n",
      "Epoch 13/50 - Train Loss: 0.025618 - Val Loss: 0.045683\n",
      "Epoch 14/50 - Train Loss: 0.032617 - Val Loss: 0.043983\n",
      "Epoch 15/50 - Train Loss: 0.032754 - Val Loss: 0.044380\n",
      "Epoch 16/50 - Train Loss: 0.034696 - Val Loss: 0.043701\n",
      "Epoch 17/50 - Train Loss: 0.032277 - Val Loss: 0.052182\n",
      "Epoch 18/50 - Train Loss: 0.029256 - Val Loss: 0.045900\n",
      "Epoch 19/50 - Train Loss: 0.031852 - Val Loss: 0.043508\n",
      "Epoch 20/50 - Train Loss: 0.033112 - Val Loss: 0.046409\n",
      "Epoch 21/50 - Train Loss: 0.031716 - Val Loss: 0.043865\n",
      "Epoch 22/50 - Train Loss: 0.028286 - Val Loss: 0.043694\n",
      "Epoch 23/50 - Train Loss: 0.028272 - Val Loss: 0.043925\n",
      "Epoch 24/50 - Train Loss: 0.030267 - Val Loss: 0.044070\n",
      "Epoch 25/50 - Train Loss: 0.029593 - Val Loss: 0.043975\n",
      "Epoch 26/50 - Train Loss: 0.032265 - Val Loss: 0.045174\n",
      "Epoch 27/50 - Train Loss: 0.028288 - Val Loss: 0.044050\n",
      "Epoch 28/50 - Train Loss: 0.027651 - Val Loss: 0.044595\n",
      "Epoch 29/50 - Train Loss: 0.030567 - Val Loss: 0.048306\n",
      "Epoch 30/50 - Train Loss: 0.026806 - Val Loss: 0.045011\n",
      "Epoch 31/50 - Train Loss: 0.031382 - Val Loss: 0.044158\n",
      "Epoch 32/50 - Train Loss: 0.029491 - Val Loss: 0.043702\n",
      "Epoch 33/50 - Train Loss: 0.027788 - Val Loss: 0.044777\n",
      "Epoch 34/50 - Train Loss: 0.025251 - Val Loss: 0.044986\n",
      "Epoch 35/50 - Train Loss: 0.031770 - Val Loss: 0.046459\n",
      "Epoch 36/50 - Train Loss: 0.029909 - Val Loss: 0.044461\n",
      "Epoch 37/50 - Train Loss: 0.026106 - Val Loss: 0.044642\n",
      "Epoch 38/50 - Train Loss: 0.031508 - Val Loss: 0.044674\n",
      "Epoch 39/50 - Train Loss: 0.026551 - Val Loss: 0.045513\n",
      "Epoch 40/50 - Train Loss: 0.028528 - Val Loss: 0.044737\n",
      "Epoch 41/50 - Train Loss: 0.030628 - Val Loss: 0.045230\n",
      "Epoch 42/50 - Train Loss: 0.029040 - Val Loss: 0.046899\n",
      "Epoch 43/50 - Train Loss: 0.025784 - Val Loss: 0.044098\n",
      "Epoch 44/50 - Train Loss: 0.028125 - Val Loss: 0.043925\n",
      "Epoch 45/50 - Train Loss: 0.028107 - Val Loss: 0.046713\n",
      "Epoch 46/50 - Train Loss: 0.028670 - Val Loss: 0.043880\n",
      "Epoch 47/50 - Train Loss: 0.027204 - Val Loss: 0.046991\n",
      "Epoch 48/50 - Train Loss: 0.029639 - Val Loss: 0.047068\n",
      "Epoch 49/50 - Train Loss: 0.027861 - Val Loss: 0.045307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:42:59,470] Trial 77 finished with value: 0.04350810870528221 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 178, 'lr': 0.00482004478277539, 'weight_decay': 2.773652620255878e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026857 - Val Loss: 0.044521\n",
      "Epoch 1/50 - Train Loss: 0.093409 - Val Loss: 0.105472\n",
      "Epoch 2/50 - Train Loss: 0.048684 - Val Loss: 0.067926\n",
      "Epoch 3/50 - Train Loss: 0.039129 - Val Loss: 0.054873\n",
      "Epoch 4/50 - Train Loss: 0.031846 - Val Loss: 0.046253\n",
      "Epoch 5/50 - Train Loss: 0.028795 - Val Loss: 0.045851\n",
      "Epoch 6/50 - Train Loss: 0.031454 - Val Loss: 0.043771\n",
      "Epoch 7/50 - Train Loss: 0.031443 - Val Loss: 0.043803\n",
      "Epoch 8/50 - Train Loss: 0.031629 - Val Loss: 0.043950\n",
      "Epoch 9/50 - Train Loss: 0.030438 - Val Loss: 0.043307\n",
      "Epoch 10/50 - Train Loss: 0.030709 - Val Loss: 0.044327\n",
      "Epoch 11/50 - Train Loss: 0.033387 - Val Loss: 0.043302\n",
      "Epoch 12/50 - Train Loss: 0.032237 - Val Loss: 0.045063\n",
      "Epoch 13/50 - Train Loss: 0.029277 - Val Loss: 0.053177\n",
      "Epoch 14/50 - Train Loss: 0.032601 - Val Loss: 0.052650\n",
      "Epoch 15/50 - Train Loss: 0.028386 - Val Loss: 0.043301\n",
      "Epoch 16/50 - Train Loss: 0.032038 - Val Loss: 0.043439\n",
      "Epoch 17/50 - Train Loss: 0.029849 - Val Loss: 0.043693\n",
      "Epoch 18/50 - Train Loss: 0.028549 - Val Loss: 0.043074\n",
      "Epoch 19/50 - Train Loss: 0.029951 - Val Loss: 0.043761\n",
      "Epoch 20/50 - Train Loss: 0.032231 - Val Loss: 0.046277\n",
      "Epoch 21/50 - Train Loss: 0.033820 - Val Loss: 0.047316\n",
      "Epoch 22/50 - Train Loss: 0.030590 - Val Loss: 0.044813\n",
      "Epoch 23/50 - Train Loss: 0.034352 - Val Loss: 0.048849\n",
      "Epoch 24/50 - Train Loss: 0.029284 - Val Loss: 0.043655\n",
      "Epoch 25/50 - Train Loss: 0.030109 - Val Loss: 0.046425\n",
      "Epoch 26/50 - Train Loss: 0.034971 - Val Loss: 0.044795\n",
      "Epoch 27/50 - Train Loss: 0.032838 - Val Loss: 0.046508\n",
      "Epoch 28/50 - Train Loss: 0.029510 - Val Loss: 0.044622\n",
      "Epoch 29/50 - Train Loss: 0.030331 - Val Loss: 0.043172\n",
      "Epoch 30/50 - Train Loss: 0.028306 - Val Loss: 0.043388\n",
      "Epoch 31/50 - Train Loss: 0.029446 - Val Loss: 0.045068\n",
      "Epoch 32/50 - Train Loss: 0.031723 - Val Loss: 0.044146\n",
      "Epoch 33/50 - Train Loss: 0.028839 - Val Loss: 0.044438\n",
      "Epoch 34/50 - Train Loss: 0.028719 - Val Loss: 0.044320\n",
      "Epoch 35/50 - Train Loss: 0.031750 - Val Loss: 0.045023\n",
      "Epoch 36/50 - Train Loss: 0.029764 - Val Loss: 0.049912\n",
      "Epoch 37/50 - Train Loss: 0.031602 - Val Loss: 0.045606\n",
      "Epoch 38/50 - Train Loss: 0.031414 - Val Loss: 0.061866\n",
      "Epoch 39/50 - Train Loss: 0.034063 - Val Loss: 0.044068\n",
      "Epoch 40/50 - Train Loss: 0.031286 - Val Loss: 0.049954\n",
      "Epoch 41/50 - Train Loss: 0.032322 - Val Loss: 0.049504\n",
      "Epoch 42/50 - Train Loss: 0.029905 - Val Loss: 0.045074\n",
      "Epoch 43/50 - Train Loss: 0.028871 - Val Loss: 0.045080\n",
      "Epoch 44/50 - Train Loss: 0.030474 - Val Loss: 0.045106\n",
      "Epoch 45/50 - Train Loss: 0.029088 - Val Loss: 0.044861\n",
      "Epoch 46/50 - Train Loss: 0.030951 - Val Loss: 0.046145\n",
      "Epoch 47/50 - Train Loss: 0.028736 - Val Loss: 0.045773\n",
      "Epoch 48/50 - Train Loss: 0.028959 - Val Loss: 0.045305\n",
      "Epoch 49/50 - Train Loss: 0.031546 - Val Loss: 0.045267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:43:21,448] Trial 78 finished with value: 0.04307432100176811 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 168, 'lr': 0.008986910952355157, 'weight_decay': 8.0526355169383e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027930 - Val Loss: 0.044749\n",
      "Epoch 1/50 - Train Loss: 0.073115 - Val Loss: 0.082461\n",
      "Epoch 2/50 - Train Loss: 0.039291 - Val Loss: 0.082188\n",
      "Epoch 3/50 - Train Loss: 0.033828 - Val Loss: 0.085970\n",
      "Epoch 4/50 - Train Loss: 0.035423 - Val Loss: 0.049495\n",
      "Epoch 5/50 - Train Loss: 0.033723 - Val Loss: 0.047816\n",
      "Epoch 6/50 - Train Loss: 0.031433 - Val Loss: 0.046907\n",
      "Epoch 7/50 - Train Loss: 0.034525 - Val Loss: 0.048949\n",
      "Epoch 8/50 - Train Loss: 0.032797 - Val Loss: 0.045146\n",
      "Epoch 9/50 - Train Loss: 0.033991 - Val Loss: 0.045803\n",
      "Epoch 10/50 - Train Loss: 0.032327 - Val Loss: 0.045814\n",
      "Epoch 11/50 - Train Loss: 0.032060 - Val Loss: 0.065257\n",
      "Epoch 12/50 - Train Loss: 0.030177 - Val Loss: 0.048541\n",
      "Epoch 13/50 - Train Loss: 0.033016 - Val Loss: 0.044529\n",
      "Epoch 14/50 - Train Loss: 0.028850 - Val Loss: 0.046577\n",
      "Epoch 15/50 - Train Loss: 0.030465 - Val Loss: 0.051465\n",
      "Epoch 16/50 - Train Loss: 0.030949 - Val Loss: 0.045137\n",
      "Epoch 17/50 - Train Loss: 0.033410 - Val Loss: 0.044979\n",
      "Epoch 18/50 - Train Loss: 0.030161 - Val Loss: 0.046844\n",
      "Epoch 19/50 - Train Loss: 0.027037 - Val Loss: 0.045143\n",
      "Epoch 20/50 - Train Loss: 0.030629 - Val Loss: 0.046322\n",
      "Epoch 21/50 - Train Loss: 0.028987 - Val Loss: 0.045564\n",
      "Epoch 22/50 - Train Loss: 0.028673 - Val Loss: 0.044764\n",
      "Epoch 23/50 - Train Loss: 0.030737 - Val Loss: 0.046064\n",
      "Epoch 24/50 - Train Loss: 0.027537 - Val Loss: 0.044922\n",
      "Epoch 25/50 - Train Loss: 0.029282 - Val Loss: 0.045911\n",
      "Epoch 26/50 - Train Loss: 0.030045 - Val Loss: 0.051535\n",
      "Epoch 27/50 - Train Loss: 0.033441 - Val Loss: 0.045488\n",
      "Epoch 28/50 - Train Loss: 0.028598 - Val Loss: 0.048725\n",
      "Epoch 29/50 - Train Loss: 0.029787 - Val Loss: 0.045711\n",
      "Epoch 30/50 - Train Loss: 0.033442 - Val Loss: 0.046553\n",
      "Epoch 31/50 - Train Loss: 0.029417 - Val Loss: 0.052226\n",
      "Epoch 32/50 - Train Loss: 0.030711 - Val Loss: 0.046862\n",
      "Epoch 33/50 - Train Loss: 0.029132 - Val Loss: 0.045727\n",
      "Epoch 34/50 - Train Loss: 0.026536 - Val Loss: 0.045313\n",
      "Epoch 35/50 - Train Loss: 0.026893 - Val Loss: 0.045846\n",
      "Epoch 36/50 - Train Loss: 0.028888 - Val Loss: 0.046046\n",
      "Epoch 37/50 - Train Loss: 0.026185 - Val Loss: 0.047011\n",
      "Epoch 38/50 - Train Loss: 0.027880 - Val Loss: 0.047408\n",
      "Epoch 39/50 - Train Loss: 0.030362 - Val Loss: 0.046047\n",
      "Epoch 40/50 - Train Loss: 0.032040 - Val Loss: 0.046909\n",
      "Epoch 41/50 - Train Loss: 0.030081 - Val Loss: 0.049359\n",
      "Epoch 42/50 - Train Loss: 0.030370 - Val Loss: 0.052988\n",
      "Epoch 43/50 - Train Loss: 0.031467 - Val Loss: 0.046206\n",
      "Epoch 44/50 - Train Loss: 0.032666 - Val Loss: 0.049726\n",
      "Epoch 45/50 - Train Loss: 0.028099 - Val Loss: 0.046198\n",
      "Epoch 46/50 - Train Loss: 0.025516 - Val Loss: 0.046721\n",
      "Epoch 47/50 - Train Loss: 0.029695 - Val Loss: 0.045614\n",
      "Epoch 48/50 - Train Loss: 0.029885 - Val Loss: 0.047097\n",
      "Epoch 49/50 - Train Loss: 0.027586 - Val Loss: 0.046469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:43:42,784] Trial 79 finished with value: 0.04452899098396301 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 149, 'lr': 0.003898763300961938, 'weight_decay': 4.145707634929115e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030512 - Val Loss: 0.046506\n",
      "Epoch 1/50 - Train Loss: 0.165780 - Val Loss: 0.222369\n",
      "Epoch 2/50 - Train Loss: 0.080635 - Val Loss: 0.082558\n",
      "Epoch 3/50 - Train Loss: 0.047663 - Val Loss: 0.110766\n",
      "Epoch 4/50 - Train Loss: 0.049092 - Val Loss: 0.072167\n",
      "Epoch 5/50 - Train Loss: 0.040079 - Val Loss: 0.078154\n",
      "Epoch 6/50 - Train Loss: 0.039955 - Val Loss: 0.066954\n",
      "Epoch 7/50 - Train Loss: 0.037856 - Val Loss: 0.053313\n",
      "Epoch 8/50 - Train Loss: 0.040982 - Val Loss: 0.062677\n",
      "Epoch 9/50 - Train Loss: 0.032032 - Val Loss: 0.050845\n",
      "Epoch 10/50 - Train Loss: 0.031775 - Val Loss: 0.045091\n",
      "Epoch 11/50 - Train Loss: 0.030841 - Val Loss: 0.047812\n",
      "Epoch 12/50 - Train Loss: 0.036881 - Val Loss: 0.044653\n",
      "Epoch 13/50 - Train Loss: 0.031491 - Val Loss: 0.048800\n",
      "Epoch 14/50 - Train Loss: 0.029671 - Val Loss: 0.045275\n",
      "Epoch 15/50 - Train Loss: 0.029398 - Val Loss: 0.047237\n",
      "Epoch 16/50 - Train Loss: 0.027258 - Val Loss: 0.047604\n",
      "Epoch 17/50 - Train Loss: 0.031208 - Val Loss: 0.049852\n",
      "Epoch 18/50 - Train Loss: 0.031639 - Val Loss: 0.043703\n",
      "Epoch 19/50 - Train Loss: 0.028161 - Val Loss: 0.043259\n",
      "Epoch 20/50 - Train Loss: 0.033354 - Val Loss: 0.043944\n",
      "Epoch 21/50 - Train Loss: 0.029237 - Val Loss: 0.052713\n",
      "Epoch 22/50 - Train Loss: 0.029271 - Val Loss: 0.044109\n",
      "Epoch 23/50 - Train Loss: 0.030213 - Val Loss: 0.044772\n",
      "Epoch 24/50 - Train Loss: 0.029039 - Val Loss: 0.046701\n",
      "Epoch 25/50 - Train Loss: 0.030235 - Val Loss: 0.048301\n",
      "Epoch 26/50 - Train Loss: 0.032681 - Val Loss: 0.048144\n",
      "Epoch 27/50 - Train Loss: 0.029615 - Val Loss: 0.060000\n",
      "Epoch 28/50 - Train Loss: 0.030415 - Val Loss: 0.045245\n",
      "Epoch 29/50 - Train Loss: 0.031637 - Val Loss: 0.049177\n",
      "Epoch 30/50 - Train Loss: 0.030082 - Val Loss: 0.056161\n",
      "Epoch 31/50 - Train Loss: 0.032848 - Val Loss: 0.046084\n",
      "Epoch 32/50 - Train Loss: 0.028804 - Val Loss: 0.043741\n",
      "Epoch 33/50 - Train Loss: 0.030482 - Val Loss: 0.046586\n",
      "Epoch 34/50 - Train Loss: 0.032174 - Val Loss: 0.044836\n",
      "Epoch 35/50 - Train Loss: 0.030361 - Val Loss: 0.043425\n",
      "Epoch 36/50 - Train Loss: 0.032514 - Val Loss: 0.043742\n",
      "Epoch 37/50 - Train Loss: 0.031134 - Val Loss: 0.043419\n",
      "Epoch 38/50 - Train Loss: 0.029796 - Val Loss: 0.043904\n",
      "Epoch 39/50 - Train Loss: 0.027929 - Val Loss: 0.044032\n",
      "Epoch 40/50 - Train Loss: 0.027881 - Val Loss: 0.043642\n",
      "Epoch 41/50 - Train Loss: 0.028529 - Val Loss: 0.045048\n",
      "Epoch 42/50 - Train Loss: 0.026993 - Val Loss: 0.044066\n",
      "Epoch 43/50 - Train Loss: 0.029035 - Val Loss: 0.045924\n",
      "Epoch 44/50 - Train Loss: 0.029751 - Val Loss: 0.043947\n",
      "Epoch 45/50 - Train Loss: 0.027905 - Val Loss: 0.044460\n",
      "Epoch 46/50 - Train Loss: 0.028715 - Val Loss: 0.043790\n",
      "Epoch 47/50 - Train Loss: 0.028712 - Val Loss: 0.050360\n",
      "Epoch 48/50 - Train Loss: 0.035690 - Val Loss: 0.058735\n",
      "Epoch 49/50 - Train Loss: 0.032867 - Val Loss: 0.049173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:44:06,955] Trial 80 finished with value: 0.0432586669921875 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 137, 'lr': 0.006104490374930188, 'weight_decay': 5.529984741567693e-06, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029970 - Val Loss: 0.048315\n",
      "Epoch 1/50 - Train Loss: 0.078548 - Val Loss: 0.069651\n",
      "Epoch 2/50 - Train Loss: 0.043918 - Val Loss: 0.078492\n",
      "Epoch 3/50 - Train Loss: 0.040472 - Val Loss: 0.050186\n",
      "Epoch 4/50 - Train Loss: 0.030143 - Val Loss: 0.046961\n",
      "Epoch 5/50 - Train Loss: 0.031538 - Val Loss: 0.047530\n",
      "Epoch 6/50 - Train Loss: 0.030617 - Val Loss: 0.046850\n",
      "Epoch 7/50 - Train Loss: 0.026907 - Val Loss: 0.044357\n",
      "Epoch 8/50 - Train Loss: 0.036223 - Val Loss: 0.044287\n",
      "Epoch 9/50 - Train Loss: 0.029051 - Val Loss: 0.044912\n",
      "Epoch 10/50 - Train Loss: 0.032561 - Val Loss: 0.045030\n",
      "Epoch 11/50 - Train Loss: 0.030045 - Val Loss: 0.046307\n",
      "Epoch 12/50 - Train Loss: 0.028956 - Val Loss: 0.045193\n",
      "Epoch 13/50 - Train Loss: 0.031139 - Val Loss: 0.053039\n",
      "Epoch 14/50 - Train Loss: 0.032787 - Val Loss: 0.044299\n",
      "Epoch 15/50 - Train Loss: 0.027018 - Val Loss: 0.045753\n",
      "Epoch 16/50 - Train Loss: 0.028939 - Val Loss: 0.044695\n",
      "Epoch 17/50 - Train Loss: 0.033104 - Val Loss: 0.044690\n",
      "Epoch 18/50 - Train Loss: 0.027978 - Val Loss: 0.050945\n",
      "Epoch 19/50 - Train Loss: 0.032319 - Val Loss: 0.048234\n",
      "Epoch 20/50 - Train Loss: 0.031551 - Val Loss: 0.061296\n",
      "Epoch 21/50 - Train Loss: 0.034398 - Val Loss: 0.046954\n",
      "Epoch 22/50 - Train Loss: 0.029514 - Val Loss: 0.044875\n",
      "Epoch 23/50 - Train Loss: 0.032278 - Val Loss: 0.044192\n",
      "Epoch 24/50 - Train Loss: 0.033146 - Val Loss: 0.051461\n",
      "Epoch 25/50 - Train Loss: 0.031917 - Val Loss: 0.045620\n",
      "Epoch 26/50 - Train Loss: 0.028165 - Val Loss: 0.044572\n",
      "Epoch 27/50 - Train Loss: 0.028829 - Val Loss: 0.043617\n",
      "Epoch 28/50 - Train Loss: 0.031364 - Val Loss: 0.043082\n",
      "Epoch 29/50 - Train Loss: 0.031685 - Val Loss: 0.043020\n",
      "Epoch 30/50 - Train Loss: 0.025759 - Val Loss: 0.043707\n",
      "Epoch 31/50 - Train Loss: 0.029947 - Val Loss: 0.050577\n",
      "Epoch 32/50 - Train Loss: 0.031215 - Val Loss: 0.050522\n",
      "Epoch 33/50 - Train Loss: 0.028886 - Val Loss: 0.042988\n",
      "Epoch 34/50 - Train Loss: 0.028026 - Val Loss: 0.044069\n",
      "Epoch 35/50 - Train Loss: 0.031730 - Val Loss: 0.043249\n",
      "Epoch 36/50 - Train Loss: 0.027070 - Val Loss: 0.043213\n",
      "Epoch 37/50 - Train Loss: 0.028513 - Val Loss: 0.043971\n",
      "Epoch 38/50 - Train Loss: 0.030849 - Val Loss: 0.049724\n",
      "Epoch 39/50 - Train Loss: 0.029999 - Val Loss: 0.057977\n",
      "Epoch 40/50 - Train Loss: 0.033291 - Val Loss: 0.043128\n",
      "Epoch 41/50 - Train Loss: 0.029251 - Val Loss: 0.042802\n",
      "Epoch 42/50 - Train Loss: 0.030201 - Val Loss: 0.043232\n",
      "Epoch 43/50 - Train Loss: 0.028429 - Val Loss: 0.046022\n",
      "Epoch 44/50 - Train Loss: 0.028844 - Val Loss: 0.046135\n",
      "Epoch 45/50 - Train Loss: 0.031293 - Val Loss: 0.053002\n",
      "Epoch 46/50 - Train Loss: 0.028387 - Val Loss: 0.043326\n",
      "Epoch 47/50 - Train Loss: 0.027491 - Val Loss: 0.044540\n",
      "Epoch 48/50 - Train Loss: 0.025856 - Val Loss: 0.043141\n",
      "Epoch 49/50 - Train Loss: 0.029680 - Val Loss: 0.043019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:44:29,470] Trial 81 finished with value: 0.0428023636341095 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 182, 'lr': 0.005171274952152748, 'weight_decay': 5.077316223325203e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.027762 - Val Loss: 0.044121\n",
      "Epoch 1/50 - Train Loss: 0.084391 - Val Loss: 0.064483\n",
      "Epoch 2/50 - Train Loss: 0.039777 - Val Loss: 0.049112\n",
      "Epoch 3/50 - Train Loss: 0.032078 - Val Loss: 0.048985\n",
      "Epoch 4/50 - Train Loss: 0.032822 - Val Loss: 0.048269\n",
      "Epoch 5/50 - Train Loss: 0.032483 - Val Loss: 0.054286\n",
      "Epoch 6/50 - Train Loss: 0.031611 - Val Loss: 0.064495\n",
      "Epoch 7/50 - Train Loss: 0.029379 - Val Loss: 0.049821\n",
      "Epoch 8/50 - Train Loss: 0.030927 - Val Loss: 0.044737\n",
      "Epoch 9/50 - Train Loss: 0.032468 - Val Loss: 0.043264\n",
      "Epoch 10/50 - Train Loss: 0.032900 - Val Loss: 0.049445\n",
      "Epoch 11/50 - Train Loss: 0.030796 - Val Loss: 0.051184\n",
      "Epoch 12/50 - Train Loss: 0.027712 - Val Loss: 0.067076\n",
      "Epoch 13/50 - Train Loss: 0.033899 - Val Loss: 0.050009\n",
      "Epoch 14/50 - Train Loss: 0.034377 - Val Loss: 0.045969\n",
      "Epoch 15/50 - Train Loss: 0.027399 - Val Loss: 0.043388\n",
      "Epoch 16/50 - Train Loss: 0.030675 - Val Loss: 0.048458\n",
      "Epoch 17/50 - Train Loss: 0.028974 - Val Loss: 0.046509\n",
      "Epoch 18/50 - Train Loss: 0.031764 - Val Loss: 0.043276\n",
      "Epoch 19/50 - Train Loss: 0.028994 - Val Loss: 0.043478\n",
      "Epoch 20/50 - Train Loss: 0.029721 - Val Loss: 0.045227\n",
      "Epoch 21/50 - Train Loss: 0.030600 - Val Loss: 0.047236\n",
      "Epoch 22/50 - Train Loss: 0.028779 - Val Loss: 0.048639\n",
      "Epoch 23/50 - Train Loss: 0.029346 - Val Loss: 0.043076\n",
      "Epoch 24/50 - Train Loss: 0.030074 - Val Loss: 0.046736\n",
      "Epoch 25/50 - Train Loss: 0.031550 - Val Loss: 0.044606\n",
      "Epoch 26/50 - Train Loss: 0.032773 - Val Loss: 0.060338\n",
      "Epoch 27/50 - Train Loss: 0.032706 - Val Loss: 0.051452\n",
      "Epoch 28/50 - Train Loss: 0.028859 - Val Loss: 0.055631\n",
      "Epoch 29/50 - Train Loss: 0.032855 - Val Loss: 0.045757\n",
      "Epoch 30/50 - Train Loss: 0.032224 - Val Loss: 0.044537\n",
      "Epoch 31/50 - Train Loss: 0.028473 - Val Loss: 0.043679\n",
      "Epoch 32/50 - Train Loss: 0.032270 - Val Loss: 0.044485\n",
      "Epoch 33/50 - Train Loss: 0.028569 - Val Loss: 0.047304\n",
      "Epoch 34/50 - Train Loss: 0.032125 - Val Loss: 0.044893\n",
      "Epoch 35/50 - Train Loss: 0.030470 - Val Loss: 0.045678\n",
      "Epoch 36/50 - Train Loss: 0.026908 - Val Loss: 0.043387\n",
      "Epoch 37/50 - Train Loss: 0.029864 - Val Loss: 0.045337\n",
      "Epoch 38/50 - Train Loss: 0.033442 - Val Loss: 0.044274\n",
      "Epoch 39/50 - Train Loss: 0.032099 - Val Loss: 0.062331\n",
      "Epoch 40/50 - Train Loss: 0.031484 - Val Loss: 0.047081\n",
      "Epoch 41/50 - Train Loss: 0.030559 - Val Loss: 0.043871\n",
      "Epoch 42/50 - Train Loss: 0.026232 - Val Loss: 0.043533\n",
      "Epoch 43/50 - Train Loss: 0.027824 - Val Loss: 0.043451\n",
      "Epoch 44/50 - Train Loss: 0.032520 - Val Loss: 0.046067\n",
      "Epoch 45/50 - Train Loss: 0.029509 - Val Loss: 0.045480\n",
      "Epoch 46/50 - Train Loss: 0.030568 - Val Loss: 0.044639\n",
      "Epoch 47/50 - Train Loss: 0.027777 - Val Loss: 0.044727\n",
      "Epoch 48/50 - Train Loss: 0.028965 - Val Loss: 0.043714\n",
      "Epoch 49/50 - Train Loss: 0.029831 - Val Loss: 0.043986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:44:51,824] Trial 82 finished with value: 0.043075837194919586 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 182, 'lr': 0.007505377036460504, 'weight_decay': 2.4829691188318517e-05, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.025287 - Val Loss: 0.047026\n",
      "Epoch 1/50 - Train Loss: 0.075972 - Val Loss: 0.161413\n",
      "Epoch 2/50 - Train Loss: 0.056491 - Val Loss: 0.072544\n",
      "Epoch 3/50 - Train Loss: 0.045735 - Val Loss: 0.079557\n",
      "Epoch 4/50 - Train Loss: 0.042368 - Val Loss: 0.096245\n",
      "Epoch 5/50 - Train Loss: 0.043762 - Val Loss: 0.075702\n",
      "Epoch 6/50 - Train Loss: 0.039922 - Val Loss: 0.065045\n",
      "Epoch 7/50 - Train Loss: 0.035160 - Val Loss: 0.053211\n",
      "Epoch 8/50 - Train Loss: 0.032080 - Val Loss: 0.071771\n",
      "Epoch 9/50 - Train Loss: 0.037335 - Val Loss: 0.087994\n",
      "Epoch 10/50 - Train Loss: 0.039945 - Val Loss: 0.047879\n",
      "Epoch 11/50 - Train Loss: 0.036527 - Val Loss: 0.080330\n",
      "Epoch 12/50 - Train Loss: 0.036740 - Val Loss: 0.044864\n",
      "Epoch 13/50 - Train Loss: 0.035172 - Val Loss: 0.055740\n",
      "Epoch 14/50 - Train Loss: 0.031349 - Val Loss: 0.049293\n",
      "Epoch 15/50 - Train Loss: 0.030624 - Val Loss: 0.044612\n",
      "Epoch 16/50 - Train Loss: 0.030123 - Val Loss: 0.056730\n",
      "Epoch 17/50 - Train Loss: 0.031653 - Val Loss: 0.043273\n",
      "Epoch 18/50 - Train Loss: 0.027920 - Val Loss: 0.043103\n",
      "Epoch 19/50 - Train Loss: 0.031532 - Val Loss: 0.052648\n",
      "Epoch 20/50 - Train Loss: 0.032334 - Val Loss: 0.044964\n",
      "Epoch 21/50 - Train Loss: 0.033498 - Val Loss: 0.042917\n",
      "Epoch 22/50 - Train Loss: 0.033535 - Val Loss: 0.048434\n",
      "Epoch 23/50 - Train Loss: 0.028688 - Val Loss: 0.044193\n",
      "Epoch 24/50 - Train Loss: 0.030939 - Val Loss: 0.044313\n",
      "Epoch 25/50 - Train Loss: 0.026738 - Val Loss: 0.046370\n",
      "Epoch 26/50 - Train Loss: 0.029813 - Val Loss: 0.043265\n",
      "Epoch 27/50 - Train Loss: 0.027919 - Val Loss: 0.054398\n",
      "Epoch 28/50 - Train Loss: 0.031083 - Val Loss: 0.055932\n",
      "Epoch 29/50 - Train Loss: 0.031017 - Val Loss: 0.044273\n",
      "Epoch 30/50 - Train Loss: 0.033567 - Val Loss: 0.044397\n",
      "Epoch 31/50 - Train Loss: 0.034413 - Val Loss: 0.067284\n",
      "Epoch 32/50 - Train Loss: 0.037307 - Val Loss: 0.042808\n",
      "Epoch 33/50 - Train Loss: 0.032215 - Val Loss: 0.044283\n",
      "Epoch 34/50 - Train Loss: 0.026907 - Val Loss: 0.043592\n",
      "Epoch 35/50 - Train Loss: 0.030906 - Val Loss: 0.042605\n",
      "Epoch 36/50 - Train Loss: 0.025450 - Val Loss: 0.044906\n",
      "Epoch 37/50 - Train Loss: 0.029288 - Val Loss: 0.043044\n",
      "Epoch 38/50 - Train Loss: 0.027418 - Val Loss: 0.043971\n",
      "Epoch 39/50 - Train Loss: 0.032743 - Val Loss: 0.047402\n",
      "Epoch 40/50 - Train Loss: 0.028558 - Val Loss: 0.043123\n",
      "Epoch 41/50 - Train Loss: 0.030942 - Val Loss: 0.044228\n",
      "Epoch 42/50 - Train Loss: 0.028422 - Val Loss: 0.047303\n",
      "Epoch 43/50 - Train Loss: 0.028512 - Val Loss: 0.043918\n",
      "Epoch 44/50 - Train Loss: 0.030166 - Val Loss: 0.043773\n",
      "Epoch 45/50 - Train Loss: 0.031398 - Val Loss: 0.043543\n",
      "Epoch 46/50 - Train Loss: 0.029174 - Val Loss: 0.045375\n",
      "Epoch 47/50 - Train Loss: 0.029154 - Val Loss: 0.043658\n",
      "Epoch 48/50 - Train Loss: 0.028850 - Val Loss: 0.046215\n",
      "Epoch 49/50 - Train Loss: 0.030141 - Val Loss: 0.044115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:45:21,136] Trial 83 finished with value: 0.04260457865893841 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 165, 'lr': 0.004879742925363757, 'weight_decay': 1.7077051419953317e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031805 - Val Loss: 0.044182\n",
      "Epoch 1/50 - Train Loss: 0.083201 - Val Loss: 0.161064\n",
      "Epoch 2/50 - Train Loss: 0.057370 - Val Loss: 0.061175\n",
      "Epoch 3/50 - Train Loss: 0.041668 - Val Loss: 0.078873\n",
      "Epoch 4/50 - Train Loss: 0.038712 - Val Loss: 0.048135\n",
      "Epoch 5/50 - Train Loss: 0.030807 - Val Loss: 0.046719\n",
      "Epoch 6/50 - Train Loss: 0.031681 - Val Loss: 0.049609\n",
      "Epoch 7/50 - Train Loss: 0.031833 - Val Loss: 0.047537\n",
      "Epoch 8/50 - Train Loss: 0.032742 - Val Loss: 0.054356\n",
      "Epoch 9/50 - Train Loss: 0.032851 - Val Loss: 0.044036\n",
      "Epoch 10/50 - Train Loss: 0.030970 - Val Loss: 0.050898\n",
      "Epoch 11/50 - Train Loss: 0.032393 - Val Loss: 0.043797\n",
      "Epoch 12/50 - Train Loss: 0.029809 - Val Loss: 0.047456\n",
      "Epoch 13/50 - Train Loss: 0.028019 - Val Loss: 0.048028\n",
      "Epoch 14/50 - Train Loss: 0.031572 - Val Loss: 0.045929\n",
      "Epoch 15/50 - Train Loss: 0.030284 - Val Loss: 0.043440\n",
      "Epoch 16/50 - Train Loss: 0.028373 - Val Loss: 0.049109\n",
      "Epoch 17/50 - Train Loss: 0.029498 - Val Loss: 0.043807\n",
      "Epoch 18/50 - Train Loss: 0.030587 - Val Loss: 0.043223\n",
      "Epoch 19/50 - Train Loss: 0.030422 - Val Loss: 0.047360\n",
      "Epoch 20/50 - Train Loss: 0.031622 - Val Loss: 0.045532\n",
      "Epoch 21/50 - Train Loss: 0.033602 - Val Loss: 0.043813\n",
      "Epoch 22/50 - Train Loss: 0.026742 - Val Loss: 0.044450\n",
      "Epoch 23/50 - Train Loss: 0.031747 - Val Loss: 0.044844\n",
      "Epoch 24/50 - Train Loss: 0.027884 - Val Loss: 0.051894\n",
      "Epoch 25/50 - Train Loss: 0.031178 - Val Loss: 0.043329\n",
      "Epoch 26/50 - Train Loss: 0.027300 - Val Loss: 0.043561\n",
      "Epoch 27/50 - Train Loss: 0.031556 - Val Loss: 0.046108\n",
      "Epoch 28/50 - Train Loss: 0.026864 - Val Loss: 0.043886\n",
      "Epoch 29/50 - Train Loss: 0.032441 - Val Loss: 0.043116\n",
      "Epoch 30/50 - Train Loss: 0.029106 - Val Loss: 0.043901\n",
      "Epoch 31/50 - Train Loss: 0.027833 - Val Loss: 0.043225\n",
      "Epoch 32/50 - Train Loss: 0.026624 - Val Loss: 0.044416\n",
      "Epoch 33/50 - Train Loss: 0.028696 - Val Loss: 0.043525\n",
      "Epoch 34/50 - Train Loss: 0.030105 - Val Loss: 0.043899\n",
      "Epoch 35/50 - Train Loss: 0.028553 - Val Loss: 0.044101\n",
      "Epoch 36/50 - Train Loss: 0.027161 - Val Loss: 0.045187\n",
      "Epoch 37/50 - Train Loss: 0.030707 - Val Loss: 0.043746\n",
      "Epoch 38/50 - Train Loss: 0.031274 - Val Loss: 0.048139\n",
      "Epoch 39/50 - Train Loss: 0.028575 - Val Loss: 0.044906\n",
      "Epoch 40/50 - Train Loss: 0.030035 - Val Loss: 0.044372\n",
      "Epoch 41/50 - Train Loss: 0.028549 - Val Loss: 0.044872\n",
      "Epoch 42/50 - Train Loss: 0.029149 - Val Loss: 0.044501\n",
      "Epoch 43/50 - Train Loss: 0.028661 - Val Loss: 0.044675\n",
      "Epoch 44/50 - Train Loss: 0.026123 - Val Loss: 0.052030\n",
      "Epoch 45/50 - Train Loss: 0.030469 - Val Loss: 0.049158\n",
      "Epoch 46/50 - Train Loss: 0.028648 - Val Loss: 0.044490\n",
      "Epoch 47/50 - Train Loss: 0.027163 - Val Loss: 0.044447\n",
      "Epoch 48/50 - Train Loss: 0.029440 - Val Loss: 0.044012\n",
      "Epoch 49/50 - Train Loss: 0.028770 - Val Loss: 0.050928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:45:43,284] Trial 84 finished with value: 0.043116215616464615 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 165, 'lr': 0.004379997789516781, 'weight_decay': 1.5176838003154622e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029038 - Val Loss: 0.044067\n",
      "Epoch 1/50 - Train Loss: 0.058213 - Val Loss: 0.109522\n",
      "Epoch 2/50 - Train Loss: 0.047246 - Val Loss: 0.066070\n",
      "Epoch 3/50 - Train Loss: 0.037077 - Val Loss: 0.048139\n",
      "Epoch 4/50 - Train Loss: 0.035005 - Val Loss: 0.045559\n",
      "Epoch 5/50 - Train Loss: 0.032320 - Val Loss: 0.048842\n",
      "Epoch 6/50 - Train Loss: 0.031433 - Val Loss: 0.045394\n",
      "Epoch 7/50 - Train Loss: 0.027723 - Val Loss: 0.043975\n",
      "Epoch 8/50 - Train Loss: 0.033073 - Val Loss: 0.044123\n",
      "Epoch 9/50 - Train Loss: 0.030052 - Val Loss: 0.044212\n",
      "Epoch 10/50 - Train Loss: 0.030898 - Val Loss: 0.048605\n",
      "Epoch 11/50 - Train Loss: 0.028530 - Val Loss: 0.051473\n",
      "Epoch 12/50 - Train Loss: 0.031620 - Val Loss: 0.045165\n",
      "Epoch 13/50 - Train Loss: 0.030624 - Val Loss: 0.046702\n",
      "Epoch 14/50 - Train Loss: 0.029755 - Val Loss: 0.049415\n",
      "Epoch 15/50 - Train Loss: 0.028915 - Val Loss: 0.046626\n",
      "Epoch 16/50 - Train Loss: 0.029378 - Val Loss: 0.047006\n",
      "Epoch 17/50 - Train Loss: 0.026134 - Val Loss: 0.046269\n",
      "Epoch 18/50 - Train Loss: 0.031132 - Val Loss: 0.051311\n",
      "Epoch 19/50 - Train Loss: 0.034892 - Val Loss: 0.054988\n",
      "Epoch 20/50 - Train Loss: 0.029168 - Val Loss: 0.047185\n",
      "Epoch 21/50 - Train Loss: 0.033509 - Val Loss: 0.048773\n",
      "Epoch 22/50 - Train Loss: 0.031149 - Val Loss: 0.043530\n",
      "Epoch 23/50 - Train Loss: 0.032882 - Val Loss: 0.046622\n",
      "Epoch 24/50 - Train Loss: 0.034441 - Val Loss: 0.055852\n",
      "Epoch 25/50 - Train Loss: 0.028547 - Val Loss: 0.044666\n",
      "Epoch 26/50 - Train Loss: 0.028142 - Val Loss: 0.043194\n",
      "Epoch 27/50 - Train Loss: 0.027690 - Val Loss: 0.048635\n",
      "Epoch 28/50 - Train Loss: 0.029881 - Val Loss: 0.051168\n",
      "Epoch 29/50 - Train Loss: 0.029867 - Val Loss: 0.045782\n",
      "Epoch 30/50 - Train Loss: 0.028688 - Val Loss: 0.045031\n",
      "Epoch 31/50 - Train Loss: 0.030003 - Val Loss: 0.044754\n",
      "Epoch 32/50 - Train Loss: 0.028771 - Val Loss: 0.043869\n",
      "Epoch 33/50 - Train Loss: 0.030017 - Val Loss: 0.044388\n",
      "Epoch 34/50 - Train Loss: 0.030548 - Val Loss: 0.043649\n",
      "Epoch 35/50 - Train Loss: 0.029043 - Val Loss: 0.046770\n",
      "Epoch 36/50 - Train Loss: 0.027331 - Val Loss: 0.045821\n",
      "Epoch 37/50 - Train Loss: 0.028761 - Val Loss: 0.044229\n",
      "Epoch 38/50 - Train Loss: 0.026526 - Val Loss: 0.044067\n",
      "Epoch 39/50 - Train Loss: 0.029212 - Val Loss: 0.046484\n",
      "Epoch 40/50 - Train Loss: 0.029954 - Val Loss: 0.044840\n",
      "Epoch 41/50 - Train Loss: 0.027135 - Val Loss: 0.046704\n",
      "Epoch 42/50 - Train Loss: 0.026998 - Val Loss: 0.046536\n",
      "Epoch 43/50 - Train Loss: 0.029283 - Val Loss: 0.047572\n",
      "Epoch 44/50 - Train Loss: 0.024588 - Val Loss: 0.044638\n",
      "Epoch 45/50 - Train Loss: 0.029058 - Val Loss: 0.048859\n",
      "Epoch 46/50 - Train Loss: 0.031124 - Val Loss: 0.045351\n",
      "Epoch 47/50 - Train Loss: 0.027548 - Val Loss: 0.044082\n",
      "Epoch 48/50 - Train Loss: 0.028483 - Val Loss: 0.045741\n",
      "Epoch 49/50 - Train Loss: 0.026287 - Val Loss: 0.044887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:46:05,182] Trial 85 finished with value: 0.04319368489086628 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 172, 'lr': 0.0057248513281191425, 'weight_decay': 5.814695593536795e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.025522 - Val Loss: 0.044742\n",
      "Epoch 1/50 - Train Loss: 0.054039 - Val Loss: 0.058380\n",
      "Epoch 2/50 - Train Loss: 0.035312 - Val Loss: 0.069345\n",
      "Epoch 3/50 - Train Loss: 0.037999 - Val Loss: 0.088289\n",
      "Epoch 4/50 - Train Loss: 0.037443 - Val Loss: 0.046322\n",
      "Epoch 5/50 - Train Loss: 0.032229 - Val Loss: 0.044803\n",
      "Epoch 6/50 - Train Loss: 0.033339 - Val Loss: 0.059395\n",
      "Epoch 7/50 - Train Loss: 0.034475 - Val Loss: 0.045118\n",
      "Epoch 8/50 - Train Loss: 0.030425 - Val Loss: 0.043551\n",
      "Epoch 9/50 - Train Loss: 0.030587 - Val Loss: 0.043618\n",
      "Epoch 10/50 - Train Loss: 0.032296 - Val Loss: 0.044249\n",
      "Epoch 11/50 - Train Loss: 0.029557 - Val Loss: 0.043936\n",
      "Epoch 12/50 - Train Loss: 0.031046 - Val Loss: 0.050490\n",
      "Epoch 13/50 - Train Loss: 0.030666 - Val Loss: 0.046742\n",
      "Epoch 14/50 - Train Loss: 0.029723 - Val Loss: 0.043167\n",
      "Epoch 15/50 - Train Loss: 0.033458 - Val Loss: 0.046416\n",
      "Epoch 16/50 - Train Loss: 0.031308 - Val Loss: 0.050680\n",
      "Epoch 17/50 - Train Loss: 0.028638 - Val Loss: 0.046027\n",
      "Epoch 18/50 - Train Loss: 0.031881 - Val Loss: 0.043287\n",
      "Epoch 19/50 - Train Loss: 0.027302 - Val Loss: 0.048971\n",
      "Epoch 20/50 - Train Loss: 0.030318 - Val Loss: 0.043246\n",
      "Epoch 21/50 - Train Loss: 0.028761 - Val Loss: 0.044169\n",
      "Epoch 22/50 - Train Loss: 0.030546 - Val Loss: 0.043109\n",
      "Epoch 23/50 - Train Loss: 0.028903 - Val Loss: 0.043824\n",
      "Epoch 24/50 - Train Loss: 0.030991 - Val Loss: 0.045912\n",
      "Epoch 25/50 - Train Loss: 0.032711 - Val Loss: 0.043563\n",
      "Epoch 26/50 - Train Loss: 0.028001 - Val Loss: 0.044570\n",
      "Epoch 27/50 - Train Loss: 0.030765 - Val Loss: 0.043121\n",
      "Epoch 28/50 - Train Loss: 0.034097 - Val Loss: 0.045484\n",
      "Epoch 29/50 - Train Loss: 0.028084 - Val Loss: 0.058974\n",
      "Epoch 30/50 - Train Loss: 0.026903 - Val Loss: 0.043340\n",
      "Epoch 31/50 - Train Loss: 0.026642 - Val Loss: 0.046254\n",
      "Epoch 32/50 - Train Loss: 0.030702 - Val Loss: 0.050997\n",
      "Epoch 33/50 - Train Loss: 0.027549 - Val Loss: 0.044943\n",
      "Epoch 34/50 - Train Loss: 0.030239 - Val Loss: 0.043236\n",
      "Epoch 35/50 - Train Loss: 0.028297 - Val Loss: 0.043258\n",
      "Epoch 36/50 - Train Loss: 0.027903 - Val Loss: 0.047897\n",
      "Epoch 37/50 - Train Loss: 0.031105 - Val Loss: 0.045952\n",
      "Epoch 38/50 - Train Loss: 0.031539 - Val Loss: 0.057753\n",
      "Epoch 39/50 - Train Loss: 0.030896 - Val Loss: 0.043568\n",
      "Epoch 40/50 - Train Loss: 0.027416 - Val Loss: 0.043533\n",
      "Epoch 41/50 - Train Loss: 0.028607 - Val Loss: 0.046683\n",
      "Epoch 42/50 - Train Loss: 0.028409 - Val Loss: 0.047357\n",
      "Epoch 43/50 - Train Loss: 0.030188 - Val Loss: 0.043763\n",
      "Epoch 44/50 - Train Loss: 0.028743 - Val Loss: 0.049526\n",
      "Epoch 45/50 - Train Loss: 0.030372 - Val Loss: 0.043531\n",
      "Epoch 46/50 - Train Loss: 0.030055 - Val Loss: 0.043107\n",
      "Epoch 47/50 - Train Loss: 0.027550 - Val Loss: 0.043241\n",
      "Epoch 48/50 - Train Loss: 0.028186 - Val Loss: 0.043395\n",
      "Epoch 49/50 - Train Loss: 0.030241 - Val Loss: 0.044736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:46:27,958] Trial 86 finished with value: 0.04310707747936249 and parameters: {'n_hidden_layers': 3, 'n_hidden_units': 194, 'lr': 0.0038898612761302004, 'weight_decay': 2.042159716289252e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030706 - Val Loss: 0.043569\n",
      "Epoch 1/50 - Train Loss: 0.107055 - Val Loss: 0.107674\n",
      "Epoch 2/50 - Train Loss: 0.055615 - Val Loss: 0.094877\n",
      "Epoch 3/50 - Train Loss: 0.049897 - Val Loss: 0.067460\n",
      "Epoch 4/50 - Train Loss: 0.045445 - Val Loss: 0.091445\n",
      "Epoch 5/50 - Train Loss: 0.039804 - Val Loss: 0.050416\n",
      "Epoch 6/50 - Train Loss: 0.040384 - Val Loss: 0.060299\n",
      "Epoch 7/50 - Train Loss: 0.034745 - Val Loss: 0.045945\n",
      "Epoch 8/50 - Train Loss: 0.032505 - Val Loss: 0.048175\n",
      "Epoch 9/50 - Train Loss: 0.030598 - Val Loss: 0.047621\n",
      "Epoch 10/50 - Train Loss: 0.028908 - Val Loss: 0.044607\n",
      "Epoch 11/50 - Train Loss: 0.034379 - Val Loss: 0.049310\n",
      "Epoch 12/50 - Train Loss: 0.031409 - Val Loss: 0.046176\n",
      "Epoch 13/50 - Train Loss: 0.029949 - Val Loss: 0.044077\n",
      "Epoch 14/50 - Train Loss: 0.027061 - Val Loss: 0.048645\n",
      "Epoch 15/50 - Train Loss: 0.031686 - Val Loss: 0.043165\n",
      "Epoch 16/50 - Train Loss: 0.029781 - Val Loss: 0.043291\n",
      "Epoch 17/50 - Train Loss: 0.030673 - Val Loss: 0.044867\n",
      "Epoch 18/50 - Train Loss: 0.029070 - Val Loss: 0.045133\n",
      "Epoch 19/50 - Train Loss: 0.031573 - Val Loss: 0.043608\n",
      "Epoch 20/50 - Train Loss: 0.030534 - Val Loss: 0.044334\n",
      "Epoch 21/50 - Train Loss: 0.031593 - Val Loss: 0.047124\n",
      "Epoch 22/50 - Train Loss: 0.030884 - Val Loss: 0.043909\n",
      "Epoch 23/50 - Train Loss: 0.032020 - Val Loss: 0.043511\n",
      "Epoch 24/50 - Train Loss: 0.027351 - Val Loss: 0.047601\n",
      "Epoch 25/50 - Train Loss: 0.028288 - Val Loss: 0.044492\n",
      "Epoch 26/50 - Train Loss: 0.031054 - Val Loss: 0.043275\n",
      "Epoch 27/50 - Train Loss: 0.028412 - Val Loss: 0.042919\n",
      "Epoch 28/50 - Train Loss: 0.031093 - Val Loss: 0.045554\n",
      "Epoch 29/50 - Train Loss: 0.029899 - Val Loss: 0.050334\n",
      "Epoch 30/50 - Train Loss: 0.030566 - Val Loss: 0.046905\n",
      "Epoch 31/50 - Train Loss: 0.030449 - Val Loss: 0.042838\n",
      "Epoch 32/50 - Train Loss: 0.030308 - Val Loss: 0.046233\n",
      "Epoch 33/50 - Train Loss: 0.030219 - Val Loss: 0.043230\n",
      "Epoch 34/50 - Train Loss: 0.030477 - Val Loss: 0.042668\n",
      "Epoch 35/50 - Train Loss: 0.027558 - Val Loss: 0.046037\n",
      "Epoch 36/50 - Train Loss: 0.027173 - Val Loss: 0.043953\n",
      "Epoch 37/50 - Train Loss: 0.031731 - Val Loss: 0.047073\n",
      "Epoch 38/50 - Train Loss: 0.029145 - Val Loss: 0.043860\n",
      "Epoch 39/50 - Train Loss: 0.028619 - Val Loss: 0.047945\n",
      "Epoch 40/50 - Train Loss: 0.029968 - Val Loss: 0.055479\n",
      "Epoch 41/50 - Train Loss: 0.034262 - Val Loss: 0.042742\n",
      "Epoch 42/50 - Train Loss: 0.030015 - Val Loss: 0.044102\n",
      "Epoch 43/50 - Train Loss: 0.030009 - Val Loss: 0.046594\n",
      "Epoch 44/50 - Train Loss: 0.029693 - Val Loss: 0.048171\n",
      "Epoch 45/50 - Train Loss: 0.030255 - Val Loss: 0.044257\n",
      "Epoch 46/50 - Train Loss: 0.031036 - Val Loss: 0.045965\n",
      "Epoch 47/50 - Train Loss: 0.027926 - Val Loss: 0.042690\n",
      "Epoch 48/50 - Train Loss: 0.029305 - Val Loss: 0.043936\n",
      "Epoch 49/50 - Train Loss: 0.026948 - Val Loss: 0.042648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:46:52,654] Trial 87 finished with value: 0.04264846444129944 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 152, 'lr': 0.002370770531901247, 'weight_decay': 6.69763121410715e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.026526 - Val Loss: 0.058087\n",
      "Epoch 1/50 - Train Loss: 0.134991 - Val Loss: 0.075060\n",
      "Epoch 2/50 - Train Loss: 0.068421 - Val Loss: 0.145794\n",
      "Epoch 3/50 - Train Loss: 0.055651 - Val Loss: 0.095603\n",
      "Epoch 4/50 - Train Loss: 0.044428 - Val Loss: 0.057880\n",
      "Epoch 5/50 - Train Loss: 0.041555 - Val Loss: 0.074375\n",
      "Epoch 6/50 - Train Loss: 0.038319 - Val Loss: 0.053412\n",
      "Epoch 7/50 - Train Loss: 0.035849 - Val Loss: 0.054105\n",
      "Epoch 8/50 - Train Loss: 0.032045 - Val Loss: 0.047173\n",
      "Epoch 9/50 - Train Loss: 0.028861 - Val Loss: 0.045851\n",
      "Epoch 10/50 - Train Loss: 0.030474 - Val Loss: 0.047589\n",
      "Epoch 11/50 - Train Loss: 0.029803 - Val Loss: 0.044556\n",
      "Epoch 12/50 - Train Loss: 0.030006 - Val Loss: 0.048106\n",
      "Epoch 13/50 - Train Loss: 0.027521 - Val Loss: 0.044230\n",
      "Epoch 14/50 - Train Loss: 0.028664 - Val Loss: 0.044986\n",
      "Epoch 15/50 - Train Loss: 0.031254 - Val Loss: 0.046687\n",
      "Epoch 16/50 - Train Loss: 0.030778 - Val Loss: 0.044522\n",
      "Epoch 17/50 - Train Loss: 0.033683 - Val Loss: 0.046383\n",
      "Epoch 18/50 - Train Loss: 0.030567 - Val Loss: 0.045057\n",
      "Epoch 19/50 - Train Loss: 0.034472 - Val Loss: 0.046026\n",
      "Epoch 20/50 - Train Loss: 0.031203 - Val Loss: 0.044443\n",
      "Epoch 21/50 - Train Loss: 0.030830 - Val Loss: 0.043521\n",
      "Epoch 22/50 - Train Loss: 0.030514 - Val Loss: 0.043969\n",
      "Epoch 23/50 - Train Loss: 0.029983 - Val Loss: 0.043602\n",
      "Epoch 24/50 - Train Loss: 0.028796 - Val Loss: 0.047560\n",
      "Epoch 25/50 - Train Loss: 0.031575 - Val Loss: 0.044232\n",
      "Epoch 26/50 - Train Loss: 0.031740 - Val Loss: 0.043201\n",
      "Epoch 27/50 - Train Loss: 0.033254 - Val Loss: 0.049104\n",
      "Epoch 28/50 - Train Loss: 0.028055 - Val Loss: 0.044187\n",
      "Epoch 29/50 - Train Loss: 0.028696 - Val Loss: 0.044036\n",
      "Epoch 30/50 - Train Loss: 0.025659 - Val Loss: 0.043454\n",
      "Epoch 31/50 - Train Loss: 0.028795 - Val Loss: 0.043291\n",
      "Epoch 32/50 - Train Loss: 0.029066 - Val Loss: 0.043583\n",
      "Epoch 33/50 - Train Loss: 0.026535 - Val Loss: 0.049869\n",
      "Epoch 34/50 - Train Loss: 0.029838 - Val Loss: 0.044435\n",
      "Epoch 35/50 - Train Loss: 0.032943 - Val Loss: 0.043743\n",
      "Epoch 36/50 - Train Loss: 0.030069 - Val Loss: 0.053109\n",
      "Epoch 37/50 - Train Loss: 0.026788 - Val Loss: 0.043204\n",
      "Epoch 38/50 - Train Loss: 0.030934 - Val Loss: 0.043134\n",
      "Epoch 39/50 - Train Loss: 0.028859 - Val Loss: 0.047278\n",
      "Epoch 40/50 - Train Loss: 0.030950 - Val Loss: 0.043224\n",
      "Epoch 41/50 - Train Loss: 0.029046 - Val Loss: 0.043134\n",
      "Epoch 42/50 - Train Loss: 0.030482 - Val Loss: 0.044572\n",
      "Epoch 43/50 - Train Loss: 0.023582 - Val Loss: 0.043527\n",
      "Epoch 44/50 - Train Loss: 0.029778 - Val Loss: 0.043455\n",
      "Epoch 45/50 - Train Loss: 0.025942 - Val Loss: 0.043839\n",
      "Epoch 46/50 - Train Loss: 0.026759 - Val Loss: 0.044035\n",
      "Epoch 47/50 - Train Loss: 0.030075 - Val Loss: 0.042992\n",
      "Epoch 48/50 - Train Loss: 0.029026 - Val Loss: 0.043245\n",
      "Epoch 49/50 - Train Loss: 0.029870 - Val Loss: 0.043047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:47:18,045] Trial 88 finished with value: 0.042991722002625465 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 155, 'lr': 0.0015761745204687725, 'weight_decay': 1.0831506229328369e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.025782 - Val Loss: 0.043274\n",
      "Epoch 1/50 - Train Loss: 0.099475 - Val Loss: 0.079045\n",
      "Epoch 2/50 - Train Loss: 0.054013 - Val Loss: 0.108044\n",
      "Epoch 3/50 - Train Loss: 0.041991 - Val Loss: 0.060451\n",
      "Epoch 4/50 - Train Loss: 0.043808 - Val Loss: 0.081220\n",
      "Epoch 5/50 - Train Loss: 0.035261 - Val Loss: 0.053511\n",
      "Epoch 6/50 - Train Loss: 0.038574 - Val Loss: 0.051554\n",
      "Epoch 7/50 - Train Loss: 0.034652 - Val Loss: 0.052021\n",
      "Epoch 8/50 - Train Loss: 0.033408 - Val Loss: 0.046354\n",
      "Epoch 9/50 - Train Loss: 0.030305 - Val Loss: 0.049413\n",
      "Epoch 10/50 - Train Loss: 0.031290 - Val Loss: 0.048107\n",
      "Epoch 11/50 - Train Loss: 0.026665 - Val Loss: 0.047186\n",
      "Epoch 12/50 - Train Loss: 0.030956 - Val Loss: 0.046020\n",
      "Epoch 13/50 - Train Loss: 0.027615 - Val Loss: 0.049665\n",
      "Epoch 14/50 - Train Loss: 0.030083 - Val Loss: 0.048954\n",
      "Epoch 15/50 - Train Loss: 0.029152 - Val Loss: 0.044702\n",
      "Epoch 16/50 - Train Loss: 0.029729 - Val Loss: 0.047458\n",
      "Epoch 17/50 - Train Loss: 0.030802 - Val Loss: 0.046807\n",
      "Epoch 18/50 - Train Loss: 0.029239 - Val Loss: 0.045231\n",
      "Epoch 19/50 - Train Loss: 0.029176 - Val Loss: 0.044747\n",
      "Epoch 20/50 - Train Loss: 0.032157 - Val Loss: 0.044319\n",
      "Epoch 21/50 - Train Loss: 0.032417 - Val Loss: 0.048294\n",
      "Epoch 22/50 - Train Loss: 0.030316 - Val Loss: 0.045805\n",
      "Epoch 23/50 - Train Loss: 0.029588 - Val Loss: 0.044082\n",
      "Epoch 24/50 - Train Loss: 0.032290 - Val Loss: 0.044118\n",
      "Epoch 25/50 - Train Loss: 0.030643 - Val Loss: 0.044063\n",
      "Epoch 26/50 - Train Loss: 0.031317 - Val Loss: 0.044786\n",
      "Epoch 27/50 - Train Loss: 0.032376 - Val Loss: 0.044839\n",
      "Epoch 28/50 - Train Loss: 0.032119 - Val Loss: 0.044972\n",
      "Epoch 29/50 - Train Loss: 0.030072 - Val Loss: 0.050350\n",
      "Epoch 30/50 - Train Loss: 0.031109 - Val Loss: 0.047430\n",
      "Epoch 31/50 - Train Loss: 0.028275 - Val Loss: 0.046688\n",
      "Epoch 32/50 - Train Loss: 0.032980 - Val Loss: 0.050830\n",
      "Epoch 33/50 - Train Loss: 0.030048 - Val Loss: 0.049905\n",
      "Epoch 34/50 - Train Loss: 0.032636 - Val Loss: 0.044613\n",
      "Epoch 35/50 - Train Loss: 0.027501 - Val Loss: 0.044968\n",
      "Epoch 36/50 - Train Loss: 0.030476 - Val Loss: 0.045425\n",
      "Epoch 37/50 - Train Loss: 0.028797 - Val Loss: 0.057292\n",
      "Epoch 38/50 - Train Loss: 0.029714 - Val Loss: 0.053712\n",
      "Epoch 39/50 - Train Loss: 0.033770 - Val Loss: 0.044284\n",
      "Epoch 40/50 - Train Loss: 0.033548 - Val Loss: 0.044235\n",
      "Epoch 41/50 - Train Loss: 0.028393 - Val Loss: 0.053874\n",
      "Epoch 42/50 - Train Loss: 0.030360 - Val Loss: 0.050828\n",
      "Epoch 43/50 - Train Loss: 0.034872 - Val Loss: 0.044559\n",
      "Epoch 44/50 - Train Loss: 0.030344 - Val Loss: 0.044698\n",
      "Epoch 45/50 - Train Loss: 0.031862 - Val Loss: 0.045273\n",
      "Epoch 46/50 - Train Loss: 0.028730 - Val Loss: 0.045481\n",
      "Epoch 47/50 - Train Loss: 0.033382 - Val Loss: 0.048716\n",
      "Epoch 48/50 - Train Loss: 0.028523 - Val Loss: 0.044317\n",
      "Epoch 49/50 - Train Loss: 0.027758 - Val Loss: 0.043965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:47:43,418] Trial 89 finished with value: 0.043965378776192665 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 151, 'lr': 0.002301226683476707, 'weight_decay': 0.0006229479765574163, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031963 - Val Loss: 0.044820\n",
      "Epoch 1/50 - Train Loss: 0.098046 - Val Loss: 0.130709\n",
      "Epoch 2/50 - Train Loss: 0.059986 - Val Loss: 0.117023\n",
      "Epoch 3/50 - Train Loss: 0.043325 - Val Loss: 0.058948\n",
      "Epoch 4/50 - Train Loss: 0.047389 - Val Loss: 0.085741\n",
      "Epoch 5/50 - Train Loss: 0.042081 - Val Loss: 0.051781\n",
      "Epoch 6/50 - Train Loss: 0.036675 - Val Loss: 0.051116\n",
      "Epoch 7/50 - Train Loss: 0.032857 - Val Loss: 0.055782\n",
      "Epoch 8/50 - Train Loss: 0.028814 - Val Loss: 0.045099\n",
      "Epoch 9/50 - Train Loss: 0.030405 - Val Loss: 0.045081\n",
      "Epoch 10/50 - Train Loss: 0.030748 - Val Loss: 0.049036\n",
      "Epoch 11/50 - Train Loss: 0.028870 - Val Loss: 0.050058\n",
      "Epoch 12/50 - Train Loss: 0.032085 - Val Loss: 0.044768\n",
      "Epoch 13/50 - Train Loss: 0.032379 - Val Loss: 0.043570\n",
      "Epoch 14/50 - Train Loss: 0.032048 - Val Loss: 0.044314\n",
      "Epoch 15/50 - Train Loss: 0.032483 - Val Loss: 0.052003\n",
      "Epoch 16/50 - Train Loss: 0.031207 - Val Loss: 0.056740\n",
      "Epoch 17/50 - Train Loss: 0.032818 - Val Loss: 0.043952\n",
      "Epoch 18/50 - Train Loss: 0.030343 - Val Loss: 0.044024\n",
      "Epoch 19/50 - Train Loss: 0.029531 - Val Loss: 0.043860\n",
      "Epoch 20/50 - Train Loss: 0.031798 - Val Loss: 0.044306\n",
      "Epoch 21/50 - Train Loss: 0.030875 - Val Loss: 0.043854\n",
      "Epoch 22/50 - Train Loss: 0.031177 - Val Loss: 0.046171\n",
      "Epoch 23/50 - Train Loss: 0.029109 - Val Loss: 0.044220\n",
      "Epoch 24/50 - Train Loss: 0.029254 - Val Loss: 0.052427\n",
      "Epoch 25/50 - Train Loss: 0.031192 - Val Loss: 0.047940\n",
      "Epoch 26/50 - Train Loss: 0.028926 - Val Loss: 0.044336\n",
      "Epoch 27/50 - Train Loss: 0.029661 - Val Loss: 0.043699\n",
      "Epoch 28/50 - Train Loss: 0.031543 - Val Loss: 0.044231\n",
      "Epoch 29/50 - Train Loss: 0.028875 - Val Loss: 0.044973\n",
      "Epoch 30/50 - Train Loss: 0.032103 - Val Loss: 0.061460\n",
      "Epoch 31/50 - Train Loss: 0.031719 - Val Loss: 0.043874\n",
      "Epoch 32/50 - Train Loss: 0.028725 - Val Loss: 0.043495\n",
      "Epoch 33/50 - Train Loss: 0.028088 - Val Loss: 0.043325\n",
      "Epoch 34/50 - Train Loss: 0.026174 - Val Loss: 0.048067\n",
      "Epoch 35/50 - Train Loss: 0.033066 - Val Loss: 0.046153\n",
      "Epoch 36/50 - Train Loss: 0.030107 - Val Loss: 0.043171\n",
      "Epoch 37/50 - Train Loss: 0.029685 - Val Loss: 0.043271\n",
      "Epoch 38/50 - Train Loss: 0.029254 - Val Loss: 0.043404\n",
      "Epoch 39/50 - Train Loss: 0.029940 - Val Loss: 0.051172\n",
      "Epoch 40/50 - Train Loss: 0.031361 - Val Loss: 0.044998\n",
      "Epoch 41/50 - Train Loss: 0.027609 - Val Loss: 0.044953\n",
      "Epoch 42/50 - Train Loss: 0.028321 - Val Loss: 0.043519\n",
      "Epoch 43/50 - Train Loss: 0.030742 - Val Loss: 0.043622\n",
      "Epoch 44/50 - Train Loss: 0.031519 - Val Loss: 0.048224\n",
      "Epoch 45/50 - Train Loss: 0.028128 - Val Loss: 0.052696\n",
      "Epoch 46/50 - Train Loss: 0.029374 - Val Loss: 0.046518\n",
      "Epoch 47/50 - Train Loss: 0.032787 - Val Loss: 0.043858\n",
      "Epoch 48/50 - Train Loss: 0.027951 - Val Loss: 0.048628\n",
      "Epoch 49/50 - Train Loss: 0.029588 - Val Loss: 0.043658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:48:11,255] Trial 90 finished with value: 0.04317132756114006 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 143, 'lr': 0.0034986195429149076, 'weight_decay': 4.065027934284054e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031378 - Val Loss: 0.044763\n",
      "Epoch 1/50 - Train Loss: 0.089417 - Val Loss: 0.100631\n",
      "Epoch 2/50 - Train Loss: 0.042841 - Val Loss: 0.075774\n",
      "Epoch 3/50 - Train Loss: 0.037738 - Val Loss: 0.064069\n",
      "Epoch 4/50 - Train Loss: 0.035153 - Val Loss: 0.044939\n",
      "Epoch 5/50 - Train Loss: 0.030494 - Val Loss: 0.059612\n",
      "Epoch 6/50 - Train Loss: 0.031721 - Val Loss: 0.054788\n",
      "Epoch 7/50 - Train Loss: 0.032996 - Val Loss: 0.048204\n",
      "Epoch 8/50 - Train Loss: 0.029863 - Val Loss: 0.050508\n",
      "Epoch 9/50 - Train Loss: 0.029448 - Val Loss: 0.059876\n",
      "Epoch 10/50 - Train Loss: 0.032642 - Val Loss: 0.047127\n",
      "Epoch 11/50 - Train Loss: 0.031897 - Val Loss: 0.046086\n",
      "Epoch 12/50 - Train Loss: 0.028212 - Val Loss: 0.044422\n",
      "Epoch 13/50 - Train Loss: 0.029766 - Val Loss: 0.043619\n",
      "Epoch 14/50 - Train Loss: 0.029983 - Val Loss: 0.044592\n",
      "Epoch 15/50 - Train Loss: 0.031383 - Val Loss: 0.046052\n",
      "Epoch 16/50 - Train Loss: 0.028945 - Val Loss: 0.048043\n",
      "Epoch 17/50 - Train Loss: 0.029633 - Val Loss: 0.046155\n",
      "Epoch 18/50 - Train Loss: 0.031913 - Val Loss: 0.052548\n",
      "Epoch 19/50 - Train Loss: 0.031802 - Val Loss: 0.045781\n",
      "Epoch 20/50 - Train Loss: 0.026983 - Val Loss: 0.051222\n",
      "Epoch 21/50 - Train Loss: 0.029676 - Val Loss: 0.045522\n",
      "Epoch 22/50 - Train Loss: 0.032690 - Val Loss: 0.046111\n",
      "Epoch 23/50 - Train Loss: 0.032004 - Val Loss: 0.043100\n",
      "Epoch 24/50 - Train Loss: 0.031724 - Val Loss: 0.047392\n",
      "Epoch 25/50 - Train Loss: 0.028688 - Val Loss: 0.055340\n",
      "Epoch 26/50 - Train Loss: 0.031826 - Val Loss: 0.052951\n",
      "Epoch 27/50 - Train Loss: 0.032212 - Val Loss: 0.054212\n",
      "Epoch 28/50 - Train Loss: 0.033456 - Val Loss: 0.044138\n",
      "Epoch 29/50 - Train Loss: 0.029514 - Val Loss: 0.043022\n",
      "Epoch 30/50 - Train Loss: 0.030474 - Val Loss: 0.043160\n",
      "Epoch 31/50 - Train Loss: 0.029394 - Val Loss: 0.052617\n",
      "Epoch 32/50 - Train Loss: 0.030643 - Val Loss: 0.053321\n",
      "Epoch 33/50 - Train Loss: 0.033636 - Val Loss: 0.045650\n",
      "Epoch 34/50 - Train Loss: 0.031816 - Val Loss: 0.046845\n",
      "Epoch 35/50 - Train Loss: 0.028904 - Val Loss: 0.044564\n",
      "Epoch 36/50 - Train Loss: 0.028265 - Val Loss: 0.044453\n",
      "Epoch 37/50 - Train Loss: 0.028292 - Val Loss: 0.043034\n",
      "Epoch 38/50 - Train Loss: 0.025983 - Val Loss: 0.044406\n",
      "Epoch 39/50 - Train Loss: 0.030450 - Val Loss: 0.043493\n",
      "Epoch 40/50 - Train Loss: 0.029248 - Val Loss: 0.046131\n",
      "Epoch 41/50 - Train Loss: 0.029894 - Val Loss: 0.042847\n",
      "Epoch 42/50 - Train Loss: 0.028930 - Val Loss: 0.043076\n",
      "Epoch 43/50 - Train Loss: 0.029504 - Val Loss: 0.043334\n",
      "Epoch 44/50 - Train Loss: 0.025857 - Val Loss: 0.043125\n",
      "Epoch 45/50 - Train Loss: 0.029814 - Val Loss: 0.042991\n",
      "Epoch 46/50 - Train Loss: 0.028015 - Val Loss: 0.043346\n",
      "Epoch 47/50 - Train Loss: 0.030753 - Val Loss: 0.045757\n",
      "Epoch 48/50 - Train Loss: 0.025589 - Val Loss: 0.042884\n",
      "Epoch 49/50 - Train Loss: 0.027957 - Val Loss: 0.043290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:48:36,192] Trial 91 finished with value: 0.042847227305173874 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 161, 'lr': 0.0047436328812941264, 'weight_decay': 6.697484100215894e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029388 - Val Loss: 0.043307\n",
      "Epoch 1/50 - Train Loss: 0.062599 - Val Loss: 0.086928\n",
      "Epoch 2/50 - Train Loss: 0.045940 - Val Loss: 0.110465\n",
      "Epoch 3/50 - Train Loss: 0.050272 - Val Loss: 0.055019\n",
      "Epoch 4/50 - Train Loss: 0.035306 - Val Loss: 0.049900\n",
      "Epoch 5/50 - Train Loss: 0.032001 - Val Loss: 0.045967\n",
      "Epoch 6/50 - Train Loss: 0.032925 - Val Loss: 0.044192\n",
      "Epoch 7/50 - Train Loss: 0.034137 - Val Loss: 0.045090\n",
      "Epoch 8/50 - Train Loss: 0.031194 - Val Loss: 0.050250\n",
      "Epoch 9/50 - Train Loss: 0.033552 - Val Loss: 0.046097\n",
      "Epoch 10/50 - Train Loss: 0.030819 - Val Loss: 0.048302\n",
      "Epoch 11/50 - Train Loss: 0.029625 - Val Loss: 0.045959\n",
      "Epoch 12/50 - Train Loss: 0.031208 - Val Loss: 0.046732\n",
      "Epoch 13/50 - Train Loss: 0.026542 - Val Loss: 0.043125\n",
      "Epoch 14/50 - Train Loss: 0.029874 - Val Loss: 0.042900\n",
      "Epoch 15/50 - Train Loss: 0.029587 - Val Loss: 0.044385\n",
      "Epoch 16/50 - Train Loss: 0.028646 - Val Loss: 0.043854\n",
      "Epoch 17/50 - Train Loss: 0.033592 - Val Loss: 0.043580\n",
      "Epoch 18/50 - Train Loss: 0.027025 - Val Loss: 0.044465\n",
      "Epoch 19/50 - Train Loss: 0.029947 - Val Loss: 0.048060\n",
      "Epoch 20/50 - Train Loss: 0.029283 - Val Loss: 0.043662\n",
      "Epoch 21/50 - Train Loss: 0.028059 - Val Loss: 0.048864\n",
      "Epoch 22/50 - Train Loss: 0.029584 - Val Loss: 0.043743\n",
      "Epoch 23/50 - Train Loss: 0.029071 - Val Loss: 0.043960\n",
      "Epoch 24/50 - Train Loss: 0.031092 - Val Loss: 0.044222\n",
      "Epoch 25/50 - Train Loss: 0.031826 - Val Loss: 0.042892\n",
      "Epoch 26/50 - Train Loss: 0.030710 - Val Loss: 0.043356\n",
      "Epoch 27/50 - Train Loss: 0.030815 - Val Loss: 0.043198\n",
      "Epoch 28/50 - Train Loss: 0.027890 - Val Loss: 0.051290\n",
      "Epoch 29/50 - Train Loss: 0.030934 - Val Loss: 0.046411\n",
      "Epoch 30/50 - Train Loss: 0.029487 - Val Loss: 0.043315\n",
      "Epoch 31/50 - Train Loss: 0.026506 - Val Loss: 0.043207\n",
      "Epoch 32/50 - Train Loss: 0.028906 - Val Loss: 0.043067\n",
      "Epoch 33/50 - Train Loss: 0.028547 - Val Loss: 0.043052\n",
      "Epoch 34/50 - Train Loss: 0.028238 - Val Loss: 0.043376\n",
      "Epoch 35/50 - Train Loss: 0.028512 - Val Loss: 0.044396\n",
      "Epoch 36/50 - Train Loss: 0.030101 - Val Loss: 0.058855\n",
      "Epoch 37/50 - Train Loss: 0.030643 - Val Loss: 0.051160\n",
      "Epoch 38/50 - Train Loss: 0.030090 - Val Loss: 0.044057\n",
      "Epoch 39/50 - Train Loss: 0.031528 - Val Loss: 0.044166\n",
      "Epoch 40/50 - Train Loss: 0.032972 - Val Loss: 0.054836\n",
      "Epoch 41/50 - Train Loss: 0.031153 - Val Loss: 0.061814\n",
      "Epoch 42/50 - Train Loss: 0.033685 - Val Loss: 0.045431\n",
      "Epoch 43/50 - Train Loss: 0.024847 - Val Loss: 0.043520\n",
      "Epoch 44/50 - Train Loss: 0.027980 - Val Loss: 0.051689\n",
      "Epoch 45/50 - Train Loss: 0.030787 - Val Loss: 0.043756\n",
      "Epoch 46/50 - Train Loss: 0.030261 - Val Loss: 0.042892\n",
      "Epoch 47/50 - Train Loss: 0.026210 - Val Loss: 0.043712\n",
      "Epoch 48/50 - Train Loss: 0.024333 - Val Loss: 0.047694\n",
      "Epoch 49/50 - Train Loss: 0.029601 - Val Loss: 0.050380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:49:01,128] Trial 92 finished with value: 0.04289150424301624 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 158, 'lr': 0.004829618719762968, 'weight_decay': 5.8012772070785417e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.025700 - Val Loss: 0.043648\n",
      "Epoch 1/50 - Train Loss: 0.114857 - Val Loss: 0.186734\n",
      "Epoch 2/50 - Train Loss: 0.058984 - Val Loss: 0.059919\n",
      "Epoch 3/50 - Train Loss: 0.053646 - Val Loss: 0.101887\n",
      "Epoch 4/50 - Train Loss: 0.044466 - Val Loss: 0.070623\n",
      "Epoch 5/50 - Train Loss: 0.041457 - Val Loss: 0.086273\n",
      "Epoch 6/50 - Train Loss: 0.040984 - Val Loss: 0.075734\n",
      "Epoch 7/50 - Train Loss: 0.036693 - Val Loss: 0.049887\n",
      "Epoch 8/50 - Train Loss: 0.033730 - Val Loss: 0.048101\n",
      "Epoch 9/50 - Train Loss: 0.030169 - Val Loss: 0.048070\n",
      "Epoch 10/50 - Train Loss: 0.029041 - Val Loss: 0.053031\n",
      "Epoch 11/50 - Train Loss: 0.034990 - Val Loss: 0.048748\n",
      "Epoch 12/50 - Train Loss: 0.033279 - Val Loss: 0.047027\n",
      "Epoch 13/50 - Train Loss: 0.030866 - Val Loss: 0.057732\n",
      "Epoch 14/50 - Train Loss: 0.032489 - Val Loss: 0.049042\n",
      "Epoch 15/50 - Train Loss: 0.028142 - Val Loss: 0.045264\n",
      "Epoch 16/50 - Train Loss: 0.030994 - Val Loss: 0.048509\n",
      "Epoch 17/50 - Train Loss: 0.032436 - Val Loss: 0.049260\n",
      "Epoch 18/50 - Train Loss: 0.027699 - Val Loss: 0.044842\n",
      "Epoch 19/50 - Train Loss: 0.032458 - Val Loss: 0.046387\n",
      "Epoch 20/50 - Train Loss: 0.034051 - Val Loss: 0.043941\n",
      "Epoch 21/50 - Train Loss: 0.030904 - Val Loss: 0.045471\n",
      "Epoch 22/50 - Train Loss: 0.028184 - Val Loss: 0.043367\n",
      "Epoch 23/50 - Train Loss: 0.028938 - Val Loss: 0.047395\n",
      "Epoch 24/50 - Train Loss: 0.030862 - Val Loss: 0.048038\n",
      "Epoch 25/50 - Train Loss: 0.028671 - Val Loss: 0.049098\n",
      "Epoch 26/50 - Train Loss: 0.031952 - Val Loss: 0.051280\n",
      "Epoch 27/50 - Train Loss: 0.029199 - Val Loss: 0.052820\n",
      "Epoch 28/50 - Train Loss: 0.035575 - Val Loss: 0.060090\n",
      "Epoch 29/50 - Train Loss: 0.029746 - Val Loss: 0.048233\n",
      "Epoch 30/50 - Train Loss: 0.035075 - Val Loss: 0.043671\n",
      "Epoch 31/50 - Train Loss: 0.030991 - Val Loss: 0.047338\n",
      "Epoch 32/50 - Train Loss: 0.028953 - Val Loss: 0.047618\n",
      "Epoch 33/50 - Train Loss: 0.031480 - Val Loss: 0.044560\n",
      "Epoch 34/50 - Train Loss: 0.029443 - Val Loss: 0.043327\n",
      "Epoch 35/50 - Train Loss: 0.033265 - Val Loss: 0.046599\n",
      "Epoch 36/50 - Train Loss: 0.030966 - Val Loss: 0.043520\n",
      "Epoch 37/50 - Train Loss: 0.031314 - Val Loss: 0.045167\n",
      "Epoch 38/50 - Train Loss: 0.027642 - Val Loss: 0.044338\n",
      "Epoch 39/50 - Train Loss: 0.028889 - Val Loss: 0.047293\n",
      "Epoch 40/50 - Train Loss: 0.032698 - Val Loss: 0.045562\n",
      "Epoch 41/50 - Train Loss: 0.032722 - Val Loss: 0.046595\n",
      "Epoch 42/50 - Train Loss: 0.031752 - Val Loss: 0.047338\n",
      "Epoch 43/50 - Train Loss: 0.032497 - Val Loss: 0.044737\n",
      "Epoch 44/50 - Train Loss: 0.028448 - Val Loss: 0.044056\n",
      "Epoch 45/50 - Train Loss: 0.035372 - Val Loss: 0.049413\n",
      "Epoch 46/50 - Train Loss: 0.029233 - Val Loss: 0.043218\n",
      "Epoch 47/50 - Train Loss: 0.030051 - Val Loss: 0.043107\n",
      "Epoch 48/50 - Train Loss: 0.028076 - Val Loss: 0.042988\n",
      "Epoch 49/50 - Train Loss: 0.026352 - Val Loss: 0.042897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:49:30,133] Trial 93 finished with value: 0.04289661906659603 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 166, 'lr': 0.005524759982998145, 'weight_decay': 1.5312319909991614e-06, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.024610 - Val Loss: 0.043103\n",
      "Epoch 1/50 - Train Loss: 0.066888 - Val Loss: 0.069353\n",
      "Epoch 2/50 - Train Loss: 0.043683 - Val Loss: 0.078423\n",
      "Epoch 3/50 - Train Loss: 0.038696 - Val Loss: 0.071776\n",
      "Epoch 4/50 - Train Loss: 0.038863 - Val Loss: 0.048589\n",
      "Epoch 5/50 - Train Loss: 0.035234 - Val Loss: 0.048886\n",
      "Epoch 6/50 - Train Loss: 0.038238 - Val Loss: 0.062129\n",
      "Epoch 7/50 - Train Loss: 0.032592 - Val Loss: 0.050059\n",
      "Epoch 8/50 - Train Loss: 0.030334 - Val Loss: 0.045229\n",
      "Epoch 9/50 - Train Loss: 0.031705 - Val Loss: 0.050760\n",
      "Epoch 10/50 - Train Loss: 0.031807 - Val Loss: 0.044812\n",
      "Epoch 11/50 - Train Loss: 0.029751 - Val Loss: 0.046428\n",
      "Epoch 12/50 - Train Loss: 0.033644 - Val Loss: 0.046607\n",
      "Epoch 13/50 - Train Loss: 0.032648 - Val Loss: 0.044196\n",
      "Epoch 14/50 - Train Loss: 0.028833 - Val Loss: 0.043655\n",
      "Epoch 15/50 - Train Loss: 0.029669 - Val Loss: 0.051700\n",
      "Epoch 16/50 - Train Loss: 0.030171 - Val Loss: 0.043663\n",
      "Epoch 17/50 - Train Loss: 0.028533 - Val Loss: 0.044853\n",
      "Epoch 18/50 - Train Loss: 0.029858 - Val Loss: 0.043767\n",
      "Epoch 19/50 - Train Loss: 0.029443 - Val Loss: 0.043359\n",
      "Epoch 20/50 - Train Loss: 0.028088 - Val Loss: 0.048298\n",
      "Epoch 21/50 - Train Loss: 0.032328 - Val Loss: 0.045291\n",
      "Epoch 22/50 - Train Loss: 0.028944 - Val Loss: 0.043408\n",
      "Epoch 23/50 - Train Loss: 0.029208 - Val Loss: 0.046528\n",
      "Epoch 24/50 - Train Loss: 0.032358 - Val Loss: 0.045079\n",
      "Epoch 25/50 - Train Loss: 0.027378 - Val Loss: 0.043426\n",
      "Epoch 26/50 - Train Loss: 0.033110 - Val Loss: 0.043753\n",
      "Epoch 27/50 - Train Loss: 0.032566 - Val Loss: 0.052374\n",
      "Epoch 28/50 - Train Loss: 0.029719 - Val Loss: 0.044560\n",
      "Epoch 29/50 - Train Loss: 0.028833 - Val Loss: 0.043773\n",
      "Epoch 30/50 - Train Loss: 0.029722 - Val Loss: 0.048155\n",
      "Epoch 31/50 - Train Loss: 0.031306 - Val Loss: 0.044462\n",
      "Epoch 32/50 - Train Loss: 0.031028 - Val Loss: 0.043549\n",
      "Epoch 33/50 - Train Loss: 0.027268 - Val Loss: 0.045813\n",
      "Epoch 34/50 - Train Loss: 0.028156 - Val Loss: 0.043761\n",
      "Epoch 35/50 - Train Loss: 0.029078 - Val Loss: 0.043487\n",
      "Epoch 36/50 - Train Loss: 0.028235 - Val Loss: 0.044791\n",
      "Epoch 37/50 - Train Loss: 0.030035 - Val Loss: 0.043416\n",
      "Epoch 38/50 - Train Loss: 0.030334 - Val Loss: 0.046065\n",
      "Epoch 39/50 - Train Loss: 0.030530 - Val Loss: 0.046257\n",
      "Epoch 40/50 - Train Loss: 0.026646 - Val Loss: 0.047397\n",
      "Epoch 41/50 - Train Loss: 0.028707 - Val Loss: 0.044225\n",
      "Epoch 42/50 - Train Loss: 0.028297 - Val Loss: 0.043497\n",
      "Epoch 43/50 - Train Loss: 0.029091 - Val Loss: 0.045686\n",
      "Epoch 44/50 - Train Loss: 0.027977 - Val Loss: 0.043639\n",
      "Epoch 45/50 - Train Loss: 0.028286 - Val Loss: 0.043987\n",
      "Epoch 46/50 - Train Loss: 0.026192 - Val Loss: 0.044538\n",
      "Epoch 47/50 - Train Loss: 0.030111 - Val Loss: 0.044271\n",
      "Epoch 48/50 - Train Loss: 0.027592 - Val Loss: 0.046128\n",
      "Epoch 49/50 - Train Loss: 0.027293 - Val Loss: 0.047620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:49:53,858] Trial 94 finished with value: 0.043358851224184036 and parameters: {'n_hidden_layers': 4, 'n_hidden_units': 128, 'lr': 0.002501255734935778, 'weight_decay': 7.088653397706867e-08, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029013 - Val Loss: 0.049137\n",
      "Epoch 1/50 - Train Loss: 0.149611 - Val Loss: 0.170541\n",
      "Epoch 2/50 - Train Loss: 0.083180 - Val Loss: 0.161409\n",
      "Epoch 3/50 - Train Loss: 0.050414 - Val Loss: 0.057621\n",
      "Epoch 4/50 - Train Loss: 0.049629 - Val Loss: 0.107441\n",
      "Epoch 5/50 - Train Loss: 0.050894 - Val Loss: 0.086763\n",
      "Epoch 6/50 - Train Loss: 0.044383 - Val Loss: 0.058431\n",
      "Epoch 7/50 - Train Loss: 0.039372 - Val Loss: 0.058687\n",
      "Epoch 8/50 - Train Loss: 0.034869 - Val Loss: 0.052636\n",
      "Epoch 9/50 - Train Loss: 0.033397 - Val Loss: 0.050264\n",
      "Epoch 10/50 - Train Loss: 0.032788 - Val Loss: 0.046144\n",
      "Epoch 11/50 - Train Loss: 0.032117 - Val Loss: 0.044631\n",
      "Epoch 12/50 - Train Loss: 0.028964 - Val Loss: 0.046624\n",
      "Epoch 13/50 - Train Loss: 0.029583 - Val Loss: 0.046856\n",
      "Epoch 14/50 - Train Loss: 0.028961 - Val Loss: 0.045633\n",
      "Epoch 15/50 - Train Loss: 0.030941 - Val Loss: 0.045204\n",
      "Epoch 16/50 - Train Loss: 0.030414 - Val Loss: 0.048352\n",
      "Epoch 17/50 - Train Loss: 0.029172 - Val Loss: 0.043589\n",
      "Epoch 18/50 - Train Loss: 0.028784 - Val Loss: 0.043311\n",
      "Epoch 19/50 - Train Loss: 0.029394 - Val Loss: 0.048219\n",
      "Epoch 20/50 - Train Loss: 0.031190 - Val Loss: 0.050474\n",
      "Epoch 21/50 - Train Loss: 0.029723 - Val Loss: 0.043555\n",
      "Epoch 22/50 - Train Loss: 0.028814 - Val Loss: 0.055875\n",
      "Epoch 23/50 - Train Loss: 0.028505 - Val Loss: 0.045797\n",
      "Epoch 24/50 - Train Loss: 0.029951 - Val Loss: 0.044556\n",
      "Epoch 25/50 - Train Loss: 0.026420 - Val Loss: 0.046104\n",
      "Epoch 26/50 - Train Loss: 0.031242 - Val Loss: 0.044032\n",
      "Epoch 27/50 - Train Loss: 0.029042 - Val Loss: 0.042709\n",
      "Epoch 28/50 - Train Loss: 0.029735 - Val Loss: 0.046756\n",
      "Epoch 29/50 - Train Loss: 0.028423 - Val Loss: 0.043827\n",
      "Epoch 30/50 - Train Loss: 0.029532 - Val Loss: 0.044501\n",
      "Epoch 31/50 - Train Loss: 0.025359 - Val Loss: 0.051298\n",
      "Epoch 32/50 - Train Loss: 0.028561 - Val Loss: 0.046954\n",
      "Epoch 33/50 - Train Loss: 0.027115 - Val Loss: 0.043631\n",
      "Epoch 34/50 - Train Loss: 0.028279 - Val Loss: 0.046171\n",
      "Epoch 35/50 - Train Loss: 0.030287 - Val Loss: 0.043101\n",
      "Epoch 36/50 - Train Loss: 0.025496 - Val Loss: 0.043377\n",
      "Epoch 37/50 - Train Loss: 0.028604 - Val Loss: 0.044156\n",
      "Epoch 38/50 - Train Loss: 0.027206 - Val Loss: 0.046398\n",
      "Epoch 39/50 - Train Loss: 0.023869 - Val Loss: 0.043884\n",
      "Epoch 40/50 - Train Loss: 0.031188 - Val Loss: 0.043370\n",
      "Epoch 41/50 - Train Loss: 0.033919 - Val Loss: 0.044597\n",
      "Epoch 42/50 - Train Loss: 0.026566 - Val Loss: 0.042798\n",
      "Epoch 43/50 - Train Loss: 0.025359 - Val Loss: 0.042753\n",
      "Epoch 44/50 - Train Loss: 0.026550 - Val Loss: 0.042981\n",
      "Epoch 45/50 - Train Loss: 0.026909 - Val Loss: 0.042885\n",
      "Epoch 46/50 - Train Loss: 0.028389 - Val Loss: 0.043215\n",
      "Epoch 47/50 - Train Loss: 0.027801 - Val Loss: 0.044631\n",
      "Epoch 48/50 - Train Loss: 0.031130 - Val Loss: 0.044295\n",
      "Epoch 49/50 - Train Loss: 0.028090 - Val Loss: 0.045078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:50:22,037] Trial 95 finished with value: 0.042708832770586014 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 160, 'lr': 0.0031384074530100754, 'weight_decay': 1.0431659207063481e-07, 'batch_size': 16}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.029506 - Val Loss: 0.046655\n",
      "Epoch 1/50 - Train Loss: 0.230953 - Val Loss: 0.231602\n",
      "Epoch 2/50 - Train Loss: 0.073180 - Val Loss: 0.077715\n",
      "Epoch 3/50 - Train Loss: 0.058513 - Val Loss: 0.132335\n",
      "Epoch 4/50 - Train Loss: 0.052100 - Val Loss: 0.099484\n",
      "Epoch 5/50 - Train Loss: 0.045568 - Val Loss: 0.066608\n",
      "Epoch 6/50 - Train Loss: 0.041720 - Val Loss: 0.079223\n",
      "Epoch 7/50 - Train Loss: 0.037856 - Val Loss: 0.086130\n",
      "Epoch 8/50 - Train Loss: 0.036126 - Val Loss: 0.069827\n",
      "Epoch 9/50 - Train Loss: 0.035953 - Val Loss: 0.061531\n",
      "Epoch 10/50 - Train Loss: 0.034996 - Val Loss: 0.065462\n",
      "Epoch 11/50 - Train Loss: 0.033724 - Val Loss: 0.053200\n",
      "Epoch 12/50 - Train Loss: 0.032693 - Val Loss: 0.050914\n",
      "Epoch 13/50 - Train Loss: 0.032965 - Val Loss: 0.055385\n",
      "Epoch 14/50 - Train Loss: 0.028860 - Val Loss: 0.046831\n",
      "Epoch 15/50 - Train Loss: 0.033170 - Val Loss: 0.053081\n",
      "Epoch 16/50 - Train Loss: 0.028884 - Val Loss: 0.046018\n",
      "Epoch 17/50 - Train Loss: 0.028661 - Val Loss: 0.049457\n",
      "Epoch 18/50 - Train Loss: 0.034153 - Val Loss: 0.045065\n",
      "Epoch 19/50 - Train Loss: 0.033811 - Val Loss: 0.048282\n",
      "Epoch 20/50 - Train Loss: 0.031560 - Val Loss: 0.047197\n",
      "Epoch 21/50 - Train Loss: 0.031246 - Val Loss: 0.044613\n",
      "Epoch 22/50 - Train Loss: 0.027495 - Val Loss: 0.052989\n",
      "Epoch 23/50 - Train Loss: 0.032373 - Val Loss: 0.044514\n",
      "Epoch 24/50 - Train Loss: 0.034371 - Val Loss: 0.053071\n",
      "Epoch 25/50 - Train Loss: 0.029533 - Val Loss: 0.048954\n",
      "Epoch 26/50 - Train Loss: 0.037975 - Val Loss: 0.048077\n",
      "Epoch 27/50 - Train Loss: 0.034989 - Val Loss: 0.049542\n",
      "Epoch 28/50 - Train Loss: 0.031466 - Val Loss: 0.045321\n",
      "Epoch 29/50 - Train Loss: 0.032528 - Val Loss: 0.051248\n",
      "Epoch 30/50 - Train Loss: 0.030980 - Val Loss: 0.044283\n",
      "Epoch 31/50 - Train Loss: 0.028026 - Val Loss: 0.049487\n",
      "Epoch 32/50 - Train Loss: 0.033609 - Val Loss: 0.044688\n",
      "Epoch 33/50 - Train Loss: 0.032449 - Val Loss: 0.046679\n",
      "Epoch 34/50 - Train Loss: 0.033897 - Val Loss: 0.045280\n",
      "Epoch 35/50 - Train Loss: 0.027985 - Val Loss: 0.045893\n",
      "Epoch 36/50 - Train Loss: 0.031197 - Val Loss: 0.046173\n",
      "Epoch 37/50 - Train Loss: 0.029425 - Val Loss: 0.045681\n",
      "Epoch 38/50 - Train Loss: 0.032313 - Val Loss: 0.047750\n",
      "Epoch 39/50 - Train Loss: 0.036094 - Val Loss: 0.048520\n",
      "Epoch 40/50 - Train Loss: 0.030733 - Val Loss: 0.046010\n",
      "Epoch 41/50 - Train Loss: 0.027915 - Val Loss: 0.045788\n",
      "Epoch 42/50 - Train Loss: 0.029114 - Val Loss: 0.045916\n",
      "Epoch 43/50 - Train Loss: 0.029038 - Val Loss: 0.045716\n",
      "Epoch 44/50 - Train Loss: 0.026488 - Val Loss: 0.045145\n",
      "Epoch 45/50 - Train Loss: 0.029886 - Val Loss: 0.047896\n",
      "Epoch 46/50 - Train Loss: 0.032541 - Val Loss: 0.045114\n",
      "Epoch 47/50 - Train Loss: 0.029283 - Val Loss: 0.046493\n",
      "Epoch 48/50 - Train Loss: 0.030553 - Val Loss: 0.047791\n",
      "Epoch 49/50 - Train Loss: 0.028053 - Val Loss: 0.046733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:50:41,527] Trial 96 finished with value: 0.04428257793188095 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 151, 'lr': 0.0017312193009700306, 'weight_decay': 3.3033583407583476e-07, 'batch_size': 32}. Best is trial 72 with value: 0.042499855160713196.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.031019 - Val Loss: 0.047138\n",
      "Epoch 1/50 - Train Loss: 0.067735 - Val Loss: 0.137215\n",
      "Epoch 2/50 - Train Loss: 0.045617 - Val Loss: 0.063743\n",
      "Epoch 3/50 - Train Loss: 0.040494 - Val Loss: 0.052800\n",
      "Epoch 4/50 - Train Loss: 0.034572 - Val Loss: 0.075618\n",
      "Epoch 5/50 - Train Loss: 0.038258 - Val Loss: 0.066384\n",
      "Epoch 6/50 - Train Loss: 0.036315 - Val Loss: 0.044690\n",
      "Epoch 7/50 - Train Loss: 0.030464 - Val Loss: 0.046974\n",
      "Epoch 8/50 - Train Loss: 0.032511 - Val Loss: 0.043584\n",
      "Epoch 9/50 - Train Loss: 0.031777 - Val Loss: 0.043761\n",
      "Epoch 10/50 - Train Loss: 0.033283 - Val Loss: 0.043766\n",
      "Epoch 11/50 - Train Loss: 0.036140 - Val Loss: 0.054610\n",
      "Epoch 12/50 - Train Loss: 0.033559 - Val Loss: 0.081904\n",
      "Epoch 13/50 - Train Loss: 0.035608 - Val Loss: 0.043537\n",
      "Epoch 14/50 - Train Loss: 0.032609 - Val Loss: 0.048356\n",
      "Epoch 15/50 - Train Loss: 0.027373 - Val Loss: 0.052314\n",
      "Epoch 16/50 - Train Loss: 0.031967 - Val Loss: 0.043061\n",
      "Epoch 17/50 - Train Loss: 0.027848 - Val Loss: 0.049174\n",
      "Epoch 18/50 - Train Loss: 0.032964 - Val Loss: 0.043430\n",
      "Epoch 19/50 - Train Loss: 0.032639 - Val Loss: 0.042566\n",
      "Epoch 20/50 - Train Loss: 0.033829 - Val Loss: 0.043914\n",
      "Epoch 21/50 - Train Loss: 0.026873 - Val Loss: 0.043398\n",
      "Epoch 22/50 - Train Loss: 0.030458 - Val Loss: 0.047051\n",
      "Epoch 23/50 - Train Loss: 0.027342 - Val Loss: 0.054943\n",
      "Epoch 24/50 - Train Loss: 0.027168 - Val Loss: 0.043123\n",
      "Epoch 25/50 - Train Loss: 0.029576 - Val Loss: 0.044209\n",
      "Epoch 26/50 - Train Loss: 0.028295 - Val Loss: 0.044182\n",
      "Epoch 27/50 - Train Loss: 0.030731 - Val Loss: 0.043828\n",
      "Epoch 28/50 - Train Loss: 0.028754 - Val Loss: 0.046044\n",
      "Epoch 29/50 - Train Loss: 0.028521 - Val Loss: 0.042627\n",
      "Epoch 30/50 - Train Loss: 0.028810 - Val Loss: 0.043717\n",
      "Epoch 31/50 - Train Loss: 0.031216 - Val Loss: 0.045020\n",
      "Epoch 32/50 - Train Loss: 0.029657 - Val Loss: 0.044176\n",
      "Epoch 33/50 - Train Loss: 0.027504 - Val Loss: 0.042620\n",
      "Epoch 34/50 - Train Loss: 0.029343 - Val Loss: 0.042338\n",
      "Epoch 35/50 - Train Loss: 0.029687 - Val Loss: 0.046692\n",
      "Epoch 36/50 - Train Loss: 0.030621 - Val Loss: 0.043358\n",
      "Epoch 37/50 - Train Loss: 0.034004 - Val Loss: 0.042609\n",
      "Epoch 38/50 - Train Loss: 0.027950 - Val Loss: 0.045610\n",
      "Epoch 39/50 - Train Loss: 0.026939 - Val Loss: 0.044271\n",
      "Epoch 40/50 - Train Loss: 0.026526 - Val Loss: 0.042771\n",
      "Epoch 41/50 - Train Loss: 0.029673 - Val Loss: 0.043706\n",
      "Epoch 42/50 - Train Loss: 0.031854 - Val Loss: 0.042932\n",
      "Epoch 43/50 - Train Loss: 0.029130 - Val Loss: 0.042920\n",
      "Epoch 44/50 - Train Loss: 0.031155 - Val Loss: 0.043461\n",
      "Epoch 45/50 - Train Loss: 0.025948 - Val Loss: 0.042865\n",
      "Epoch 46/50 - Train Loss: 0.031030 - Val Loss: 0.049713\n",
      "Epoch 47/50 - Train Loss: 0.027336 - Val Loss: 0.043373\n",
      "Epoch 48/50 - Train Loss: 0.028404 - Val Loss: 0.046052\n",
      "Epoch 49/50 - Train Loss: 0.028752 - Val Loss: 0.043784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:51:13,342] Trial 97 finished with value: 0.042338233441114426 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 202, 'lr': 0.0030908578887602826, 'weight_decay': 1.2191937471295825e-07, 'batch_size': 16}. Best is trial 97 with value: 0.042338233441114426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.030764 - Val Loss: 0.048785\n",
      "Epoch 1/50 - Train Loss: 0.139773 - Val Loss: 0.062924\n",
      "Epoch 2/50 - Train Loss: 0.052887 - Val Loss: 0.121125\n",
      "Epoch 3/50 - Train Loss: 0.046123 - Val Loss: 0.057113\n",
      "Epoch 4/50 - Train Loss: 0.039416 - Val Loss: 0.080374\n",
      "Epoch 5/50 - Train Loss: 0.038590 - Val Loss: 0.057173\n",
      "Epoch 6/50 - Train Loss: 0.035431 - Val Loss: 0.058237\n",
      "Epoch 7/50 - Train Loss: 0.033586 - Val Loss: 0.047031\n",
      "Epoch 8/50 - Train Loss: 0.027849 - Val Loss: 0.045527\n",
      "Epoch 9/50 - Train Loss: 0.029084 - Val Loss: 0.055028\n",
      "Epoch 10/50 - Train Loss: 0.034307 - Val Loss: 0.045223\n",
      "Epoch 11/50 - Train Loss: 0.029703 - Val Loss: 0.046336\n",
      "Epoch 12/50 - Train Loss: 0.031614 - Val Loss: 0.044448\n",
      "Epoch 13/50 - Train Loss: 0.029068 - Val Loss: 0.044338\n",
      "Epoch 14/50 - Train Loss: 0.028719 - Val Loss: 0.051361\n",
      "Epoch 15/50 - Train Loss: 0.030187 - Val Loss: 0.046332\n",
      "Epoch 16/50 - Train Loss: 0.029034 - Val Loss: 0.044742\n",
      "Epoch 17/50 - Train Loss: 0.032042 - Val Loss: 0.044624\n",
      "Epoch 18/50 - Train Loss: 0.031556 - Val Loss: 0.048021\n",
      "Epoch 19/50 - Train Loss: 0.030141 - Val Loss: 0.046734\n",
      "Epoch 20/50 - Train Loss: 0.028869 - Val Loss: 0.045841\n",
      "Epoch 21/50 - Train Loss: 0.026356 - Val Loss: 0.044181\n",
      "Epoch 22/50 - Train Loss: 0.030579 - Val Loss: 0.043661\n",
      "Epoch 23/50 - Train Loss: 0.028881 - Val Loss: 0.044570\n",
      "Epoch 24/50 - Train Loss: 0.029007 - Val Loss: 0.047514\n",
      "Epoch 25/50 - Train Loss: 0.030599 - Val Loss: 0.044375\n",
      "Epoch 26/50 - Train Loss: 0.030679 - Val Loss: 0.043647\n",
      "Epoch 27/50 - Train Loss: 0.030381 - Val Loss: 0.055401\n",
      "Epoch 28/50 - Train Loss: 0.030782 - Val Loss: 0.043813\n",
      "Epoch 29/50 - Train Loss: 0.030996 - Val Loss: 0.043537\n",
      "Epoch 30/50 - Train Loss: 0.031623 - Val Loss: 0.046100\n",
      "Epoch 31/50 - Train Loss: 0.029073 - Val Loss: 0.043559\n",
      "Epoch 32/50 - Train Loss: 0.030939 - Val Loss: 0.045262\n",
      "Epoch 33/50 - Train Loss: 0.026364 - Val Loss: 0.045523\n",
      "Epoch 34/50 - Train Loss: 0.030687 - Val Loss: 0.043528\n",
      "Epoch 35/50 - Train Loss: 0.030397 - Val Loss: 0.046212\n",
      "Epoch 36/50 - Train Loss: 0.030569 - Val Loss: 0.049888\n",
      "Epoch 37/50 - Train Loss: 0.031376 - Val Loss: 0.043665\n",
      "Epoch 38/50 - Train Loss: 0.031970 - Val Loss: 0.044186\n",
      "Epoch 39/50 - Train Loss: 0.033346 - Val Loss: 0.044388\n",
      "Epoch 40/50 - Train Loss: 0.029939 - Val Loss: 0.051669\n",
      "Epoch 41/50 - Train Loss: 0.027212 - Val Loss: 0.044371\n",
      "Epoch 42/50 - Train Loss: 0.028549 - Val Loss: 0.043663\n",
      "Epoch 43/50 - Train Loss: 0.031092 - Val Loss: 0.044731\n",
      "Epoch 44/50 - Train Loss: 0.027672 - Val Loss: 0.043764\n",
      "Epoch 45/50 - Train Loss: 0.030405 - Val Loss: 0.044030\n",
      "Epoch 46/50 - Train Loss: 0.027939 - Val Loss: 0.045460\n",
      "Epoch 47/50 - Train Loss: 0.027486 - Val Loss: 0.043969\n",
      "Epoch 48/50 - Train Loss: 0.030718 - Val Loss: 0.044301\n",
      "Epoch 49/50 - Train Loss: 0.028111 - Val Loss: 0.044329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:51:46,631] Trial 98 finished with value: 0.04352760873734951 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 226, 'lr': 0.0010405986069050926, 'weight_decay': 1.3941001072487548e-07, 'batch_size': 16}. Best is trial 97 with value: 0.042338233441114426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028514 - Val Loss: 0.046537\n",
      "Epoch 1/50 - Train Loss: 0.064576 - Val Loss: 0.077747\n",
      "Epoch 2/50 - Train Loss: 0.043013 - Val Loss: 0.090350\n",
      "Epoch 3/50 - Train Loss: 0.038714 - Val Loss: 0.054948\n",
      "Epoch 4/50 - Train Loss: 0.038864 - Val Loss: 0.046889\n",
      "Epoch 5/50 - Train Loss: 0.034117 - Val Loss: 0.053923\n",
      "Epoch 6/50 - Train Loss: 0.036771 - Val Loss: 0.053955\n",
      "Epoch 7/50 - Train Loss: 0.028781 - Val Loss: 0.048512\n",
      "Epoch 8/50 - Train Loss: 0.034797 - Val Loss: 0.047540\n",
      "Epoch 9/50 - Train Loss: 0.031268 - Val Loss: 0.045682\n",
      "Epoch 10/50 - Train Loss: 0.033759 - Val Loss: 0.043646\n",
      "Epoch 11/50 - Train Loss: 0.037235 - Val Loss: 0.043989\n",
      "Epoch 12/50 - Train Loss: 0.034583 - Val Loss: 0.045597\n",
      "Epoch 13/50 - Train Loss: 0.032455 - Val Loss: 0.044939\n",
      "Epoch 14/50 - Train Loss: 0.025920 - Val Loss: 0.044775\n",
      "Epoch 15/50 - Train Loss: 0.031751 - Val Loss: 0.050231\n",
      "Epoch 16/50 - Train Loss: 0.029224 - Val Loss: 0.058708\n",
      "Epoch 17/50 - Train Loss: 0.034432 - Val Loss: 0.049249\n",
      "Epoch 18/50 - Train Loss: 0.034271 - Val Loss: 0.045629\n",
      "Epoch 19/50 - Train Loss: 0.033771 - Val Loss: 0.044963\n",
      "Epoch 20/50 - Train Loss: 0.033173 - Val Loss: 0.045813\n",
      "Epoch 21/50 - Train Loss: 0.029297 - Val Loss: 0.056765\n",
      "Epoch 22/50 - Train Loss: 0.031427 - Val Loss: 0.043485\n",
      "Epoch 23/50 - Train Loss: 0.028232 - Val Loss: 0.045236\n",
      "Epoch 24/50 - Train Loss: 0.033069 - Val Loss: 0.056280\n",
      "Epoch 25/50 - Train Loss: 0.032873 - Val Loss: 0.047272\n",
      "Epoch 26/50 - Train Loss: 0.027605 - Val Loss: 0.044031\n",
      "Epoch 27/50 - Train Loss: 0.026340 - Val Loss: 0.049245\n",
      "Epoch 28/50 - Train Loss: 0.031880 - Val Loss: 0.048521\n",
      "Epoch 29/50 - Train Loss: 0.029402 - Val Loss: 0.044966\n",
      "Epoch 30/50 - Train Loss: 0.029007 - Val Loss: 0.053590\n",
      "Epoch 31/50 - Train Loss: 0.031453 - Val Loss: 0.053647\n",
      "Epoch 32/50 - Train Loss: 0.030196 - Val Loss: 0.043685\n",
      "Epoch 33/50 - Train Loss: 0.031426 - Val Loss: 0.044438\n",
      "Epoch 34/50 - Train Loss: 0.029471 - Val Loss: 0.046152\n",
      "Epoch 35/50 - Train Loss: 0.030187 - Val Loss: 0.043197\n",
      "Epoch 36/50 - Train Loss: 0.026347 - Val Loss: 0.043307\n",
      "Epoch 37/50 - Train Loss: 0.030499 - Val Loss: 0.052162\n",
      "Epoch 38/50 - Train Loss: 0.027452 - Val Loss: 0.044369\n",
      "Epoch 39/50 - Train Loss: 0.030135 - Val Loss: 0.045524\n",
      "Epoch 40/50 - Train Loss: 0.027342 - Val Loss: 0.044413\n",
      "Epoch 41/50 - Train Loss: 0.026898 - Val Loss: 0.043521\n",
      "Epoch 42/50 - Train Loss: 0.029527 - Val Loss: 0.043558\n",
      "Epoch 43/50 - Train Loss: 0.028219 - Val Loss: 0.043667\n",
      "Epoch 44/50 - Train Loss: 0.026163 - Val Loss: 0.051800\n",
      "Epoch 45/50 - Train Loss: 0.029062 - Val Loss: 0.046545\n",
      "Epoch 46/50 - Train Loss: 0.031595 - Val Loss: 0.048193\n",
      "Epoch 47/50 - Train Loss: 0.030935 - Val Loss: 0.055105\n",
      "Epoch 48/50 - Train Loss: 0.031319 - Val Loss: 0.051461\n",
      "Epoch 49/50 - Train Loss: 0.032998 - Val Loss: 0.044026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 00:52:18,632] Trial 99 finished with value: 0.043197497725486755 and parameters: {'n_hidden_layers': 5, 'n_hidden_units': 203, 'lr': 0.0035035645918513533, 'weight_decay': 2.0329188919215605e-07, 'batch_size': 16}. Best is trial 97 with value: 0.042338233441114426.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 - Train Loss: 0.028830 - Val Loss: 0.044556\n",
      "Best Hyperparameters: {'n_hidden_layers': 5, 'n_hidden_units': 202, 'lr': 0.0030908578887602826, 'weight_decay': 1.2191937471295825e-07, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameters to search over\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 5)\n",
    "    n_hidden_units = trial.suggest_int(\"n_hidden_units\", 32, 256)\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-8, 1e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])  # Match the original hp['batch_sz']\n",
    "\n",
    "    # Create train & validation loaders (following the original code)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize MLP model\n",
    "    model = BasicMLP(\n",
    "        N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "        N_HIDDEN_LAYERS=n_hidden_layers,\n",
    "        N_HIDDEN_UNITS=n_hidden_units,\n",
    "        N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "        loss_function=LOSS_FUNC,\n",
    "    )\n",
    "\n",
    "    # Train and return validation loss\n",
    "    val_loss = model.train_model(train_loader, val_loader, epochs=50, lr=lr, weight_decay=weight_decay, device=device)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=\"mlp_hyperparameter_optimization_phy12\", storage=\"sqlite:///mlp_hyperparameter_optimization_phy.db\", load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.134501 - Val Loss: 0.184140\n",
      "Epoch 2/50 - Train Loss: 0.065482 - Val Loss: 0.085816\n",
      "Epoch 3/50 - Train Loss: 0.049851 - Val Loss: 0.076358\n",
      "Epoch 4/50 - Train Loss: 0.046105 - Val Loss: 0.092902\n",
      "Epoch 5/50 - Train Loss: 0.044509 - Val Loss: 0.075473\n",
      "Epoch 6/50 - Train Loss: 0.039025 - Val Loss: 0.068223\n",
      "Epoch 7/50 - Train Loss: 0.033124 - Val Loss: 0.054876\n",
      "Epoch 8/50 - Train Loss: 0.031373 - Val Loss: 0.053747\n",
      "Epoch 9/50 - Train Loss: 0.029960 - Val Loss: 0.049394\n",
      "Epoch 10/50 - Train Loss: 0.030145 - Val Loss: 0.049671\n",
      "Epoch 11/50 - Train Loss: 0.031745 - Val Loss: 0.045235\n",
      "Epoch 12/50 - Train Loss: 0.028513 - Val Loss: 0.048130\n",
      "Epoch 13/50 - Train Loss: 0.031109 - Val Loss: 0.046529\n",
      "Epoch 14/50 - Train Loss: 0.031802 - Val Loss: 0.043908\n",
      "Epoch 15/50 - Train Loss: 0.029657 - Val Loss: 0.044213\n",
      "Epoch 16/50 - Train Loss: 0.030727 - Val Loss: 0.046160\n",
      "Epoch 17/50 - Train Loss: 0.029687 - Val Loss: 0.043333\n",
      "Epoch 18/50 - Train Loss: 0.027721 - Val Loss: 0.043228\n",
      "Epoch 19/50 - Train Loss: 0.031443 - Val Loss: 0.046607\n",
      "Epoch 20/50 - Train Loss: 0.029604 - Val Loss: 0.044425\n",
      "Epoch 21/50 - Train Loss: 0.029160 - Val Loss: 0.048709\n",
      "Epoch 22/50 - Train Loss: 0.031700 - Val Loss: 0.045801\n",
      "Epoch 23/50 - Train Loss: 0.031870 - Val Loss: 0.043817\n",
      "Epoch 24/50 - Train Loss: 0.029110 - Val Loss: 0.044390\n",
      "Epoch 25/50 - Train Loss: 0.030406 - Val Loss: 0.042980\n",
      "Epoch 26/50 - Train Loss: 0.030215 - Val Loss: 0.047785\n",
      "Epoch 27/50 - Train Loss: 0.031559 - Val Loss: 0.056115\n",
      "Epoch 28/50 - Train Loss: 0.034532 - Val Loss: 0.051555\n",
      "Epoch 29/50 - Train Loss: 0.029937 - Val Loss: 0.049903\n",
      "Epoch 30/50 - Train Loss: 0.035003 - Val Loss: 0.053088\n",
      "Epoch 31/50 - Train Loss: 0.031786 - Val Loss: 0.045908\n",
      "Epoch 32/50 - Train Loss: 0.028995 - Val Loss: 0.043406\n",
      "Epoch 33/50 - Train Loss: 0.032109 - Val Loss: 0.043412\n",
      "Epoch 34/50 - Train Loss: 0.029762 - Val Loss: 0.045562\n",
      "Epoch 35/50 - Train Loss: 0.029885 - Val Loss: 0.042734\n",
      "Epoch 36/50 - Train Loss: 0.029743 - Val Loss: 0.055104\n",
      "Epoch 37/50 - Train Loss: 0.031249 - Val Loss: 0.047376\n",
      "Epoch 38/50 - Train Loss: 0.031662 - Val Loss: 0.042894\n",
      "Epoch 39/50 - Train Loss: 0.025930 - Val Loss: 0.045142\n",
      "Epoch 40/50 - Train Loss: 0.029153 - Val Loss: 0.045047\n",
      "Epoch 41/50 - Train Loss: 0.027583 - Val Loss: 0.043582\n",
      "Epoch 42/50 - Train Loss: 0.027906 - Val Loss: 0.053057\n",
      "Epoch 43/50 - Train Loss: 0.030631 - Val Loss: 0.044657\n",
      "Epoch 44/50 - Train Loss: 0.027508 - Val Loss: 0.042739\n",
      "Epoch 45/50 - Train Loss: 0.030112 - Val Loss: 0.043104\n",
      "Epoch 46/50 - Train Loss: 0.028955 - Val Loss: 0.045274\n",
      "Epoch 47/50 - Train Loss: 0.027516 - Val Loss: 0.043480\n",
      "Epoch 48/50 - Train Loss: 0.028326 - Val Loss: 0.042842\n",
      "Epoch 49/50 - Train Loss: 0.027962 - Val Loss: 0.046503\n",
      "Epoch 50/50 - Train Loss: 0.026951 - Val Loss: 0.043778\n",
      "Model saved as best_mlp_no2.pth in Model folder\n"
     ]
    }
   ],
   "source": [
    "best_params = {'n_hidden_layers': 5, 'n_hidden_units': 202, 'lr': 0.0030908578887602826, 'weight_decay': 1.2191937471295825e-07, 'batch_size': 16}\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = BasicMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=best_params[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=best_params[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "    loss_function=LOSS_FUNC,\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "best_model.train_model(train_loader, val_loader, epochs=50, lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"], device=device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(best_model.state_dict(), f\"{MODEL_PATH}/best_mlp_no2.pth\")\n",
    "print(\"Model saved as best_mlp_no2.pth in Model folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO2</td>\n",
       "      <td>1.37</td>\n",
       "      <td>93.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0   min    max\n",
       "0        NO2  1.37  93.95"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "df_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE Loss: 176.053607\n",
      "Test RMSE Loss: 14.748276\n",
      "Test SMAPE Loss: 37.653960%\n"
     ]
    }
   ],
   "source": [
    "best_model.load_state_dict(torch.load(f\"{MODEL_PATH}/best_mlp_no2.pth\"))\n",
    "best_model.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "mse, rmse_val, smape_val = best_model.test_model(test_loader, min_value=min_value, max_value=max_value, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 176.0536072608543, Test RMSE: 14.748276010799184, Test SMAPE: 37.653960122002495\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test MSE: {mse}, Test RMSE: {rmse_val}, Test SMAPE: {smape_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "# Iterate through the test set and collect predictions & ground truth\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x_test, y_true = batch  # Get input and ground truth\n",
    "        x_test = x_test.to(\"cpu\")  # Ensure data is on CPU if needed\n",
    "\n",
    "        # Get predictions\n",
    "        y_pred = best_model(x_test)\n",
    "\n",
    "        # Store results\n",
    "        y_preds.append(y_pred.cpu())\n",
    "        y_trues.append(y_true.cpu())\n",
    "\n",
    "# Convert lists to tensors\n",
    "y_preds = torch.cat(y_preds, dim=0).numpy()\n",
    "y_trues = torch.cat(y_trues, dim=0).numpy()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_trues.flatten(), label=\"Ground Truth (NO)\", linestyle=\"-\", color=\"blue\")\n",
    "plt.scatter(range(len(y_preds.flatten())), y_preds.flatten(), label=\"Predictions\", color=\"black\", s=10)\n",
    "\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"NO Level\")\n",
    "plt.title(\"Predictions vs. Ground Truth\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MSE Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_baseline = {\"n_hidden_layers\": 4, \"n_hidden_units\": 194, \"lr\": 0.0025532081590650484, \"weight_decay\": 1.3029599229861795e-08, \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.056461 - Val Loss: 0.078200\n",
      "Epoch 2/50 - Train Loss: 0.033327 - Val Loss: 0.034181\n",
      "Epoch 3/50 - Train Loss: 0.024612 - Val Loss: 0.036997\n",
      "Epoch 4/50 - Train Loss: 0.020388 - Val Loss: 0.027525\n",
      "Epoch 5/50 - Train Loss: 0.017908 - Val Loss: 0.028525\n",
      "Epoch 6/50 - Train Loss: 0.016590 - Val Loss: 0.028329\n",
      "Epoch 7/50 - Train Loss: 0.018531 - Val Loss: 0.022859\n",
      "Epoch 8/50 - Train Loss: 0.017043 - Val Loss: 0.021024\n",
      "Epoch 9/50 - Train Loss: 0.015320 - Val Loss: 0.021388\n",
      "Epoch 10/50 - Train Loss: 0.014659 - Val Loss: 0.020307\n",
      "Epoch 11/50 - Train Loss: 0.015040 - Val Loss: 0.022117\n",
      "Epoch 12/50 - Train Loss: 0.014961 - Val Loss: 0.022487\n",
      "Epoch 13/50 - Train Loss: 0.015082 - Val Loss: 0.023689\n",
      "Epoch 14/50 - Train Loss: 0.014757 - Val Loss: 0.021779\n",
      "Epoch 15/50 - Train Loss: 0.015012 - Val Loss: 0.020729\n",
      "Epoch 16/50 - Train Loss: 0.014366 - Val Loss: 0.020447\n",
      "Epoch 17/50 - Train Loss: 0.014167 - Val Loss: 0.021481\n",
      "Epoch 18/50 - Train Loss: 0.014991 - Val Loss: 0.021874\n",
      "Epoch 19/50 - Train Loss: 0.015447 - Val Loss: 0.020984\n",
      "Epoch 20/50 - Train Loss: 0.016376 - Val Loss: 0.025494\n",
      "Epoch 21/50 - Train Loss: 0.014786 - Val Loss: 0.025825\n",
      "Epoch 22/50 - Train Loss: 0.015110 - Val Loss: 0.022120\n",
      "Epoch 23/50 - Train Loss: 0.013966 - Val Loss: 0.020869\n",
      "Epoch 24/50 - Train Loss: 0.014886 - Val Loss: 0.020164\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbest_params_baseline[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mbest_model_baseline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params_baseline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbest_params_baseline\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_model_baseline\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/best_mlp_no2_baseline.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/forecasting_smog_PEML/src/modelling/MLP.py:51\u001b[0m, in \u001b[0;36mBasicMLP.train_model\u001b[0;34m(self, train_loader, val_loader, epochs, lr, weight_decay, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(u)\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(output, y, u, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function)  \u001b[38;5;66;03m# Compute loss based on selected function\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/forecasting_smog_PEML/venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/forecasting_smog_PEML/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/forecasting_smog_PEML/venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "best_model_baseline = BasicMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=best_params_baseline[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=best_params_baseline[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__()\n",
    "    loss_function=\"MSE\",\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params_baseline[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params_baseline[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "best_model_baseline.train_model(train_loader, val_loader, epochs=50, lr=best_params_baseline[\"lr\"], weight_decay=best_params_baseline[\"weight_decay\"], device=device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(best_model_baseline.state_dict(), f\"{MODEL_PATH}/best_mlp_no2_baseline.pth\")\n",
    "print(\"Model saved as best_mlp_no2_baseline.pth in Model folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_baseline.load_state_dict(torch.load(f\"{MODEL_PATH}/best_mlp_no2_baseline.pth\"))\n",
    "best_model_baseline.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params_baseline[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "mse_baseline, rmse_val_baseline, smape_val_baseline = best_model_baseline.test_model(test_loader, min_value=min_value, max_value=max_value, device=\"cpu\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
