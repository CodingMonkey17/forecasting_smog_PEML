{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline Simple MLP with just MSE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a Optuna hyperparameters optimisation and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n",
    "## **IMPORTANT NOTE**: \n",
    "- do preprocessing from ``preprocess.ipynb`` to obtain data in ``data/data_combined``, before starting this notebook\n",
    "- make sure the notebook is under ``src`` directory before running!\n",
    "- change the global variables defined below for the desired years of data, loss function and NN type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "\n",
    "from modelling.MLP import BasicMLP\n",
    "from modelling import *\n",
    "\n",
    "\n",
    "import optuna\n",
    "import threading\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/rachel/forecasting_smog_PEML/src')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/home/rachel/forecasting_smog_PEML/src/config.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MODIFY THESE GLOBAL VARIABLES FOR YOUR MODEL SCENARIO\n",
    "all other variables are defined in config.py\n",
    "\n",
    "LOSS_FUNC: choose from \n",
    "- MSE\n",
    "- LinearShift_MSE\n",
    "- PDE_nmer_const\n",
    "- PDE_nmer_piece\n",
    "- PINN\n",
    "\n",
    "CITY: choose from\n",
    "- Utrecht\n",
    "- Amsterdam (for testing transferability)\n",
    "- Multi (extended area around utrecht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this according to the data you want to use\n",
    "YEARS = [2017, 2018, 2020, 2021, 2022, 2023]\n",
    "TRAIN_YEARS = [2017, 2018, 2020, 2021, 2022]\n",
    "VAL_YEARS = [2021, 2022, 2023]\n",
    "TEST_YEARS = [2021, 2022, 2023]\n",
    "\n",
    "# for loss.py and also naming of file names\n",
    "LOSS_FUNC = \"MSE\" # choose from the above list\n",
    "NN_TYPE = \"MLP\" \n",
    "CITY = 'Utrecht' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Generation of paths and filenames according to data years, loss func, NN type\n",
    "- will be used throughout the whole notebook\n",
    "- check ``config.py`` for global variables defined outside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years:  allyears\n",
      "idx_dict:  {'NO2_TUINDORP_IDX': 5, 'NO2_BREUKELEN_IDX': 4, 'WIND_DIR_IDX': 0, 'WIND_SPEED_IDX': 2}\n",
      "station_names:  ['tuindorp', 'breukelen']\n",
      "main_station:  breukelen\n",
      "RESULTS_PATH:  /home/rachel/forecasting_smog_PEML/src/results/Utrecht\n",
      "MODEL_PATH:  /home/rachel/forecasting_smog_PEML/src/results/Utrecht/models\n",
      "MINMAX_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/Utrecht/all_years/pollutants_minmax_allyears.csv\n",
      "DATASET_PATH:  /home/rachel/forecasting_smog_PEML/data/data_combined/Utrecht/all_years\n",
      "Y_PHY_FILENAME:  y_phy_batchsize16_MSE_allyears_Utrecht\n",
      "MODEL_PATH_NAME:  best_MLP_no2_MSE_allyears_Utrecht.pth\n",
      "RESULTS_METRICS_FILENAME:  results_MLP_no2_MSE_allyears_Utrecht.csv\n",
      "BESTPARAMS_FILENAME:  best_params_MLP_no2_MSE_allyears_Utrecht.txt\n",
      "PLOT_FILENAME:  plot_MLP_no2_MSE_allyears_Utrecht.png\n"
     ]
    }
   ],
   "source": [
    "years, idx_dict , station_names, main_station, RESULTS_PATH, MODEL_PATH, DATASET_PATH, MINMAX_PATH, Y_PHY_FILENAME,  MODEL_PATH_NAME,RESULTS_METRICS_FILENAME, BESTPARAMS_FILENAME, PLOT_FILENAME  = init_paths(CITY, YEARS, LOSS_FUNC, NN_TYPE)\n",
    "print(\"years: \", years)\n",
    "print(\"idx_dict: \", idx_dict)\n",
    "print(\"station_names: \", station_names)\n",
    "print(\"main_station: \", main_station)\n",
    "print(\"RESULTS_PATH: \", RESULTS_PATH)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "print(\"DATASET_PATH: \", DATASET_PATH)\n",
    "print(\"Y_PHY_FILENAME: \", Y_PHY_FILENAME)\n",
    "print(\"MODEL_PATH_NAME: \", MODEL_PATH_NAME)\n",
    "print(\"RESULTS_METRICS_FILENAME: \", RESULTS_METRICS_FILENAME)\n",
    "print(\"BESTPARAMS_FILENAME: \", BESTPARAMS_FILENAME)\n",
    "print(\"PLOT_FILENAME: \", PLOT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2017_combined_u.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported train_2018_combined_u.csv\n",
      "Imported train_2020_combined_u.csv\n",
      "Imported train_2021_combined_u.csv\n",
      "Imported train_2022_combined_u.csv\n",
      "Warning: train_2023_combined_u.csv does not exist.\n",
      "Imported train_2017_combined_y.csv\n",
      "Imported train_2018_combined_y.csv\n",
      "Imported train_2020_combined_y.csv\n",
      "Imported train_2021_combined_y.csv\n",
      "Imported train_2022_combined_y.csv\n",
      "Warning: train_2023_combined_y.csv does not exist.\n",
      "Warning: val_2017_combined_u.csv does not exist.\n",
      "Warning: val_2018_combined_u.csv does not exist.\n",
      "Warning: val_2020_combined_u.csv does not exist.\n",
      "Imported val_2021_combined_u.csv\n",
      "Imported val_2022_combined_u.csv\n",
      "Imported val_2023_combined_u.csv\n",
      "Warning: val_2017_combined_y.csv does not exist.\n",
      "Warning: val_2018_combined_y.csv does not exist.\n",
      "Warning: val_2020_combined_y.csv does not exist.\n",
      "Imported val_2021_combined_y.csv\n",
      "Imported val_2022_combined_y.csv\n",
      "Imported val_2023_combined_y.csv\n",
      "Warning: test_2017_combined_u.csv does not exist.\n",
      "Warning: test_2018_combined_u.csv does not exist.\n",
      "Warning: test_2020_combined_u.csv does not exist.\n",
      "Imported test_2021_combined_u.csv\n",
      "Imported test_2022_combined_u.csv\n",
      "Imported test_2023_combined_u.csv\n",
      "Warning: test_2017_combined_y.csv does not exist.\n",
      "Warning: test_2018_combined_y.csv does not exist.\n",
      "Warning: test_2020_combined_y.csv does not exist.\n",
      "Imported test_2021_combined_y.csv\n",
      "Imported test_2022_combined_y.csv\n",
      "Imported test_2023_combined_y.csv\n",
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', YEARS, DATASET_PATH)\n",
    "train_output_frames = get_dataframes('train', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', YEARS, DATASET_PATH)\n",
    "val_output_frames = get_dataframes('val', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', YEARS, DATASET_PATH)\n",
    "test_output_frames = get_dataframes('test', 'y', YEARS, DATASET_PATH)\n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    len(TRAIN_YEARS),                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    len(VAL_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    len(TEST_YEARS),\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmation that the dataset has column indexes the same as those in ``config.py``\n",
    "Indexes are used mainly for the physics calculations, in order to accurately extract the information needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2_TUINDORP_IDX index matches in index: 5\n",
      "NO2_BREUKELEN_IDX index matches in index: 4\n",
      "WIND_DIR_IDX index matches in index: 0\n",
      "WIND_SPEED_IDX index matches in index: 2\n",
      "All station indexes match.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = list(train_dataset.u[0])  # Convert Index to list\n",
    "check_station_indexes(column_names, idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = f\"{RESULTS_PATH}/best_params/{BESTPARAMS_FILENAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read params from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best parms from /home/rachel/forecasting_smog_PEML/src/results/Utrecht/best_params/best_params_MLP_no2_MSE_allyears_Utrecht.txt\n",
      "Loaded Best Parameters: {'n_hidden_layers': 2, 'n_hidden_units': 256, 'lr': 8.394595694372765e-05, 'weight_decay': 3.300707449214965e-07, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    best_params = json.load(f)  # Automatically converts it to a dictionary\n",
    "\n",
    "print(f\"Loading best parms from {file_path}\")\n",
    "print(\"Loaded Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Saving Model\n",
    "Model saved in ``src/results/models/best_MLP_no2_MSE_allyears.pth``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "# Train the model with the best hyperparameters\n",
    "best_model_baseline = BasicMLP(\n",
    "    N_INPUT_UNITS=train_dataset.__n_features_in__(),\n",
    "    N_HIDDEN_LAYERS=best_params[\"n_hidden_layers\"],\n",
    "    N_HIDDEN_UNITS=best_params[\"n_hidden_units\"],\n",
    "    N_OUTPUT_UNITS=train_dataset.__n_features_out__(),\n",
    "    loss_function=\"MSE\",\n",
    ")\n",
    "\n",
    "# Create train & validation loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Train-Val\n",
    "Plot saved in ``src/results/trainval_plots/trainval_plot_MLP_no2_MSE_allyears.png``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Save Results\n",
    "Results saved in ``src/results/metrics/results_MLP_no2_MSE_allyears.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model of MLP MSE allyears from /home/rachel/forecasting_smog_PEML/src/results/Utrecht/models/best_MLP_no2_MSE_allyears_Utrecht.pth\n",
      "Test MSE Loss: 49.193053\n",
      "Test RMSE Loss: 7.013776\n",
      "Test SMAPE Loss: 28.573230%\n",
      "Mean Inference Time per Forward Pass: 0.017123 s Â± 0.009056 s\n"
     ]
    }
   ],
   "source": [
    "best_model_baseline.load_state_dict(torch.load(f\"{MODEL_PATH}/{MODEL_PATH_NAME}\", map_location = device))\n",
    "print(f\"Loading best model of {NN_TYPE} {LOSS_FUNC} {years} from {MODEL_PATH}/{MODEL_PATH_NAME}\")\n",
    "best_model_baseline.eval()\n",
    "\n",
    "# Create the DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "df_minmax = pd.read_csv(MINMAX_PATH, sep=';')\n",
    "min_value = df_minmax[\"min\"].values\n",
    "max_value = df_minmax[\"max\"].values\n",
    "mse, rmse, smape, inference_time_mean, inference_time_std = best_model_baseline.test_model(test_loader, min_value=min_value, max_value=max_value, device=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved as results_MLP_no2_MSE_allyears_Utrecht.csv in Results/metrics folder\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the CSV file path\n",
    "results_csv_path = f\"{RESULTS_PATH}/metrics/{RESULTS_METRICS_FILENAME}\"\n",
    "\n",
    "# Read original header and row\n",
    "with open(results_csv_path, mode=\"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    original_header = next(reader)\n",
    "    original_row = next(reader)\n",
    "\n",
    "# Convert to dict for easy manipulation\n",
    "data = dict(zip(original_header, original_row))\n",
    "\n",
    "# Update inference time\n",
    "data[\"Inference Time\"] = str(inference_time_mean)\n",
    "\n",
    "# Insert Inference Time Std right after Inference Time\n",
    "new_header = []\n",
    "new_row = []\n",
    "\n",
    "for col in original_header:\n",
    "    new_header.append(col)\n",
    "    new_row.append(data[col])\n",
    "    if col == \"Inference Time\":\n",
    "        new_header.append(\"Inference Time Std\")\n",
    "        new_row.append(str(inference_time_std))\n",
    "\n",
    "# Write updated CSV\n",
    "with open(results_csv_path, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(new_header)\n",
    "    writer.writerow(new_row)\n",
    "\n",
    "\n",
    "print(f\"Results saved as {RESULTS_METRICS_FILENAME} in Results/metrics folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
