{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n",
    "\n",
    "In preprocessing the data, the following steps are taken:\n",
    "\n",
    "> * Prepare packages and setup  \n",
    "> * Load in the data  \n",
    "> * Tidy the data and store metadata  \n",
    "> * Inspect data with various metrics  \n",
    "> * Inspect data with visualisations \n",
    "> * Select locations\n",
    "> * Select timeframe\n",
    "> * Feature engineering\n",
    "> * Perform train-validation-test-split  \n",
    "> * Normalisation  \n",
    "> * Create big combined normalised dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prepare packages and setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (6, 2) # landscape plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Global\" variables (we're still in a notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'tinus'                        # for working directory compatibility\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "SUBSET_MONTHS = bool(1)                 # If true, only the months specified in the list below will be\n",
    "                                        # used for the training, validation and testing set\n",
    "START_MON = '08'                        # starting month for the data\n",
    "END_MON = '12'                          # ending month for the data\n",
    "\n",
    "# ============================================================================+\n",
    "\n",
    "# Sensor locations in the case of Utrecht area:\n",
    "DE_BILT = 'S260'                        # starting (and only used) location for meteorological data\n",
    "TUINDORP = 'NL10636'                    # starting location for contamination data\n",
    "BREUKELEN = 'NL10641'                   # 'goal' location for contamination data\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "# At multiple locations, a sys.exit() can be used to halt the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load in the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding is in ISO-8859-15: https://data.rivm.nl/data/luchtmeetnet/readme.pdf\n",
    "\n",
    "def read_contaminant_csv_from_data_raw(component, year):\n",
    "    if DEVICE == 'tinus':               # adjust/delete this if on another machine\n",
    "        os.chdir(r\"c:\\Users\\vwold\\Documents\\thesis\\bsc_thesis\\preprocessing\")                      \n",
    "\n",
    "    rows_to_skip = 9                    # to not include metadata                                    \n",
    "    return pd.read_csv(f\"../data/data_raw/{year}_{component}.csv\", \n",
    "                       sep = ';', encoding = 'ISO-8859-15', skiprows = rows_to_skip)\n",
    "\n",
    "\n",
    "def read_meteo_csv_from_data_raw(year):\n",
    "    if DEVICE == 'tinus':               # adjust/delete this if on another machine\n",
    "        os.chdir(r\"c:\\Users\\vwold\\Documents\\thesis\\bsc_thesis\\preprocessing\") \n",
    "\n",
    "    return pd.read_csv(f\"../data/data_raw/{year}_meteo_Utrecht.csv\",\n",
    "                       sep = ';', encoding = 'UTF-8', index_col = 0) \n",
    "                                                          \n",
    "                                                            \n",
    "def read_four_contaminants(year, contaminants):\n",
    "    df1 = read_contaminant_csv_from_data_raw(contaminants[0], year)\n",
    "    df2 = read_contaminant_csv_from_data_raw(contaminants[1], year)\n",
    "    df3 = read_contaminant_csv_from_data_raw(contaminants[2], year)\n",
    "    df4 = read_contaminant_csv_from_data_raw(contaminants[3], year)\n",
    "    return df1, df2, df3, df4\n",
    "\n",
    "\n",
    "def read_two_meteo_years(yr1, yr2):\n",
    "    df1 = read_meteo_csv_from_data_raw(yr1)\n",
    "    df2 = read_meteo_csv_from_data_raw(yr2)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contaminants = ['PM25', 'PM10', 'O3', 'NO2']\n",
    "\n",
    "df_PM25_2016_raw, df_PM10_2016_raw, df_O3_2016_raw, df_NO2_2016_raw = \\\n",
    "    read_four_contaminants(2016, contaminants)\n",
    "df_PM25_2017_raw, df_PM10_2017_raw, df_O3_2017_raw, df_NO2_2017_raw = \\\n",
    "    read_four_contaminants(2017, contaminants)\n",
    "df_PM25_2018_raw, df_PM10_2018_raw, df_O3_2018_raw, df_NO2_2018_raw = \\\n",
    "    read_four_contaminants(2018, contaminants)\n",
    "df_PM25_2019_raw, df_PM10_2019_raw, df_O3_2019_raw, df_NO2_2019_raw = \\\n",
    "    read_four_contaminants(2019, contaminants)\n",
    "df_PM25_2020_raw, df_PM10_2020_raw, df_O3_2020_raw, df_NO2_2020_raw = \\\n",
    "    read_four_contaminants(2020, contaminants)\n",
    "df_PM25_2021_raw, df_PM10_2021_raw, df_O3_2021_raw, df_NO2_2021_raw = \\\n",
    "    read_four_contaminants(2021, contaminants)\n",
    "df_PM25_2022_raw, df_PM10_2022_raw, df_O3_2022_raw, df_NO2_2022_raw = \\\n",
    "    read_four_contaminants(2022, contaminants)\n",
    "df_PM25_2023_raw, df_PM10_2023_raw, df_O3_2023_raw, df_NO2_2023_raw = \\\n",
    "    read_four_contaminants(2023, contaminants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_2016_raw = read_meteo_csv_from_data_raw(2016)\n",
    "df_meteo_2017_raw = read_meteo_csv_from_data_raw(2017)\n",
    "df_meteo_2018_raw = read_meteo_csv_from_data_raw(2018)\n",
    "df_meteo_2019_raw = read_meteo_csv_from_data_raw(2019)\n",
    "df_meteo_2020_raw = read_meteo_csv_from_data_raw(2020)\n",
    "df_meteo_2021_raw = read_meteo_csv_from_data_raw(2021)\n",
    "df_meteo_2022_raw = read_meteo_csv_from_data_raw(2022)\n",
    "df_meteo_2023_raw = read_meteo_csv_from_data_raw(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_meteo_2016_raw.head(1))\n",
    "# print(df_meteo_2017_raw.head(1))\n",
    "# print(df_meteo_2018_raw.head(1))\n",
    "# print(df_meteo_2019_raw.head(1))\n",
    "# print(df_meteo_2020_raw.head(1))\n",
    "# print(df_meteo_2021_raw.head(1))\n",
    "print(df_meteo_2022_raw.head(1))\n",
    "print(df_meteo_2023_raw.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tidy the data and store metadata**\n",
    "\n",
    "First, tidy the contaminant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_contains_NaN(df, col):\n",
    "    \"\"\"Checks if column contains NaNs, returns True if so, False if not\"\"\"\n",
    "    return df[col].isna().any()\n",
    "\n",
    "\n",
    "def get_component(df):\n",
    "    return f\"{df['Component'].iloc[0]}\"\n",
    "\n",
    "\n",
    "def get_unit(df):\n",
    "    return f\"{df['Eenheid'].iloc[0]}\"\n",
    "\n",
    "\n",
    "def get_metadata(df):\n",
    "    \"\"\"Returns dictionary with component and unit of contaminant\"\"\"\n",
    "    metadata = {'comp' : get_component(df),\n",
    "                'unit' : get_unit(df)}\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def remove_unuseful_cols(df, cols):\n",
    "    \"\"\"Removes cols from df\"\"\"\n",
    "    return df.drop(cols, axis = 1)\n",
    "\n",
    "\n",
    "def change_contaminant_date_format(df):\n",
    "    \"\"\"Changes the date format yyyy-mm-dd hh:mm\"\"\"\n",
    "    try:\n",
    "        df['Begindatumtijd'] = pd.to_datetime(df['Begindatumtijd'],\n",
    "                                              format = '%Y%m%d %H:%M')\n",
    "    except ValueError:\n",
    "        df['Begindatumtijd'] = pd.to_datetime(df['Begindatumtijd'],\n",
    "                                              format = 'ISO8601')\n",
    "    df.rename(columns = {'Begindatumtijd' : 'DateTime'},\n",
    "              inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def strip_dot1_of_col_names(col_names): \n",
    "    \"\"\"Removes '.1' from col names (which were caused by duplicate cols in the raw data)\"\"\"\n",
    "    return [name.removesuffix('.1') for name in np.asarray(col_names)]\n",
    "\n",
    "\n",
    "def resolve_split_columns(df):          \n",
    "    \"\"\"Groups cols which were split over two columns in the raw data\"\"\"\n",
    "    df.columns = strip_dot1_of_col_names(df.columns)\n",
    "\n",
    "    # tranpose; group by duplicate column names; no sorting; sum the cols;\n",
    "    # minimum count is 1 to get NaN when column is empty; transpose again\n",
    "    return df.transpose().groupby(by = df.columns, sort = False).sum(min_count = 1).transpose()\n",
    "\n",
    "\n",
    "def fill_NaNs_forward(df):\n",
    "    \"\"\"Fills in NaNs by copying last value - forward fill\"\"\"\n",
    "    return df.ffill(axis = 1)\n",
    "\n",
    "\n",
    "def fill_NaNs_linear(df):\n",
    "    \"\"\"Fills in NaNs by linear interpolation, with a maximum of a week (168 hours)\"\"\"\n",
    "    return df.interpolate(method = 'linear', limit = 24 * 7)\n",
    "\n",
    "\n",
    "def subset_month_range(df, start_mon, end_mon, year):\n",
    "    \"\"\"Returns df subsetted by month range\"\"\"\n",
    "    return df[f'{year}-{start_mon}' : f'{year}-{end_mon}']\n",
    "\n",
    "\n",
    "def delete_feb_29th(df):\n",
    "    \"\"\"Deletes the 29th of February from a df\"\"\"\n",
    "    return df[~((df.index.month == 2) & (df.index.day == 29))]\n",
    "\n",
    "def delete_firework_days(df):\n",
    "    \"\"\"Deletes the 31st of December and 1st of January\"\"\"\n",
    "    return df[~(((df.index.month == 12) & (df.index.day == 31)) | ((df.index.month == 1) & (df.index.day == 1)))]\n",
    "\n",
    "\n",
    "def delete_empty_columns(df):\n",
    "    \"\"\"Drops cols with more than 25% NaNs\"\"\"          \n",
    "    threshold = df.shape[0] * 0.75\n",
    "    return df.dropna(thresh = threshold, axis = 1)\n",
    "\n",
    "\n",
    "def tidy_raw_contaminant_data(df, year, fill_NaNs = True):\n",
    "    \"\"\"Tidies raw contaminant data by various preprocessing steps\"\"\"\n",
    "    df.columns = df.columns.str.strip() # remove leading and trailing ws in col names\n",
    "    df = remove_unuseful_cols(df, ['Component', 'Bep.periode', 'Eenheid', 'Einddatumtijd'])\n",
    "                                        # change format to yr-mm-dd hr-mn\n",
    "    df = change_contaminant_date_format(df)         \n",
    "                                        # set the index to the dates ('datetime')\n",
    "    df = df.set_index('DateTime', drop = True)\n",
    "    df = resolve_split_columns(df)      # concat sensor data split over two columns\n",
    "\n",
    "    if fill_NaNs:\n",
    "        df = fill_NaNs_linear(df)       # fill in NaNs using linear interpolation\n",
    "                                        \n",
    "    if SUBSET_MONTHS:\n",
    "        df = subset_month_range(df, START_MON, END_MON, year)\n",
    "    df = delete_feb_29th(df)            # delete the 29th of February\n",
    "    df = delete_firework_days(df)       # delete the 31st of December and 1st of January\n",
    "    df = delete_empty_columns(df)       # drop columns which remained too empty after interpolating\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PM25_2016_meta = get_metadata(df_PM25_2016_raw)\n",
    "PM10_2016_meta = get_metadata(df_PM10_2016_raw)\n",
    "O3_2016_meta   = get_metadata(df_O3_2016_raw)\n",
    "NO2_2016_meta  = get_metadata(df_NO2_2016_raw)\n",
    "PM25_2017_meta = get_metadata(df_PM25_2017_raw)\n",
    "PM10_2017_meta = get_metadata(df_PM10_2017_raw)\n",
    "O3_2017_meta   = get_metadata(df_O3_2017_raw)\n",
    "NO2_2017_meta  = get_metadata(df_NO2_2017_raw)\n",
    "PM25_2018_meta = get_metadata(df_PM25_2018_raw)\n",
    "PM10_2018_meta = get_metadata(df_PM10_2018_raw)\n",
    "O3_2018_meta   = get_metadata(df_O3_2018_raw)\n",
    "NO2_2018_meta  = get_metadata(df_NO2_2018_raw)\n",
    "PM25_2019_meta = get_metadata(df_PM25_2019_raw)\n",
    "PM10_2019_meta = get_metadata(df_PM10_2019_raw)\n",
    "O3_2019_meta   = get_metadata(df_O3_2019_raw)\n",
    "NO2_2019_meta  = get_metadata(df_NO2_2019_raw)\n",
    "PM25_2020_meta = get_metadata(df_PM25_2020_raw)\n",
    "PM10_2020_meta = get_metadata(df_PM10_2020_raw)\n",
    "O3_2020_meta   = get_metadata(df_O3_2020_raw)\n",
    "NO2_2020_meta  = get_metadata(df_NO2_2020_raw)\n",
    "PM25_2021_meta = get_metadata(df_PM25_2021_raw)\n",
    "PM10_2021_meta = get_metadata(df_PM10_2021_raw)\n",
    "O3_2021_meta   = get_metadata(df_O3_2021_raw)\n",
    "NO2_2021_meta  = get_metadata(df_NO2_2021_raw)\n",
    "PM25_2022_meta = get_metadata(df_PM25_2022_raw)\n",
    "PM10_2022_meta = get_metadata(df_PM10_2022_raw)\n",
    "O3_2022_meta   = get_metadata(df_O3_2022_raw)\n",
    "NO2_2022_meta  = get_metadata(df_NO2_2022_raw)\n",
    "PM25_2023_meta = get_metadata(df_PM25_2023_raw)\n",
    "PM10_2023_meta = get_metadata(df_PM10_2023_raw)\n",
    "O3_2023_meta   = get_metadata(df_O3_2023_raw)\n",
    "NO2_2023_meta  = get_metadata(df_NO2_2023_raw)\n",
    "\n",
    "df_PM25_2016_tidy = tidy_raw_contaminant_data(df_PM25_2016_raw, '2016')\n",
    "df_PM10_2016_tidy = tidy_raw_contaminant_data(df_PM10_2016_raw, '2016')\n",
    "df_O3_2016_tidy   = tidy_raw_contaminant_data(df_O3_2016_raw, '2016')\n",
    "df_NO2_2016_tidy  = tidy_raw_contaminant_data(df_NO2_2016_raw, '2016')\n",
    "df_PM25_2017_tidy = tidy_raw_contaminant_data(df_PM25_2017_raw, '2017')\n",
    "df_PM10_2017_tidy = tidy_raw_contaminant_data(df_PM10_2017_raw, '2017')\n",
    "df_O3_2017_tidy   = tidy_raw_contaminant_data(df_O3_2017_raw, '2017')\n",
    "df_NO2_2017_tidy  = tidy_raw_contaminant_data(df_NO2_2017_raw, '2017')\n",
    "df_PM25_2018_tidy = tidy_raw_contaminant_data(df_PM25_2018_raw, '2018')\n",
    "df_PM10_2018_tidy = tidy_raw_contaminant_data(df_PM10_2018_raw, '2018')\n",
    "df_O3_2018_tidy   = tidy_raw_contaminant_data(df_O3_2018_raw, '2018')\n",
    "df_NO2_2018_tidy  = tidy_raw_contaminant_data(df_NO2_2018_raw, '2018')\n",
    "df_PM25_2019_tidy = tidy_raw_contaminant_data(df_PM25_2019_raw, '2019')\n",
    "df_PM10_2019_tidy = tidy_raw_contaminant_data(df_PM10_2019_raw, '2019')\n",
    "df_O3_2019_tidy   = tidy_raw_contaminant_data(df_O3_2019_raw, '2019')\n",
    "df_NO2_2019_tidy  = tidy_raw_contaminant_data(df_NO2_2019_raw, '2019')\n",
    "df_PM25_2020_tidy = tidy_raw_contaminant_data(df_PM25_2020_raw, '2020')\n",
    "df_PM10_2020_tidy = tidy_raw_contaminant_data(df_PM10_2020_raw, '2020')\n",
    "df_O3_2020_tidy   = tidy_raw_contaminant_data(df_O3_2020_raw, '2020')\n",
    "df_NO2_2020_tidy  = tidy_raw_contaminant_data(df_NO2_2020_raw, '2020')\n",
    "df_PM25_2021_tidy = tidy_raw_contaminant_data(df_PM25_2021_raw, '2021')\n",
    "df_PM10_2021_tidy = tidy_raw_contaminant_data(df_PM10_2021_raw, '2021')\n",
    "df_O3_2021_tidy   = tidy_raw_contaminant_data(df_O3_2021_raw, '2021')\n",
    "df_NO2_2021_tidy  = tidy_raw_contaminant_data(df_NO2_2021_raw, '2021')\n",
    "df_PM25_2022_tidy = tidy_raw_contaminant_data(df_PM25_2022_raw, '2022')\n",
    "df_PM10_2022_tidy = tidy_raw_contaminant_data(df_PM10_2022_raw, '2022')\n",
    "df_O3_2022_tidy   = tidy_raw_contaminant_data(df_O3_2022_raw, '2022')\n",
    "df_NO2_2022_tidy  = tidy_raw_contaminant_data(df_NO2_2022_raw, '2022')\n",
    "df_PM25_2023_tidy = tidy_raw_contaminant_data(df_PM25_2023_raw, '2023')\n",
    "df_PM10_2023_tidy = tidy_raw_contaminant_data(df_PM10_2023_raw, '2023')\n",
    "df_O3_2023_tidy   = tidy_raw_contaminant_data(df_O3_2023_raw, '2023')\n",
    "df_NO2_2023_tidy  = tidy_raw_contaminant_data(df_NO2_2023_raw, '2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_PM25_2016_tidy.shape)\n",
    "# print(df_PM10_2016_tidy.shape)\n",
    "# print(df_O3_2016_tidy.shape)\n",
    "# print(df_NO2_2016_tidy.shape)\n",
    "print(df_PM25_2017_tidy.shape)\n",
    "print(df_PM10_2017_tidy.shape)\n",
    "print(df_O3_2017_tidy.shape)\n",
    "print(df_NO2_2017_tidy.shape)\n",
    "print(df_PM25_2018_tidy.shape)\n",
    "print(df_PM10_2018_tidy.shape)\n",
    "print(df_O3_2018_tidy.shape)\n",
    "print(df_NO2_2018_tidy.shape)\n",
    "# print(df_PM25_2019_tidy.shape)\n",
    "# print(df_PM10_2019_tidy.shape)\n",
    "# print(df_O3_2019_tidy.shape)\n",
    "# print(df_NO2_2019_tidy.shape)\n",
    "print(df_PM25_2020_tidy.shape)\n",
    "print(df_PM10_2020_tidy.shape)\n",
    "print(df_O3_2020_tidy.shape)\n",
    "print(df_NO2_2020_tidy.shape)\n",
    "print(df_PM25_2021_tidy.shape)\n",
    "print(df_PM10_2021_tidy.shape)\n",
    "print(df_O3_2021_tidy.shape)\n",
    "print(df_NO2_2021_tidy.shape)\n",
    "print(df_PM25_2022_tidy.shape)\n",
    "print(df_PM10_2022_tidy.shape)\n",
    "print(df_O3_2022_tidy.shape)\n",
    "print(df_NO2_2022_tidy.shape)\n",
    "print(df_PM25_2023_tidy.shape)\n",
    "print(df_PM10_2023_tidy.shape)\n",
    "print(df_O3_2023_tidy.shape)\n",
    "print(df_NO2_2023_tidy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, tidy the meteorological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_meteo_date_format(df):\n",
    "    \"\"\"Changes the date format to yyyy-mm-dd hh:mm\"\"\"\n",
    "    df['DateTime'] = pd.to_datetime(df['YYYYMMDD'].astype(str) + ' ' + df['HH'].astype(str), \n",
    "                                    format = '%Y%m%d %H')\n",
    "    df = remove_unuseful_cols(df, ['YYYYMMDD', 'HH'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_WD_990_with_NaN(df, col):\n",
    "    \"\"\"Replaces all occurrences of 990 in the WD column with 0\"\"\"\n",
    "    df[col] = df[col].replace(990, np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_tidy_raw_meteo_data(df, col, only_260, year, fill_NaNs = True):\n",
    "    \"\"\"Tidies the raw meteo data by various preprocessing steps, and returns the selected col(s)\"\"\"\n",
    "    df.columns = df.columns.str.strip() # remove leading and trailing ws in col names\n",
    "    if '# STN' in df.columns:           # change col name of stations\n",
    "        df = df.rename(columns = {'# STN' : 'STN'})\n",
    "    df = remove_unuseful_cols(df, ['T10N', 'FF', 'VV', 'N', 'U',\n",
    "                                   'WW', 'IX', 'M', 'R', 'O', 'S', 'Y'])\n",
    "    df['HH'] = df['HH'].subtract(1)     # 1-24 to 0-23 hour range\n",
    "    df = change_meteo_date_format(df)   # create DateTime column\n",
    "    df = df.set_index('DateTime')       # set DateTime as index\n",
    "\n",
    "                                        # separate the three stations:\n",
    "    df = df[[col, 'STN']].copy()        # keep only selected col and station name col\n",
    "    if col == 'DD':                     # 990 (change in DD (or WD)) -> 0, for more even influence\n",
    "        df = replace_WD_990_with_NaN(df, col)\n",
    "\n",
    "                                        # continue with the 260 station:\n",
    "    df_260 = remove_unuseful_cols(df[df['STN'] == 260], 'STN')\n",
    "\n",
    "    if fill_NaNs:\n",
    "        df_260 = fill_NaNs_linear(df_260).astype('float64')\n",
    "\n",
    "    if SUBSET_MONTHS:\n",
    "        df_260 = subset_month_range(df_260, START_MON, END_MON, year)\n",
    "\n",
    "    df_260 = df_260.rename(columns = {df.columns[0] : 'S260'})\n",
    "    df_260 = delete_feb_29th(df_260)\n",
    "    df_260 = delete_firework_days(df_260)\n",
    "\n",
    "    if only_260:                        # return only the 260 station (only_260 var is unused)\n",
    "        return df_260\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_DeBilt = True                      # True: only De Bilt is used\n",
    "\n",
    "# df_temp_2016_tidy = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'T', only_DeBilt)\n",
    "# df_dewP_2016_tidy = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'TD', only_DeBilt)\n",
    "# df_WD_2016_tidy   = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'DD', only_DeBilt)\n",
    "# df_Wvh_2016_tidy  = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'FH', only_DeBilt)\n",
    "# df_Wmax_2016_tidy = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'FX', only_DeBilt)\n",
    "# df_preT_2016_tidy = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'DR', only_DeBilt)\n",
    "# df_preS_2016_tidy = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'RH', only_DeBilt)\n",
    "# df_SQ_2016_tidy   = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'SQ', only_DeBilt)\n",
    "# df_Q_2016_tidy    = extract_tidy_raw_meteo_data(df_meteo_2016_raw, 'Q', only_DeBilt)\n",
    "\n",
    "df_temp_2017_tidy = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'T', only_DeBilt, '2017')\n",
    "df_dewP_2017_tidy = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'TD', only_DeBilt, '2017')\n",
    "df_WD_2017_tidy   = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'DD', only_DeBilt, '2017')\n",
    "df_Wvh_2017_tidy  = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'FH', only_DeBilt, '2017')\n",
    "df_Wmax_2017_tidy = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'FX', only_DeBilt, '2017')\n",
    "df_preT_2017_tidy = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'DR', only_DeBilt, '2017')\n",
    "df_P_2017_tidy    = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'P', only_DeBilt, '2017')\n",
    "df_preS_2017_tidy = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'RH', only_DeBilt, '2017')\n",
    "df_SQ_2017_tidy   = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'SQ', only_DeBilt, '2017')\n",
    "df_Q_2017_tidy    = extract_tidy_raw_meteo_data(df_meteo_2017_raw, 'Q', only_DeBilt, '2017')\n",
    "\n",
    "df_temp_2018_tidy = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'T', only_DeBilt, '2018')\n",
    "df_dewP_2018_tidy = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'TD', only_DeBilt, '2018')\n",
    "df_WD_2018_tidy   = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'DD', only_DeBilt, '2018')\n",
    "df_Wvh_2018_tidy  = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'FH', only_DeBilt, '2018')\n",
    "df_Wmax_2018_tidy = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'FX', only_DeBilt, '2018')\n",
    "df_preT_2018_tidy = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'DR', only_DeBilt, '2018')\n",
    "df_P_2018_tidy    = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'P', only_DeBilt, '2018')\n",
    "df_preS_2018_tidy = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'RH', only_DeBilt, '2018')\n",
    "df_SQ_2018_tidy   = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'SQ', only_DeBilt, '2018')\n",
    "df_Q_2018_tidy    = extract_tidy_raw_meteo_data(df_meteo_2018_raw, 'Q', only_DeBilt, '2018')\n",
    "\n",
    "# df_temp_2019_tidy = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'T', only_DeBilt, '2019')\n",
    "# df_dewP_2019_tidy = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'TD', only_DeBilt, '2019')\n",
    "# df_WD_2019_tidy   = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'DD', only_DeBilt, '2019')\n",
    "# df_Wvh_2019_tidy  = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'FH', only_DeBilt, '2019')\n",
    "# df_Wmax_2019_tidy = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'FX', only_DeBilt, '2019')\n",
    "# df_preT_2019_tidy = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'DR', only_DeBilt, '2019')\n",
    "# df_P_2019_tidy    = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'P', only_DeBilt, '2019')\n",
    "# df_preS_2019_tidy = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'RH', only_DeBilt, '2019')\n",
    "# df_SQ_2019_tidy   = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'SQ', only_DeBilt, '2019')\n",
    "# df_Q_2019_tidy    = extract_tidy_raw_meteo_data(df_meteo_2019_raw, 'Q', only_DeBilt, '2019')\n",
    "\n",
    "df_temp_2020_tidy = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'T', only_DeBilt, '2020')\n",
    "df_dewP_2020_tidy = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'TD', only_DeBilt, '2020')\n",
    "df_WD_2020_tidy   = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'DD', only_DeBilt, '2020')\n",
    "df_Wvh_2020_tidy  = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'FH', only_DeBilt, '2020')\n",
    "df_Wmax_2020_tidy = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'FX', only_DeBilt, '2020')\n",
    "df_preT_2020_tidy = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'DR', only_DeBilt, '2020')\n",
    "df_P_2020_tidy    = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'P', only_DeBilt, '2020')\n",
    "df_preS_2020_tidy = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'RH', only_DeBilt, '2020')\n",
    "df_SQ_2020_tidy   = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'SQ', only_DeBilt, '2020')\n",
    "df_Q_2020_tidy    = extract_tidy_raw_meteo_data(df_meteo_2020_raw, 'Q', only_DeBilt, '2020')\n",
    "\n",
    "df_temp_2021_tidy = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'T', only_DeBilt, '2021')\n",
    "df_dewP_2021_tidy = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'TD', only_DeBilt, '2021')\n",
    "df_WD_2021_tidy   = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'DD', only_DeBilt, '2021')\n",
    "df_Wvh_2021_tidy  = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'FH', only_DeBilt, '2021')\n",
    "df_Wmax_2021_tidy = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'FX', only_DeBilt, '2021')\n",
    "df_preT_2021_tidy = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'DR', only_DeBilt, '2021')\n",
    "df_P_2021_tidy    = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'P', only_DeBilt, '2021')\n",
    "df_preS_2021_tidy = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'RH', only_DeBilt, '2021')\n",
    "df_SQ_2021_tidy   = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'SQ', only_DeBilt, '2021')\n",
    "df_Q_2021_tidy    = extract_tidy_raw_meteo_data(df_meteo_2021_raw, 'Q', only_DeBilt, '2021')\n",
    "\n",
    "df_temp_2022_tidy = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'T', only_DeBilt, '2022')\n",
    "df_dewP_2022_tidy = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'TD', only_DeBilt, '2022')\n",
    "df_WD_2022_tidy   = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'DD', only_DeBilt, '2022')\n",
    "df_Wvh_2022_tidy  = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'FH', only_DeBilt, '2022')\n",
    "df_Wmax_2022_tidy = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'FX', only_DeBilt, '2022')\n",
    "df_preT_2022_tidy = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'DR', only_DeBilt, '2022')\n",
    "df_P_2022_tidy    = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'P', only_DeBilt, '2022')\n",
    "df_preS_2022_tidy = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'RH', only_DeBilt, '2022')\n",
    "df_SQ_2022_tidy   = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'SQ', only_DeBilt, '2022')\n",
    "df_Q_2022_tidy    = extract_tidy_raw_meteo_data(df_meteo_2022_raw, 'Q', only_DeBilt, '2022')\n",
    "\n",
    "df_temp_2023_tidy = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'T', only_DeBilt, '2023')\n",
    "df_dewP_2023_tidy = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'TD', only_DeBilt, '2023')\n",
    "df_WD_2023_tidy   = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'DD', only_DeBilt, '2023')\n",
    "df_Wvh_2023_tidy  = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'FH', only_DeBilt, '2023')\n",
    "df_Wmax_2023_tidy = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'FX', only_DeBilt, '2023')\n",
    "df_preT_2023_tidy = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'DR', only_DeBilt, '2023')\n",
    "df_P_2023_tidy    = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'P', only_DeBilt, '2023')\n",
    "df_preS_2023_tidy = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'RH', only_DeBilt, '2023')\n",
    "df_SQ_2023_tidy   = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'SQ', only_DeBilt, '2023')\n",
    "df_Q_2023_tidy    = extract_tidy_raw_meteo_data(df_meteo_2023_raw, 'Q', only_DeBilt, '2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_WD_2022_tidy.head(2))\n",
    "print(df_WD_2022_tidy.tail(2))\n",
    "print(df_P_2023_tidy.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_temp_2017_tidy.shape)\n",
    "print(df_dewP_2017_tidy.shape)\n",
    "print(df_WD_2017_tidy.shape)\n",
    "print(df_Wvh_2017_tidy.shape)\n",
    "print(df_Wmax_2017_tidy.shape)\n",
    "print(df_preT_2017_tidy.shape)\n",
    "print(df_P_2017_tidy.shape)\n",
    "print(df_preS_2017_tidy.shape)\n",
    "print(df_SQ_2017_tidy.shape)\n",
    "print(df_Q_2017_tidy.shape)\n",
    "\n",
    "# print(df_temp_2019_tidy.shape)\n",
    "# print(df_dewP_2019_tidy.shape)\n",
    "# print(df_WD_2019_tidy.shape)\n",
    "# print(df_Wvh_2019_tidy.shape)\n",
    "# print(df_Wmax_2019_tidy.shape)\n",
    "# print(df_preT_2019_tidy.shape)\n",
    "# print(df_P_2019_tidy.shape)\n",
    "# print(df_preS_2019_tidy.shape)\n",
    "# print(df_SQ_2019_tidy.shape)\n",
    "# print(df_Q_2019_tidy.shape)\n",
    "\n",
    "print(df_temp_2023_tidy.shape)\n",
    "print(df_dewP_2023_tidy.shape)\n",
    "print(df_WD_2023_tidy.shape)\n",
    "print(df_Wvh_2023_tidy.shape)\n",
    "print(df_Wmax_2023_tidy.shape)\n",
    "print(df_preT_2023_tidy.shape)\n",
    "print(df_P_2023_tidy.shape)\n",
    "print(df_preS_2023_tidy.shape)\n",
    "print(df_SQ_2023_tidy.shape)\n",
    "print(df_Q_2023_tidy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspect data with various metrics**\n",
    "\n",
    "min, mean, max of day, month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_sensor_value(df, sensor):\n",
    "    return df[sensor].min()\n",
    "\n",
    "\n",
    "def get_mean_sensor_value(df, sensor):\n",
    "    return df[sensor].mean()\n",
    "\n",
    "\n",
    "def get_max_sensor_value(df, sensor):\n",
    "    return df[sensor].max()\n",
    "\n",
    "\n",
    "def get_min_per_day(df, sensor):\n",
    "    return pd.Series(data = df[sensor].resample('D', origin = 'start').min())\n",
    "\n",
    "\n",
    "def get_mean_per_day(df, sensor):\n",
    "    return pd.Series(data = df[sensor].resample('D', origin = 'start').mean())\n",
    "\n",
    "\n",
    "def get_max_per_day(df, sensor):\n",
    "    return pd.Series(data = df[sensor].resample('D', origin = 'start').max())\n",
    "\n",
    "\n",
    "def get_min_per_month(df, sensor):      # resample by month, take min(), shift to 15th of month\n",
    "    return pd.Series(data = \\\n",
    "                     df[sensor].resample('MS', convention = 'start').min().shift(14, 'D'))\n",
    "                     \n",
    "\n",
    "def get_mean_per_month(df, sensor):     # resample by month, take mean(), shift to 15th of month\n",
    "    return pd.Series(data = \\\n",
    "                     df[sensor].resample('MS', convention = 'start').mean().shift(14, 'D'))\n",
    "\n",
    "\n",
    "def get_max_per_month(df, sensor):      # resample by month, take max(), shift to 15th of month\n",
    "    return pd.Series(data = \\\n",
    "                     df[sensor].resample('MS', convention = 'start').max().shift(14, 'D'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_measurement_count(df, col):\n",
    "    \"\"\"Returns number of measurements\"\"\"\n",
    "    return df[col].count()\n",
    "\n",
    "\n",
    "def print_index_sampling_info(df):\n",
    "    \"\"\"Prints various sampling metrics of the index\"\"\"\n",
    "    print(f'Sample time distribution  =\\n{df.index.to_series().diff().value_counts()}')\n",
    "    print(f'Most frequent sample time = {df.index.to_series().diff().median()}')\n",
    "    print(f'Mean sample time          = {df.index.to_series().diff().mean()}')\n",
    "\n",
    "\n",
    "def print_sensor_metrics_min_mean_max_entries(df, sensor, meta):\n",
    "    \"\"\"Prints the min, mean, max, and number of entries of a sensor\"\"\"\n",
    "    if not sensor in df.columns:\n",
    "        return print(f\"{meta['comp']} measurements for sensor {sensor} are not avaiable\\n\")\n",
    "\n",
    "    print(f\"[min, mean, max] for sensor {sensor} measuring {meta['comp']} {meta['unit']}:\")\n",
    "    print(f\"[{get_min_sensor_value(df, sensor):.4f}, {get_mean_sensor_value(df, sensor):.4f}, {get_max_sensor_value(df, sensor):.4f}] with n = {get_col_measurement_count(df, sensor)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def get_daily_sensor_metrics(df, sensor):\n",
    "    return get_min_per_day(df, sensor), get_mean_per_day(df, sensor), get_max_per_day(df, sensor)\n",
    "\n",
    "\n",
    "def get_monthly_sensor_metrics(df, sensor):\n",
    "    return get_min_per_month(df, sensor), get_mean_per_month(df, sensor), get_max_per_month(df, sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_sensor_metrics_min_mean_max_entries(df_PM25_2016_tidy, TUINDORP, PM25_2016_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_PM25_2017_tidy, TUINDORP, PM25_2017_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_PM25_2018_tidy, TUINDORP, PM25_2018_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM25_2019_tidy, TUINDORP, PM25_2019_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM25_2020_tidy, TUINDORP, PM25_2020_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM25_2021_tidy, TUINDORP, PM25_2021_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM25_2022_tidy, TUINDORP, PM25_2022_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_sensor_metrics_min_mean_max_entries(df_PM10_2016_tidy, TUINDORP, PM10_2016_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_PM10_2017_tidy, TUINDORP, PM10_2017_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_PM10_2018_tidy, TUINDORP, PM10_2018_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM10_2019_tidy, TUINDORP, PM10_2019_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM10_2020_tidy, TUINDORP, PM10_2020_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM10_2021_tidy, TUINDORP, PM10_2021_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_PM10_2022_tidy, TUINDORP, PM10_2022_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_sensor_metrics_min_mean_max_entries(df_O3_2016_tidy, TUINDORP, O3_2016_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_O3_2017_tidy, TUINDORP, O3_2017_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_O3_2018_tidy, TUINDORP, O3_2018_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_O3_2019_tidy, TUINDORP, O3_2019_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_O3_2020_tidy, TUINDORP, O3_2020_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_O3_2021_tidy, TUINDORP, O3_2021_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_O3_2022_tidy, TUINDORP, O3_2022_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_sensor_metrics_min_mean_max_entries(df_NO2_2016_tidy, TUINDORP, NO2_2016_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_NO2_2017_tidy, TUINDORP, NO2_2017_meta)\n",
    "# print_sensor_metrics_min_mean_max_entries(df_NO2_2018_tidy, TUINDORP, NO2_2018_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_NO2_2019_tidy, TUINDORP, NO2_2019_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_NO2_2020_tidy, TUINDORP, NO2_2020_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_NO2_2021_tidy, TUINDORP, NO2_2021_meta)\n",
    "print_sensor_metrics_min_mean_max_entries(df_NO2_2022_tidy, TUINDORP, NO2_2022_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_PM25_2016_raw, df_PM10_2016_raw, df_O3_2016_raw, df_NO2_2016_raw\n",
    "del df_PM25_2017_raw, df_PM10_2017_raw, df_O3_2017_raw, df_NO2_2017_raw\n",
    "del df_PM25_2018_raw, df_PM10_2018_raw, df_O3_2018_raw, df_NO2_2018_raw\n",
    "del df_PM25_2019_raw, df_PM10_2019_raw, df_O3_2019_raw, df_NO2_2019_raw\n",
    "del df_PM25_2020_raw, df_PM10_2020_raw, df_O3_2020_raw, df_NO2_2020_raw\n",
    "del df_PM25_2021_raw, df_PM10_2021_raw, df_O3_2021_raw, df_NO2_2021_raw\n",
    "del df_PM25_2022_raw, df_PM10_2022_raw, df_O3_2022_raw, df_NO2_2022_raw\n",
    "del df_PM25_2023_raw, df_PM10_2023_raw, df_O3_2023_raw, df_NO2_2023_raw\n",
    "del df_meteo_2016_raw\n",
    "del df_meteo_2017_raw\n",
    "del df_meteo_2018_raw\n",
    "del df_meteo_2019_raw\n",
    "del df_meteo_2020_raw\n",
    "del df_meteo_2021_raw\n",
    "del df_meteo_2022_raw\n",
    "del df_meteo_2023_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspect data with visualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_style():\n",
    "    sns.set_theme()\n",
    "    sns.axes_style('darkgrid')\n",
    "    sns.set_palette('dark') \n",
    "    sns.set_context('notebook')\n",
    "    \n",
    "\n",
    "def plot_sensor(df, sensor, info = ''):\n",
    "    \"\"\"Plots all measurements for one sensor against time\"\"\"\n",
    "    if sensor not in df:\n",
    "        return print(f\"Sensor {sensor} is not avaiable\\n\")\n",
    "\n",
    "    set_style()\n",
    "    sns.lineplot(data = df, x = 'DateTime', y = sensor, color = '#800000')\n",
    "\n",
    "    plt.title(f\"Sensor {sensor} plotted against time - {info}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.xticks(rotation = 18)\n",
    "    plt.ylabel(f\"Measurement value\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sensor_meta(df, sensor, meta):\n",
    "    \"\"\"Plots all measurements for one sensor against time, with metadata\"\"\"\n",
    "    if sensor not in df:\n",
    "        return print(f\"{meta['comp']} measurements for sensor {sensor} are not avaiable\\n\")\n",
    "\n",
    "    set_style()\n",
    "    sns.lineplot(data = df, x = 'DateTime', y = sensor, color = '#800000')\n",
    "\n",
    "    plt.title(f\"Sensor {sensor} plotted against time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.xticks(rotation = 18)\n",
    "    plt.ylabel(f\"{meta['comp']} value in {meta['unit']}\")\n",
    "    plt.show()\n",
    "\n",
    "                                        \n",
    "def plot_min_mean_max(df, sensor, meta):\n",
    "    \"\"\"Plots min, mean, max of a sensor against time\"\"\"\n",
    "    mins, means, maxs = get_daily_sensor_metrics(df, sensor)\n",
    "    \n",
    "    set_style()\n",
    "\n",
    "    sns.lineplot(data = mins.to_frame(), x = mins.index, y = mins.values, \n",
    "                 label = 'min')#, color = '#FED116')\n",
    "    sns.lineplot(data = means.to_frame(), x = means.index, y = means.values, \n",
    "                 label = 'mean')#, color = '#CD1127')\n",
    "    sns.lineplot(data = maxs.to_frame(), x = maxs.index, y = maxs.values, \n",
    "                 label = 'max')#, color = '#013893')\n",
    "\n",
    "    plt.title(f\"Sensor {sensor}'s min, max, mean plotted against time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.xticks(rotation = 18)\n",
    "    plt.ylabel(f\"{meta['comp']} value in {meta['unit']}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_day_vs_month(df, sensor, meta):\n",
    "    \"\"\"Plots daily vs monthly average of a sensor against time\"\"\"\n",
    "    days = get_mean_per_day(df, sensor)\n",
    "    mons = get_mean_per_month(df, sensor)\n",
    "\n",
    "    set_style()\n",
    "\n",
    "    sns.lineplot(data = days.to_frame(), x = days.index, \n",
    "                 y = days.values, label = 'day')\n",
    "    plt.stem(mons.index, mons.values, basefmt = ' ', \n",
    "             linefmt = '--r', markerfmt = 'or', label = 'month')\n",
    "    # for more customization visit:\n",
    "    # https://stackoverflow.com/questions/38984959/how-can-i\n",
    "    # -get-the-stemlines-color-to-match-the-marker-color-in-a-stem-plot\n",
    "\n",
    "    plt.title(f\"Sensor {sensor}'s daily and monthly average plotted against time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.xticks(rotation = 18)\n",
    "    plt.ylabel(f\"{meta['comp']} value in {meta['unit']}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Select locations**\n",
    "\n",
    "Here, we'll select the locations we want to use. The I/O-task can be either 0-dimensional, or 1-dimensional.  \n",
    "\n",
    "EDIT: The project is continued with a one-dimensional set-up, but some code might still be accustomed to both possible set-ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_sensors(df, sensors):\n",
    "    \"\"\"Subsets sensor in the vicinity of Groningen, Friesland, and Drenthe\"\"\"\n",
    "\n",
    "    if isinstance(sensors, str):        # subset one sensor, so a str,\n",
    "        return df.loc[:, sensors]       \n",
    "    else:                               # else, subset multiple from a list\n",
    "        return df.loc[:, df.columns.isin(sensors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_1D = [TUINDORP, BREUKELEN]\n",
    "\n",
    "df_PM25_2017_tidy_subset_1D = subset_sensors(df_PM25_2017_tidy, sensors_1D)\n",
    "df_PM10_2017_tidy_subset_1D = subset_sensors(df_PM10_2017_tidy, sensors_1D)\n",
    "df_O3_2017_tidy_subset_1D = subset_sensors(df_O3_2017_tidy, sensors_1D)\n",
    "df_NO2_2017_tidy_subset_1D = subset_sensors(df_NO2_2017_tidy, sensors_1D)\n",
    "df_PM25_2018_tidy_subset_1D = subset_sensors(df_PM25_2018_tidy, sensors_1D)\n",
    "df_PM10_2018_tidy_subset_1D = subset_sensors(df_PM10_2018_tidy, sensors_1D)\n",
    "df_O3_2018_tidy_subset_1D = subset_sensors(df_O3_2018_tidy, sensors_1D)\n",
    "df_NO2_2018_tidy_subset_1D = subset_sensors(df_NO2_2018_tidy, sensors_1D)\n",
    "df_PM25_2020_tidy_subset_1D = subset_sensors(df_PM25_2020_tidy, sensors_1D)\n",
    "df_PM10_2020_tidy_subset_1D = subset_sensors(df_PM10_2020_tidy, sensors_1D)\n",
    "df_O3_2020_tidy_subset_1D = subset_sensors(df_O3_2020_tidy, sensors_1D)\n",
    "df_NO2_2020_tidy_subset_1D = subset_sensors(df_NO2_2020_tidy, sensors_1D)\n",
    "df_PM25_2021_tidy_subset_1D = subset_sensors(df_PM25_2021_tidy, sensors_1D)\n",
    "df_PM10_2021_tidy_subset_1D = subset_sensors(df_PM10_2021_tidy, sensors_1D)\n",
    "df_O3_2021_tidy_subset_1D = subset_sensors(df_O3_2021_tidy, sensors_1D)\n",
    "df_NO2_2021_tidy_subset_1D = subset_sensors(df_NO2_2021_tidy, sensors_1D)\n",
    "df_PM25_2022_tidy_subset_1D = subset_sensors(df_PM25_2022_tidy, sensors_1D)\n",
    "df_PM10_2022_tidy_subset_1D = subset_sensors(df_PM10_2022_tidy, sensors_1D)\n",
    "df_O3_2022_tidy_subset_1D = subset_sensors(df_O3_2022_tidy, sensors_1D)\n",
    "df_NO2_2022_tidy_subset_1D = subset_sensors(df_NO2_2022_tidy, sensors_1D)\n",
    "df_PM25_2023_tidy_subset_1D = subset_sensors(df_PM25_2023_tidy, sensors_1D)\n",
    "df_PM10_2023_tidy_subset_1D = subset_sensors(df_PM10_2023_tidy, sensors_1D)\n",
    "df_O3_2023_tidy_subset_1D = subset_sensors(df_O3_2023_tidy, sensors_1D)\n",
    "df_NO2_2023_tidy_subset_1D = subset_sensors(df_NO2_2023_tidy, sensors_1D)\n",
    "\n",
    "del df_PM25_2017_tidy, df_PM10_2017_tidy, df_O3_2017_tidy, df_NO2_2017_tidy\n",
    "del df_PM25_2018_tidy, df_PM10_2018_tidy, df_O3_2018_tidy, df_NO2_2018_tidy\n",
    "del df_PM25_2020_tidy, df_PM10_2020_tidy, df_O3_2020_tidy, df_NO2_2020_tidy\n",
    "del df_PM25_2021_tidy, df_PM10_2021_tidy, df_O3_2021_tidy, df_NO2_2021_tidy\n",
    "del df_PM25_2022_tidy, df_PM10_2022_tidy, df_O3_2022_tidy, df_NO2_2022_tidy\n",
    "del df_PM25_2023_tidy, df_PM10_2023_tidy, df_O3_2023_tidy, df_NO2_2023_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Select timeframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes before plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_NO2_2016_tidy_subset_1D.shape, df_O3_2016_tidy_subset_1D.shape,\n",
    "#       df_PM25_2016_tidy_subset_1D.shape, df_PM10_2016_tidy_subset_1D.shape)\n",
    "print(df_NO2_2017_tidy_subset_1D.shape, df_O3_2017_tidy_subset_1D.shape,\n",
    "        df_PM25_2017_tidy_subset_1D.shape, df_PM10_2017_tidy_subset_1D.shape)\n",
    "print(df_NO2_2018_tidy_subset_1D.shape, df_O3_2018_tidy_subset_1D.shape,\n",
    "        df_PM25_2018_tidy_subset_1D.shape, df_PM10_2018_tidy_subset_1D.shape)\n",
    "# print(df_NO2_2019_tidy_subset_1D.shape, df_O3_2019_tidy_subset_1D.shape,\n",
    "#         df_PM25_2019_tidy_subset_1D.shape, df_PM10_2019_tidy_subset_1D.shape)\n",
    "print(df_NO2_2020_tidy_subset_1D.shape, df_O3_2020_tidy_subset_1D.shape,\n",
    "        df_PM25_2020_tidy_subset_1D.shape, df_PM10_2020_tidy_subset_1D.shape)\n",
    "print(df_NO2_2021_tidy_subset_1D.shape, df_O3_2021_tidy_subset_1D.shape,\n",
    "        df_PM25_2021_tidy_subset_1D.shape, df_PM10_2021_tidy_subset_1D.shape)\n",
    "print(df_NO2_2022_tidy_subset_1D.shape, df_O3_2022_tidy_subset_1D.shape,\n",
    "        df_PM25_2022_tidy_subset_1D.shape, df_PM10_2022_tidy_subset_1D.shape)\n",
    "\n",
    "# # Add dummy column for missing cols\n",
    "# df_O3_2016_tidy_subset_1D[TUINDORP] = np.nan\n",
    "# df_O3_2019_tidy_subset_1D[TUINDORP] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "def plot_corr_matrix_pearson(df, threshold = 0, method = 'pearson'):\n",
    "    \"\"\"Plot a diagonal correlation matrix using the assembled dataframe\"\"\"\n",
    "    corr = df.corr(method)\n",
    "    if threshold:\n",
    "        corr = corr[corr.abs() > threshold]\n",
    "\n",
    "    mask = np.triu(np.ones_like(corr, dtype = bool))\n",
    "\n",
    "    f, ax = plt.subplots(figsize = (7, 5))\n",
    "    # # cmap = sns.diverging_palette(230, 20, as_cmap = True)\n",
    "    # cmap = sns.diverging_palette(0, 255, s = 100, sep = 1, as_cmap = True)\n",
    "\n",
    "    # sns.heatmap(corr, mask = mask, cmap = cmap, center = 0,\n",
    "    #             square = True, linewidths = .5, cbar_kws = {\"shrink\": .5});\n",
    "    sns.heatmap(corr, mask = mask,\n",
    "                vmin = -1, vmax = 1, center = 0,\n",
    "                square = True, linewidth = .5, cbar_kws = {\"shrink\": .75})\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "def plot_corr_matrix_other_method(df, method = 'kendall'):\n",
    "    \"\"\"Plot a diagonal correlation matrix using the assembled dataframe\"\"\"\n",
    "    corr = df.corr(method)\n",
    "    mask = np.triu(np.ones_like(corr, dtype = bool))\n",
    "    f, ax = plt.subplots(figsize = (7, 5))\n",
    "    sns.heatmap(corr, mask = mask,\n",
    "                vmin = -1, vmax = 1, center = 0,\n",
    "                square = True, linewidth = .5, cbar_kws = {\"shrink\": .75})\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the paper (README.md) for an analysis of the correlation plots created by the functions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Perform train-validation-test-split**\n",
    "\n",
    "*Training data:*  \n",
    "September, October, November, December of 2017, 2018, 2020, and September, October, first 2 weeks of November of 2021, 2022  \n",
    "\n",
    "\n",
    "*Validation data:*  \n",
    "Last 2 weeks of November, first week of December of 2021, 2022, and September and first 2 weeks of October of 2023  \n",
    "\n",
    "\n",
    "*Testing data:*  \n",
    "Last 3 weeks of December of 2022, and last 2 weeks of October and November of 2023  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_split(df, days_vali, days_test):\n",
    "    \"\"\"Performs a train-validation-test split on the data\"\"\"\n",
    "                                        # partition points of val/test \n",
    "                                        # set expressed in hours for indexing\n",
    "    pp_vali = int(df.shape[0] - (days_vali + days_test) * 24)\n",
    "    pp_test = int(pp_vali + days_test * 24)\n",
    "    \n",
    "    df_train = df[ : pp_vali]\n",
    "    df_vali  = df[pp_vali : pp_test]\n",
    "    df_test  = df[pp_test : ]\n",
    "\n",
    "    return df_train, df_vali, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_data_split_without_train(df, days_vali, days_test):\n",
    "    \"\"\"Performs a train-validation-test split on the data\"\"\"\n",
    "                                        # partition points of val/test \n",
    "                                        # set expressed in hours for indexing\n",
    "    pp_vali = int(days_vali * 24)\n",
    "    pp_test = int(pp_vali + days_test * 24)\n",
    "    \n",
    "    df_vali  = df[ : pp_vali]\n",
    "    df_test  = df[pp_vali : pp_test]\n",
    "\n",
    "    return df_vali, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two variables, the size of the validation and testing set is set (in days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_vali = 21\n",
    "days_test = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test-split is performed. Each component is split separately. In the case of a one-dimensional prediction task, additional contaminant data is split as well. (All data remains segregate for now for proper normalisation later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final version we'll consider the data mentioned above, this is the splitting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PM25_2017_train_1D = df_PM25_2017_tidy_subset_1D.copy()\n",
    "df_PM10_2017_train_1D = df_PM10_2017_tidy_subset_1D.copy()\n",
    "df_NO2_2017_train_1D  = df_NO2_2017_tidy_subset_1D.copy()\n",
    "df_O3_2017_train_1D   = df_O3_2017_tidy_subset_1D.copy()\n",
    "df_temp_2017_train = df_temp_2017_tidy.copy()\n",
    "df_dewP_2017_train = df_dewP_2017_tidy.copy()\n",
    "df_WD_2017_train   = df_WD_2017_tidy.copy()\n",
    "df_Wvh_2017_train  = df_Wvh_2017_tidy.copy()\n",
    "df_P_2017_train    = df_P_2017_tidy.copy()\n",
    "df_SQ_2017_train   = df_SQ_2017_tidy.copy()\n",
    "\n",
    "df_PM25_2018_train_1D = df_PM25_2018_tidy_subset_1D.copy()\n",
    "df_PM10_2018_train_1D = df_PM10_2018_tidy_subset_1D.copy()\n",
    "df_NO2_2018_train_1D  = df_NO2_2018_tidy_subset_1D.copy()\n",
    "df_O3_2018_train_1D   = df_O3_2018_tidy_subset_1D.copy()\n",
    "df_temp_2018_train = df_temp_2018_tidy.copy()\n",
    "df_dewP_2018_train = df_dewP_2018_tidy.copy()\n",
    "df_WD_2018_train   = df_WD_2018_tidy.copy()\n",
    "df_Wvh_2018_train  = df_Wvh_2018_tidy.copy()\n",
    "df_P_2018_train    = df_P_2018_tidy.copy()\n",
    "df_SQ_2018_train   = df_SQ_2018_tidy.copy()\n",
    "\n",
    "df_PM25_2020_train_1D = df_PM25_2020_tidy_subset_1D.copy()\n",
    "df_PM10_2020_train_1D = df_PM10_2020_tidy_subset_1D.copy()\n",
    "df_NO2_2020_train_1D  = df_NO2_2020_tidy_subset_1D.copy()\n",
    "df_O3_2020_train_1D   = df_O3_2020_tidy_subset_1D.copy()\n",
    "df_temp_2020_train = df_temp_2020_tidy.copy()\n",
    "df_dewP_2020_train = df_dewP_2020_tidy.copy()\n",
    "df_WD_2020_train   = df_WD_2020_tidy.copy()\n",
    "df_Wvh_2020_train  = df_Wvh_2020_tidy.copy()\n",
    "df_P_2020_train    = df_P_2020_tidy.copy()\n",
    "df_SQ_2020_train   = df_SQ_2020_tidy.copy()\n",
    "\n",
    "df_PM25_2021_train_1D, df_PM25_2021_val_1D, df_PM25_2021_test_1D = \\\n",
    "    perform_data_split(df_PM25_2021_tidy_subset_1D, days_vali, days_test)\n",
    "df_PM10_2021_train_1D, df_PM10_2021_val_1D, df_PM10_2021_test_1D = \\\n",
    "    perform_data_split(df_PM10_2021_tidy_subset_1D, days_vali, days_test)\n",
    "df_NO2_2021_train_1D,  df_NO2_2021_val_1D,  df_NO2_2021_test_1D  = \\\n",
    "    perform_data_split(df_NO2_2021_tidy_subset_1D, days_vali, days_test)\n",
    "df_O3_2021_train_1D,   df_O3_2021_val_1D,   df_O3_2021_test_1D   = \\\n",
    "    perform_data_split(df_O3_2021_tidy_subset_1D, days_vali, days_test)\n",
    "df_temp_2021_train, df_temp_2021_val, df_temp_2021_test = \\\n",
    "    perform_data_split(df_temp_2021_tidy, days_vali, days_test)\n",
    "df_dewP_2021_train, df_dewP_2021_val, df_dewP_2021_test = \\\n",
    "    perform_data_split(df_dewP_2021_tidy, days_vali, days_test)\n",
    "df_WD_2021_train,   df_WD_2021_val,   df_WD_2021_test   = \\\n",
    "    perform_data_split(df_WD_2021_tidy, days_vali, days_test)\n",
    "df_Wvh_2021_train,  df_Wvh_2021_val,  df_Wvh_2021_test  = \\\n",
    "    perform_data_split(df_Wvh_2021_tidy, days_vali, days_test)\n",
    "df_P_2021_train,    df_P_2021_val,    df_P_2021_test    = \\\n",
    "    perform_data_split(df_P_2021_tidy, days_vali, days_test)\n",
    "df_SQ_2021_train,   df_SQ_2021_val,   df_SQ_2021_test   = \\\n",
    "    perform_data_split(df_SQ_2021_tidy, days_vali, days_test)\n",
    "\n",
    "df_PM25_2022_train_1D, df_PM25_2022_val_1D, df_PM25_2022_test_1D = \\\n",
    "    perform_data_split(df_PM25_2022_tidy_subset_1D, days_vali, days_test)\n",
    "df_PM10_2022_train_1D, df_PM10_2022_val_1D, df_PM10_2022_test_1D = \\\n",
    "    perform_data_split(df_PM10_2022_tidy_subset_1D, days_vali, days_test)\n",
    "df_NO2_2022_train_1D,  df_NO2_2022_val_1D,  df_NO2_2022_test_1D  = \\\n",
    "    perform_data_split(df_NO2_2022_tidy_subset_1D, days_vali, days_test)\n",
    "df_O3_2022_train_1D,   df_O3_2022_val_1D,   df_O3_2022_test_1D   = \\\n",
    "    perform_data_split(df_O3_2022_tidy_subset_1D, days_vali, days_test)\n",
    "df_temp_2022_train, df_temp_2022_val, df_temp_2022_test = \\\n",
    "    perform_data_split(df_temp_2022_tidy, days_vali, days_test)\n",
    "df_dewP_2022_train, df_dewP_2022_val, df_dewP_2022_test = \\\n",
    "    perform_data_split(df_dewP_2022_tidy, days_vali, days_test)\n",
    "df_WD_2022_train,   df_WD_2022_val,   df_WD_2022_test   = \\\n",
    "    perform_data_split(df_WD_2022_tidy, days_vali, days_test)\n",
    "df_Wvh_2022_train,  df_Wvh_2022_val,  df_Wvh_2022_test  = \\\n",
    "    perform_data_split(df_Wvh_2022_tidy, days_vali, days_test)\n",
    "df_P_2022_train,    df_P_2022_val,    df_P_2022_test    = \\\n",
    "    perform_data_split(df_P_2022_tidy, days_vali, days_test)\n",
    "df_SQ_2022_train,   df_SQ_2022_val,   df_SQ_2022_test   = \\\n",
    "    perform_data_split(df_SQ_2022_tidy, days_vali, days_test)\n",
    "\n",
    "df_PM25_2023_val_1D, df_PM25_2023_test_1D = \\\n",
    "    perform_data_split_without_train(df_PM25_2023_tidy_subset_1D, 63, 63)\n",
    "df_PM10_2023_val_1D, df_PM10_2023_test_1D = \\\n",
    "    perform_data_split_without_train(df_PM10_2023_tidy_subset_1D, 63, 63)\n",
    "df_NO2_2023_val_1D,  df_NO2_2023_test_1D  = \\\n",
    "    perform_data_split_without_train(df_NO2_2023_tidy_subset_1D, 63, 63)\n",
    "df_O3_2023_val_1D,   df_O3_2023_test_1D   = \\\n",
    "    perform_data_split_without_train(df_O3_2023_tidy_subset_1D, 63, 63)\n",
    "df_temp_2023_val,    df_temp_2023_test = \\\n",
    "    perform_data_split_without_train(df_temp_2023_tidy, 63, 63)\n",
    "df_dewP_2023_val,    df_dewP_2023_test = \\\n",
    "    perform_data_split_without_train(df_dewP_2023_tidy, 63, 63)\n",
    "df_WD_2023_val,      df_WD_2023_test   = \\\n",
    "    perform_data_split_without_train(df_WD_2023_tidy, 63, 63)\n",
    "df_Wvh_2023_val,     df_Wvh_2023_test  = \\\n",
    "    perform_data_split_without_train(df_Wvh_2023_tidy, 63, 63)\n",
    "df_P_2023_val,       df_P_2023_test    = \\\n",
    "    perform_data_split_without_train(df_P_2023_tidy, 63, 63)\n",
    "df_SQ_2023_val,      df_SQ_2023_test   = \\\n",
    "    perform_data_split_without_train(df_SQ_2023_tidy, 63, 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_split_ratios(dfs_train: list, df_val, df_test, comp):\n",
    "    \"\"\"Prints the splitting ratios (useful after the train-validaiton-test split)\"\"\"\n",
    "    total_len = sum([len(df) for df in dfs_train]) + len(df_val) + len(df_test)\n",
    "    print(f\"[train/validation/test] %-ratio for {comp} data is: \", end = '')\n",
    "    print(f\"[{round((sum([len(df) for df in dfs_train])) / total_len * 100, 1)}/\", end = '')\n",
    "    print(f\"{round(len(df_val) / total_len * 100, 1)}/{round(len(df_test) / total_len * 100, 1)}]\")\n",
    "\n",
    "\n",
    "print_split_ratios([df_PM25_2017_train_1D,\n",
    "                    df_PM25_2018_train_1D,\n",
    "                    df_PM25_2020_train_1D,\n",
    "                    df_PM25_2021_train_1D,\n",
    "                    df_PM25_2022_train_1D],\n",
    "                    pd.concat([df_PM25_2021_val_1D,\n",
    "                               df_PM25_2022_val_1D,\n",
    "                               df_PM25_2023_val_1D]),\n",
    "                    pd.concat([df_PM25_2021_test_1D,\n",
    "                               df_PM25_2022_test_1D,\n",
    "                               df_PM25_2023_test_1D]),\n",
    "                    'PM25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_PM25_2023_val_1D.head())\n",
    "print(df_PM25_2023_val_1D.tail())\n",
    "print(df_PM25_2023_test_1D.head())\n",
    "print(df_PM25_2023_test_1D.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Normalisation**\n",
    "\n",
    "Linear scaling: $x' = (x - x_{min}) / (x_{max} - x_{min})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_minimum(df):\n",
    "    \"\"\"Returns minimum of entire dataframe\"\"\"\n",
    "    return np.min(df.min())             \n",
    "\n",
    "\n",
    "def get_df_maximum(df):\n",
    "    \"\"\"Returns maximum of entire dataframe\"\"\"\n",
    "    return np.max(df.max())\n",
    "\n",
    "\n",
    "def calc_combined_min_max_params(dfs: list):\n",
    "    \"\"\"\"Returns min and max of two dataframes combined\"\"\"\n",
    "    min = np.min([get_df_minimum(df) for df in dfs])\n",
    "    max = np.max([get_df_maximum(df) for df in dfs])\n",
    "    return min, max\n",
    "\n",
    "\n",
    "def normalise_linear(df, min, max):\n",
    "    \"\"\"Performs linear scaling (minmax) on dataframe\"\"\"\n",
    "    return (df - min) / (max - min)\n",
    "\n",
    "\n",
    "def normalise_linear_inv(df_norm, min, max):\n",
    "    \"\"\"Performs inverse linear scaling (minmax) on dataframe\"\"\"\n",
    "    return df_norm * (max - min) + min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise each component/contaminant separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PM25_min_train, PM25_max_train = calc_combined_min_max_params([\n",
    "                                                            df_PM25_2017_train_1D,\n",
    "                                                            df_PM25_2018_train_1D,\n",
    "                                                            df_PM25_2020_train_1D,\n",
    "                                                            df_PM25_2021_train_1D,\n",
    "                                                            df_PM25_2022_train_1D,\n",
    "                                                            ])\n",
    "PM10_min_train, PM10_max_train = calc_combined_min_max_params([\n",
    "                                                            df_PM10_2017_train_1D,\n",
    "                                                            df_PM10_2018_train_1D,\n",
    "                                                            df_PM10_2020_train_1D,\n",
    "                                                            df_PM10_2021_train_1D,\n",
    "                                                            df_PM10_2022_train_1D,\n",
    "                                                            ])\n",
    "O3_min_train,   O3_max_train   = calc_combined_min_max_params([\n",
    "                                                            df_O3_2017_train_1D,\n",
    "                                                            df_O3_2018_train_1D,\n",
    "                                                            df_O3_2020_train_1D,\n",
    "                                                            df_O3_2021_train_1D,\n",
    "                                                            df_O3_2022_train_1D,\n",
    "                                                            ])\n",
    "NO2_min_train,  NO2_max_train  = calc_combined_min_max_params([\n",
    "                                                            df_NO2_2017_train_1D,\n",
    "                                                            df_NO2_2018_train_1D,\n",
    "                                                            df_NO2_2020_train_1D,\n",
    "                                                            df_NO2_2021_train_1D,\n",
    "                                                            df_NO2_2022_train_1D,\n",
    "                                                            ])\n",
    "temp_min_train, temp_max_train = calc_combined_min_max_params([\n",
    "                                                            df_temp_2017_train,\n",
    "                                                            df_temp_2018_train,\n",
    "                                                            df_temp_2020_train,\n",
    "                                                            df_temp_2021_train,\n",
    "                                                            df_temp_2022_train,\n",
    "                                                            ])\n",
    "dewP_min_train, dewP_max_train = calc_combined_min_max_params([\n",
    "                                                            df_dewP_2017_train,\n",
    "                                                            df_dewP_2018_train,\n",
    "                                                            df_dewP_2020_train,\n",
    "                                                            df_dewP_2021_train,\n",
    "                                                            df_dewP_2022_train,\n",
    "                                                            ])\n",
    "WD_min_train,   WD_max_train   = calc_combined_min_max_params([\n",
    "                                                            df_WD_2017_train,\n",
    "                                                            df_WD_2018_train,\n",
    "                                                            df_WD_2020_train,\n",
    "                                                            df_WD_2021_train,\n",
    "                                                            df_WD_2022_train,\n",
    "                                                            ])\n",
    "Wvh_min_train,  Wvh_max_train  = calc_combined_min_max_params([\n",
    "                                                            df_Wvh_2017_train,\n",
    "                                                            df_Wvh_2018_train,\n",
    "                                                            df_Wvh_2020_train,\n",
    "                                                            df_Wvh_2021_train,\n",
    "                                                            df_Wvh_2022_train,\n",
    "                                                            ])\n",
    "P_min_train,    P_max_train    = calc_combined_min_max_params([\n",
    "                                                            df_P_2017_train,\n",
    "                                                            df_P_2018_train,\n",
    "                                                            df_P_2020_train,\n",
    "                                                            df_P_2021_train,\n",
    "                                                            df_P_2022_train,\n",
    "                                                            ])\n",
    "SQ_min_train,   SQ_max_train   = calc_combined_min_max_params([\n",
    "                                                            df_SQ_2017_train,\n",
    "                                                            df_SQ_2018_train,\n",
    "                                                            df_SQ_2020_train,\n",
    "                                                            df_SQ_2021_train,\n",
    "                                                            df_SQ_2022_train,\n",
    "                                                            ])\n",
    "\n",
    "df_minmax = pd.DataFrame({'NO2':  [NO2_min_train, NO2_max_train],\n",
    "                          'O3':   [O3_min_train, O3_max_train],\n",
    "                          'PM10': [PM10_min_train, PM10_max_train],\n",
    "                          'PM25': [PM25_min_train, PM25_max_train]},\n",
    "                          index = ['min', 'max']).T\n",
    "print(df_minmax)\n",
    "df_minmax.to_csv(f\"../data/dataset_final/contaminant_minmax.csv\", \n",
    "                 index = True, sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NO2_2017_train_norm_1D = normalise_linear(df_NO2_2017_train_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_2018_train_norm_1D = normalise_linear(df_NO2_2018_train_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_2020_train_norm_1D = normalise_linear(df_NO2_2020_train_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_2021_train_norm_1D = normalise_linear(df_NO2_2021_train_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_2021_val_norm_1D = normalise_linear(df_NO2_2021_val_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_2021_test_norm_1D = normalise_linear(df_NO2_2021_test_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_2022_train_norm_1D = normalise_linear(df_NO2_2022_train_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_val_2022_norm_1D = normalise_linear(df_NO2_2022_val_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_test_2022_norm_1D = normalise_linear(df_NO2_2022_test_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_val_2023_norm_1D = normalise_linear(df_NO2_2023_val_1D, NO2_min_train, NO2_max_train)\n",
    "df_NO2_test_2023_norm_1D = normalise_linear(df_NO2_2023_test_1D, NO2_min_train, NO2_max_train)\n",
    "\n",
    "df_O3_2017_train_norm_1D = normalise_linear(df_O3_2017_train_1D, O3_min_train, O3_max_train)\n",
    "df_O3_2018_train_norm_1D = normalise_linear(df_O3_2018_train_1D, O3_min_train, O3_max_train)\n",
    "df_O3_2020_train_norm_1D = normalise_linear(df_O3_2020_train_1D, O3_min_train, O3_max_train)\n",
    "df_O3_2021_train_norm_1D = normalise_linear(df_O3_2021_train_1D, O3_min_train, O3_max_train)\n",
    "df_O3_2021_val_norm_1D = normalise_linear(df_O3_2021_val_1D, O3_min_train, O3_max_train)\n",
    "df_O3_2021_test_norm_1D = normalise_linear(df_O3_2021_test_1D, O3_min_train, O3_max_train)\n",
    "df_O3_2022_train_norm_1D = normalise_linear(df_O3_2022_train_1D, O3_min_train, O3_max_train)\n",
    "df_O3_val_2022_norm_1D = normalise_linear(df_O3_2022_val_1D, O3_min_train, O3_max_train)\n",
    "df_O3_test_2022_norm_1D = normalise_linear(df_O3_2022_test_1D, O3_min_train, O3_max_train)\n",
    "df_O3_val_2023_norm_1D = normalise_linear(df_O3_2023_val_1D, O3_min_train, O3_max_train)\n",
    "df_O3_test_2023_norm_1D = normalise_linear(df_O3_2023_test_1D, O3_min_train, O3_max_train)\n",
    "\n",
    "df_PM10_2017_train_norm_1D = normalise_linear(df_PM10_2017_train_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_2018_train_norm_1D = normalise_linear(df_PM10_2018_train_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_2020_train_norm_1D = normalise_linear(df_PM10_2020_train_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_2021_train_norm_1D = normalise_linear(df_PM10_2021_train_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_2021_val_norm_1D = normalise_linear(df_PM10_2021_val_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_2021_test_norm_1D = normalise_linear(df_PM10_2021_test_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_2022_train_norm_1D = normalise_linear(df_PM10_2022_train_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_val_2022_norm_1D = normalise_linear(df_PM10_2022_val_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_test_2022_norm_1D = normalise_linear(df_PM10_2022_test_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_val_2023_norm_1D = normalise_linear(df_PM10_2023_val_1D, PM10_min_train, PM10_max_train)\n",
    "df_PM10_test_2023_norm_1D = normalise_linear(df_PM10_2023_test_1D, PM10_min_train, PM10_max_train)\n",
    "\n",
    "df_PM25_2017_train_norm_1D = normalise_linear(df_PM25_2017_train_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_2018_train_norm_1D = normalise_linear(df_PM25_2018_train_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_2020_train_norm_1D = normalise_linear(df_PM25_2020_train_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_2021_train_norm_1D = normalise_linear(df_PM25_2021_train_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_2021_val_norm_1D = normalise_linear(df_PM25_2021_val_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_2021_test_norm_1D = normalise_linear(df_PM25_2021_test_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_2022_train_norm_1D = normalise_linear(df_PM25_2022_train_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_val_2022_norm_1D = normalise_linear(df_PM25_2022_val_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_test_2022_norm_1D = normalise_linear(df_PM25_2022_test_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_val_2023_norm_1D = normalise_linear(df_PM25_2023_val_1D, PM25_min_train, PM25_max_train)\n",
    "df_PM25_test_2023_norm_1D = normalise_linear(df_PM25_2023_test_1D, PM25_min_train, PM25_max_train)\n",
    "\n",
    "df_temp_2017_train_norm = normalise_linear(df_temp_2017_train, temp_min_train, temp_max_train)\n",
    "df_temp_2018_train_norm = normalise_linear(df_temp_2018_train, temp_min_train, temp_max_train)\n",
    "df_temp_2020_train_norm = normalise_linear(df_temp_2020_train, temp_min_train, temp_max_train)\n",
    "df_temp_2021_train_norm = normalise_linear(df_temp_2021_train, temp_min_train, temp_max_train)\n",
    "df_temp_2021_val_norm = normalise_linear(df_temp_2021_val, temp_min_train, temp_max_train)\n",
    "df_temp_2021_test_norm = normalise_linear(df_temp_2021_test, temp_min_train, temp_max_train)\n",
    "df_temp_2022_train_norm = normalise_linear(df_temp_2022_train, temp_min_train, temp_max_train)\n",
    "df_temp_val_2022_norm = normalise_linear(df_temp_2022_val, temp_min_train, temp_max_train)\n",
    "df_temp_test_2022_norm = normalise_linear(df_temp_2022_test, temp_min_train, temp_max_train)\n",
    "df_temp_val_2023_norm = normalise_linear(df_temp_2023_val, temp_min_train, temp_max_train)\n",
    "df_temp_test_2023_norm = normalise_linear(df_temp_2023_test, temp_min_train, temp_max_train)\n",
    "\n",
    "df_dewP_2017_train_norm = normalise_linear(df_dewP_2017_train, dewP_min_train, dewP_max_train)\n",
    "df_dewP_2018_train_norm = normalise_linear(df_dewP_2018_train, dewP_min_train, dewP_max_train)\n",
    "df_dewP_2020_train_norm = normalise_linear(df_dewP_2020_train, dewP_min_train, dewP_max_train)\n",
    "df_dewP_2021_train_norm = normalise_linear(df_dewP_2021_train, dewP_min_train, dewP_max_train)\n",
    "df_dewP_2021_val_norm = normalise_linear(df_dewP_2021_val, dewP_min_train, dewP_max_train)\n",
    "df_dewP_2021_test_norm = normalise_linear(df_dewP_2021_test, dewP_min_train, dewP_max_train)\n",
    "df_dewP_2022_train_norm = normalise_linear(df_dewP_2022_train, dewP_min_train, dewP_max_train)\n",
    "df_dewP_val_2022_norm = normalise_linear(df_dewP_2022_val, dewP_min_train, dewP_max_train)\n",
    "df_dewP_test_2022_norm = normalise_linear(df_dewP_2022_test, dewP_min_train, dewP_max_train)\n",
    "df_dewP_val_2023_norm = normalise_linear(df_dewP_2023_val, dewP_min_train, dewP_max_train)\n",
    "df_dewP_test_2023_norm = normalise_linear(df_dewP_2023_test, dewP_min_train, dewP_max_train)\n",
    "\n",
    "df_WD_2017_train_norm = normalise_linear(df_WD_2017_train, WD_min_train, WD_max_train)\n",
    "df_WD_2018_train_norm = normalise_linear(df_WD_2018_train, WD_min_train, WD_max_train)\n",
    "df_WD_2020_train_norm = normalise_linear(df_WD_2020_train, WD_min_train, WD_max_train)\n",
    "df_WD_2021_train_norm = normalise_linear(df_WD_2021_train, WD_min_train, WD_max_train)\n",
    "df_WD_2021_val_norm = normalise_linear(df_WD_2021_val, WD_min_train, WD_max_train)\n",
    "df_WD_2021_test_norm = normalise_linear(df_WD_2021_test, WD_min_train, WD_max_train)\n",
    "df_WD_2022_train_norm = normalise_linear(df_WD_2022_train, WD_min_train, WD_max_train)\n",
    "df_WD_val_2022_norm = normalise_linear(df_WD_2022_val, WD_min_train, WD_max_train)\n",
    "df_WD_test_2022_norm = normalise_linear(df_WD_2022_test, WD_min_train, WD_max_train)\n",
    "df_WD_val_2023_norm = normalise_linear(df_WD_2023_val, WD_min_train, WD_max_train)\n",
    "df_WD_test_2023_norm = normalise_linear(df_WD_2023_test, WD_min_train, WD_max_train)\n",
    "\n",
    "df_Wvh_2017_train_norm = normalise_linear(df_Wvh_2017_train, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_2018_train_norm = normalise_linear(df_Wvh_2018_train, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_2020_train_norm = normalise_linear(df_Wvh_2020_train, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_2021_train_norm = normalise_linear(df_Wvh_2021_train, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_2021_val_norm = normalise_linear(df_Wvh_2021_val, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_2021_test_norm = normalise_linear(df_Wvh_2021_test, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_2022_train_norm = normalise_linear(df_Wvh_2022_train, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_val_2022_norm = normalise_linear(df_Wvh_2022_val, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_test_2022_norm = normalise_linear(df_Wvh_2022_test, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_val_2023_norm = normalise_linear(df_Wvh_2023_val, Wvh_min_train, Wvh_max_train)\n",
    "df_Wvh_test_2023_norm = normalise_linear(df_Wvh_2023_test, Wvh_min_train, Wvh_max_train)\n",
    "\n",
    "df_P_2017_train_norm = normalise_linear(df_P_2017_train, P_min_train, P_max_train)\n",
    "df_P_2018_train_norm = normalise_linear(df_P_2018_train, P_min_train, P_max_train)\n",
    "df_P_2020_train_norm = normalise_linear(df_P_2020_train, P_min_train, P_max_train)\n",
    "df_P_2021_train_norm = normalise_linear(df_P_2021_train, P_min_train, P_max_train)\n",
    "df_P_2021_val_norm = normalise_linear(df_P_2021_val, P_min_train, P_max_train)\n",
    "df_P_2021_test_norm = normalise_linear(df_P_2021_test, P_min_train, P_max_train)\n",
    "df_P_2022_train_norm = normalise_linear(df_P_2022_train, P_min_train, P_max_train)\n",
    "df_P_val_2022_norm = normalise_linear(df_P_2022_val, P_min_train, P_max_train)\n",
    "df_P_test_2022_norm = normalise_linear(df_P_2022_test, P_min_train, P_max_train)\n",
    "df_P_val_2023_norm = normalise_linear(df_P_2023_val, P_min_train, P_max_train)\n",
    "df_P_test_2023_norm = normalise_linear(df_P_2023_test, P_min_train, P_max_train)\n",
    "\n",
    "df_SQ_2017_train_norm = normalise_linear(df_SQ_2017_train, SQ_min_train, SQ_max_train)\n",
    "df_SQ_2018_train_norm = normalise_linear(df_SQ_2018_train, SQ_min_train, SQ_max_train)\n",
    "df_SQ_2020_train_norm = normalise_linear(df_SQ_2020_train, SQ_min_train, SQ_max_train)\n",
    "df_SQ_2021_train_norm = normalise_linear(df_SQ_2021_train, SQ_min_train, SQ_max_train)\n",
    "df_SQ_2021_val_norm = normalise_linear(df_SQ_2021_val, SQ_min_train, SQ_max_train)\n",
    "df_SQ_2021_test_norm = normalise_linear(df_SQ_2021_test, SQ_min_train, SQ_max_train)\n",
    "df_SQ_2022_train_norm = normalise_linear(df_SQ_2022_train, SQ_min_train, SQ_max_train)\n",
    "df_SQ_val_2022_norm = normalise_linear(df_SQ_2022_val, SQ_min_train, SQ_max_train)\n",
    "df_SQ_test_2022_norm = normalise_linear(df_SQ_2022_test, SQ_min_train, SQ_max_train)\n",
    "df_SQ_val_2023_norm = normalise_linear(df_SQ_2023_val, SQ_min_train, SQ_max_train)\n",
    "df_SQ_test_2023_norm = normalise_linear(df_SQ_2023_test, SQ_min_train, SQ_max_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions_KDE(data, title):\n",
    "    \"\"\"Plots the distribution of a sensor's measurements\"\"\"\n",
    "    set_style()\n",
    "    \n",
    "    if isinstance(data, pd.Series):      # distinguish between Series and DataFrame\n",
    "        sns.kdeplot(data)\n",
    "    else:\n",
    "        for column in data.columns:\n",
    "            sns.kdeplot(data, x = column)\n",
    "\n",
    "    plt.xlim(right = 1)\n",
    "    plt.ylim(top = 10)\n",
    "    plt.title(f\"Measurement distributions - {title}\")\n",
    "    plt.xlabel('Measurement value')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_multiple_distributions(data: list, title, metadata):\n",
    "    \"\"\"Plots the distribution of sensors' measurements\"\"\"\n",
    "    set_style()\n",
    "    \n",
    "    # if isinstance(data1, pd.Series):    # distinguish between Series and DataFrame\n",
    "    #     sns.kdeplot(data1, label = '1')\n",
    "    #     sns.kdeplot(data2, label = '2')\n",
    "    #     sns.kdeplot(data3, label = '3')\n",
    "    # else:\n",
    "        # for column in data1.columns:\n",
    "        #     sns.kdeplot(data1, x = column, label = '1')\n",
    "        # for column in data2.columns:\n",
    "        #     sns.kdeplot(data2, x = column, label = '2')\n",
    "        # for column in data3.columns:\n",
    "        #     sns.kdeplot(data3, x = column, label = '3')\n",
    "    for idx, df in enumerate(data):\n",
    "        for column in df.columns:\n",
    "            sns.kdeplot(df, x = column, label = idx + 1)\n",
    "\n",
    "    plt.xlim(right = 1)\n",
    "    plt.ylim(top = 10)\n",
    "    plt.title(f\"Measurement dist.s for {metadata['comp']} - {title}\")\n",
    "    plt.xlabel('Measurement value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Create big combined normalised dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_frames_vertically(dfs, keys):           # https://pandas.pydata.org/docs/user_guide/cookbook.html\n",
    "    \"\"\"\n",
    "    Concatenates a list of dataframes into one dataframe with a MultiIndex,\n",
    "    where the first level is the key and the second level is the original index.\n",
    "    The values get unionized over the columns, and the index is sorted by date.\n",
    "    \"\"\"\n",
    "    frames = [df.rename(columns = {df.columns[0] : 'Groningen'}) for df in dfs]\n",
    "    \n",
    "    return pd.concat(objs = frames, \n",
    "                     axis = 0,          # concat over row axis while unionizing column-axis\n",
    "                     join = 'outer',    # create MultiIndex, rename MultiIndex, then sort on date\n",
    "                     keys = keys).rename_axis(['Component', 'DateTime']).sort_index(level = 'DateTime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tails(data, title):            # https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
    "    \"\"\"Plots violin plot of a sensor's different component measurements\"\"\"\n",
    "    set_style()\n",
    "\n",
    "    if isinstance(data, pd.Series):     # interpret as Series\n",
    "        df = pd.DataFrame(data).reset_index()\n",
    "        df.columns = (['Component', 'DateTime', 'Value'])\n",
    "        sns.violinplot(data = df, x = 'Component', y = 'Value', hue = 'Component', legend = False)\n",
    "    else:\n",
    "        df = data.reset_index()         # interpret as DataFrame\n",
    "        df.columns = (['Component', 'DateTime', 'Value'])\n",
    "        sns.violinplot(data = df, x = 'Component', y = 'Value', hue = 'Component', legend = False)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylim(top = 1.0)\n",
    "    plt.xlabel('Component')\n",
    "    plt.ylabel('Normalised value')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['PM25', 'PM10', 'O3', 'NO2',\n",
    "        'temp', 'dewP', 'WD', 'Wvh', 'p', 'SQ']\n",
    "\n",
    "# Create input dataframes (u)\n",
    "frames_train_2017_1D_u = [df_PM25_2017_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_PM10_2017_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_O3_2017_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_NO2_2017_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_temp_2017_train_norm,\n",
    "                              df_dewP_2017_train_norm,\n",
    "                              df_WD_2017_train_norm,\n",
    "                              df_Wvh_2017_train_norm,\n",
    "                              df_P_2017_train_norm,\n",
    "                              df_SQ_2017_train_norm]\n",
    "frames_train_2018_1D_u = [df_PM25_2018_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_PM10_2018_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_O3_2018_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_NO2_2018_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_temp_2018_train_norm,\n",
    "                              df_dewP_2018_train_norm,\n",
    "                              df_WD_2018_train_norm,\n",
    "                              df_Wvh_2018_train_norm,\n",
    "                              df_P_2018_train_norm,\n",
    "                              df_SQ_2018_train_norm]\n",
    "frames_train_2020_1D_u = [df_PM25_2020_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_PM10_2020_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_O3_2020_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_NO2_2020_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_temp_2020_train_norm,\n",
    "                              df_dewP_2020_train_norm,\n",
    "                              df_WD_2020_train_norm,\n",
    "                              df_Wvh_2020_train_norm,\n",
    "                              df_P_2020_train_norm,\n",
    "                              df_SQ_2020_train_norm]\n",
    "frames_train_2021_1D_u = [df_PM25_2021_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_PM10_2021_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_O3_2021_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_NO2_2021_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_temp_2021_train_norm,\n",
    "                              df_dewP_2021_train_norm,\n",
    "                              df_WD_2021_train_norm,\n",
    "                              df_Wvh_2021_train_norm,\n",
    "                              df_P_2021_train_norm,\n",
    "                              df_SQ_2021_train_norm]\n",
    "frames_val_2021_1D_u = [df_PM25_2021_val_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_PM10_2021_val_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_O3_2021_val_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_NO2_2021_val_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_temp_2021_val_norm,\n",
    "                            df_dewP_2021_val_norm,\n",
    "                            df_WD_2021_val_norm,\n",
    "                            df_Wvh_2021_val_norm,\n",
    "                            df_P_2021_val_norm,\n",
    "                            df_SQ_2021_val_norm]\n",
    "frames_test_2021_1D_u = [df_PM25_2021_test_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_PM10_2021_test_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_O3_2021_test_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_NO2_2021_test_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_temp_2021_test_norm,\n",
    "                             df_dewP_2021_test_norm,\n",
    "                             df_WD_2021_test_norm,\n",
    "                             df_Wvh_2021_test_norm,\n",
    "                             df_P_2021_test_norm,\n",
    "                             df_SQ_2021_test_norm]\n",
    "frames_train_2022_1D_u = [df_PM25_2022_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_PM10_2022_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_O3_2022_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_NO2_2022_train_norm_1D.loc[:, [TUINDORP]],\n",
    "                              df_temp_2022_train_norm,\n",
    "                              df_dewP_2022_train_norm,\n",
    "                              df_WD_2022_train_norm,\n",
    "                              df_Wvh_2022_train_norm,\n",
    "                              df_P_2022_train_norm,\n",
    "                              df_SQ_2022_train_norm]\n",
    "frames_val_2022_1D_u = [df_PM25_val_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_PM10_val_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_O3_val_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_NO2_val_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_temp_val_2022_norm,\n",
    "                            df_dewP_val_2022_norm,\n",
    "                            df_WD_val_2022_norm,\n",
    "                            df_Wvh_val_2022_norm,\n",
    "                            df_P_val_2022_norm,\n",
    "                            df_SQ_val_2022_norm]\n",
    "frames_val_2023_1D_u = [df_PM25_val_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_PM10_val_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_O3_val_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_NO2_val_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                            df_temp_val_2023_norm,\n",
    "                            df_dewP_val_2023_norm,\n",
    "                            df_WD_val_2023_norm,\n",
    "                            df_Wvh_val_2023_norm,\n",
    "                            df_P_val_2023_norm,\n",
    "                            df_SQ_val_2023_norm]\n",
    "frames_test_2022_1D_u = [df_PM25_test_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_PM10_test_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_O3_test_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_NO2_test_2022_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_temp_test_2022_norm,\n",
    "                             df_dewP_test_2022_norm,\n",
    "                             df_WD_test_2022_norm,\n",
    "                             df_Wvh_test_2022_norm,\n",
    "                             df_P_test_2022_norm,\n",
    "                             df_SQ_test_2022_norm]\n",
    "frames_test_2023_1D_u = [df_PM25_test_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_PM10_test_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_O3_test_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_NO2_test_2023_norm_1D.loc[:, [TUINDORP]],\n",
    "                             df_temp_test_2023_norm,\n",
    "                             df_dewP_test_2023_norm,\n",
    "                             df_WD_test_2023_norm,\n",
    "                             df_Wvh_test_2023_norm,\n",
    "                             df_P_test_2023_norm,\n",
    "                             df_SQ_test_2023_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_train_2017_1D_y = [df_PM25_2017_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_PM10_2017_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_O3_2017_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_NO2_2017_train_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_train_2018_1D_y = [df_PM25_2018_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_PM10_2018_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_O3_2018_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_NO2_2018_train_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_train_2020_1D_y = [df_PM25_2020_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_PM10_2020_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_O3_2020_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_NO2_2020_train_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_train_2021_1D_y = [df_PM25_2021_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_PM10_2021_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_O3_2021_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_NO2_2021_train_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_val_2021_1D_y = [df_PM25_2021_val_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_PM10_2021_val_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_O3_2021_val_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_NO2_2021_val_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_test_2021_1D_y = [df_PM25_2021_test_norm_1D.loc[:, [BREUKELEN]],\n",
    "                             df_PM10_2021_test_norm_1D.loc[:, [BREUKELEN]],\n",
    "                             df_O3_2021_test_norm_1D.loc[:, [BREUKELEN]],\n",
    "                             df_NO2_2021_test_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_train_2022_1D_y = [df_PM25_2022_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_PM10_2022_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_O3_2022_train_norm_1D.loc[:, [BREUKELEN]],\n",
    "                              df_NO2_2022_train_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_val_2022_1D_y = [df_PM25_val_2022_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_PM10_val_2022_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_O3_val_2022_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_NO2_val_2022_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_val_2023_1D_y = [df_PM25_val_2023_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_PM10_val_2023_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_O3_val_2023_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_NO2_val_2023_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_test_2022_1D_y = [df_PM25_test_2022_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_PM10_test_2022_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_O3_test_2022_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_NO2_test_2022_norm_1D.loc[:, [BREUKELEN]]]\n",
    "frames_test_2023_1D_y = [df_PM25_test_2023_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_PM10_test_2023_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_O3_test_2023_norm_1D.loc[:, [BREUKELEN]],\n",
    "                            df_NO2_test_2023_norm_1D.loc[:, [BREUKELEN]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling of the dataset is done in the model files, not here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_manual_dict_of_dfs(dfs, components):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of dataframes with the components as keys.\n",
    "    Important:\n",
    "    It assumes they are in the same order as the components list!\n",
    "    \"\"\"\n",
    "    return dict(zip(components, dfs))\n",
    "\n",
    "\n",
    "def sort_dict_of_dfs(dfs_dict, components_sorted):\n",
    "    \"\"\"Sorts a dictionary of dataframes by the given components list\"\"\"\n",
    "    return [dfs_dict[name] for name in components_sorted]\n",
    "\n",
    "\n",
    "def print_dict_of_dfs(dfs):\n",
    "    \"\"\"Prints a dictionary of dataframes in a concise manner\"\"\"\n",
    "    for key, df in dfs.items():\n",
    "        print(key)\n",
    "        print(df.head(2))\n",
    "\n",
    "\n",
    "def print_dfs_sorted(dfs):\n",
    "    \"\"\"Prints a list of dataframes in a concise manner\"\"\"\n",
    "    for df in dfs:\n",
    "        print(df.head(2))\n",
    "\n",
    "\n",
    "def concat_frames_horizontally(dfs, components):\n",
    "    \"\"\"\n",
    "    Concatenates a list of dataframes into one dataframe where:\n",
    "    - the x-axis (axis = 1) is the DateTime index;\n",
    "    - the y-axis (axis = 0) is the component axis.\n",
    "    With this approach, the diagrams cannot be automatically sorted.\n",
    "    Hence, this is done manually through the following sorting order:\n",
    "    NO2, O3, PM10, PM25, Q, SQ, WD, Wmax, Wvh, dewP, temp.\n",
    "\n",
    "    The following steps are taken:\n",
    "    1. Create a dictionary of dataframes with the components as keys;\n",
    "    2. Sort the components list alphabetically;\n",
    "    3. Sort the dictionary by the components list;\n",
    "    4. Concatenate the sorted dataframes;\n",
    "    5. Drop the old column names (i.e. sensor names).\n",
    "    \"\"\"\n",
    "    dfs_dict = make_manual_dict_of_dfs(dfs, components)\n",
    "    components_sorted = sorted(components)\n",
    "    dfs_sorted = sort_dict_of_dfs(dfs_dict, components_sorted)\n",
    "    \n",
    "    df = pd.concat(objs = dfs_sorted, \n",
    "                   axis = 1,            # concat over column axis    \n",
    "                   keys = components_sorted).sort_index(level = 'DateTime')\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = ['PM25', 'PM10', 'O3', 'NO2',\n",
    "              'temp', 'dewP', 'WD', 'Wvh', 'p', 'SQ']\n",
    "target_keys = ['PM25', 'PM10', 'O3', 'NO2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(input_keys))\n",
    "print(sorted(target_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in frames_val_2023_1D_u:\n",
    "    print(df.index.tz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2023 data seemed to have a different kind of DateTime index, namely one that was timezone-aware instead of timezone-naive. Hence, we delete the timezone information here for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in frames_val_2023_1D_u:\n",
    "    df.index = df.index.tz_localize(None)\n",
    "for df in frames_test_2023_1D_u:\n",
    "    df.index = df.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2017_horizontal_u = concat_frames_horizontally(frames_train_2017_1D_u, input_keys)\n",
    "df_train_2018_horizontal_u = concat_frames_horizontally(frames_train_2018_1D_u, input_keys)\n",
    "df_train_2020_horizontal_u = concat_frames_horizontally(frames_train_2020_1D_u, input_keys)\n",
    "df_train_2021_horizontal_u = concat_frames_horizontally(frames_train_2021_1D_u, input_keys)\n",
    "df_val_2021_horizontal_u = concat_frames_horizontally(frames_val_2021_1D_u, input_keys)\n",
    "df_test_2021_horizontal_u = concat_frames_horizontally(frames_test_2021_1D_u, input_keys)\n",
    "df_train_2022_horizontal_u = concat_frames_horizontally(frames_train_2022_1D_u, input_keys)\n",
    "df_val_2022_horizontal_u = concat_frames_horizontally(frames_val_2022_1D_u, input_keys)\n",
    "df_val_2023_horizontal_u = concat_frames_horizontally(frames_val_2023_1D_u, input_keys)\n",
    "df_test_2022_horizontal_u = concat_frames_horizontally(frames_test_2022_1D_u, input_keys)\n",
    "df_test_2023_horizontal_u = concat_frames_horizontally(frames_test_2023_1D_u, input_keys)\n",
    "\n",
    "df_train_2017_horizontal_y = concat_frames_horizontally(frames_train_2017_1D_y, target_keys)\n",
    "df_train_2018_horizontal_y = concat_frames_horizontally(frames_train_2018_1D_y, target_keys)\n",
    "df_train_2020_horizontal_y = concat_frames_horizontally(frames_train_2020_1D_y, target_keys)\n",
    "df_train_2021_horizontal_y = concat_frames_horizontally(frames_train_2021_1D_y, target_keys)\n",
    "df_val_2021_horizontal_y = concat_frames_horizontally(frames_val_2021_1D_y, target_keys)\n",
    "df_test_2021_horizontal_y = concat_frames_horizontally(frames_test_2021_1D_y, target_keys)\n",
    "df_train_2022_horizontal_y = concat_frames_horizontally(frames_train_2022_1D_y, target_keys)\n",
    "df_val_2022_horizontal_y = concat_frames_horizontally(frames_val_2022_1D_y, target_keys)\n",
    "df_val_2023_horizontal_y = concat_frames_horizontally(frames_val_2023_1D_y, target_keys)\n",
    "df_test_2022_horizontal_y = concat_frames_horizontally(frames_test_2022_1D_y, target_keys)\n",
    "df_test_2023_horizontal_y = concat_frames_horizontally(frames_test_2023_1D_y, target_keys)\n",
    "\n",
    "# print(df_train_2017_horizontal_u.shape)\n",
    "# print(df_train_2018_horizontal_u.shape)\n",
    "# print(df_train_2019_horizontal_u.shape)\n",
    "print(df_train_2020_horizontal_u.shape)\n",
    "# print(df_train_2021_horizontal_u.shape)\n",
    "# print(df_train_2022_horizontal_u.shape)\n",
    "print(df_val_2022_horizontal_u.shape)\n",
    "print(df_test_2022_horizontal_u.shape)\n",
    "# print(df_train_2017_horizontal_y.shape)\n",
    "# print(df_train_2018_horizontal_y.shape)\n",
    "# print(df_train_2019_horizontal_y.shape)\n",
    "print(df_train_2020_horizontal_y.shape)\n",
    "# print(df_train_2021_horizontal_y.shape)\n",
    "# print(df_train_2022_horizontal_y.shape)\n",
    "print(df_val_2022_horizontal_y.shape)\n",
    "print(df_test_2022_horizontal_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataframes to data_combined/ folder. The windowing will be performed by a PyTorch Dataset class in the model notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2017_horizontal_u.to_csv(\"../data/data_combined/train_2017_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2018_horizontal_u.to_csv(\"../data/data_combined/train_2018_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2020_horizontal_u.to_csv(\"../data/data_combined/train_2020_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2021_horizontal_u.to_csv(\"../data/data_combined/train_2021_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_val_2021_horizontal_u.to_csv(\"../data/data_combined/val_2021_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_test_2021_horizontal_u.to_csv(\"../data/data_combined/test_2021_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2022_horizontal_u.to_csv(\"../data/data_combined/train_2022_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_val_2022_horizontal_u.to_csv(\"../data/data_combined/val_2022_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_val_2023_horizontal_u.to_csv(\"../data/data_combined/val_2023_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_test_2022_horizontal_u.to_csv(\"../data/data_combined/test_2022_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_test_2023_horizontal_u.to_csv(\"../data/data_combined/test_2023_combined_u.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "\n",
    "df_train_2017_horizontal_y.to_csv(\"../data/data_combined/train_2017_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2018_horizontal_y.to_csv(\"../data/data_combined/train_2018_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2020_horizontal_y.to_csv(\"../data/data_combined/train_2020_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2021_horizontal_y.to_csv(\"../data/data_combined/train_2021_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_val_2021_horizontal_y.to_csv(\"../data/data_combined/val_2021_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_test_2021_horizontal_y.to_csv(\"../data/data_combined/test_2021_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_train_2022_horizontal_y.to_csv(\"../data/data_combined/train_2022_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_val_2022_horizontal_y.to_csv(\"../data/data_combined/val_2022_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_val_2023_horizontal_y.to_csv(\"../data/data_combined/val_2023_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_test_2022_horizontal_y.to_csv(\"../data/data_combined/test_2022_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')\n",
    "df_test_2023_horizontal_y.to_csv(\"../data/data_combined/test_2023_combined_y.csv\", index = True, sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions_KDE(data, title):\n",
    "    \"\"\"Plots the distribution of a sensor's measurements\"\"\"\n",
    "    set_style()\n",
    "    \n",
    "    if isinstance(data, pd.Series):      # distinguish between Series and DataFrame\n",
    "        sns.kdeplot(data, label = data.name)\n",
    "    else:\n",
    "        for column in data.columns:\n",
    "            sns.kdeplot(data, x = column, label = column)\n",
    "\n",
    "    plt.xlim(left = -0.1, right = 1)\n",
    "    plt.ylim(top = 10)\n",
    "    plt.title(f\"Measurement distributions - {title}\")\n",
    "    plt.xlabel('Measurement value')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_y_combined = pd.concat([\n",
    "    df_train_2017_horizontal_y,\n",
    "    df_train_2018_horizontal_y,\n",
    "    df_train_2020_horizontal_y,\n",
    "    df_train_2021_horizontal_y,\n",
    "    df_train_2022_horizontal_y\n",
    "])\n",
    "df_val_y_combined = pd.concat([\n",
    "    df_val_2021_horizontal_y,\n",
    "    df_val_2022_horizontal_y,\n",
    "    df_val_2023_horizontal_y\n",
    "])\n",
    "df_test_y_combined = pd.concat([\n",
    "    df_test_2021_horizontal_y,\n",
    "    df_test_2022_horizontal_y,\n",
    "    df_test_2023_horizontal_y\n",
    "])\n",
    "\n",
    "plot_distributions_KDE(df_training_y_combined, 'training data - y')\n",
    "plot_distributions_KDE(df_val_y_combined, 'validation data - y')\n",
    "plot_distributions_KDE(df_test_y_combined, 'test data - y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
